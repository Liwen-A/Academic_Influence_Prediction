paperId,abstract,year,referenceCount,citationCount,influentialCitationCount,year0_citation_count,year1_citation_count,year2_citation_count,year3_citation_count,year4_citation_count,year5_citation_count,year6_citation_count,year7_citation_count,year8_citation_count,year9_citation_count
a411f6a0e6473137ac1a538f7cee65722fa3584f,"Genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. Knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. The goal of the Gene Ontology Consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. To this end, three independent ontologies accessible on the World-Wide Web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component.",2000,35,31100,1372,0,0,0,0,1,0,0,1,0,1
82e320e06b1c717b0d924d257aa7b6710f53a38e,"1. Oxygen is a toxic gas - an introductionto oxygen toxicity and reactive species 2. The chemistry of free radicals and related 'reactive species' 3. Antioxidant defences Endogenous and Diet Derived 4. Cellular responses to oxidative stress: adaptation, damage, repair, senescence and death 5. Measurement of reactive species 6. Reactive species can pose special problems needing special solutions. Some examples. 7. Reactive species can be useful some more examples 8. Reactive species can be poisonous: their role in toxicology 9. Reactive species and disease: fact, fiction or filibuster? 10. Ageing, nutrition, disease, and therapy: A role for antioxidants?",1985,0,20977,1140,0,0,0,0,0,0,1,0,0,0
fd495d6cf7c3169bc58550fdf32be6e16e2800f8,"The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. The goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. We describe details of our aims and methods, identify current challenges, compare Bioconductor to other open bioinformatics projects, and provide working examples.",2004,69,11702,986,0,0,1,0,7,451,826,920,966,1006
30eecc8a7b7346a5e0c3a6648b0e156faad3a786,"cellular viewpoint. That is all very well. In any case academics will swallow almost anything if it's their job. Even if the botanists of the 18th century, the well-intentioned of the 19th century and the latterday enthusiasts of this century believed in the cell, is it an approach that is truly relevant? Such must surely be the attitude of today's sceptical medical students, and of many of their now middle-aged predecessors. Well, let a battle-scarred predecessor speak. Even a conventional diet of comparative and classical morphology, combined with exposure to prefashionable Eltonian ecology, and a native distrust of big-business 'nouvelle vague' biology has not blinkered your reviewer to the virtues of Alberts' Molecular Biology of the Cell. This is undoubtedly a landmark for the student and his teacher in the field of preclinical medicine. Many American undergraduate texts in biochemistry and molecular genetics clearly outsell competing English publications. Only English textbooks in immunology remain popular, and effective sellers in this field. The successful texts in these areas eschew traditional academic contrapuntal argument. The new bestselling formula involves clarity of presentation, the effective use of multi-coloured diagrams and the detailed exposition of basic processes. Such didactic approaches suit undergraduates today, and certainly ease the hardship for their seniors needing a refresher",1983,0,8546,650,0,6,11,7,18,12,11,16,26,19
667b43c8adad628c60c20810197cdeec6679714e,"Countless millions of people have died from tuberculosis, a chronic infectious disease caused by the tubercle bacillus. The complete genome sequence of the best-characterized strain of Mycobacterium tuberculosis, H37Rv, has been determined and analysed in order to improve our understanding of the biology of this slow-growing pathogen and to help the conception of new prophylactic and therapeutic interventions. The genome comprises 4,411,529 base pairs, contains around 4,000 genes, and has a very high guanine + cytosine content that is reflected in the biased amino-acid content of the proteins. M. tuberculosis differs radically from other bacteria in that a very large portion of its coding capacity is devoted to the production of enzymes involved in lipogenesis and lipolysis, and to two new families of glycine-rich proteins with a repetitive structure that may represent a source of antigenic variation.",1998,48,7413,404,61,288,335,362,366,301,329,350,328,297
ddf06cf0d375fb9404fe30c5f1d7858d74080e9c,The European Molecular Biology Open Software Suite (EMBOSS) is a mature package of software tools developed for the molecular biology community. It includes a comprehensive set of applications for molecular sequence analysis and other tasks and integrates popular third-party software packages under a consistent interface. EMBOSS includes extensive C programming libraries and is a platform to develop and release software in the true open source spirit.,2000,4,7763,588,3,10,22,85,127,216,255,308,348,346
b65ec8eec4933b3a9425d7dc981fdc27e4260077,"Vascular endothelial growth factor (VEGF) is a key regulator of physiological angiogenesis during embryogenesis, skeletal growth and reproductive functions. VEGF has also been implicated in pathological angiogenesis associated with tumors, intraocular neovascular disorders and other conditions. The biological effects of VEGF are mediated by two receptor tyrosine kinases (RTKs), VEGFR-1 and VEGFR-2, which differ considerably in signaling properties. Non-signaling co-receptors also modulate VEGF RTK signaling. Currently, several VEGF inhibitors are undergoing clinical testing in several malignancies. VEGF inhibition is also being tested as a strategy for the prevention of angiogenesis, vascular leakage and visual loss in age-related macular degeneration.",2003,151,8467,465,35,253,343,392,448,520,507,562,542,626
a462ee3d64edf189e075451ca7cb728bcc9e5fb3,"A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century.",2004,130,7015,296,71,215,292,332,349,427,434,444,531,480
73d7d1adf83ee16e12e6b442ec15357da218c06d,"Diabetes-specific microvascular disease is a leading cause of blindness, renal failure and nerve damage, and diabetes-accelerated atherosclerosis leads to increased risk of myocardial infarction, stroke and limb amputation. Four main molecular mechanisms have been implicated in glucose-mediated vascular damage. All seem to reflect a single hyperglycaemia-induced process of overproduction of superoxide by the mitochondrial electron-transport chain. This integrating paradigm provides a new conceptual framework for future research and drug discovery.",2001,183,7809,418,3,39,172,208,285,315,374,423,421,445
ede7e9d3e65a6c3a834b12b6456e55ff88d926cd,"The prime objective for every life form is to deliver its genetic material, intact and unchanged, to the next generation. This must be achieved despite constant assaults by endogenous and environmental agents on the DNA. To counter this threat, life has evolved several systems to detect DNA damage, signal its presence and mediate its repair. Such responses, which have an impact on a wide range of cellular events, are biologically significant because they prevent diverse human diseases. Our improving understanding of DNA-damage responses is providing new avenues for disease management.",2009,134,4184,215,4,139,217,257,304,351,376,411,452,369
1debd200d72b1fde3b71e3e87753b10e51fbbae0,"Introduction to amphibia - the world of amphibians, historical resume, prospects for the future. Part 1 Life History: reproductive strategies - reproductive cycles, reproductive mode, quantitative aspects, parental care, evolution of reproductive strategies courtship and mating - location of breeding site, secondary sexual characters, courtship behaviour, fertilization and oviposition, sexual selection, evolution of mating systems vocalization - anuran communication system, mechanisms of sound production and reception, kinds of vocalizations and their functions, abiotic factors affecting vocalization, interspecific significance of vocalization, phylogenetic implications of vocalization eggs and development - spermatozoa and fertilization, egg structure, egg development, hatching and birth, development and amphibian diversity larvae - morphology of larvae, adaptive types of larvae, physiology and ecology, social behaviour, evolutionary significance of larvae metamorphosis - endocrine control, other biochemical changes, morphological changes, neoteny, ecological and evolutionary significance of metamorphosis. Part 2 Ecology: relationships with the environment - water economy, temperature, gas exchange, energy metabolism and energy budgets, ecological synthesis food and feeding - prey selection, location of prey, capture of prey, evolution of prey-capturing mechanisms and strategies enemies and defence - diseases, parasites, predators, anti-predator mechanisms, evolution of defence mechanisms population biology - characteristics of individuals, movements and territoriality, demography, factors regulating populations community ecology and species diversity - community structure, species diversity, evolution of amphibian communities. Part 3 Morphology: musculoskeletal system - skull and hyobranchium, axial system, appendicular system, integration of functional units integumentary, sensory and visceral systems - integument, sensory receptor systems, nervous system, circulatory and respiratory systems, urogenital system, digestive system, endocrine glands, evolutionary considerations. Part 4 Evolution: origin and early evolution - nature of a tetrapod, primitive tetrapods, tetrapod affinities (lungfishes or lobe-fins?), diversity and evolution of early tetrapods, status of the lissamphibia cytogenetic, molecular and genomic evolution - cytogenetics, molecular evolution, genomic evolution phylogeny - caudata, gymnophiona, anura biogeography - biogeographic principles, historical setting, lissamphibia, caudata, gymnophiona, anura classification.",1986,0,4551,355,4,21,37,34,52,51,57,51,48,68
3ed436b2b92d50a4af31015774472101652eb983,"Living in an oxygenated environment has required the evolution of effective cellular strategies to detect and detoxify metabolites of molecular oxygen known as reactive oxygen species. Here we review evidence that the appropriate and inappropriate production of oxidants, together with the ability of organisms to respond to oxidative stress, is intricately connected to ageing and life span.",2000,116,8085,282,4,65,154,169,225,258,304,305,340,331
3e8d033a962a04c5b25d58d81135cf469186ed49,"D ifferent areas in biology are often criticized for being either too myopic and not asking questions about how their results fit into a larger framework or for being too broad and ignoring important details. A few research programs are now attempting to make general claims derived from simple, micro-level phenomena. This is great progress for biology, resolving some of the tensions between different approaches. A merging of these fields may lead to even greater advances. One of the most exciting of these new programs is ecological stoichiometry (ES), which has been spearheaded by Sterner and Elser. Their recent book provides an overview of this developing field. With clear exposition they explain how analysis at the level of molecules can have broad implications for many macro-level ecological phenomena. Moreover, they keep their theory closely tied to available data and to the design of new experiments, thereby bridging the gap between two dividing perspectives in biology. As such, Ecological Stoichiometry contains a much needed overview of this research program as well as a good example of future directions in biology. Sterner and Elser use the first part of their book to develop the essential ideas behind ES. The foremost assumption is that an entire organism can be treated as a single, extremely large molecule. Humans, for example, are reducible to the formula:",2002,17,3513,566,7,18,61,100,120,131,128,140,198,181
55c30317e9e3027d77025add26341bf8618c080a,,1981,0,3838,485,0,6,17,25,37,36,46,55,54,70
c1b0e411a0ab95363236fedaedc4103739d44efb,"Samenvatting:The book for introductory microbiology, Brock's Biology of Microorganismscontinues its long tradition of impeccable scholarship, outstanding art,and accuracy. It balances the most current coverage with the majorclassical concepts essential for understanding the science. A six-partpresentation covers principles of microbiology; evolutionary microbiologyand microbial diversity; metabolic diversity and microbial ecology;immunology, pathogenicity, and host responses; microbial diseases; andmicroorganisms as tools for industry and research. For researchers, groupleaders, senior scientists in pharmaceuticals, chemicals and biochemicalbiotechnology companies, and public health laboratories.",1996,0,4448,233,0,2,11,14,37,50,78,97,127,144
2943c165f95fecb7a4f6ae37703a11697962f4be,"Allometric scaling relations, including the 3/4 power law for metabolic rates, are characteristic of all organisms and are here derived from a general model that describes how essential materials are transported through space-filling fractal networks of branching tubes. The model assumes that the energy dissipated is minimized and that the terminal tubes do not vary with body size. It provides a complete analysis of scaling relations for mammalian circulatory systems that are in agreement with data. More generally, the model predicts structural and functional properties of vertebrate cardiovascular and respiratory systems, plant vascular systems, insect tracheal tubes, and other distribution networks.",1997,41,3981,292,5,31,49,56,76,85,116,150,156,166
4b80558804d91c9e421ec3d1e24d0c87567cffb9,"The study of eye movements and oculomotor disorders has, for four decades, greatly benefitted from the application of control theoretic concepts. This paper is an example of a complementary approach based on the theory of nonlinear dynamical systems. Recently, a nonlinear dynamics model of the saccadic system was developed, comprising a symmetric piecewise-smooth system of six first-order autonomous ordinary differential equations. A preliminary numerical investigation of the model revealed that in addition to generating normal saccades, it could also simulate inaccurate saccades, and the oscillatory instability known as congenital nystagmus (CN). By varying the parameters of the model, several types of CN oscillations were produced, including jerk, bidirectional jerk and pendular nystagmus. The aim of this study was to investigate the bifurcations and attractors of the model, in order to obtain a classification of the simulated oculomotor behaviours. The application of standard stability analysis techniques, together with numerical work, revealed that the equations have a rich bifurcation structure. In addition to Hopf, homoclinic and saddlenode bifurcations organised by a Takens-Bogdanov point, the equations can undergo nonsmooth pitchfork bifurcations and nonsmooth gluing bifurcations. Evidence was also found for the existence of Hopf-initiated canards. The simulated jerk CN waveforms were found to correspond to a pair of post-canard symmetry-related limit cycles, which exist in regions of parameter space where the equations are a slow-fast system. The slow and fast phases of the simulated oscillations were attributed to the geometry of the corresponding slow manifold. The simulated bidirectional jerk and Corresponding author O.E. Akman: The School of Mathematics, The University of Manchester, P.O. Box 88, Manchester M60 1QD, UK. Present address: Mathematics Institute, University of Warwick, Coventry CV4 7AL, UK. e-mail: oakman@maths.warwick.ac.uk D.S. Broomhead: The School of Mathematics, The University of Manchester, P.O. Box 88, Manchester M60 1QD, UK. e-mail: david.broomhead@manchester.ac.uk R.V. Abadi: Faculty of Life Sciences, Moffat Building, The University of Manchester, Sackville St, Manchester M60 1QD, UK. e-mail: richard.abadi@manchester.ac.uk R.A. Clement: Visual Sciences Unit, Institute of Child Health, U.C.L., London WC1N 1EH, UK. e-mail: R.clement@ich.ucl.ac.uk Mathematics Subject Classification (2000): 37N25",1961,15,4737,433,1,0,0,0,0,0,0,0,1,0
4e98194e61ce6de2f033947ec65370ce0cf1d97d,,1979,0,7583,269,42,77,106,130,149,145,188,146,186,190
1a1761eb00bdce0c1c7887a29168852d0cada096,"Astrocytes are specialized glial cells that outnumber neurons by over fivefold. They contiguously tile the entire central nervous system (CNS) and exert many essential complex functions in the healthy CNS. Astrocytes respond to all forms of CNS insults through a process referred to as reactive astrogliosis, which has become a pathological hallmark of CNS structural lesions. Substantial progress has been made recently in determining functions and mechanisms of reactive astrogliosis and in identifying roles of astrocytes in CNS disorders and pathologies. A vast molecular arsenal at the disposal of reactive astrocytes is being defined. Transgenic mouse models are dissecting specific aspects of reactive astrocytosis and glial scar formation in vivo. Astrocyte involvement in specific clinicopathological entities is being defined. It is now clear that reactive astrogliosis is not a simple all-or-none phenomenon but is a finely gradated continuum of changes that occur in context-dependent manners regulated by specific signaling events. These changes range from reversible alterations in gene expression and cell hypertrophy with preservation of cellular domains and tissue structure, to long-lasting scar formation with rearrangement of tissue structure. Increasing evidence points towards the potential of reactive astrogliosis to play either primary or contributing roles in CNS disorders via loss of normal astrocyte functions or gain of abnormal effects. This article reviews (1) astrocyte functions in healthy CNS, (2) mechanisms and functions of reactive astrogliosis and glial scar formation, and (3) ways in which reactive astrocytes may cause or contribute to specific CNS disorders and lesions.",2009,289,3439,219,1,31,129,184,256,285,312,328,374,399
87de747cb11577979a9398cb5f42d3927a38fabb,■ Abstract Contributions from the field of population biology hold promise for understanding and managing invasiveness; invasive species also offer excellent opportunities to study basic processes in population biology. Life history studies and demographic models may be valuable for examining the introduction of invasive species and identifying life history stages where management will be most effective. Evolutionary processes may be key features in determining whether invasive species establish and spread. Studies of genetic diversity and evolutionary changes should be useful for,2001,142,3393,241,0,15,45,89,123,122,146,167,192,222
e625c8787d34a5113ee817f52d0debfb95fcb1ca,"Rho GTPases are molecular switches that control a wide variety of signal transduction pathways in all eukaryotic cells. They are known principally for their pivotal role in regulating the actin cytoskeleton, but their ability to influence cell polarity, microtubule dynamics, membrane transport pathways and transcription factor activity is probably just as significant. Underlying this biological complexity is a simple biochemical idea, namely that by switching on a single GTPase, several distinct signalling pathways can be coordinately activated. With spatial and temporal activation of multiple switches factored in, it is not surprising to find Rho GTPases having such a prominent role in eukaryotic cell biology.",2002,92,4454,173,1,152,315,316,322,274,271,258,278,250
cd709dbb5085f28971ba3ba6ecc65800cf9ecf5b,"This review focuses on the mechanisms regulating the synthesis, secretion, biological actions, and therapeutic relevance of the incretin peptides glucose-dependent insulinotropic polypeptide (GIP) and glucagon-like peptide-1 (GLP-1). The published literature was reviewed, with emphasis on recent advances in our understanding of the biology of GIP and GLP-1. GIP and GLP-1 are both secreted within minutes of nutrient ingestion and facilitate the rapid disposal of ingested nutrients. Both peptides share common actions on islet beta-cells acting through structurally distinct yet related receptors. Incretin-receptor activation leads to glucose-dependent insulin secretion, induction of beta-cell proliferation, and enhanced resistance to apoptosis. GIP also promotes energy storage via direct actions on adipose tissue, and enhances bone formation via stimulation of osteoblast proliferation and inhibition of apoptosis. In contrast, GLP-1 exerts glucoregulatory actions via slowing of gastric emptying and glucose-dependent inhibition of glucagon secretion. GLP-1 also promotes satiety and sustained GLP-1-receptor activation is associated with weight loss in both preclinical and clinical studies. The rapid degradation of both GIP and GLP-1 by the enzyme dipeptidyl peptidase-4 has led to the development of degradation-resistant GLP-1-receptor agonists and dipeptidyl peptidase-4 inhibitors for the treatment of type 2 diabetes. These agents decrease hemoglobin A1c (HbA1c) safely without weight gain in subjects with type 2 diabetes. GLP-1 and GIP integrate nutrient-derived signals to control food intake, energy absorption, and assimilation. Recently approved therapeutic agents based on potentiation of incretin action provide new physiologically based approaches for the treatment of type 2 diabetes.",2007,249,2814,233,8,80,120,177,191,248,256,229,249,265
e84cc71e71ee1d52df52b45ff6c95167fd447308,"Molecular cell biology , Molecular cell biology , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1986,0,5755,136,0,10,24,32,19,40,34,47,45,62
bb2d0d378f82472e4286b451f9b9fb584922321d,"To understand biology at the system level, we must examine the structure and dynamics of cellular and organismal function, rather than the characteristics of isolated parts of a cell or organism. Properties of systems, such as robustness, emerge as central issues, and understanding these properties may have an impact on the future of medicine. However, many breakthroughs in experimental devices, advanced software, and analytical methods are required before the achievements of systems biology can live up to their much-touted potential.",2002,12,4018,139,29,114,141,165,180,240,232,251,281,285
f25de4182a75525841ad40b0b8bfe7337a9cf792,"MOTIVATION
Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.


RESULTS
We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.


AVAILABILITY
The specification of SBML Level 1 is freely available from http://www.sbml.org/",2003,86,3167,276,33,77,119,177,167,168,215,199,200,218
3d6e5f2caa50a415cee56acf5860905022250e6a,"As a discipline, phylogenetics is becoming transformed by a flood of molecular data. These data allow broad questions to be asked about the history of life, but also present difficult statistical and computational problems. Bayesian inference of phylogeny brings a new perspective to a number of outstanding issues in evolutionary biology, including the analysis of large phylogenetic trees and complex evolutionary models and the detection of the footprint of natural selection in DNA sequences.",2001,34,2558,477,0,40,96,119,127,146,112,128,172,149
acb5571a8ab6ae2be5fbabd04451162006fcab49,"Denitrification is a distinct means of energy conservation, making use of N oxides as terminal electron acceptors for cellular bioenergetics under anaerobic, microaerophilic, and occasionally aerobic conditions. The process is an essential branch of the global N cycle, reversing dinitrogen fixation, and is associated with chemolithotrophic, phototrophic, diazotrophic, or organotrophic metabolism but generally not with obligately anaerobic life. Discovered more than a century ago and believed to be exclusively a bacterial trait, denitrification has now been found in halophilic and hyperthermophilic archaea and in the mitochondria of fungi, raising evolutionarily intriguing vistas. Important advances in the biochemical characterization of denitrification and the underlying genetics have been achieved with Pseudomonas stutzeri, Pseudomonas aeruginosa, Paracoccus denitrificans, Ralstonia eutropha, and Rhodobacter sphaeroides. Pseudomonads represent one of the largest assemblies of the denitrifying bacteria within a single genus, favoring their use as model organisms. Around 50 genes are required within a single bacterium to encode the core structures of the denitrification apparatus. Much of the denitrification process of gram-negative bacteria has been found confined to the periplasm, whereas the topology and enzymology of the gram-positive bacteria are less well established. The activation and enzymatic transformation of N oxides is based on the redox chemistry of Fe, Cu, and Mo. Biochemical breakthroughs have included the X-ray structures of the two types of respiratory nitrite reductases and the isolation of the novel enzymes nitric oxide reductase and nitrous oxide reductase, as well as their structural characterization by indirect spectroscopic means. This revealed unexpected relationships among denitrification enzymes and respiratory oxygen reductases. Denitrification is intimately related to fundamental cellular processes that include primary and secondary transport, protein translocation, cytochrome c biogenesis, anaerobic gene regulation, metalloprotein assembly, and the biosynthesis of the cofactors molybdopterin and heme D1. An important class of regulators for the anaerobic expression of the denitrification apparatus are transcription factors of the greater FNR family. Nitrate and nitric oxide, in addition to being respiratory substrates, have been identified as signaling molecules for the induction of distinct N oxide-metabolizing enzymes.",1997,1002,3066,275,0,21,39,36,57,78,78,75,97,117
fd08d0e4326a8686e784f011a6181b873c7b85ad,"Based on the work of the Tropical Soil Biology and Fertility (TSBF) Programme, this is a handbook of recommended and validated methods for the characterization and analysis of tropical soils, with the aim of achieving sustainable use of soil resources. The objectives of the programme revolve around five main themes: synchrony of nutrient release and plant growth demands; management of soil organic matter; soil water balance; effects and management of soil fauna; and integration of biological processes into the maintenance of soil fertility. The methods given are endorsed by the International Soil Science Society and are part of the International Union of Biological Sciences and the UNESCO Man and the Biosphere Programme.",1990,0,2864,275,0,6,5,12,16,25,35,31,54,52
066e24bd08719e6163903f7e630f49296c5e009d,"The establishment of a vascular supply is required for organ development and differentiation as well as for tissue repair and reproductive functions in the adult1. Neovascularization (angiogenesis) is also implicated in the pathogenesis of a number of disorders. These include: proliferative retinopathies, age-related macular degeneration, tumors, rheumatoid arthritis, and psoriasis1,2. A strong correlation has been noted between density of microvessels in primary breast cancers and their nodal metastases and patient survival3. Similarly, a correlation has been reported between vascularity and invasive behavior in several other tumors4–6.",1997,245,4445,171,15,88,196,219,203,215,236,277,223,228
f91c8be04929b0443b026f582e14450a5804d7df,,1990,0,2818,232,0,20,17,26,24,32,29,34,28,50
f54a5c14e14feda0bf4b8fb30a8a3958adce16bf,"Prostaglandins and leukotrienes are potent eicosanoid lipid mediators derived from phospholipase-released arachidonic acid that are involved in numerous homeostatic biological functions and inflammation. They are generated by cyclooxygenase isozymes and 5-lipoxygenase, respectively, and their biosynthesis and actions are blocked by clinically relevant nonsteroidal anti-inflammatory drugs, the newer generation coxibs (selective inhibitors of cyclooxygenase-2), and leukotriene modifiers. The prime mode of prostaglandin and leukotriene action is through specific G protein-coupled receptors, many of which have been cloned recently, thus enabling specific receptor agonist and antagonist development. Important insights into the mechanisms of inflammatory responses, pain, and fever have been gleaned from our current understanding of eicosanoid biology.",2001,67,3341,147,2,64,119,137,160,142,188,186,161,188
336dc5ace3125bd95cccbf7a9ff53beced98584b,"DAMBE (data analysis in molecular biology and evolution) is an integrated software package for converting, manipulating, statistically and graphically describing, and analyzing molecular sequence data with a user-friendly Windows 95/98/2000/NT interface. DAMBE is free and can be downloaded from http://web.hku.hk/~xxia/software/software.htm. The current version is 4.0.36.",2001,13,2184,472,5,22,36,74,69,69,121,159,144,164
95ff3f28e7fc3fdfe025e6d6cd49b9d98e85b3db,1. Introduction 2. Archetypes of the weak hydrogen bond 3. Other weak and non-conventional hydrogen bonds 4. The weak hydrogen bond in supramolecular chemistry 5. The weak hydrogen bond in biological structures 6. Conclusions Appendix,1999,0,3703,137,8,25,76,114,127,170,192,198,216,229
98f9550b50d0aede9d8b10651a5757b45a722ac6,"Approximately one percent of the human genome encodes proteins that either regulate or are regulated by direct interaction with members of the Rho family of small GTPases. Through a series of complex biochemical networks, these highly conserved molecular switches control some of the most fundamental processes of cell biology common to all eukaryotes, including morphogenesis, polarity, movement, and cell division. In the first part of this review, we present the best characterized of these biochemical pathways; in the second part, we attempt to integrate these molecular details into a biological context.",2005,194,2779,185,4,103,165,209,203,226,199,227,241,207
0a7c0cbdeec3ef2157c5c4f606f079542b2c56fb,"SUMMARY Tetracyclines were discovered in the 1940s and exhibited activity against a wide range of microorganisms including gram-positive and gram-negative bacteria, chlamydiae, mycoplasmas, rickettsiae, and protozoan parasites. They are inexpensive antibiotics, which have been used extensively in the prophlylaxis and therapy of human and animal infections and also at subtherapeutic levels in animal feed as growth promoters. The first tetracycline-resistant bacterium, Shigella dysenteriae, was isolated in 1953. Tetracycline resistance now occurs in an increasing number of pathogenic, opportunistic, and commensal bacteria. The presence of tetracycline-resistant pathogens limits the use of these agents in treatment of disease. Tetracycline resistance is often due to the acquisition of new genes, which code for energy-dependent efflux of tetracyclines or for a protein that protects bacterial ribosomes from the action of tetracyclines. Many of these genes are associated with mobile plasmids or transposons and can be distinguished from each other using molecular methods including DNA-DNA hybridization with oligonucleotide probes and DNA sequencing. A limited number of bacteria acquire resistance by mutations, which alter the permeability of the outer membrane porins and/or lipopolysaccharides in the outer membrane, change the regulation of innate efflux systems, or alter the 16S rRNA. New tetracycline derivatives are being examined, although their role in treatment is not clear. Changing the use of tetracyclines in human and animal health as well as in food production is needed if we are to continue to use this class of broad-spectrum antimicrobials through the present century.",2001,394,3128,197,3,34,52,59,72,87,116,106,125,120
7c72b917a38b09e6d3ab19d28a4344ba54edb6ae,"Part I. Exact String Matching: The Fundamental String Problem: 1. Exact matching: fundamental preprocessing and first algorithms 2. Exact matching: classical comparison-based methods 3. Exact matching: a deeper look at classical methods 4. Semi-numerical string matching Part II. Suffix Trees and their Uses: 5. Introduction to suffix trees 6. Linear time construction of suffix trees 7. First applications of suffix trees 8. Constant time lowest common ancestor retrieval 9. More applications of suffix trees Part III. Inexact Matching, Sequence Alignment and Dynamic Programming: 10. The importance of (sub)sequence comparison in molecular biology 11. Core string edits, alignments and dynamic programming 12. Refining core string edits and alignments 13. Extending the core problems 14. Multiple string comparison: the Holy Grail 15. Sequence database and their uses: the motherlode Part IV. Currents, Cousins and Cameos: 16. Maps, mapping, sequencing and superstrings 17. Strings and evolutionary trees 18. Three short topics 19. Models of genome-level mutations.",1997,0,3106,176,4,21,32,49,77,110,130,160,175,204
b7c89b0247fc7009ab51dd0873b5f6c3b6ec144e,"Electron-transfer reactions between ions and 
molecules in solution have been the subject of 
considerable experimental study during the past 
three decades. Experimental results have also been 
obtained on related phenomena, such as reactions 
between ions or molecules and electrodes, charge-transfer 
spectra, photoelectric emission spectra of 
ionic solutions, chemiluminescent electron transfers, 
electron transfer through frozen media, and 
electron transfer through thin hydrocarbon-like 
films on electrodes.",1985,276,5704,66,3,23,48,73,55,75,85,95,119,155
11fba3d8fd169d54526c4c360d88fad4a6e0d1a9,,1987,0,4764,80,21,92,114,193,210,192,201,170,259,310
ec94b60c8e678e4fd7473d743babc23cba45a280,"Cellular functions, such as signal transmission, are carried out by 'modules' made up of many species of interacting molecules. Understanding how modules work has depended on combining phenomenological analysis with molecular studies. General principles that govern the structure and behaviour of modules may be discovered with help from synthetic sciences such as engineering and computer science, from stronger interactions between experiment and theory in cell biology, and from an appreciation of evolutionary constraints.",1999,30,3464,79,2,34,56,81,116,165,176,198,215,212
5345c358466cb5e11801687820b247afb999b01a,"The authors regret the inability to cite all of the primary literature contributing to this review due to length considerations. The authors thank F. Chan, T. Migone, M. Peter, J. Puck, R. Siegel, H. Walczak, and J. Wang for insightful comments on the manuscript. N. K. is a Scholar of the Leukemia Society. Supported in part by grants from the National Institutes of Health (R. M. L., N. K.).",2001,241,3570,111,56,141,210,172,192,182,180,184,204,193
b2528cd8034ea7cb49313ebd872804b93dfabed3,,2007,0,2679,162,84,94,146,130,159,185,194,216,195,192
6072973ad37f4e5763e8495d562eebe24af0696b,"Retinoids play an important role in development, differentiation, and homeostasis. The discovery of retinoid receptors belonging to the superfamily of nuclear ligand‐activated transcriptional regulators has revolutionized our molecular understanding as to how these structurally simple molecules exert their pleiotropic effects. Diversity in the control of gene expression by retinoid signals is generated through complexity at different levels of the signaling pathway. A major source of diversity originates from the existence of two families of retinoid acid (RA) receptors (R), the RAR isotypes (α, β, and γ) and the three RXR isotypes (α, β, and γ), and their numerous isoforms, which bind as RXR/RAR heterodimers to the polymorphic cis.acting response elements of RA target genes. The possibility of cross‐modulation (cross‐talk) with cell‐surface receptors signaling pathways, as well as the finding that RARs and RXRs interact with multiple putative coactivators and/or corepressors, generates additional levels of complexity for the array of combinatorial effects that underlie the pleiotropic effects of retinoids. This review focuses on recent developments, particularly in the area of structure‐ function relationships.—Chambon, P. A decade of molecular biology of retinoic acid receptors. FASEB J. 10, 940‐954 (1996)",1996,119,2867,153,5,76,112,139,185,177,143,167,154,181
33b776abcc465b79c54623e213a4dfe079da0ca4,"General methods bacterial strains and cloning vectors enzymes that modify DNA and RNA in vitro amplification of DNA using - the polymerase chain reaction (PCR) and the thermostable Taq DNA polymerase, introduction DNA restriction fragment analysis and preparation, introduction in vitro labeling of probes and filter hybridization plasmid DNA preparation for E. Coli hosts preparation of DNA from Lambda bacteriophage clones subcloning fragments into plasmid vectors and plasmid construction preparation of genomic DNA preparation and analysis of RNA from eukaryotic cells - overview geonomic cloning generatino of cDNA libaries preparation of subtractive cDNA, introduction chain termination sequencing transfection of mammalian cells, introduction basic methods of protein analysis in situ hybridization transgenic mouse preparation detection and in vitro generation of specific mutations in genes and cDNAs, introduction appendices.",1986,0,3456,60,3,26,112,175,232,254,212,191,120,161
3537ee388a43c5df1f985ddcc97382e61dbe33d6,"Human natural killer (NK) cells comprise approximately 15% of all circulating lymphocytes. Owing to their early production of cytokines and chemokines, and ability to lyse target cells without prior sensitization, NK cells are crucial components of the innate immune system. Human NK cells can be divided into two subsets based on their cell-surface density of CD56--CD56(bright) and CD56(dim)--each with distinct phenotypic properties. Now, there is ample evidence to suggest that these NK-cell subsets have unique functional attributes and, therefore, distinct roles in the human immune response. The CD56(dim) NK-cell subset is more naturally cytotoxic and expresses higher levels of Ig-like NK receptors and FCgamma receptor III (CD16) than the CD56(bright) NK-cell subset. By contrast, the CD56(bright) subset has the capacity to produce abundant cytokines following activation of monocytes, but has low natural cytotoxicity and is CD16(dim) or CD16(-). In addition, we will discuss other cell-surface receptors expressed differentially by human NK-cell subsets and the distinct functional properties of these subsets.",2001,64,2541,204,0,34,54,71,103,100,97,119,114,144
bbe46d4075ba4fd4344d614753a6a6b65e3c15e5,"Professional phagocytes generate high levels of reactive oxygen species (ROS) using a superoxide-generating NADPH oxidase as part of their armoury of microbicidal mechanisms. The multicomponent phagocyte oxidase (Phox), which has been well characterized over the past three decades, includes the catalytic subunit gp91phox. Lower levels of ROS are seen in non-phagocytic cells, but are usually thought to be 'accidental' byproducts of aerobic metabolism. The discovery of a family of superoxide-generating homologues of gp91phox has led to the concept that ROS are 'intentionally' generated in these cells with distinctive cellular functions related to innate immunity, signal transduction and modification of the extracellular matrix.",2004,83,2712,160,16,96,141,145,162,176,185,194,196,187
752c9501216b9514bf24e7cf09e9657fdb550b94,"The prostaglandin endoperoxide H synthases-1 and 2 (PGHS-1 and PGHS-2; also cyclooxygenases-1 and 2, COX-1 and COX-2) catalyze the committed step in prostaglandin synthesis. PGHS-1 and 2 are of particular interest because they are the major targets of nonsteroidal anti-inflammatory drugs (NSAIDs) including aspirin, ibuprofen, and the new COX-2 inhibitors. Inhibition of the PGHSs with NSAIDs acutely reduces inflammation, pain, and fever, and long-term use of these drugs reduces fatal thrombotic events, as well as the development of colon cancer and Alzheimer's disease. In this review, we examine how the structures of these enzymes relate mechanistically to cyclooxygenase and peroxidase catalysis, and how differences in the structure of PGHS-2 confer on this isozyme differential sensitivity to COX-2 inhibitors. We further examine the evidence for independent signaling by PGHS-1 and PGHS-2, and the complex mechanisms for regulation of PGHS-2 gene expression.",2000,255,2674,146,3,45,99,127,141,171,189,172,166,148
ffac9d302a4f5a18bc69320e97ed7a4264469bd3,"Preface. Reference System Usage. Frequently Used Abbreviations. Introduction and Historical Perspectives. Ethylene Analysis and Properties of the Gas. The Biosynthesis of Ethylene. Regulation of Ethylene Production by Internal, Environmental, and Stress Factors. Roles and Physiological Effects of Ethylene in Plant Physiology: Dormancy, Growth and Development. Fruit Ripening, Abscission and Postharvest Disorders. The Mechanisms of Ethylene Action. Ethylene in the Environment. The Role of Ethylene in Agriculture. References. Index.",1973,0,3074,169,2,10,15,31,12,19,26,30,19,25
a72c494aba62e75c696566d924f7b70da9e4c3fe,"Since its publication in 2000, Biochemistry & Molecular Biology of Plants, has been hailed as a major contribution to the plant sciences literature and critical acclaim has been matched by global sales success. Maintaining the scope and focus of the first edition, the second will provide a major update, include much new material and reorganise some chapters to further improve the presentation. This book is meticulously organised and richly illustrated, having over 1,000 full-colour illustrations and 500 photographs. It is divided into five parts covering: Compartments: Cell Reproduction: Energy Flow; Metabolic and Developmental Integration; and Plant Environment and Agriculture. Specific changes to this edition include: Completely revised with over half of the chapters having a major rewrite. Includes two new chapters on signal transduction and responses to pathogens. Restructuring of section on cell reproduction for improved presentation. Dedicated website to include all illustrative material. Biochemistry & Molecular Biology of Plants holds a unique place in the plant sciences literature as it provides the only comprehensive, authoritative, integrated single volume book in this essential field of study.",2002,0,3146,64,96,112,127,150,196,227,225,200,232,261
140f7c7d5fafba3fc9911476c6cbc911b35e93a0,"Hydrophilic polymers are the center of research emphasis in nanotechnology because of their perceived “intelligence”. They can be used as thin films, scaffolds, or nanoparticles in a wide range of biomedical and biological applications. Here we highlight recent developments in engineering uncrosslinked and crosslinked hydrophilic polymers for these applications. Natural, biohybrid, and synthetic hydrophilic polymers and hydrogels are analyzed and their thermodynamic responses are discussed. In addition, examples of the use of hydrogels for various therapeutic applications are given. We show how such systems’ intelligent behavior can be used in sensors, microarrays, and imaging. Finally, we outline challenges for the future in integrating hydrogels into biomedical applications.",2006,274,3088,52,5,29,67,120,156,202,231,245,276,263
90c3c31cee756baa411d2f2be30fd03f7e7e3c08,"Predicting which species are probable invaders has been a long-standing goal of ecologists, but only recently have quantitative methods been used to achieve such a goal. Although restricted to few taxa, these studies reveal clear relationships between the characteristics of releases and the species involved, and the successful establishment and spread of invaders. For example, the probability of bird establishment increases with the number of individuals released and the number of release events. Also, the probability of plant invasiveness increases if the species has a history of invasion and reproduces vegetatively. These promising quantitative approaches should be more widely applied to allow us to predict patterns of invading species more successfully.",2001,34,2607,157,9,25,59,106,98,125,162,149,159,153
0f22ce024aa171f62f5677b68afda409ee29fcb0,The comparative method for studying adaptation why worry about phylogeny? reconstructing phylogenetic trees and ancestral character states comparative analysis of discrete data comparative analysis of continuous variables determining the form of comparative relationships.,1991,0,2849,95,10,65,128,81,122,123,122,139,122,124
f0e58ab853ba27ae924cf11813a924bd69c72f65,"Living organisms have regular patterns and routines that involve obtaining food and carrying out life history stages such as breeding, migrating, molting, and hibernating. The acquisition, utilization, and storage of energy reserves (and other resources) are critical to lifetime reproductive success. There are also responses to predictable changes, e.g., seasonal, and unpredictable challenges, i.e., storms and natural disasters. Social organization in many populations provides advantages through cooperation in providing basic necessities and beneficial social support. But there are disadvantages owing to conflict in social hierarchies and competition for resources. Here we discuss the concept of allostasis, maintaining stability through change, as a fundamental process through which organisms actively adjust to both predictable and unpredictable events. Allostatic load refers to the cumulative cost to the body of allostasis, with allostatic overload being a state in which serious pathophysiology can occur. Using the balance between energy input and expenditure as the basis for applying the concept of allostasis, we propose two types of allostatic overload. Type 1 allostatic overload occurs when energy demand exceeds supply, resulting in activation of the emergency life history stage. This serves to direct the animal away from normal life history stages into a survival mode that decreases allostatic load and regains positive energy balance. The normal life cycle can be resumed when the perturbation passes. Type 2 allostatic overload begins when there is sufficient or even excess energy consumption accompanied by social conflict and other types of social dysfunction. The latter is the case in human society and certain situations affecting animals in captivity. In all cases, secretion of glucocorticosteroids and activity of other mediators of allostasis such as the autonomic nervous system, CNS neurotransmitters, and inflammatory cytokines wax and wane with allostatic load. If allostatic load is chronically high, then pathologies develop. Type 2 allostatic overload does not trigger an escape response, and can only be counteracted through learning and changes in the social structure.",2003,97,2548,133,8,36,51,59,71,107,113,132,138,152
666db4c90f03cb71cd87a220a0ae4ddd910610d3,"
 Publisher Summary
 
 Studies of cytotoxicity by human lymphocytes revealed not only that both allogeneic and syngeneic tumor cells were lysed in a non-MHC-restricted fashion, but also that lymphocytes from normal donors were often cytotoxic. Lymphocytes from any healthy donor, as well as peripheral blood and spleen lymphocytes from several experimental animals, in the absence of known or deliberate sensitization, were found to be spontaneously cytotoxic in vitro for some normal fresh cells, most cultured cell lines, immature hematopoietic cells, and tumor cells. This type of nonadaptive, non-MHC-restricted cellmediated cytotoxicity was defined as “natural” cytotoxicity, and the effector cells mediating natural cytotoxicity were functionally defined as natural killer (NK) cells. The existence of NK cells has prompted a reinterpretation of both the studies of specific cytotoxicity against spontaneous human tumors and the theory of immune surveillance, at least in its most restrictive interpretation. Unlike cytotoxic T cells, NK cells cannot be demonstrated to have clonally distributed specificity, restriction for MHC products at the target cell surface, or immunological memory. NK cells cannot yet be formally assigned to a single lineage based on the definitive identification of a stem cell, a distinct anatomical location of maturation, or unique genotypic rearrangements.
 
",1989,1219,3024,78,2,22,76,89,128,101,95,132,126,139
83cb6105617265494e4184527a91a49165ef19e3,"Wolbachia are common intracellular bacteria that are found in arthropods and nematodes. These alphaproteobacteria endosymbionts are transmitted vertically through host eggs and alter host biology in diverse ways, including the induction of reproductive manipulations, such as feminization, parthenogenesis, male killing and sperm–egg incompatibility. They can also move horizontally across species boundaries, resulting in a widespread and global distribution in diverse invertebrate hosts. Here, we review the basic biology of Wolbachia, with emphasis on recent advances in our understanding of these fascinating endosymbionts.",2008,110,2065,123,3,36,57,111,132,154,153,154,215,199
84f5633b1e4cd3b1ec034b8e2610e4e959950884,"Recognized more than a decade ago, NKT cells differentiate from mainstream thymic precursors through instructive signals emanating during TCR engagement by CD1d-expressing cortical thymocytes. Their semi-invariant alphabeta TCRs recognize isoglobotrihexosylceramide, a mammalian glycosphingolipid, as well as microbial alpha-glycuronylceramides found in the cell wall of Gram-negative, lipopolysaccharide-negative bacteria. This dual recognition of self and microbial ligands underlies innate-like antimicrobial functions mediated by CD40L induction and massive Th1 and Th2 cytokine and chemokine release. Through reciprocal activation of NKT cells and dendritic cells, synthetic NKT ligands constitute promising new vaccine adjuvants. NKT cells also regulate a range of immunopathological conditions, but the mechanisms and the ligands involved remain unknown. NKT cell biology has emerged as a new field of research at the frontier between innate and adaptive immunity, providing a powerful model to study fundamental aspects of the cell and structural biology of glycolipid trafficking, processing, and recognition.",2007,254,1999,145,43,135,161,163,200,183,176,160,153,114
a75b4efbf7503f370fe751002ad3a34a00ed7ec3,"Malignant astrocytic gliomas such as glioblastoma are the most common and lethal intracranial tumors. These cancers exhibit a relentless malignant progression characterized by widespread invasion throughout the brain, resistance to traditional and newer targeted therapeutic approaches, destruction of normal brain tissue, and certain death. The recent confluence of advances in stem cell biology, cell signaling, genome and computational science and genetic model systems have revolutionized our understanding of the mechanisms underlying the genetics, biology and clinical behavior of glioblastoma. This progress is fueling new opportunities for understanding the fundamental basis for development of this devastating disease and also novel therapies that, for the first time, portend meaningful clinical responses.",2007,350,2168,124,0,46,112,153,199,186,201,183,199,173
3558b67481229e3fc23ebb9246d5423961288d61,"A free radical is any molecule that has an odd number of electrons. Free radicals, which can occur in both organic (i.e., quinones) and inorganic molecules (i.e., O(2)), are highly reactive and, therefore, transient. Free radicals are generated in vivo as by products of normal metabolism. They are also produced when an organism is exposed to ionizing radiation, to drugs capable of redox cycling, or to xenobiotics that can form free radical metabolites in situ. Cellular targets at risk from free radical damage depend on the nature of the radical and its site of generation. In this review we survey cellular sources of free radicals and the reactions they can undergo and discuss cellular defenses and adaptive mechanisms.",1982,0,3171,65,0,2,31,57,59,74,102,116,98,133
10d6778bc45aebcd58d336b4062b935861d2fe8a,"In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.",1998,19,15423,1676,0,0,0,1,0,0,0,1,2,1
4f97bfe016c1c6e1661f774fac8232728db3d1b6,"Basal ganglia disorders are a heterogeneous group of clinical syndromes with a common anatomic locus within the basal ganglia. To account for the variety of clinical manifestations associated with insults to various parts of the basal ganglia we propose a model in which specific types of basal ganglia disorders are associated with changes in the function of subpopulations of striatal projection neurons. This model is based on a synthesis of experimental animal and post-mortem human anatomic and neurochemical data. Hyperkinetic disorders, which are characterized by an excess of abnormal movements, are postulated to result from the selective impairment of striatal neurons projecting to the lateral globus pallidus. Hypokinetic disorders, such as Parkinson's disease, are hypothesized to result from a complex series of changes in the activity of striatal projection neuron subpopulations resulting in an increase in basal ganglia output. This model suggests that the activity of subpopulations of striatal projection neurons is differentially regulated by striatal afferents and that different striatal projection neuron subpopulations may mediate different aspects of motor control.",1989,153,4872,299,1,31,85,158,57,90,93,119,111,110
f3c4e8f65b7c988d08373f23b7b191534f5677e1,"“Grid” computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. In this article, the authors define this new field. First, they review the “Grid problem,” which is defined as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources—what is referred to as virtual organizations. In such settings, unique authentication, authorization, resource access, resource discovery, and other challenges are encountered. It is this class of problem that is addressed by Grid technologies. Next, the authors present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. The authors describe requirements that they believe any such mechanisms must satisfy and discuss the importance of defining a compact set of intergrid protocols to enable interoperability among different Grid systems. Finally, the authors discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. They maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.",2001,69,4777,247,27,140,271,381,500,469,497,410,487,377
57e3d0266d3f0a5f7ff529acd86b0a4a3ad5231a,"Functional neuroimaging studies have started unravelling unexpected functional attributes for the posteromedial portion of the parietal lobe, the precuneus. This cortical area has traditionally received little attention, mainly because of its hidden location and the virtual absence of focal lesion studies. However, recent functional imaging findings in healthy subjects suggest a central role for the precuneus in a wide spectrum of highly integrated tasks, including visuo-spatial imagery, episodic memory retrieval and self-processing operations, namely first-person perspective taking and an experience of agency. Furthermore, precuneus and surrounding posteromedial areas are amongst the brain structures displaying the highest resting metabolic rates (hot spots) and are characterized by transient decreases in the tonic activity during engagement in non-self-referential goal-directed actions (default mode of brain function). Therefore, it has recently been proposed that precuneus is involved in the interwoven network of the neural correlates of self-consciousness, engaged in self-related mental representations during rest. This hypothesis is consistent with the selective hypometabolism in the posteromedial cortex reported in a wide range of altered conscious states, such as sleep, drug-induced anaesthesia and vegetative states. This review summarizes the current knowledge about the macroscopic and microscopic anatomy of precuneus, together with its wide-spread connectivity with both cortical and subcortical structures, as shown by connectional and neurophysiological findings in non-human primates, and links these notions with the multifaceted spectrum of its behavioural correlates. By means of a critical analysis of precuneus activation patterns in response to different mental tasks, this paper provides a useful conceptual framework for matching the functional imaging findings with the specific role(s) played by this structure in the higher-order cognitive functions in which it has been implicated. Specifically, activation patterns appear to converge with anatomical and connectivity data in providing preliminary evidence for a functional subdivision within the precuneus into an anterior region, involved in self-centred mental imagery strategies, and a posterior region, subserving successful episodic memory retrieval.",2006,175,3843,148,18,65,102,191,162,205,224,278,320,332
cf6df5a001f5058016ba2e0e234bfb539ee051da,"""Grid"" computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high-performance orientation. In this article, we define this new field. First, we review the ""Grid problem,"" which we define as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources-what we refer to as virtual organizations. In such settings, we encounter unique authentication, authorization, resource access, resource discovery, and other challenges. It is this class of problem that is addressed by Grid technologies. Next, we present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. We describe requirements that we believe any such mechanisms must satisfy, and we discuss the central role played by the intergrid protocols that enable interoperability among different Grid systems. Finally, we discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. We maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.",2001,64,4067,165,47,133,300,355,445,473,409,339,304,280
cce9ff2a9df6d171e35c3532cae28f7e230e95bd,"Anatomical and physiological observations in monkeys indicate that the primate visual system consists of several separate and independent subdivisions that analyze different aspects of the same retinal image: cells in cortical visual areas 1 and 2 and higher visual areas are segregated into three interdigitating subdivisions that differ in their selectivity for color, stereopsis, movement, and orientation. The pathways selective for form and color seem to be derived mainly from the parvocellular geniculate subdivisions, the depth- and movement-selective components from the magnocellular. At lower levels, in the retina and in the geniculate, cells in these two subdivisions differ in their color selectivity, contrast sensitivity, temporal properties, and spatial resolution. These major differences in the properties of cells at lower levels in each of the subdivisions led to the prediction that different visual functions, such as color, depth, movement, and form perception, should exhibit corresponding differences. Human perceptual experiments are remarkably consistent with these predictions. Moreover, perceptual experiments can be designed to ask which subdivisions of the system are responsible for particular visual abilities, such as figure/ground discrimination or perception of depth from perspective or relative movement--functions that might be difficult to deduce from single-cell response properties.",1988,93,3141,114,4,38,85,61,170,114,114,129,106,91
d848e0df740036b3c8b10371b4624a1599d39191,"Publisher Summary This chapter investigates the anatomy and taxonomy of protein structures. A protein is a polypeptide chain made up of amino acid residues linked together in a definite sequence. Amino acids are “handed,” and naturally occurring proteins contain only L-amino acids. A simple mnemonic for that purpose is the “corncrib.” The sequence of side chains determines all that is unique about a particular protein, including its biological function and its specific three-dimensional structure. The major possible routes to knowledge of three-dimensional protein structure are prediction from the amino acid sequence and analysis of spectroscopic measurements such as circular dichroism, laser Raman spectroscopy, and nuclear magnetic resonance. The analysis and discussion of protein structure is based on the results of three-dimensional X-ray crystallography of globular proteins. The basic elements of protein structures are discussed. The most useful level at which protein structures are to be categorized is the domain, as there are many cases of multiple-domain proteins in which each separate domain resembles other entire smaller proteins. The simplest type of stable protein structure consists of polypeptide backbone wrapped more or less uniformly around the outside of a single hydrophobic core. The outline of the taxonomy is also provided in the chapter.",1981,370,3007,53,1,15,36,45,64,59,61,81,110,124
a781b50b70bdc73f17b05de344261ae8d2c8b8cc,"Clinically oriented anatomy , Clinically oriented anatomy , کتابخانه دیجیتال جندی شاپور اهواز",1985,0,2463,146,2,0,3,4,1,8,9,6,13,10
6493953038c9e83ff423df8096ed4f9287443079,"Design work and design knowledge in Information Systems (IS) is important for both research and practice. Yet there has been comparatively little critical attention paid to the problem of specifying design theory so that it can be communicated, justified, and developed cumulatively. In this essay we focus on the structural components or anatomy of design theories in IS as a special class of theory. In doing so, we aim to extend the work of Walls, Widemeyer and El Sawy (1992) on the specification of information systems design theories (ISDT), drawing on other streams of thought on design research and theory to provide a basis for a more systematic and useable formulation of these theories. We identify eight separate components of design theories: (1) purpose and scope, (2) constructs, (3) principles of form and function, (4) artifact mutability, (5) testable propositions, (6) justificatory knowledge (kernel theories), (7) principles of implementation, and (8) an expository instantiation. This specification includes components missing in the Walls et al. adaptation of Dubin (1978) and Simon (1969) and also addresses explicitly problems associated with the role of instantiations and the specification of design theories for methodologies and interventions as well as for products and applications. The essay is significant as the unambiguous establishment of design knowledge as theory gives a sounder base for arguments for the rigor and legitimacy of IS as an applied discipline and for its continuing progress. A craft can proceed with the copying of one example of a design artifact by one artisan after another. A discipline cannot.",2007,146,1486,212,6,38,54,84,116,140,139,135,136,118
e54de9c464468d7c540924ec82262152c2353636,,2005,0,1554,233,0,14,29,35,40,44,59,88,156,164
5fcf7ce74b7c145433c496b665f9e2e08a080cea,"There are some economic forces so powerful that they constantly break through all barriers erected for their suppression. Such, for example, are the forces of supply and demand which have resisted alike medieval efforts to abolish usury and contemporary attempts to control prices. In this paper I discuss what I believe to be another such mechanism which has colored the past and seems likely to stamp its character on the future. It helps us to understand the prospective roles of a wide variety of economic services: municipal government, education, the performing arts, restaurants, and leisure time activity. I will argue that inherent in the technological structure of each of these activities are forces working almost unavoidably for progressive and cumulative increases in the real costs incurred in supplying them. As a consequence, efforts to offset these cost increases, while they may succeed temporarily, in the long run are merely palliatives which can have no significant effect on the underlying trends. The justification of a macroeconomic model should reside primarily in its ability to provide insights into the workings of observed phenomena. Its aggregation of diverse variables usually deny it the elegance and the rigor that are provided by microeconomic analysis at its best. Yet macro models have succeeded in explaining the structure of practical problems and in offering guidance for policy to a degree that has so far eluded the more painstaking modes of economic analysis. This article hopes to follow in the tradition-the structure of its basic model is rudimentary. Yet it can perhaps shed some light on a variety of economic problems of our generation.",2007,1,1326,231,57,58,55,65,84,85,82,81,91,84
a4ded2846b5e20d9228f8e676a6b0539cb2d0fdb,"This review describes normal bone anatomy and physiology as an introduction to the subsequent articles in this section that discuss clinical applications of iliac crest bone biopsy. The normal anatomy and functions of the skeleton are reviewed first, followed by a general description of the processes of bone modeling and remodeling. The bone remodeling process regulates the gain and loss of bone mineral density in the adult skeleton and directly influences bone strength. Thorough understanding of the bone remodeling process is critical to appreciation of the value of and interpretation of the results of iliac crest bone histomorphometry. Osteoclast recruitment, activation, and bone resorption is discussed in some detail, followed by a review of osteoblast recruitment and the process of new bone formation. Next, the collagenous and noncollagenous protein components and function of bone extracellular matrix are summarized, followed by a description of the process of mineralization of newly formed bone matrix. The actions of biomechanical forces on bone are sensed by the osteocyte syncytium within bone via the canalicular network and intercellular gap junctions. Finally, concepts regarding bone remodeling, osteoclast and osteoblast function, extracellular matrix, matrix mineralization, and osteocyte function are synthesized in a summary of the currently understood functional determinants of bone strength. This information lays the groundwork for understanding the utility and clinical applications of iliac crest bone biopsy.",2008,58,1495,142,1,7,14,37,50,103,99,136,173,193
ff01a8facdc76d951053e4f157641ae88bea8294,"The use of positron emission tomography to measure regional changes in average blood flow during processing of individual auditory and visual words provides support for multiple, parallel routes between localized sensory-specific, phonological, articulatory and semantic-coding areas.",1988,48,2483,67,9,27,39,96,52,46,63,91,155,87
70fc87984662b932b9c2671266b86a5ef8d57648,"Membrane traffic in eukaryotic cells involves transport of vesicles that bud from a donor compartment and fuse with an acceptor compartment. Common principles of budding and fusion have emerged, and many of the proteins involved in these events are now known. However, a detailed picture of an entire trafficking organelle is not yet available. Using synaptic vesicles as a model, we have now determined the protein and lipid composition; measured vesicle size, density, and mass; calculated the average protein and lipid mass per vesicle; and determined the copy number of more than a dozen major constituents. A model has been constructed that integrates all quantitative data and includes structural models of abundant proteins. Synaptic vesicles are dominated by proteins, possess a surprising diversity of trafficking proteins, and, with the exception of the V-ATPase that is present in only one to two copies, contain numerous copies of proteins essential for membrane traffic and neurotransmitter uptake.",2006,68,1887,94,2,73,108,150,125,134,122,132,138,133
ad526584216d24cbe5bfa3d103c9c819ccc5bc88,"Despite intensive work on language-brain relations, and a fairly impressive accumulation of knowledge over the last several decades, there has been little progress in developing large-scale models of the functional anatomy of language that integrate neuropsychological, neuroimaging, and psycholinguistic data. Drawing on relatively recent developments in the cortical organization of vision, and on data from a variety of sources, we propose a new framework for understanding aspects of the functional anatomy of language which moves towards remedying this situation. The framework posits that early cortical stages of speech perception involve auditory fields in the superior temporal gyrus bilaterally (although asymmetrically). This cortical processing system then diverges into two broad processing streams, a ventral stream, which is involved in mapping sound onto meaning, and a dorsal stream, which is involved in mapping sound onto articulatory-based representations. The ventral stream projects ventro-laterally toward inferior posterior temporal cortex (posterior middle temporal gyrus) which serves as an interface between sound-based representations of speech in the superior temporal gyrus (again bilaterally) and widely distributed conceptual representations. The dorsal stream projects dorso-posteriorly involving a region in the posterior Sylvian fissure at the parietal-temporal boundary (area Spt), and ultimately projecting to frontal regions. This network provides a mechanism for the development and maintenance of ""parity"" between auditory and motor representations of speech. Although the proposed dorsal stream represents a very tight connection between processes involved in speech perception and speech production, it does not appear to be a critical component of the speech perception process under normal (ecologically natural) listening conditions, that is, when speech input is mapped onto a conceptual representation. We also propose some degree of bi-directionality in both the dorsal and ventral pathways. We discuss some recent empirical tests of this framework that utilize a range of methods. We also show how damage to different components of this framework can account for the major symptom clusters of the fluent aphasias, and discuss some recent evidence concerning how sentence-level processing might be integrated into the framework.",2004,147,1730,89,5,32,63,58,66,86,80,109,116,112
7470f48f46326c72301c5e3d7480125b3c9b1cb2,Two- and three-dimensional (3D) white matter atlases were created on the basis of high-spatial-resolution diffusion tensor magnetic resonance (MR) imaging and 3D tract reconstruction. The 3D trajectories of 17 prominent white matter tracts could be reconstructed and depicted. Tracts were superimposed on coregistered anatomic MR images to parcel the white matter. These parcellation maps were then compared with coregistered diffusion tensor imaging color maps to assign visible structures. The results showed (a). which anatomic structures can be identified on diffusion tensor images and (b). where these anatomic units are located at each section level and orientation. The atlas may prove useful for educational and clinical purposes.,2004,39,1704,93,9,43,61,68,107,109,119,141,118,125
b01af4869bef8fa1d1066c3670fe235e53bae902,"Drugs of abuse are very powerful reinforcers, and even in conditions of limited access (where the organism is not dependent) these drugs will motivate high rates of operant responding. This presumed hedonic property and the drugs' neuropharmacological specificity provide a means of studying the neuropharmacology and neuroanatomy of brain reward. Three major brain systems appear to be involved in drug reward--dopamine, opioid and GABA. Evidence suggests a midbrain-forebrain-extrapyramidal circuit with its focus in the nucleus accumbens. Data implicating dopamine and opioid systems in indirect sympathomimetic and opiate reward include critical elements in both the nucleus accumbens and ventral tegmental areas. Ethanol reward appears to depend on an interaction with the GABAA receptor complex but may also involve common elements such as dopamine and opioid peptides in this midbrain-forebrain-extrapyramidal circuit. These results suggest that brain reward systems have a multidetermined neuropharmacological basis that may involve some common neuroanatomical elements.",1992,75,2093,47,1,25,42,60,103,101,132,134,124,100
9a0bad06edecaac3ba1ff77278adc8e5deb1873d,,1991,0,1935,35,5,5,12,14,31,27,34,48,45,63
cc445ba3fd9c5cdc017e9e2ed4fd6f148a53bae5,"The central melanocortin system is perhaps the best-characterized neuronal pathway involved in the regulation of energy homeostasis. This collection of circuits is unique in having the capability of sensing signals from a staggering array of hormones, nutrients and afferent neural inputs. It is likely to be involved in integrating long-term adipostatic signals from leptin and insulin, primarily received by the hypothalamus, with acute signals regulating hunger and satiety, primarily received by the brainstem. The system is also unique from a regulatory point of view in that it is composed of fibers expressing both agonists and antagonists of melanocortin receptors. Given that the central melanocortin system is an active target for development of drugs for the treatment of obesity, diabetes and cachexia, it is important to understand the system in its full complexity, including the likelihood that the system also regulates the cardiovascular and reproductive systems.",2005,119,1366,116,6,75,100,74,83,70,114,91,116,98
a6a03388556176a8c0477252bfbad7eb820cd7e3,"Binding of one protein to another is involved in nearly all biological functions, yet the principles governing the interaction of proteins are not fully understood. To analyze the contributions of individual amino acid residues in protein-protein binding we have compiled a database of 2325 alanine mutants for which the change in free energy of binding upon mutation to alanine has been measured (available at http://motorhead. ucsf.edu/thorn/hotspot). Our analysis shows that at the level of side-chains there is little correlation between buried surface area and free energy of binding. We find that the free energy of binding is not evenly distributed across interfaces; instead, there are hot spots of binding energy made up of a small subset of residues in the dimer interface. These hot spots are enriched in tryptophan, tyrosine and arginine, and are surrounded by energetically less important residues that most likely serve to occlude bulk solvent from the hot spot. Occlusion of solvent is found to be a necessary condition for highly energetic interactions.",1998,81,1726,76,0,18,31,41,40,56,68,72,61,83
bacb5ee17014b1d68cb443c542e7836ccf3d5a1a,"We describe a sensor-driven, or sentient, platform for context-aware computing that enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user's current working desktop to follow them as they move around the environment.",1999,23,1620,78,2,13,51,58,79,136,128,112,119,118
7b6805e028b02a5be1ca5112c9bd366c3c211629,"The study of social dilemmas is the study of the tension between individual and collective rationality. In a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. The first part of this review is a discussion of categories of social dilemmas and how they are modeled. The key two-person social dilemmas (Prisoner's Dilemma, Assurance, Chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. The second part is an extended treatment of possible solutions for social dilemmas. These solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: Motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. Strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. Solu...",1998,151,1507,131,1,4,7,7,14,23,23,43,41,49
e5f052609c6080a3a7ec6ff89ee39f706487234d,* The Dog and Its Relatives Introduction and Breeds Phylogenetic Relationships of Canids to Other Carnivores Origin and Domestication of the Dog * Prenatal Development * The Integument * The Skeleton * Arthrology * The Muscular System * The Digestive Apparatus and Abdomen * The Respiratory System * The Urogenital System * The Endocrine System * The Heart and Arteries * The Veins * The Lymphatic System * Introduction to the Nervous System * The Autonomic Nervous System * Spinal Cord and Meninges * The Spinal Nerves * The Brain * Cranial Nerves and Cutaneous Innervation of the Head * The Ear * The Eye,1979,0,1943,158,0,0,4,6,2,6,3,6,9,10
5de2b6d837c9d457f38e30e7cff17beeef0768c7,,1950,0,2499,85,0,1,1,2,4,2,6,0,0,5
6d4dc0712fe632a76a3aa22acfe6ec11bb2b6d1a,,1967,2,2104,130,0,0,3,1,3,6,2,11,5,6
17c9e48463c4fd5da19b00bbbf44741bbba06237,"A converging body of literature over the last 50 years has implicated the amygdala in assigning emotional significance or value to sensory information. In particular, the amygdala has been shown to be an essential component of the circuitry underlying fear-related responses. Disorders in the processing of fear-related information are likely to be the underlying cause of some anxiety disorders in humans such as posttraumatic stress. The amygdaloid complex is a group of more than 10 nuclei that are located in the midtemporal lobe. These nuclei can be distinguished both on cytoarchitectonic and connectional grounds. Anatomical tract tracing studies have shown that these nuclei have extensive intranuclear and internuclear connections. The afferent and efferent connections of the amygdala have also been mapped in detail, showing that the amygdaloid complex has extensive connections with cortical and subcortical regions. Analysis of fear conditioning in rats has suggested that long-term synaptic plasticity of inputs to the amygdala underlies the acquisition and perhaps storage of the fear memory. In agreement with this proposal, synaptic plasticity has been demonstrated at synapses in the amygdala in both in vitro and in vivo studies. In this review, we examine the anatomical and physiological substrates proposed to underlie amygdala function.",2003,365,1378,124,2,21,40,40,47,54,65,69,73,86
43f1d8775ba09eee5f785449597ecc783afde484,Anatomy of the prefrontal cortex chemical neurotransmission animal neuropsychology neurophysiology human neuropsychology neuroimaging overview of prefrontal functions - the temporal organization of behaviour.,1997,0,1562,46,62,44,80,109,83,100,107,94,92,91
53be8684b3965956e4bbbed771377a6a9a118d16,"Practice of a novel task leads to improved performance. The brain mechanisms associated with practice-induced improvement in performance are largely unknown. To address this question we have examined the functional anatomy of the human brain with positron emission tomography (PET) during the naive and practiced performance of a simple verbal response selection task (saying an appropriate verb for a visually presented noun). As a control state, subjects were asked to repeat the visually presented nouns. Areas of the brain most active during naive performance (anterior cingulate, left prefrontal and left posterior temporal cortices, and the right cerebellar hemisphere), compared to repeating the visually presented nouns, were all significantly less active during practiced performance. These changes were accompanied by changes in the opposite direction in sylvian-insular cortex bilaterally and left medial extrastriate cortex. In effect, brief practice made the cortical circuitry used for verbal response selection indistinguishable from simple word repetition. Introduction of a novel list of words reversed the learning-related effects. These results indicate that two distinct circuits can be used for verbal response selection and normal subjects can change the brain circuits used during task performance following less than 15 min of practice. One critical factor in determining the circuitry used appears to be the degree to which a task is learned or automatic.",1994,40,1526,60,9,60,144,72,95,65,118,81,56,80
095cadc155e88a41a1158632a9422d13066057c9,"Adaptation to climate variability and change is important both for impact assessment (to estimate adaptations which are likely to occur) and for policy development (to advise on or prescribe adaptations). This paper proposes an ""anatomy of adaptation"" to systematically specify and differentiate adaptations, based upon three questions: (i) adapt to what? (ii) who or what adapts? and (iii) how does adaptation occur? Climatic stimuli include changes in long-term mean conditions and variability about means, both current and future, and including extremes. Adaptation depends fundamentally on the characteristics of the system of interest, including its sensitivities and vulnerabilities. The nature of adaptation processes and forms can be distinguished by numerous attributes including timing, purposefulness, and effect. The paper notes the contribution of conceptual and numerical models and empirical studies to the understanding of adaptation, and outlines approaches to the normative evaluation of adaptation measures and strategies.",2000,89,1500,76,3,14,11,17,20,29,28,27,50,62
b606205a102ebc3b1a610893b97d258a85084910,"Duplexes of 21–23 nucleotide (nt) RNAs are the sequence‐specific mediators of RNA interference (RNAi) and post‐transcriptional gene silencing (PTGS). Synthetic, short interfering RNAs (siRNAs) were examined in Drosophila melanogaster embryo lysate for their requirements regarding length, structure, chemical composition and sequence in order to mediate efficient RNAi. Duplexes of 21 nt siRNAs with 2 nt 3′ overhangs were the most efficient triggers of sequence‐specific mRNA degradation. Substitution of one or both siRNA strands by 2′‐deoxy or 2′‐O‐methyl oligonucleotides abolished RNAi, although multiple 2′‐deoxynucleotide substitutions at the 3′ end of siRNAs were tolerated. The target recognition process is highly sequence specific, but not all positions of a siRNA contribute equally to target recognition; mismatches in the centre of the siRNA duplex prevent target RNA cleavage. The position of the cleavage site in the target RNA is defined by the 5′ end of the guide siRNA rather than its 3′ end. These results provide a rational basis for the design of siRNAs in future gene targeting experiments.",2001,57,1495,97,3,49,124,156,146,126,110,94,70,92
8204a4d51b7c3848c181fbca017217b5e0da95c1,"In 1616, Sir William Harvey was the first to describe the importance of right ventricular (RV) function in his seminal treatise, De Motu Cordis : “Thus the right ventricle may be said to be made for the sake of transmitting blood through the lungs, not for nourishing them.”1,2 For many years that followed, emphasis in cardiology was placed on left ventricular (LV) physiology, overshadowing the study of the RV. In the first half of the 20th century, the study of RV function was limited to a small group of investigators who were intrigued by the hypothesis that human circulation could function adequately without RV contractile function.3 Their studies, however, were based on an open pericardial dog model, which failed to take into account the complex nature of ventricular interaction. In the early 1950s through the 1970s, cardiac surgeons recognized the importance of right-sided function as they evaluated procedures to palliate right-heart hypoplasia. Since then, the importance of RV function has been recognized in heart failure, RV myocardial infarction, congenital heart disease and pulmonary hypertension. More recently, advances in echocardiography and magnetic resonance imaging have created new opportunities for the study of RV anatomy and physiology.

The goal of the present review is to offer a clinical perspective on RV structure and function. In the first part, we discuss the anatomy, physiology, aging, and assessment of the RV. In the second part, we discuss the pathophysiology, clinical importance, and management of RV failure.

### Macroscopic Anatomy of the RV

In the normal heart, the RV is the most anteriorly situated cardiac chamber and lies immediately behind the sternum. In the absence of transposition of great arteries, the RV is delimited by the annulus of the tricuspid valve and by the pulmonary valve. As suggested by Goor and Lillehi,4 the RV can be described in …",2008,173,1168,39,5,36,61,57,90,88,94,124,115,110
475556bff31ab426f808645b8c902ada765a87d6,"OBJECTIVE: To describe the functional anatomy of the ankle complex as it relates to lateral ankle instability and to describe the pathomechanics and pathophysiology of acute lateral ankle sprains and chronic ankle instability. DATA SOURCES: I searched MEDLINE (1985-2001) and CINAHL (1982-2001) using the key words ankle sprain and ankle instability. DATA SYNTHESIS: Lateral ankle sprains are among the most common injuries incurred during sports participation. The ankle functions as a complex with contributions from the talocrural, subtalar, and inferior tibiofibular joints. Each of these joints must be considered in the pathomechanics and pathophysiology of lateral ankle sprains and chronic ankle instability. Lateral ankle sprains typically occur when the rearfoot undergoes excessive supination on an externally rotated lower leg. Recurrent ankle sprain is extremely common; in fact, the most common predisposition to suffering a sprain is the history of having suffered a previous ankle sprain. Chronic ankle instability may be due to mechanical instability, functional instability, or most likely, a combination of these 2 phenomena. Mechanical instability may be due to specific insufficiencies such as pathologic laxity, arthrokinematic changes, synovial irritation, or degenerative changes. Functional instability is caused by insufficiencies in proprioception and neuromuscular control. CONCLUSIONS/RECOMMENDATIONS: Lateral ankle sprains are often inadequately treated, resulting in frequent recurrence of ankle sprains. Appreciation of the complex anatomy and mechanics of the ankle joint and the pathomechanics and pathophysiology related to acute and chronic ankle instability is integral to the process of effectively evaluating and treating ankle injuries.",2002,147,1208,129,0,1,3,11,26,31,38,46,51,50
3971135f684ed9ce58d85336b8e81ab825de08c8,"We performed a comprehensive cognitive, neuroimaging, and genetic study of 31 patients with primary progressive aphasia (PPA), a decline in language functions that remains isolated for at least 2 years. Detailed speech and language evaluation was used to identify three different clinical variants: nonfluent progressive aphasia (NFPA; n = 11), semantic dementia (SD; n = 10), and a third variant termed logopenic progressive aphasia (LPA; n = 10). Voxel‐based morphometry (VBM) on MRIs showed that, when all 31 PPA patients were analyzed together, the left perisylvian region and the anterior temporal lobes were atrophied. However, when each clinical variant was considered separately, distinctive patterns emerged: (1) NFPA, characterized by apraxia of speech and deficits in processing complex syntax, was associated with left inferior frontal and insular atrophy; (2) SD, characterized by fluent speech and semantic memory deficits, was associated with anterior temporal damage; and (3) LPA, characterized by slow speech and impaired syntactic comprehension and naming, showed atrophy in the left posterior temporal cortex and inferior parietal lobule. Apolipoprotein E ε4 haplotype frequency was 20% in NFPA, 0% in SD, and 67% in LPA. Cognitive, genetic, and anatomical features indicate that different PPA clinical variants may correspond to different underlying pathological processes.",2004,88,1280,101,3,22,34,61,60,72,75,85,113,109
6626c83757db9edf5b69f1a44ae82198efcd01a6,"Two thousand four hundred human permanent teeth were decalcified, injected with dye, and cleared in order to determine the number of root canals and their different types, the ramifications of the main root canals, the location of apical foramina and transverse anastomoses, and the frequency of apical deltas.",1984,35,1589,160,0,2,1,3,7,3,6,5,5,6
2001275e7a9c8d7e3ccc1c4d2b2543d5a8936292,2007 Guidelines for the Management of Arterial Hypertension : The Task Force for the Management of Arterial Hypertension of the European Society of Hypertension (ESH) and of the European Society of Cardiology (ESC).,2007,1505,5102,221,0,1,24,5,396,460,493,705,705,731
b58c40da4051134954fccf0ab765b314f58ff18b,,1996,123,13795,240,0,0,1,1,0,0,0,0,0,0
a4017f4928c8aae4ae2ecb37e86238b5af7b1e6c,2007 Guidelines for the Management of Arterial Hypertension : The Task Force for the Management of Arterial Hypertension of the European Society of Hypertension (ESH) and of the European Society of Cardiology (ESC).,2007,893,8675,83,275,585,750,747,660,718,793,875,773,639
ec85e9b8ba314c9e8c583631ed61bfd109ffbefb,"Cardiac imaging is an integral part of the evaluation of patients with all forms of heart disease. Unfortunately, each imaging modality, including nuclear cardiology, echocardiography, cardiovascular magnetic resonance imaging, cardiac computed tomography, coronary angiography, and cardiac positron emission tomography, has adopted its own separate and sometimes markedly differing nomenclature, as well as methods of orientation and segmentation of the heart. The lack of common nomenclature and views has resulted in difficulties in optimal patient management, communication between modalities, interpretation of results, and combined research. Attempts by several subspecialty organizations in the past have improved but not resolved these terminology issues. To ultimately resolve these differences, a remarkable committee was convened: The American Heart Association Writing Group on Myocardial Segmentation and Registration for Cardiac Imaging. This writing group was composed of members from the following organizations: the American Society of Echocardiography, the American Society of Nuclear Cardiology, the North American Society of Cardiac Imaging, the Society for Cardiac Angiography and Interventions, and the Society of Cardiovascular Magnetic Resonance. Their task was a real challenge: to come to an agreement upon all aspects of nomenclature and anatomic descriptions of the heart. Their efforts have resulted in agreements for the following issues: orientation of the heart, naming of cardiac planes, number of segments of the left ventricle, common displays, and finally, regional nomenclature and locations. This extraordinary effort is being published in this issue of the Journal of Nuclear Cardiology as well as in Circulation, Catheterization and Cardiovascular Intervention, The International Journal of Cardiovascular Imaging, Journal of Cardiovascular Magnetic Resonance, and Journal of the American Society of Echocardiography. The virtually simultaneous publication of this statement in many journals is a real tribute to the importance of this document. However, the ultimate test of the success of this writing group will be the widespread acceptance of these tools to better communication amongst modalities, ultimately resulting in better patient care. Gary V. Heller, MD, PhD President, American Society of Nuclear Cardiology",2002,12,3546,190,3,26,60,85,110,146,173,223,203,218
8d51c5f7e40c5d8d4767c7fc4990648c6f0b5853,"Guidelines and Expert Consensus Documents aim to present management recommendations based on all of the relevant evidence on a particular subject in order to help physicians select the best possible management strategies for the individual patient suffering from a specific condition, taking into account the impact on outcome and also the risk–benefit ratio of a particular diagnostic or therapeutic procedure. Numerous studies have demonstrated that patient outcomes improve when guideline recommendations, based on the rigorous assessment of evidence-based research, are applied in clinical practice.

A great number of Guidelines and Expert Consensus Documents have been issued in recent years by the European Society of Cardiology (ESC) and also by other organizations or related societies. The profusion of documents can put at stake the authority and credibility of guidelines, particularly if discrepancies appear between different documents on the same issue, as this can lead to confusion in the minds of physicians. In order to avoid these pitfalls, the ESC and other organizations have issued recommendations for formulating and issuing Guidelines and Expert Consensus Documents. The ESC recommendations for guidelines production can be found on the ESC website.1 It is beyond the scope of this preamble to recall all but the basic rules.

In brief, the ESC appoints experts in the field to carry out a comprehensive review of the literature, with a view to making a critical evaluation of the use of diagnostic and therapeutic procedures and assessing the risk–benefit ratio of the therapies recommended for management and/or prevention of a given condition. Estimates of expected health outcomes are included, where data exist. The strength of evidence for or against particular procedures or treatments is weighed according to predefined scales for grading recommendations and levels of evidence, as outlined in what follows.

The Task Force members of the writing panels, …",2006,317,3524,103,3,39,132,208,283,298,356,436,425,453
44ed95d53ec3563e8287626ea69a2876fcf4fd83,,2004,188,7448,76,160,317,431,497,546,474,527,508,488,513
016affda2fb90ddf94a48b5b67b06c55259c4ebb,"This document was developed by a consensus conference initiated by Kristian Thygesen, MD, and Joseph S. Alpert, MD, after formal approval by Lars Rydén, MD, President of the European Society of Cardiology (ESC), and Arthur Garson, MD, President of the American College of Cardiology (ACC). All of the participants were selected for their expertise in the field they represented, with approximately one-half of the participants selected from each organization. Participants were instructed to review the scientific evidence in their area of expertise and to attend the consensus conference with prepared remarks. The first draft of the document was prepared during the consensus conference itself. Sources of funding appear in Appendix A. The recommendations made in this document represent the attitudes and opinions of the participants at the time of the conference, and these recommendations were revised subsequently. The conclusions reached will undoubtedly need to be revised as new scientific evidence becomes available. This document has been reviewed by members of the ESC Committee for Scientific and Clinical Initiatives and by members of the Board of the ESC who approved the document on April 15, 2000.*",2000,69,3679,55,4,58,128,170,216,272,291,307,242,208
2fa8bf2cbb77fab5803e4ba6081017a22bad1a16,"ESC Committee for Practice Guidelines (CPG), Silvia G. Priori (Chairperson) (Italy), Jean-Jacques Blanc (France), Andrzej Budaj (Poland), John Camm (UK), Veronica Dean (France), Jaap Deckers (The Netherlands), Kenneth Dickstein (Norway), John Lekakis (Greece), Keith McGregor (France), Marco Metra (Italy), Joao Morais (Portugal), Ady Osterspey (Germany), Juan Tamargo (Spain), Jose Luis Zamorano (Spain)  Document Reviewers, Marco Metra (CPG Review Coordinator) (Italy), Michael Bohm (Germany), Alain Cohen-Solal (France), Martin Cowie (UK), Ulf Dahlstrom (Sweden), Kenneth Dickstein (Norway), Gerasimos S. Filippatos (Greece), Edoardo Gronda (Italy), Richard Hobbs (UK), John K. Kjekshus (Norway), John McMurray (UK), Lars Ryden (Sweden), Gianfranco Sinagra (Italy), Juan Tamargo (Spain), Michal Tendera (Poland), Dirk van Veldhuisen (The Netherlands), Faiez Zannad (France)

Guidelines and Expert Consensus Documents aim to present all the relevant evidence on a particular issue in order to help physicians to weigh the benefits and risks of a particular diagnostic or therapeutic procedure. They should be helpful in everyday clinical decision-making.

A great number of Guidelines and Expert Consensus Documents have been issued in recent years by the European Society of Cardiology (ESC) and by different organizations and other related societies. This profusion can put at stake the authority and validity of guidelines, which can only be guaranteed if they have been developed by an unquestionable decision-making process. This is one of the reasons why the ESC and others have issued recommendations for formulating and issuing Guidelines and Expert Consensus Documents.

In spite of the fact that standards for issuing good quality Guidelines and Expert Consensus Documents are well defined, recent surveys of Guidelines and Expert Consensus Documents published in peer-reviewed journals between 1985 and 1998 have shown that methodological standards were not complied with in the vast majority of cases. It is therefore of great importance that guidelines and recommendations are presented in formats that are …",2005,194,5758,102,127,325,416,452,466,399,449,458,682,712
a20a63b283ac8c044823cbc538e7729eca35ab8f,"Nuclear cardiology, echocardiography, cardiovascular magnetic resonance (CMR), cardiac computed tomography (CT), positron emission computed tomography (PET), and coronary angiography are imaging modalities that have been used to measure myocardial perfusion, left ventricular function, and coronary anatomy for clinical management and research. Although there are technical differences between these modalities, all of them image the myocardium and the adjacent cavity. However, the orientation of the heart, angle selection for cardiac planes, number of segments, slice display and thickness, nomenclature for segments, and assignment of segments to coronary arterial territories have evolved independently within each field. This evolution has been based on the inherent strengths and weaknesses of the technique and the practical clinical application of these modalities as they are used for patient management. This independent evolution has resulted in a lack of standardization and has made accurate intra- and cross-modality comparisons for clinical patient management and research very difficult, if not, at times, impossible.

Attempts to standardize these options for all cardiac imaging modalities should be based on the sound principles that have evolved from cardiac anatomy and clinical needs.1–3⇓⇓ Selection of standardized methods must be based on the following criteria:





An earlier special report from the American Heart Association, American College of Cardiology, and Society of Nuclear Medicine4 defined standards for plane selection and display orientation for serial …",2002,14,3304,13,6,28,65,108,142,174,154,188,195,199
6a29b1e2a193f72b56feee4e9100275006117ce4,"Non-thrombotic PE does not represent a distinct clinical syndrome. It may be due to a variety of embolic materials and result in a wide spectrum of clinical presentations, making the diagnosis difficult. With the exception of severe air and fat embolism, the haemodynamic consequences of non-thrombotic emboli are usually mild. Treatment is mostly supportive but may differ according to the type of embolic material and clinical severity.",2008,441,2708,174,5,102,178,209,274,252,320,312,251,245
ef66d43779cc8e6c050f841229be9dcc0cb2552f,"Classifications of heart muscle diseases have proved to be exceedingly complex and in many respects contradictory. Indeed, the precise language used to describe these diseases is profoundly important. A new contemporary and rigorous classification of cardiomyopathies (with definitions) is proposed here. This reference document affords an important framework and measure of clarity to this heterogeneous group of diseases. Of particular note, the present classification scheme recognizes the rapid evolution of molecular genetics in cardiology, as well as the introduction of several recently described diseases, and is unique in that it incorporates ion channelopathies as a primary cardiomyopathy.",2006,26,2641,69,15,85,109,138,184,203,184,222,205,208
223fcc19b392d2e83ff0be826862cd49708c66db,"The American College of Cardiology (ACC)/American Heart Association (AHA) Task Force on Practice Guidelines was formed to make recommendations regarding the diagnosis and treatment of patients with known or suspected cardiovascular disease. Coronary artery disease (CAD) is the leading cause of death in the United States. Unstable angina (UA) and the closely related condition non–ST-segment elevation myocardial infarction (NSTEMI) are very common manifestations of this disease. These life-threatening disorders are a major cause of emergency medical care and hospitalizations in the United States. In 1996, the National Center for Health Statistics reported 1 433 000 hospitalizations for UA or NSTEMI. In recognition of the importance of the management of this common entity and of the rapid advances in the management of this condition, the need to revise guidelines published by the Agency for Health Care Policy and Research (AHCPR) and the National Heart, Lung and Blood Institute in 1994 was evident. This Task Force therefore formed the current committee to develop guidelines for the management of UA and NSTEMI. The present guidelines supersede the 1994 guidelines.

The customary ACC/AHA classifications I, II, and III summarize both the evidence and expert opinion and provide final recommendations for both patient evaluation and therapy:

Class I: Conditions for which there is evidence and/or general agreement that a given procedure or treatment is useful and effective .

Class II: Conditions for which there is conflicting evidence and/or a divergence of opinion about the usefulness/efficacy of a procedure or treatment. 

Class IIa: Weight of evidence/opinion is in favor of usefulness/efficacy. 

Class IIb: Usefulness/efficacy is less well established by evidence/opinion. 

Class III: Conditions for which there is evidence and/or general agreement that the procedure/treatment is not useful/effective and in some cases may be harmful. 

The weight of the evidence was ranked highest (A) if the data …",2002,650,2826,59,117,190,226,268,264,174,162,145,125,119
5e409725d53f259e051a1ad7872da336b80a8c94,"Definition of MI. Criteria for acute, evolving or recent MI. Either one of the following criteria satisfies the diagnosis for an acute, evolving or recent MI: 1) Typical rise and gradual fall (troponin) or more rapid rise and fall (CK-MB) of biochemical markers of myocardial necrosis with at least one of the following: a) ischemic symptoms; b) development of pathologic Qwaves on the ECG; c) ECG changes indicative of ischemia (ST segment elevation or depression); or d) coronary artery intervention (e.g., coronary angioplasty). 2) Pathologic findings of an acute MI. Criteria for established MI. Any one of the following criteria satisfies the diagnosis for established MI: 1) Development of new pathologic Q waves on serial ECGs. The patient may or may not remember previous symptoms. Biochemical markers of myocardial necrosis may have normalized, depending on the length of time that has passed since the infarct developed. 2) Pathologic findings of a healed or healing MI.",2000,70,2824,101,1,40,72,111,132,193,218,208,228,221
ffde91f2a0cd6263ff55dd7c5d71b033fbde85ff,"It is important that the medical profession plays a significant role in critically evaluating the use of diagnostic procedures and therapies as they are introduced and tested in the detection, management, or prevention of disease states. Rigorous and expert analysis of the available data documenting absolute and relative benefits and risks of those procedures and therapies can produce helpful guidelines that improve the effectiveness of care, optimize patient outcomes, and favorably affect the overall cost of care by focusing resources on the most effective strategies.

The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of such guidelines in the area of cardiovascular disease since 1980. The ACC/AHA Task Force on Practice Guidelines, whose charge is to develop, update, or revise practice guidelines for important cardiovascular diseases and procedures, directs this effort. The Task Force is pleased to have this guideline developed in conjunction with the European Society of Cardiology (ESC). Writing committees are charged with the task of performing an assessment of the evidence and acting as an independent group of authors to develop or update written recommendations for clinical practice.

Experts in the subject under consideration have been selected from all 3 organizations to examine subject-specific data and write guidelines. The process includes additional representatives from other medical practitioner and specialty groups when appropriate. Writing committees are specifically charged to perform a formal literature review, weigh the strength of evidence for or against a particular treatment or procedure, and include estimates of expected health outcomes where data exist. Patient-specific modifiers, comorbidities, and issues of patient preference that might influence the choice of particular tests or therapies are considered as well as frequency of follow-up and cost effectiveness. When available, information from studies on cost will be considered; however, review …",2006,1216,2494,81,6,99,198,210,220,223,227,244,262,209
1487335adcfb75344d99544e133559ec4620acd8,"For several years the European Society of Hypertension (ESH) and the European Society of Cardiology (ESC) decided not to produce their own guidelines on the diagnosis and treatment of hypertension but to endorse the guidelines on hypertension issued by the World Health Organization (WHO) and International Society of Hypertension (ISH)1,2 with some adaptation to reflect the situation in Europe. However, in 2003 the decision was taken to publish ESH/ESC specific guidelines3 based on the fact that, because the WHO/ISH Guidelines address countries widely varying in the extent of their health care and availability of economic resource, they contain diagnostic and therapeutic recommendations that may be not totally appropriate for European countries. In Europe care provisions may often allow a more in-depth diagnostic assessment of cardiovascular risk and organ damage of hypertensive individuals as well as a wider choice of antihypertensive treatment.

The 2003 ESH/ESC Guidelines3 were well received by the clinical world and have been the most widely quoted paper in the medical literature in the last two years.4 However, since 2003 considerable additional evidence on important issues related to diagnostic and treatment approaches to hypertension has become available and therefore updating of the previous guidelines has been found advisable.

In preparing the new guidelines the Committee established by the ESH and ESC has agreed to adhere to the principles informing the 2003 Guidelines, namely 1) to try to offer the best available and most balanced recommendation to all health care providers involved in the management of hypertension, 2) to address this aim again by an extensive and critical review of the data accompanied by a series of boxes where specific recommendations are given, as well as by a concise set of practice recommendations to be published soon thereafter as already done in 2003; …",2007,1247,2394,69,27,141,199,229,246,278,320,208,129,122
f6dbc98c59690ebe8527c8e5e2aee49948736bf8,,1996,8,3015,62,9,29,51,54,99,97,100,95,123,142
24b2ae931645336f34b96d336e2c669dbccc81db,"Clinical Periodontology and Implant Dentistry 6ed - Libros de Medicina - Periodoncia - 252,00",2008,0,1107,64,57,52,57,74,90,102,100,107,69,98
8eb2a10b79dbbba7cdf2b0ffa7acf30e2952baa6,Discusses the performativity of economics and proposes theoretical directions to study it from a sociological perspective.,2006,92,999,115,3,11,17,36,56,51,70,81,96,80
8f966e607f2bbc89b1c68ce18eb503bf8d8aa59d,This literature review concerns the impact of immigration on the economy of the host country focusing on the experience of the United States. The emphasis is on the period from the 1970s to the 1990s. The author shows that research earlier in this period generally concluded that the economic effects of immigration were positive but that more recent research on later migrations have generally concluded that immigration may be having an adverse effect on the earnings of native unskilled workers and be placing an increased burden on welfare programs. The importance of such economic analysis for the formulation of appropriate migration policies is stressed.,1994,113,2490,196,2,13,33,47,55,60,60,79,93,111
b6d7e6a2763da443c3386edfe70bf46c07059da1,"There is now clear scientific evidence that emissions from economic activity, particularly the burning of fossil fuels for energy, are causing changes to the Earth´s climate. A sound understanding of the economics of climate change is needed in order to underpin an effective global response to this challenge. The Stern Review is an independent, rigourous and comprehensive analysis of the economic aspects of this crucial issue. It has been conducted by Sir Nicholas Stern, Head of the UK Government Economic Service, and a former Chief Economist of the World Bank. The Economics of Climate Change will be invaluable for all students of the economics and policy implications of climate change, and economists, scientists and policy makers involved in all aspects of climate change.",2007,0,8384,711,241,485,697,771,822,819,772,725,576,542
c04677651cb0545ccf080abeb0958fc907f70c4a,"THE new institutional economics is preoccupied with the origins, incidence, and ramifications of transaction costs. Indeed, if transaction costs are negligible, the organization of economic activity is irrelevant, since any advantages one mode of organization appears to hold over another will simply be eliminated by costless contracting. But despite the growing realization that transaction costs are central to the study of economics,' skeptics remain. Stanley Fischer's complaint is typical: ""Transaction costs have a well-deserved bad name as a theoretical device ... [partly] because there is a suspicion that almost anything can be rationalized by invoking suitably specified transaction costs.""2 Put differently, there are too many degrees of freedom; the concept wants for definition.",1979,1,8960,477,0,8,8,16,33,25,30,40,46,50
a6462e546ad432ddbe3e7d9e5854f029204f57e6,"This paper provides an introduction and “user guide” to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a “quasi-experimental” design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD. ( JEL C21, C31)",2009,202,3986,453,39,94,159,194,264,298,368,466,466,407
23908e4166066894e2ace9c76cb6a375d99ad8dc,"Economic geography in an era of global competition poses a paradox. In theory, location should no longer be a source of competitive advantage. Open global markets, rapid transportation, and high-speed communications should allow any company to source any thing from any place at any time. But in practice, Michael Porter demonstrates, location remains central to competition. Today's economic map of the world is characterized by what Porter calls clusters: critical masses in one place of linked industries and institutions--from suppliers to universities to government agencies--that enjoy unusual competitive success in a particular field. The most famous example are found in Silicon Valley and Hollywood, but clusters dot the world's landscape. Porter explains how clusters affect competition in three broad ways: first, by increasing the productivity of companies based in the area; second, by driving the direction and pace of innovation; and third, by stimulating the formation of new businesses within the cluster. Geographic, cultural, and institutional proximity provides companies with special access, closer relationships, better information, powerful incentives, and other advantages that are difficult to tap from a distance. The more complex, knowledge-based, and dynamic the world economy becomes, the more this is true. Competitive advantage lies increasingly in local things--knowledge, relationships, and motivation--that distant rivals cannot replicate. Porter challenges the conventional wisdom about how companies should be configured, how institutions such as universities can contribute to competitive success, and how governments can promote economic development and prosperity.",1998,0,8267,651,3,35,76,103,152,222,253,268,284,361
72910077a29caf411dbb03148997c72b47e65ab0,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",1993,109,6825,560,112,128,131,131,133,161,173,189,199,187
6732435ab6e0eb4e0d14662cf724f3be54807a35,"This paper considers how identity, a person's sense of self, affects economic outcomes. We incorporate the psychology and sociology of identity into an economic model of behavior. In the utility function we propose, identity is associated with different social categories and how people in these categories should behave. We then construct a simple game-theoretic model showing how identity can affect individual interactions. The paper adapts these models to gender discrimination in the workplace, the economics of poverty and social exclusion, and the household division of labor. In each case, the inclusion of identity substantively changes conclusions of previous economic analysis.",2000,144,4589,414,18,28,52,51,91,119,99,136,180,176
6c8409565460774b18eb020832343e647d094573,"Bringing together leaf trait data spanning 2,548 species and 175 sites we describe, for the first time at global scale, a universal spectrum of leaf economics consisting of key chemical, structural and physiological properties. The spectrum runs from quick to slow return on investments of nutrients and dry mass in leaves, and operates largely independently of growth form, plant functional type or biome. Categories along the spectrum would, in general, describe leaf economic variation at the global scale better than plant functional types, because functional types overlap substantially in their leaf traits. Overall, modulation of leaf traits and trait relationships by climate is surprisingly modest, although some striking and significant patterns can be seen. Reliable quantification of the leaf economics spectrum and its interaction with climate will prove valuable for modelling nutrient fluxes and vegetation boundaries under changing land-use and climate.",2004,69,5301,299,13,58,101,114,134,158,197,253,283,327
11040e39cb84397ac26f063058fefcb394da9400,"This paper examines the progressive development of the new institutional economics over the past quarter century. It begins by distinguishing four levels of social analysis, with special emphasis on the institutional environment and the institutions of governance. It then turns to some of the good ideas out of which the NIE works: the description of human actors, feasibility, firms as governance structures, and operationalization. Applications, including privatization, are briefly discussed. Its empirical successes, public policy applications, and other accomplishments notwithstanding, there is a vast amount of unfinished business.",2000,138,4906,329,9,36,78,90,111,137,114,176,201,239
35cb80cc9825527ce57f4f3c36c2862d73a0ee62,"Introduction General and theoretical issues in the economics of the performing arts: The economic structure of performing arts firms the nature of consumer demand for the performing arts structure, conduct and performance in the performing arts industry the positive theory of assistance empirical characteristics of the performing arts: characteristics of companies characteristics of audiences characteristics characteristics of performing arts industries the pattern of assistnace Public policy and the performing arts arguments for public assistnace market efficiency considerations arguments for public assistnace: equity and merit good considerations objectives of performing arts policy forms of assistance levels of assistance allocation of assistance alocation of assistance conclusion: some policy issues in persspective appendices notes list of references.",1979,0,232,7,0,0,1,1,3,0,1,4,4,3
674aae795b11a895e5fc2699a5b97fcb725cedfa,"The work cited by the Nobel committee was done jointly with Amos Tversky (1937-1996) during a long and unusually close collaboration. Together, we explored the psychology of intuitive beliefs and choices and examined their bounded rationality. Herbert A. Simon (1955, 1979) had proposed much earlier that decision makers should be viewed as boundedly rational, and had offered a model in which utility maximization was replaced by satisficing. Our research attempted to obtain a map of bounded rationality, by exploring the systematic biases that separate the beliefs that people have and the choices they make from the optimal beliefs and choices assumed in rational-agent models. The",2003,189,4247,301,4,45,62,84,139,153,181,176,219,227
30c40f8c16848fa14540ef47afe0cc8b381b1ff9,"People like to help those who are helping them and to hurt those who are hurting them. Outcomes rejecting such motivations are called fairness equilibria. Outcomes are mutual-max when each person maximizes the other's material payoffs, and mutual-min when each person minimizes the other's payoffs. It is shown that every mutual-max or mutual-min Nash equilibrium is a fairness equilibrium. If payoffs are small, fairness equilibria are roughly the set of mutual-max and mutual-min outcomes; if payoffs are large, fairness equilibria are roughly the set of Nash equilibria. Several economic examples are considered and possible welfare implications of fairness are explored. Copyright 1993 by American Economic Association.",1993,43,5061,298,2,6,4,23,19,37,48,104,99,136
c35be599aede0ee2ed502a2bb6e4d7ea592af36f,"The transaction cost approach to the study of economic organization regards the transaction as the basic unit of analysis and holds that an understanding of transaction cost economizing is central to the study of organizations. Applications of this approach require that transactions be dimensionalized and that alternative governance structures be described. Economizing is accomplished by assigning transactions to governance structures in a discriminating way. The approach applies both to the determination of efficient boundaries, as between firms and markets, and to the organization of internal transactions, including the design of employment relations. The approach is compared and contrasted with selected parts of the organization theory literature.",1981,60,5585,330,1,5,6,11,16,18,23,30,15,27
61d66e74e0d6973baf01ced1ddc27bc182c88bce,"The event study is an important research tool in economics and finance. The goal of an event study is to measure the effects of an economic event on the value of firms. Event study methods exploit the fact that, given rationality in the marketplace, the effects of an event will be reflected immediately in security prices. Thus the impact can be measured by examining security prices surrounding the event. In this paper event study methods are described including some of the potential complications. An example is included to illustrate the approach.",1997,41,3436,330,3,13,8,23,37,38,51,69,70,98
6cbb20a29588438b23d1545bec9736d69f450327,"What determines the size and form of redistributive programs, the extent and type of public goods provision, the burden of taxation across alternative tax bases, the size of government deficits, and the stance of monetary policy during the course of business and electoral cycles? A large and rapidly growing literature in political economics attempts to answer these questions. But so far there is little consensus on the answers and disagreement on the appropriate mode of analysis. Combining the best of three separate traditions—the theory of macroeconomic policy, public choice, and rational choice in political science—Torsten Persson and Guido Tabellini suggest a unified approach to the field. As in modern macroeconomics, individual citizens behave rationally, their preferences over economic outcomes inducing preferences over policy. As in public choice, the delegation of policy decisions to elected representatives may give rise to agency problems between voters and politicians. And, as in rational choice, political institutions shape the procedures for setting policy and electing politicians. The authors outline a common method of analysis, establish several new results, and identify the main outstanding problems.",2000,0,3757,216,15,49,94,154,169,184,193,204,261,256
d67f30652aa34fb99b5c42e67af3946f5de9afb0,,1985,10,5795,276,2,4,6,10,10,23,31,31,51,60
fc206e1723d6897dc8fcb6078efb05b9111b3b8e,"This classic text has introduced generations of students to the economic theory of consumer behaviour. Written by 2015 Nobel Laureate Angus Deaton and John Muellbauer, the book begins with a self-contained presentation of the basic theory and its use in applied econometrics. These early chapters also include elementary extensions of the theory to labour supply, durable goods, the consumption function, and rationing. The rest of the book is divided into three parts. In the first of these the authors discuss restrictions on choice and aggregation problems. The next part consists of chapters on consumer index numbers; household characteristics, demand, and household welfare comparisons; and social welfare and inequality. The last part extends the coverage of consumer behaviour to include the quality of goods and household production theory, labour supply and human capital theory, the consumption function and intertemporal choice, the demand for durable goods, and choice under uncertainty.",1980,0,3919,269,1,5,21,24,44,34,52,54,48,50
ff259e78776c738b94760ce73c044bfaf2a22f55,"This article is intended to enhance the position of stakeholder theory as an integrating theme for the business and society field. It offers an instrumental theory of stakeholder management based on a synthesis of the stakeholder concept, economic theory, behavioral science, and ethics. The core theory—that a subset of ethical principles (trust, trustworthiness, and cooperativeness) can result in significant competitive advantage—is supplemented by nine research propositions along with some research and policy implications.",1995,110,3277,283,2,7,23,26,38,31,48,42,51,55
6c3cd55660a48af4d9614c3f44ef69ce8eff0dfe,"Global climate change poses a threat to the well-being of humans and other living things through impacts on ecosystem functioning, biodiversity, capital productivity, and human health. Climate change economics attends to this issue by offering theoretical insights and empirical findings relevant to the design of policies to reduce, avoid, or adapt to climate change. This economic analysis has yielded new estimates of mitigation benefits, improved understanding of costs in the presence of various market distortions or imperfections, better tools for making policy choices under uncertainty, and alternate mechanisms for allowing flexibility in policy responses. These contributions have influenced the formulation and implementation of a range of climate change policies at the domestic and international levels.",1995,42,3086,362,0,4,0,5,6,4,6,6,2,7
d044dafc85a0b40237ae0ee40e76d00a98a62e29,"A systematic treatment of the economics of the modern firm, this book draws on the insights of a variety of areas in modern economics and other disciplines, but presents a coherent, consistent, innovative treatment of the central problems in organizations of motivating people and coordinating their activities.",1992,0,4464,177,2,23,35,55,88,99,117,146,180,189
d060e917cc7cdecb72807feea6432496c1c31a02,"The course to be followed by a motorist having a preselected destination is indicated by a plurality of markers set in the center of the path to be followed by the motorist, each set of markers having a different color to correspond to a different preselected destination.",1985,20,3398,244,2,4,15,17,20,15,30,35,31,28
23272b54be9ac651186bf19b2299f59873b0997c,"Policy makers view public sector-sponsored employment and training programs and other active labor market policies as tools for integrating the unemployed and economically disadvantaged into the work force. Few public sector programs have received such intensive scrutiny, and been subjected to so many different evaluation strategies. This chapter examines the impacts of active labor market policies, such as job training, job search assistance, and job subsidies, and the methods used to evaluate their effectiveness. Previous evaluations of policies in OECD countries indicate that these programs usually have at best a modest impact on participants' labor market prospects. But at the same time, they also indicate that there is considerable heterogeneity in the impact of these programs. For some groups, a compelling case can be made that these policies generate high rates of return, while for other groups these policies have had no impact and may have been harmful. Our discussion of the methods used to evaluate these policies has more general interest. We believe that the same issues arise generally in the social sciences and are no easier to address elsewhere. As a result, a major focus of this chapter is on the methodological lessons learned from evaluating these programs. One of the most important of these lessons is that there is no inherent method of choice for conducting program evaluations. The choice between experimental and non-experimental methods or among alternative econometric estimators should be guided by the underlying economic models, the available data, and the questions being addressed. Too much emphasis has been placed on formulating alternative econometric methods for correcting for selection bias and too little given to the quality of the underlying data. Although it is expensive, obtaining better data is the only way to solve the evaluation problem in a convincing way. However, better data are not synonymous with social experiments.",1999,199,3312,186,17,46,84,133,136,175,153,208,230,180
4f19d7f17067f3edb5ee3efaa161b2ad24876df4,"To harness the full power of computer technology, economists need to use a broad range of mathematical techniques. In this book, Kenneth Judd presents techniques from the numerical analysis and applied mathematics literatures and shows how to use them in economic analyses. The book is divided into five parts. Part I provides a general introduction. Part II presents basics from numerical analysis on R^n, including linear equations, iterative methods, optimization, nonlinear equations, approximation methods, numerical integration and differentiation, and Monte Carlo methods. Part III covers methods for dynamic problems, including finite difference methods, projection methods, and numerical dynamic programming. Part IV covers perturbation and asymptotic solution methods. Finally, Part V covers applications to dynamic equilibrium analysis, including solution methods for perfect foresight models and rational expectation models. A web site contains supplementary material including programs and answers to exercises.",1998,4,2896,319,9,30,58,83,91,102,133,143,177,160
4e77fd7c955f8fd70d9e1c6cb670fa1e38ad7157,"Part 1 Rise of science-related technology: introduction evolutionary theory in economics and technical change process innovations materials innovations product and system innovation paradigm change. Part 2 Innovations and the firms: the microeconomics of innovation success and failure, the role of marketing and user-producer networks innovation, size of firm, economies of scale and scope uncertainty, project evaluation and finance of innovation, management strategy and theory of the firm. Part 3 Macroeconomics of innovation - science, technology and economic growth globalization and multinational corporations underdevelopment and catching up. Part 4 Innovation and public policies: market failure and aspects of public support for innovation technical change, employment and skills environmental issues technological assessment. Appendix: measurement and definitions.",1975,0,4611,103,1,2,5,7,4,18,10,13,4,21
620351e3a30767e78c301a2cb193965b8268efc4,"This book, which comprises eight chapters, presents a comprehensive critical survey of the results and methods of laboratory experiments in economics. The first chapter provides an introduction to experimental economics as a whole, with the remaining chapters providing surveys by leading practitioners in areas of economics that have seen a concentration of experiments: public goods, coordination problems, bargaining, industrial organization, asset markets, auctions, and individual decision making.",1997,8,3846,67,41,75,75,115,118,186,202,219,189,236
ddbbf87a2a0ec74f9c62239ddf3abdb45afd9c7d,"N RECENT YEARS, public and professional interest in schools has been heightened by a spate of reports, many of them critical of current school policy.' These policy documents have added to persistent and long-standing concerns about the cost, effectiveness, and fairness of the current school structure, and have made schooling once again a serious public issue. As in the past, however, any renewed interest in education is likely to be short-lived, doomed to dissipate as frustration over the inability of policy to improve school practice sets in. This frustration about school policy relates directly to knowledge about the educational production process and in turn to underlying research on schools. Although the educational process has been extensively researched, clear policy prescriptions flowing from this research have been difficult to derive.2 There exists, however, a consistency to the research findings that does have an immediate application to school policy: Schools differ dramatically in ""quality,""",1986,109,3076,204,2,7,15,20,23,40,44,36,49,49
715254ac0ffa87857c00b91b1b4e6464a5c56ee6,The identification of sellers and the discovery of their prices is given as an example of the role of the search for information in economic life.,1961,0,3949,133,1,0,0,1,1,0,2,3,1,6
dfc06f7f7eb83d2c0e2a1d67ecb95b520a406843,"We examine the economics of financing small business in private equity and debt markets. Firms are viewed through a financial growth cycle paradigm in which different capital structures are optimal at different points in the cycle. We show the sources of small business finance, and how capital structure varies with firm size and age. The interconnectedness of small firm finance is discussed along with the impact of the macroeconomic environment. We also analyze a number of research and policy issues, review the literature, and suggest topics for future research.",1998,265,2216,302,1,8,12,47,44,40,65,55,73,79
e291333a380e1de8c758e1e14d2dccaf414899e0,"This paper summarizes evidence on the effects of early environments on child, adolescent, and adult achievement. Life cycle skill formation is a dynamic process in which early inputs strongly affect the productivity of later inputs.",2006,43,2795,127,12,29,68,96,114,122,173,213,193,211
a0c06ad17e0a731ead51d0853992f6b06fb3a4b5,1. The Original Affluent Society2. The Domestic Mode of Production: The Structure of Underproduction3. The Domestic Mode of Production: Intensification of Production4. The Spirit of the Gift5. On the Sociology of Primitive Exchange6. Exchange Value and the Diplomacy of Primitive Trade,1972,0,3546,132,0,5,4,8,10,23,20,24,28,22
c3b220081c6543f265edae74568e47dcf6f270c3,"This paper shows that reciprocity has powerful implications for many economic domains. It is an important determinant in the enforcement of contracts and social norms and enhances the possibilities of collective action greatly. Reciprocity may render the provision of explicit incentive inefficient because the incentives may crowd out voluntary co-operation. It strongly limits the effects to competition in markets with incomplete contracts and gives rise to noncompetitive wage differences. Finally, reciprocity it is also a strong force contributing to the existence of incomplete contracts.",2000,187,3173,149,32,57,75,101,96,126,146,167,184,166
050443122604df9197368e874bd80bc8180e6e31,Manufacturing is undergoing a revolution. The mass production model is being replaced by a vision of a flexible multiproduct firm that emphasizes quality and speedy response to market conditions while utilizing technologically advanced equipment and new forms of organization. The authors' optimizing model of the firm generates many of the observed patterns that mark modern manufacturing. Central to the authors' results is a method of handling optimization and comparative statics problems that requires neither differentiability nor convexity. Copyright 1990 by American Economic Association.,1990,9,2847,136,4,12,10,24,28,32,45,45,60,54
3ee2a3694161433364273b60dadd5b061b5caa9b,"Undoubtedly one of the highlights of the 1999 Conference was the plenary session in which Professors David Held and Mahdi Elmandjra came together to discuss the theme of ‘“Globalization”: Democracy and Diversity’. The Conference also witnessed the launch of Global Transformations (Polity Press, 1999), at which David Held was joined by two of his three coauthors, Professor Anthony McGrew and Dr Jonathan Perraton. Global Transformations is the product of almost a decade’s work by a research team (based at the Open University and supported by the ESRC) who have produced what James. N. Rosenau has called ‘the definitive work on globalization’. It is a study which not only synthesises an extraordinary amount of information from research on globalization in a range of social science disciplines, but also makes its own distinctive contribution to our understanding of the complex range of forces which are reshaping the world order. We are delighted to be able to reproduce here an ‘executive summary’ of Global Transformations that summarises the major findings of this 500-page survey in just six thousand words.",1999,1,2768,111,8,43,76,123,179,160,157,160,163,172
f8efc9c04fcc543943552f5e168b36318cea3e1b,"Wood performs several essential functions in plants, including mechanically supporting aboveground tissue, storing water and other resources, and transporting sap. Woody tissues are likely to face physiological, structural and defensive trade-offs. How a plant optimizes among these competing functions can have major ecological implications, which have been under-appreciated by ecologists compared to the focus they have given to leaf function. To draw together our current understanding of wood function, we identify and collate data on the major wood functional traits, including the largest wood density database to date (8412 taxa), mechanical strength measures and anatomical features, as well as clade-specific features such as secondary chemistry. We then show how wood traits are related to one another, highlighting functional trade-offs, and to ecological and demographic plant features (growth form, growth rate, latitude, ecological setting). We suggest that, similar to the manifold that tree species leaf traits cluster around the 'leaf economics spectrum', a similar 'wood economics spectrum' may be defined. We then discuss the biogeography, evolution and biogeochemistry of the spectrum, and conclude by pointing out the major gaps in our current knowledge of wood functional traits.",2009,120,2046,166,9,45,67,110,108,146,158,212,201,245
7fbc2485837aa460ce48b90d3869a5d2f06ae306,"This study analyzes organization of economic activity within and between markets and hierarchies. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains. Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration. Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition. The analysis explains why primary work groups of the peer group and simple hierarchy types arise. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such as bounded rationality and opportunism, occur. Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The analysis attempts better to assess the employment relation in circumstances where workers acquire, during the course of the employment, significant job-specific skills and knowledge. The study compares alternative labor-contracting modes and demonstrates that collective organization is helpful in enhancing the acquisition of idiosyncratic knowledge and skills by the work force. The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies. Discusses the market structure in relation to technical and organizational innovation. The study proposes a systems approach to the innovation process. Its purpose is to permit the realization of the distinctive advantages of both small and large firms which apply at different stages of the innovation process. The analysis also examines the relation of organizational innovation to technological innovation. (AT)",1975,0,2890,162,0,1,1,0,3,2,5,4,4,3
2a0fc90991ab74e18be81208eafd1d9c3d81a428,"Abstract Science policy issues have recently joined technology issues in being acknowledged to have strategic importance for national ‘competitiveness’ and ‘economic security’. The economics literature addressed specifically to science and its interdependences with technological progress has been quite narrowly focused and has lacked an overarching conceptual framework to guide empirical studies and public policy discussions in this area. The emerging ‘new economics of science’, described by this paper, offers a way to remedy these deficiencies. It makes use of insights from the theory of games of incomplete information to synthesize the classic approach of Arrow and Nelson in examining the implications of the characteristics of information for allocative efficiency in research activities, on the one hand, with the functionalist analysis of institutional structures, reward systems and behavioral norms of ‘open science’ communities-associated with the sociology of science in the tradition of Merton-on the other. An analysis is presented of the gross features of the institutions and norms distinguishing open science from other modes of organizing scientific research, which shows that the collegiate reputation-based reward system functions rather well in satisfying the requirement of social efficiency in increasing the stock of reliable knowledge. At a more fine-grain level of examination, however, the detailed workings of the system based on the pursuit of priority are found to cause numerous inefficiencies in the allocation of basic and applied science resources, both within given fields and programs and across time. Another major conclusion, arrived at in the context of examining policy measures and institutional reforms proposed to promote knowledge transfers between university-based open science and commercial R&D, is that there are no economic forces that operate automatically to maintain dynamic efficiency in the interactions of these two (organizational) spheres. Ill-considered institutional experiments, which destroy their distinctive features if undertaken on a sufficient scale, may turn out to be very costly in terms of long-term economic performance.",1994,80,2333,149,5,6,19,12,24,34,36,45,52,71
91cb3b78f73b6fd1717bb3e31544f056a8f4f4a9,"IN RECENT years has not felt his gorge rise upon learning the staggeringly high salary of a shortstop, a movie star, an opera singer? A basketball player on a losing team earns $1.2 million; an author sells the paperback rights to his book for $800,000; a television interviewer switches networks and signs a contract calling for her to receive an annual income of just under $2 million. And the gorge continues to rise. The spectacle of people doing work that doesn't always seem overweighted with significance for annual (and, in the case of rock singers, sometimes nightly) sums of money that figure to exceed what you and I may earn in our lifetimes this, as they say nowadays, does not give off good vibes. What's going on here? What we are talking about, of course, is the phenomenon of superstars, wherein relatively small numbers of people earn enormous amounts of money and seem to dominate the fields in which they are engaged. This phenomenon appears to be increasingly important in the modern world certainly, with the breakdown of economic privacy, it is an increasingly visible phenomenon. The very word superstar implies inflation in our most precious currency, language; to be a star would have been sufficient in my youth. Yet we appear to be stuck with the term. As for the phenomenon itself, viewed from the standpoint of an economist, it may not be as puzzling as it at first glimpse seems. The first thing to be said in this connection is that certain economic activities admit extreme concentration of both personal reward and market size among a handful of participants. Every economic activity supports considerable diversity of talent and significant inequality in the personal distribution of rewards. Activities where superstars are found differ from those in which most of us make our livings by supporting much less diversity and much more inequality in the distribution of earnings. The bulk of earnings goes to relatively small numbers of practitioners typically, the few regarded as among the best in their fields. Similar distributions of earnings in the industrial sector would ultimately come to the attention of the Federal Trade Commission or the",1981,17,2465,176,0,2,4,5,2,4,2,3,0,5
47b40e819edf53c09e9ec7c4f735fb5cec72a713,"This article surveys the economic analysis of public enforcement of law – the use of public agents (inspectors, tax auditors, police, prosecutors) to detect and to sanction violators of legal rules. We first discuss the basic elements of the theory: the probability of imposition of sanctions, the magnitude and form of sanctions (fines, imprisonment), and the rule of liability. We then examine a variety of extensions, including the costs of imposing fines, mistakes, marginal deterrence, settlement, selfreporting, repeat offences, and incapacitation.",2007,59,1977,132,68,124,137,172,182,155,186,147,118,113
9906a97c7d2431ea447eef3ec769ef625de2c338,"With climate change as prototype example, this paper analyzes the implications of structural uncertainty for the economics of low-probability, high-impact catastrophes. Even when updated by Bayesian learning, uncertain structural parameters induce a critical tail fattening of posterior-predictive distributions. Such fattened tails have strong implications for situations, like climate change, where a catastrophe is theoretically possible because prior knowledge cannot place sufficiently narrow bounds on overall damages. This paper shows that the economic consequences of fat-tailed structural uncertainty (along with unsureness about high-temperature damages) can readily outweigh the effects of discounting in climate-change policy analysis.",2009,52,1535,118,100,120,149,125,143,151,111,114,101,87
614625b071af366b441fc15408850c04fd304459,"Plant design and economics for chemical engineers , Plant design and economics for chemical engineers , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1958,0,2821,192,0,1,0,0,2,0,0,0,1,1
09d6a448068fbfa44712993ec689f38ea47bf4a9,TABLE OF CONTENTS PAGE Executive Summary i-xxvii Preface & Acknowledgements,2006,83,1710,185,9,100,163,164,149,159,143,139,138,109
896172e1d3a5d8a06ce6ef0187f53f1cf3803755,Commercial production of intracellular microalgal metabolites requires the following: (1) large-scale monoseptic production of the appropriate microalgal biomass; (2) recovery of the biomass from a relatively dilute broth; (3) extraction of the metabolite from the biomass; and (4) purification of the crude extract. This review examines the options available for recovery of the biomass and the intracellular metabolites from the biomass. Economics of monoseptic production of microalgae in photobioreactors and the downstream recovery of metabolites are discussed using eicosapentaenoic acid (EPA) recovery as a representative case study.,2003,94,1947,136,5,1,8,13,17,27,30,69,117,125
d55b23466d157f238673f9dfb70f334e2de93891,"As firms switch from defined‐benefit plans to defined‐contribution plans, employees bear more responsibility for making decisions about how much to save. The employees who fail to join the plan or who participate at a very low level appear to be saving at less than the predicted life cycle savings rates. Behavioral explanations for this behavior stress bounded rationality and self‐control and suggest that at least some of the low‐saving households are making a mistake and would welcome aid in making decisions about their saving. In this paper, we propose such a prescriptive savings program, called Save More Tomorrow™ (hereafter, the SMarT program). The essence of the program is straightforward: people commit in advance to allocating a portion of their future salary increases toward retirement savings. We report evidence on the first three implementations of the SMarT program. Our key findings, from the first implementation, which has been in place for four annual raises, are as follows: (1) a high proportion (78 percent) of those offered the plan joined, (2) the vast majority of those enrolled in the SMarT plan (80 percent) remained in it through the fourth pay raise, and (3) the average saving rates for SMarT program participants increased from 3.5 percent to 13.6 percent over the course of 40 months. The results suggest that behavioral economics can be used to design effective prescriptive programs for important economic decisions.",2004,59,2279,67,34,47,72,68,75,99,112,143,129,124
47e32579b853d2647e6e9a70ef6ff317cd38b7d3,"How much and how fast should the globe reduce greenhouse-gas emissions? How should nations balance the costs of the reductions against the damages and dangers of climate change? This question has been addressed by the recent ""Stern Review on the Economics of Climate Change,"" which answers these questions clearly and unambiguously. We need urgent, sharp, and immediate reductions in greenhouse-gas emissions. An analysis of the ""Stern Review"" finds that these recommendations depend decisively on the assumption of a near-zero social discount rate. The Review's unambiguous conclusions about the need for extreme immediate action will not survive the substitution of discounting assumptions that are consistent with today's market place.",2006,49,2147,57,15,171,209,241,225,252,235,222,185,145
dac864eeb1b5de388e96a6fab4c15cf3d6d7ba5b,,1963,0,3147,22,2,1,4,5,7,2,5,14,5,11
98fe960b3f5354a987e33e62ab1de060a42ec9ec,"Economic theorists traditionally banish discussions of information to footnotes. Serious consideration of costs of communication, imperfect knowledge, and the like would, it is believed, complicate without informing. This paper, which analyzes competitive markets in which the characteristics of the commodities exchanged are not fully known to at least one of the parties to the transaction, suggests that this comforting myth is false. Some of the most important conclusions of economic theory are not robust to considerations of imperfect information.",1976,21,2530,91,1,7,8,11,7,15,23,24,26,22
a3d2dcfed78248ac3944c6b00ef97ed36807a1c1,"The advantages and disadvantages of expanding the standard economic model by more realistic behavioral assumptions have received much attention. The issue raised in this article is whether it is useful to complicate-or perhaps to enrichthe model of the profit-seeking firm by considering the preferences that people have for being treated fairly and for treating others fairly. The absence of considerations of fairness and loyalty from standard economic theory is one of the most striking contrasts between this body of theory and other social sciences-and also between economic theory and lay intuitions about human behavior. Actions in many domains commonly conform to standards of decency that are more restrictive than the legal ones: the institutions of tipping and lost-and-found offices rest on expectations of such actions. Nevertheless, the standard microeconomic model of the profitmaximizing firm assigns essentially no role to The traditional assumption that fairness is irrelevant to economic analysis is questioned. Even profit-maximizing firms will have an incentive to act in a manner that is perceived as fair if the individuals with whom they deal are willing to resist unfair transactions and punish unfair firms at some cost to themselves. Three experiments demonstrated that willingness to enforce fairness is common. Community standards for actions affecting customers, tenants, and employees were studied in telephone surveys. The rules of fairness, some of which are not obvious, help explain some anomalous market phenomena. * The research for this paper was supported by the Department of Fisheries and Oceans Canada. Kahneman and Thaler were also supported, respectively, by the U.S. Office of Naval Research and by the Alfred P. Sloan Foundation. Conversations with J. Brander, R. Frank, and A. Tversky were very helpful. We also thank Leslie McPherson and Daniel Treisman for their assistance. The paper presented at the conference and commented on by the discussants included a detailed report of study 3, which is only summarized here. It did not contain study 1, which was incomplete at the time. Daniel Kahneman is now in the Department of Psychology, University of California, Berkeley 94720.",1986,22,2185,98,4,0,8,5,13,14,17,14,44,16
83a569a41faaa1819d718559d89cd2b2e2e1c50c,"Milton Friedman's 1953 essay 'The methodology of positive economics' remains the most cited, influential, and controversial piece of methodological writing in twentieth-century economics. Since its appearance, the essay has shaped the image of economics as a scientific discipline, both within and outside of the academy. At the same time, there has been an ongoing controversy over the proper interpretation and normative evaluation of the essay. Perceptions have been sharply divided, with some viewing economics as a scientific success thanks to its adherence to Friedman's principles, others taking it as a failure for the same reason. In this book, a team of world-renowned experts in the methodology of economics cast new light on Friedman's methodological arguments and practices from a variety of perspectives. It provides the 21st century reader with an invaluable assessment of the impact and contemporary significance of Friedman's seminal work.",2009,0,1378,110,96,100,100,89,102,93,99,98,94,104
ed3ddf12573aebe49008dbb6c6f2fe2fe42756ca,"R ECENTLY, orbiting evidence of unAmerican technological competition has focused attention on the role played by scientific research in our political economy. Since Sputnik it has become almost trite to argue that we are not spending as much on basic scientific research as we should. But, though dollar figures have been suggested, they have not been based on economic analysis of what is meant by ""as much as we should."" And, once that question is raised, another immediately comes to mind. Economists often argue that opportunities for private profit draw resources where society most desires them. Why, therefore, does not basic research draw more resources through privateprofit opportunity, if, in fact, we are not spending as much on basic scientific research as is ""socially desirable""? In order to answer some of these questions, it seems useful to examine the simple economics of basic research. How much are we spending on basic research? How much should we be spending? Under what conditions will these figures tend to be different? Is basic research marked by these conditions? If so, what can we do to eliminate or reduce the discrepancy? How much are we spending on basic research? In 1953, the latest date for which relatively sophisticated estimates are available, total expenditure on research and development was about $5.4 billion. Of that total, much more than half was for engineering development, much less than half for scientific research. Even less of the total, about $435 million in 1953, was spent on ""basic research."" All evidence indicates that since 1953 expenditure on research and development has increased markedly; $10 billion seems a reasonable estimate for 1957. Expenditure on basic research has also increased at a rapid rate, perhaps at a faster rate than total research and development expenditure. But basic-research expenditure today is probably under $1 billion, less than one-quarter of 1 per cent of gross national product.' How much should we spend on basic research? Replacing the Xi of the familiar literature on welfare economics with ""basic research"" provides the theoretical answer. From a given expenditure on science we may expect a given flow, over time, of benefits that would not have been created had none of our resources been directed to basic research. This flow of benefits (properly discounted) may be defined as the social value of a given expenditure on basic research. However, if we allocate a given quantity of resources to science, this implies that we are not allocating these resources to other activities and, hence, that we are depriving ourselves of a flow of future benefits that we could have obtained had we directed these resources elsewhere. The discounted flow of bene-",1959,0,2682,123,0,0,0,0,0,0,3,0,1,1
4740121953761c263ddfc31d10daa2b941e446b4,"While innovation processes toward sustainable development (eco-innovations) have received increasing attention during the past years, theoretical and methodological approaches to analyze these processes are poorly developed. Against this background, the term eco-innovation is introduced in this paper addressing explicitly three kinds of changes towards sustainable development: technological, social and institutional innovation. Secondly, the potential contribution of neoclassical and (co-)evolutionary approaches from environmental and innovation economics to eco-innovation research is discussed. Three peculiarities of eco-innovation are identified: the double externality problem, the regulatory push:pull effect and the increasing importance of social and institutional innovation. While the first two are widely ignored in innovation economics, the third is at the least not elaborated appropriately. The consideration of these peculiarities may help to overcome market failure by establishing a specific eco-innovation policy and to avoid a ‘technology bias’ through a broader understanding of innovation. Finally, perspectives for a specific contribution of ecological economics to eco-innovation research are drawn. It is argued that methodological pluralism as established in ecological economics would be very beneficial for eco-innovation research. A theoretical framework integrating elements from both neoclassical and evolutionary approaches should be pursued in order to consider the complexity of factors influencing innovation decisions as well as the specific role of regulatory instruments. And the experience gathered in ecological economics integrating ecological, social and economic aspects of sustainable development is highly useful for opening up innovation research to social and institutional changes. © 2000 Elsevier Science B.V. All rights reserved.",2000,39,1779,154,0,5,3,14,13,15,15,27,39,43
c9cc18de3c1269b24151785b29b8cab7ac39474b,This paper explores the interface between personality psychology and economics. We examine the predictive power of personality and the stability of personality traits over the life cycle. We develop simple analytical frameworks for interpreting the evidence in personality psychology and suggest promising avenues for future research.,2008,739,1391,135,50,56,84,99,109,128,157,132,147,167
71e8e2a9f0d164962dc6caca4e0ba8cc8fbca57b,"Contemplation of the world’s disappearing supplies of minerals, forests, and other exhaustible assets has led to demands for regulation of their exploitation. The feeling that these products are now too cheap for the good of future generations, that they are being selfishly exploited at too rapid a rate, and that in consequence of their excessive cheapness they are being produced and consumed wastefully has given rise to the conservation movement. The method ordinarily proposed to stop the wholesale devastation of irreplaceable natural resources, or of natural resources replaceable only with difficulty and long delay, is to forbid production at certain times and in certain regions or to hamper production by insisting that obsolete and inefficient methods be continued. The prohibitions against oil and mineral development and cutting timber on certain government lands have this justification, as have also closed seasons for fish and game and statutes forbidding certain highly efficient means of catching fish. Taxation would be a more economic method than publicly ordained inefficiency in the case of purely commercial activities such as mining and fishing for profit, if not also for sport fishing. However, the opposition of those who are making the profits, with the apathy of everyone else, is usually sufficient to prevent the diversion into the public treasury of any considerable part of the proceeds of the exploitation of natural resources.",1991,11,1819,193,6,13,12,12,11,13,16,20,14,30
f78d5f79225374bb58506ed50a3b0aabda0f8ad8,"A simple practical questionnaire technique for routine clinical use, the Dermatology Life Quality Index (DLQI) is described. One hundred and twenty patients with different skin diseases were asked about the impact of their disease and its treatment on their lives; a questionnaire, the DLQI, was developed based on their answers. The DLQI was then completed by 200 consecutive new‐patients attending a dermatology clinic. This study confirmed that a topic eczema, psoriasis and generalized pruritus have a greater impact on quality of life than acne, basal cell carcinomas and viral warts. The DLQI was also completed by 100 healthy volunteers; their mean score was very low (1.6%, s.d. 3.5) compared with the mean score for the dermatology patients (24.2%, s.d. 20.9). The reliability of the DLQI was examined in 53 patients using a 1 week test‐retest method and reliability was found to be high (γs=0.99).",1994,14,3673,212,0,9,14,16,14,21,37,38,40,93
391adf80e9a43ded3f591b911136876513ee1c39,Introduction biology and pathophysiology of skin disorders presenting in the skin and mucous membranes dermatology and internal medicine diseases due to microbial agents therapeutics paediatric and geriatric dermatology.,1971,0,4224,37,1,7,16,23,25,26,19,21,14,26
d72324dc5a2c194ace5e94e5176a1293c9a64762,"Rook's textbook of dermatology , Rook's textbook of dermatology , کتابخانه مرکزی دانشگاه علوم پزشکی ایران",2004,116,2025,52,1,19,53,76,88,81,86,132,145,165
58a51ea337d47ee3cab36977105a1d7bbcfbe67f,".."".surpasses the standards set in all [previous editions]""-Archives of Dermatology, review of prior editionWidely acclaimed dermatological treatise is completely revised and updated with over forty new chapters and an entirely new section on dermatologic surgery. Important new chapters include growth factors, thermoregulation, cell biology, vitiligo, albinism and more. Firmly places dermatology within the continuum of internal medicine and deploys over 1,600 exquisite color photos to depict clinical states.",2003,0,2083,54,46,60,77,63,54,61,96,93,121,102
2da0fec6bbaa835a32750c47a9e337c5172f8c39,"Medicine, it is said, is an ever-changing science. With cutting edge research and new clinical experience exponentially expanding the realms of medical science, there is a felt need to continually update our knowledge base. ‘Fitzpatrick’ has remained a highly respected textbook, a ‘Bible’ in the practice of Dermatology, Even since the first edition of this textbook was released in 1971, newer editions have been introduced earlier than the previous ones; every five years now compared to every eight years earlier; a measure of the accelerating pace of change. 
 
The new edition is different from the previous one right from the moment it is opened. The arrangement and organization of various sections and their contents have been completely rehashed. Basic sciences, which occupied a section of its own in the previous edition, are segregated into its components which are now clubbed along with their sections of relevance, bringing with it the advantage of continuity Some reshuffling, for instance, moves Kawasaki's Syndrome from the section on ‘Bacterial Diseases’ to the section on ‘Inflammatory and Vascular Disorders’. Section-end references are truncated and readers are directed to a website for the rest, thus re-appropriating premium print space for text. 
 
Along with updating of previous concepts, new chapters on Public Health in Dermatology, Complementary and Alternative Dermatology, Drug Interactions, 
 
Dermatology in Bioterrorism and Biological Warfare have been added, underscoring globally emerging newer advances in Dermatology which the new age dermatologists should be aware of. 
 
Three column text, new stand-out font, different color codes for headings and sub headings, numerous clinical and histopathological color plates, self explanatory figurative representations, graphs, flow-charts and tables make the assimilation of the vast data a cake walk. ‘At A Glance’ windows and boxes summarizing the differential diagnoses and treatment protocols give a fish-eye view of various aspects of dermatological disorders, something that is not only crisp and precise but also relevant and easy to remember. The reader gets a ready reference point with flip-book views of section name and representative thumbnail pictures on the free margins of both sides of every page. 
 
In a nutshell, this marvelous textbook replete with latest looks, is a completely revised and updated edition with great clarity and easily ‘digestable’ reference material. With this edition, ‘Fitzpatrick’ has further cemented its place in the practice of Dermatology.",2008,0,1327,0,81,143,161,168,124,155,152,113,53,37
beb8de7f3bd175fa9a1d0a0830f2882c7d91e130,,2008,0,832,87,36,47,46,50,55,47,69,45,78,78
d93ffe572b15a163e2ec1336a4e507b0b7a766f0,"Research in IT must address the design tasks faced by practitioners. Real problems must be properly conceptualized and represented, appropriate techniques for their solution must be constructed, and solutions must be implemented and evaluated using appropriate criteria. If significant progress is to be made, IT research must also develop an understanding of how and why IT systems work or do not work. Such an understanding must tie together natural laws governing IT systems with natural laws governing the environments in which they operate. This paper presents a two dimensional framework for research in information technology. The first dimension is based on broad types of design and natural science research activities: build, evaluate, theorize, and justify. The second dimension is based on broad types of outputs produced by design research: representational constructs, models, methods, and instantiations. We argue that both design science and natural science activities are needed to insure that IT research is both relevant and effective.",1995,90,3794,442,1,0,1,2,5,15,22,19,18,40
6fcaad2b337e0786a08486c3f71c9c9afddca930,"One: Descriptive.- One The Scientific Description of Personality.- Personality and Taxonomy: The Problem of Classification.- Type and Trait Theories: The Modern View.- Type-Trait Theories and Factor Analysis.- Situationism versus Type-Trait Theories.- Two The Development of a Paradigm.- Origins of Personality Theory.- The Beginnings of Modern Research.- Psychoticism as a Dimension of Personality.- Impulsiveness and Sensation Seeking: A Special Case.- The Question of Validity.- Three The Universality of P, E, and N.- Genetic Factors.- Personality in Animals.- Cross-cultural Studies.- Longitudinal Studies of Personality.- Four Alternative Systems of Personality Description.- R.B. Cattell and 16PF.- The Guilford-Zimmerman Factors.- The NEO Model of Personality.- The Minnesota Multiphasic Personality Inventor.- The California Psychological Inventory.- The Edwards Personal Preference Schedule and the Jackson Personality Research Form.- Other Systems.- Summary.- Five The Cognitive Dimension: Intelligence as a Component of Personality.- Galton versus Binet: IQ and Reaction Time.- The Psychophysiology of Intelligence.- The Theory of Intelligence.- Six Summary and Conclusions.- Two: Causal.- Seven Theories of Personality and Performance.- H. J. Eysenck (1957).- H. J. Eysenck (1967a).- Gray's Theory.- Brebner's Theory.- Eight The Psychophysiology of Personality.- Theoretical Background.- Extraversion.- Neuroticism.- Theoretical Implications.- Nine Extraversion, Arousal, and Performance.- Conditioning.- Sensitivity to Stimulation.- Vigilance.- Verbal Learning and Verbal Memory.- Psychomotor Performance.- Perceptual Phenomena.- Conclusions.- Ten Neuroticism, Anxiety, and Performance.- The State-Trait Approach.- Theories of Anxiety and Performance.- Worry and Performance.- Efficiency and Effectiveness.- Anxiety ? Task Interactions.- Attentional Mechanisms.- Learning and Memory.- Conclusions.- Eleven Social Behavior.- Social Interaction.- Sexual Behavior.- Educational Achievement.- Occupational Performance.- Antisocial Behavior and Crime.- Psychiatric Disorders.- Conclusions.- Three: Epilogue.- Twelve Is There a Paradigm in Personality Research?.- References and Bibliography.- Author Index.",1985,0,2054,111,1,24,46,14,25,25,41,24,53,36
e25083587debd035d2963d3ccdeb3eef477f083b,"In section 3.3 of [i]Philosophy of Natural Science[/i], Hempel argues that crucial tests are not sufficient enough to prove a given hypothesis or to disprove them. Hempel states what some may believe why a crucial test can prove or disprove a hypothesis. If there are two competing hypothesis which involve the same subject and no available evidence favors one or the other, then there exists a test, which will produce conflicting outcomes for the different hypotheses. This test is the so-called crucial test would then presumably refute one hypothesis while supporting the other. Hempel then presents his side of this argument using an example of past experiments involving the nature of light. He describes how Foucault performed an experiment involving the velocity of light through air and water. This experiment was meant to show whether light consists of waves or extremely small particles as presented by Newton. Foucault’s experiment was performed and the resulting outcome was used to refute Newton’s view of light as small particles traveling at a high velocity. Hempel believed that this test was not strong enough to completely support or refute either view of light. The experiment relied on the assumption that light as waves would travel faster in air than in water. However Hempel argues that the conception of light as streams of particles was too indefinite to assume that it would travel slower in air without additional assumptions about the motion of particles and their surrounding medium. So even though the results may seem to support and prove the wave hypothesis, it doesn’t necessarily disprove the particle theory. In fact Einstein later proposed a theory, which eliminated the classical wave theory, using support from an experiment by Lenard in 1903. But again as in the previous example one of the hypotheses was not definitely refuted, in this case being the wave theory. M.-M. V.",1966,0,1635,44,0,1,1,6,8,10,13,6,10,12
e22c08e168d74b55bd618a9026635fb60d5c7508,"Despite their successful use in many conscientious studies involving outdoor learning applications, mobile learning systems still have certain limitations. For instance, because students cannot obtain real-time, contextaware content in outdoor locations such as historical sites, endangered animal habitats, and geological landscapes, they are unable to search, collect, share, and edit information by using information technology. To address such concerns, this work proposes an environment of ubiquitous learning with educational resources (EULER) based on radio frequency identification (RFID), augmented reality (AR), the Internet, ubiquitous computing, embedded systems, and database technologies. EULER helps teachers deliver lessons on site and cultivate student competency in adopting information technology to improve learning. To evaluate its effectiveness, we used the proposed EULER for natural science learning at the Guandu Nature Park in Taiwan. The participants were elementary school teachers and students. The analytical results revealed that the proposed EULER improves student learning. Moreover, the largely positive feedback from a post-study survey confirms the effectiveness of EULER in supporting outdoor learning and its ability to attract the interest of students.",2009,40,180,9,2,3,10,16,17,19,17,26,11,21
a0e6a2a784c38a0052beec3c23419e9c11f22a60,"This study has three major purposes, including designing mobile natural-science learning activities that rest on the 5E Learning Cycle, examining the effects of these learning activities on students’ performances of learning aquatic plants, and exploring students’ perceptions toward these learning activities. A case-study method is utilized and the science club with 46 fourth-grade students is selected as the study case in the study. Besides, a set of quantitative and qualitative data were collected from the case to document the learning effects of and the students’ perceptions of the learning activities, and to discuss factors underlying these effects and students’ perceptions. The results indicate that the learning activities can enhance students’ scientific performances, including both knowledge and understanding levels. Students’ perceptions of these learning activities appear to be positive. The study identifies two factors that are prominent in the positive effects: students’ engaging in “mobile-technology supported” observation during their scientific inquiry; and students’ engaging in “mobiletechnology supported” manipulation during their scientific inquiry. Finally, the conclusions that our study has drawn could constitute a useful guide for educational practitioners concerned with the potentials of mobile computing in school settings.",2009,33,127,7,0,0,7,10,16,18,21,11,16,9
30a0ba6032b19c5d2ef61ec2281c814482f223bf,"The context and conduct of Arctic research are changing. In Nunavut, funding agencies, licensing bodies, and new regulatory agencies established under the Nunavut Land Claims Agreement require researchers to engage and consult with Inuit communities during all phases of research, to provide local training and other benefits, and to communicate project results effectively. Researchers are also increasingly expected to incorporate traditional knowledge into their work and to design studies that are relevant to local interests and needs. In this paper, we explore the challenges that researchers and communities experience in meeting these requirements by reviewing case studies of three natural science projects in Nunavut. Together, these projects exemplify both success and failure in negotiating research relationships. The case studies highlight three principal sources of researcher-community conflict: 1) debate surrounding acceptable impacts of research and the nature and extent of local benefits that research projects can and should provide; 2) uncertainty over who has the power and authority to dictate terms and conditions under which projects should be licensed; and 3) the appropriate research methodology and design to balance local expectations and research needs. The Nunavut research licensing process under the Scientists Act is an important opportunity for communities, scientists, and regulatory agencies to negotiate power relationships. However, the standards and procedures used to evaluate research impact remain unclear, as does the role of communities in the decision-making process for research licensing. The case studies also demonstrate the critical role of trust and rapport, forged through early and frequent communication, efforts to provide local training, and opportunities for community members to observe, participate in, and derive employment from project activities. Clarifying research policies in Nunavut is one step to improving relations between scientists and communities. In addition, steps need to be taken at both policy and project levels to train researchers, educate funding programs, mobilize institutions, and empower communities, thus strengthening the capacity of all stakeholders in northern research.",2009,15,91,6,5,5,6,8,8,5,6,6,9,12
6650c025463e0537afdb6e452a5eaa8cd2dbd8b8,"THE trickle of water moved very much like a caterpillar hunting for food. Its anterior was like a smooth, domeshaped wall of water that was prevented from answering the call of gravity by some invisible force... Natural Science James Lucas THE trickle of water moved very much like a caterpillar hunt ing for food. Its anterior was like a smooth, domeshaped wall of water that was prevented from answering the call of gravity by some invisible force. Al though the water was apparently very wet, the grains of dust that it pushed before it remained as dry as they had ever been, and the little tricklet even picked grains of dust up on its back without their sinking into it or getting wet. It sent out a creeping branch much like the pseudopod of an amoeba, which soon came to a standstill and grew in size. Suddenly a small part of it broke through and moved off in another direction. II I was riding the tractor in a field of young blades of corn, when a dark bird with white belly appeared that reminded me much of a sea-gull. Its wings were long and pointed, and its plumage looked more like large scales than feathers. It glided over the field for thirty yards and even gained al t i tude wtihout a single beat of the wings. Its eyes were black and glistening, with a white circle about them that seemed to represent the evil thoughts behind it. Then , within the space of one second, its long beak pointed suddenly downwards, it d ropped straight to the earth like a falling rock, then swooped u p within three inches of the ground with a long, white straw in its beak.",1871,1,0,0,0,0,0,0,0,0,0,0,0,0
15f688bdc381f2bc19a1411ec650a38ebec0c7a9,,1998,0,159,3,2,0,1,3,1,10,4,1,3,2
febde76b600bdcb5fa8005eda4da979e2311e965,,1892,0,57,2,0,0,0,0,0,0,0,0,0,0
0bdfc5a5fd0ef6e7bb4e993f7fa4d64228acefea,,1899,0,0,0,0,0,0,0,0,0,0,0,0,0
61c80563df60e566d3cbfe6d26ea91d557860a9d,"IN order to meet as fully as possible the needs of those who desire work in nature study, the subject will be presented in three closely related courses. Course I deals with the subject in its general aspects, and Courses II and III deal with special topics in considerable detail. During the first three weeks, Course I will be open only to those who enter for the first half of the term. During the last three weeks it will be open (I) to those who enter at that time and (2) to those who are taking either Course II or Course III, and who may desire a somewhat broader survey of the field. The primaryand grammar-grade teachers who elect Course I will be assigned to separate sections, and the work will be adapted to each. The sections will be divided into groups for convenience, and each group will be assigned according to the choice of the individual to definite work upon which each student will be expected to make reports as often as necessary. The subjects given to the groups will be selected from the subjoined syllabus of topics. Each group will report to the entire class, so that the mutual relations of the different lines of study will appear. The topics for discussion will be assigned to different groups for presentation, through which the pedagogic aspects of the subject will be considered.",1901,1,0,0,0,0,0,0,0,0,0,0,0,0
30c9dbd8c68e009b1205da2c6ed7d3a57aebca7c,,1901,0,0,0,0,0,0,0,0,0,0,0,0,0
a022df6e1db5f2e1be32388ca33cfc0391bb490d,"EVERY observation is made and repeated for the purpose of defining a picture or image. Number work is a means of defining a picture or image through a determination of quantity. In the application of number, therefore, the same principles must be observed that are employed in defining the image by other means. I. There must be a clear conception of the image to be developed. Number work, not less than drawing, painting, modeling, and written composition, depends upon the primary",1902,0,0,0,0,0,0,0,0,0,0,0,0,0
7fe1bc4a8e62195178294117a3ab64b97fe47d22,"Most behavior analysts emphasize that a science of behavior must be a natural science as opposed to a social science or any other such description. But what does this mean and why is it important? I explore these questions by attempting to characterize the designation “natural science” and briefly surveying what behavior analysis has in common with other putative natural sciences. Included in the discussion are problems of agency, background assumptions, mechanism, and some examples of shared problems and issues with other natural sciences. Special focus is placed on behavior analysis as a biological science and the implications of that status.",2009,34,19,0,0,1,2,0,3,0,4,2,2,1
f502a77d80062f2dd42ed0b065ded02a5868c2e9,"Following an initial discussion of the general nature of interpretation in contemporary psychology, and social and natural science, relevant views of Charles Taylor and Thomas Kuhn are considered in some detail. Although both Taylor and Kuhn agree that interpretation in the social or human sciences differs in some ways from interpretation in the natural sciences, they disagree about the nature and origins of such difference. Our own analysis follows, in which we consider differences in interpretation between the natural and social sciences (psychology in particular) in terms of Ian Hacking's use of Elizabeth Anscombe's conceptualization of actions as intentional acts under particular descriptions. We conclude that both Taylor and Kuhn are correct to point to differences in interpretation between the natural and social sciences. We also argue that in psychology, such interpretive differences, contra Kuhn and pro Taylor, are qualitative rather than quantitative. They arise from the nature of persons as self-interpretive, reactive beings who act under socioculturally sanctioned, linguistic descriptions. The actions of psychological persons may display qualitative differences over time and across contexts as these descriptions, including social scientific and psychological findings and interpretations, change. In contrast, even when descriptions in natural science change, such changes do not spawn changes in the self-interpretations and intentional actions of the focal phenomena of natural science. We also make the point that much current confusion surrounding interpretation in science arises from the unwarranted tendency of some commentators to treat interpretation as subjective, in ways that ignore the objective grounding of interpretation within regulated social practices, including scientific practices sanctioned by scientific communities.",2009,9,15,0,1,0,0,1,2,0,2,2,2,2
7f4f566b160c7abbf57da0fa51ae119640c28770,"The study aims at establishing whether Foundation Phase schooling provides a proper foundation for the promotion of scientific literacy. Natural Science in the Foundation Phase is understood as scientific knowledge, process skills, and values and attitudes, which together should foster scientific literacy. Influential perspectives on learning, and teaching methods appropriate to Natural Science education in the Foundation Phase, are reviewed, and the Natural Science Learning Area in the RNCS discussed in the context of global trends in curriculum development. Finally the findings of an empirical survey on the perceptions of Foundation Phase teachers with regard to Natural Science teaching and learning, are presented. Major findings include the following: (1) Scientific literacy is currently not a curriculum priority in the Foundation Phase, due mainly to meagre time allocation and lack of applicable Learning Outcomes. (2) Although teachers appear predominantly positive towards the Learning Area, significant shortcomings need to be addressed before Natural Science teaching in the Foundation Phase may claim to provide the required basis for promoting scientific literacy.",2009,129,12,1,0,0,0,1,4,1,2,1,0,1
a78d7e14769e88809543d71159c6b16f6b21b702,How was the hypothetical character of theories of experiencethought about throughout the history of science? The essays cover periods from the middle ages to the 19th and 20th centuries. It is fascinating to see how natural scientists and philosophers were increasingly forced to realize that a natural science without hypotheses is not possible.,2009,0,12,0,0,1,1,3,1,2,3,0,0,1
4b4a4b295a5e8221b0165e65a5a2aeaa58dbea85,"Analytical table of contents Preface Introduction: rationality Part I. Representing: 1. What is scientific realism? 2. Building and causing 3. Positivism 4. Pragmatism 5. Incommensurability 6. Reference 7. Internal realism 8. A surrogate for truth Part II. Intervening: 9. Experiment 10. Observation 11. Microscopes 12. Speculation, calculation, models, approximations 13. The creation of phenomena 14. Measurement 15. Baconian topics 16. Experimentation and scientific realism Further reading Index.",1983,0,1243,47,0,1,1,4,1,3,1,3,11,10
1610cf06f9281bfbda5368d15c286ad6a3191d80,"In this study, mixed research methods were adopted to examine the online game-based learning environment for ""Go Go Bugs"". The purpose of this study was to determine if an online game-based learning environment can engage and motivate students to learn more natural science by using game features than the control group, which had the same learning environment, but no game features. This experiment examined the engagement by participants' frequencies of returning to the learning environment after school hours, within a time period of two weeks. The results of the comparison of pre- and post- surveys showed significant improvement in increasing participants' interest in learning natural science in a game-based learning environment over that of the non-game-based learning environment. Both quantitative and qualitative results from this study indicated that this game-based learning environment successfully motivated participants in exploring natural science and engaging in the learning activities. However, there was no significant result showing that the game-based learning environment improves students' learning achievement more than with the non-game-based learning environment.",2007,28,50,3,0,1,0,2,7,2,7,3,1,8
e28fc8015226ba662b5a6468e3b2f33837358fd2,"Information processes and computation continue to be found abundantly in the deep structures of many fields. Computing is not---in fact, never was---a science only of the artificial.",2007,11,203,9,4,14,25,15,16,23,20,15,13,7
4d09ecd48768f8945dbec99323d03f7eb62b8316,Preface 1. Metaphysical foundations of phoronomy 2. Metaphysical foundations of dynamics 3. Metaphysical foundations of mechanics 4. Metaphysical foundations of phenomenology.,2007,0,209,8,10,10,9,10,9,8,16,19,16,14
cec073bd5d5418c7ff112e5807fef1089a59d453,"This study examines the relationship between citation frequency and the human capital of teams of authors. Analysis of a random sample of articles published in top natural science journals shows that articles co-authored by teams including frequently cited scholars and teams whose members have diverse disciplinary backgrounds have greater citation frequency. The institutional prestige, the percentage of team members at U. S. institutions and the variety of disciplines represented by team member backgrounds do not influence citation frequency. The study introduces a method for evaluating the extent of multidisciplinarity that accounts for the relatedness of disciplines or authors.",2009,46,22,1,1,1,0,2,4,2,1,4,2,0
050f600c03ea03d13839cd108acdbd1e0d59b96d,"The psychology classic a detailed study of scientific theories of human nature and the possible ways in which human behavior can be predicted and controlled from one of the most influential behaviorists of the twentieth century and the author of ""Walden Two."" This is an important book, exceptionally well written, and logically consistent with the basic premise of the unitary nature of science. Many students of society and culture would take violent issue with most of the things that Skinner has to say, but even those who disagree most will find this a stimulating book. Samuel M. Strong, ""The American Journal of Sociology"" This is a remarkable book remarkable in that it presents a strong, consistent, and all but exhaustive case for a natural science of human behavior It ought to be valuable for those whose preferences lie with, as well as those whose preferences stand against, a behavioristic approach to human activity. Harry Prosch, ""Ethics""""",1953,13,6841,265,0,5,2,10,12,16,11,10,17,15
413886ce56be596e068c8e069f1c3ad7a6e563b8,"I. METAPHYSICS, ONTOLOGY, AND LOGIC II. OBJECTS AND PROPERTIES III. METAPHYSICS AND NATURAL SCIENCE IV. TRUTH, TRUTHMAKING, AND METAPHYSICAL REALISM",2006,0,266,12,4,4,5,10,8,15,9,36,14,22
7fa07c973ff7d7a84c8658c25a6532fb1e018b1c,"Regional-scale restoration is a tool of growing importance in environmental management, and the number, scope, and complexity of restoration programs is increasing. Although the importance of natural science to the success of such projects generally is recognized, the actual use of natural science in these programs rarely has been evaluated. We used techniques of program evaluation to examine the use of natural science in six American and three Western European regional-scale restoration programs. Our results suggest that ensuring the technical rigor and directed application of the science is important to program development and delivery. However, the influence of science may be constrained if strategies for its integration into the broader program are lacking. Consequently, the influence of natural science in restoration programs is greatest when formal mechanisms exist for incorporating science into programs, for example, via a framework for integration of science and policy. Our evaluation proposes a model that can be used to enhance the influence of natural science in regional-scale restoration programs in the United States and elsewhere.",2006,39,23,5,0,1,3,4,5,1,0,3,1,2
8a2ce9609450a02cffe7d6c90ef98ed900880f2f,"In this paper, we studied the research areas of Chinese natural science basic research from a point view of complex network. Two research areas are considered to be connected if they appear in one fund proposal. The explicit network of such connections using data from 1999 to 2004 is constructed. The analysis of the real data shows that the degree distribution of the research areas network (RAN) may be better fitted by the exponential distribution. It displays small world effect in which randomly chosen pairs of research areas are typically separated by only a short path of intermediate research areas. The average distance of RAN decreases with time, while the average clustering coefficient increases with time, which indicates that the scientific study would like to be integrated together in terms of the studied areas. The relationship between the clustering coefficient C(k) and the degree k indicates that there is no hierarchical organization in RAN.",2005,112,26,0,0,1,2,4,4,5,1,1,1,1
63b37ee975c40e74bd26819ec6c3fe341656fe7f,"This paper reviews the recent literature on monetary policy rules. We exploit the monetary policy design problem within a simple baseline theoretical framework. We then consider the implications of adding various real world complications. Among other things, we show that the optimal policy implicitly incorporates inflation targeting. We also characterize the gains from making a credible commitment to fight inflation. In contrast to conventional wisdom, we show that gains from commitment may emerge even if the central bank is not trying to inadvisedly push output above its natural level. We also consider the implications of frictions such as imperfect information.",1999,186,5463,621,20,85,194,227,305,292,274,320,368,343
ec1d6bbda7331fc074cca94f0bc07127327dea8b,"Integrated studies of coupled human and natural systems reveal new and complex patterns and processes not evident when studied by social or natural scientists separately. Synthesis of six case studies from around the world shows that couplings between human and natural systems vary across space, time, and organizational units. They also exhibit nonlinear dynamics with thresholds, reciprocal feedback loops, time lags, resilience, heterogeneity, and surprises. Furthermore, past couplings have legacy effects on present conditions and future possibilities.",2007,75,2590,91,3,31,88,116,135,149,190,212,232,223
310beca34914ca073a88f04a2b3ae12b302dcd81,"Abstract The second phase of the Mpumalanga Secondary Science Initiative (MSSI) was launched in Mpumalanga Province, South Africa in 2003. The MSSI seeks to improve the teaching and learning of mathematics and science in all secondary schools in the province over a period of three years. To achieve this goal an in-service system has been developed. The long-term research of the MSSI is aimed at examining the effect of the intervention on the three science learning outcomes as defined by the new curriculum. Tests were developed to assess 10 learners in each of Grades 8 and 9 in 40 schools, representing a 6% sample. This paper reports on the methodology of the baseline assessments undertaken and the extent of learners' attainment levels in the three Natural Science outcomes. It then looks at the results in terms of learners' socio-economic backgrounds. Finally it discusses some of the dilemmas encountered in this research, such as the issue of validity in the light of possible misalignments between the instrument, practice and policy.",2005,26,22,3,1,1,1,1,6,3,3,0,1,2
c3d62bc0ab3e900f0fbf33949754ed9919139164,"This article discusses the results of a qualitative study, based on case studies, aimed at: (a) assessing a group of Portuguese secondary school natural science teachers regarding their conceptions of the nature, teaching and learning of science; (b) studying possible impacts of recent controversies surrounding scientific and technological issues on these conceptions and on teachers' classroom practices. Five teachers, with different backgrounds and teaching experience, were observed during classes and interviewed with the purpose of studying: (a) the relationship between their conceptions and classroom practices; and (b) the factors that impede or enhance this relationship. Subsequently, observation notes and interview transcriptions were systematically analysed. The socio-scientific controversies recently discussed in Portugal seem to have had an impact on teachers' (1) conceptions about the nature, teaching and learning of science; and (2) classroom practice. However, not all teachers were able to teach according to their conceptions. Some factors seem to mediate the relationship between teachers' conceptions and classroom practice: National Curriculum, national exams, teachers' previous experience as scientists, and personal educational priorities or aims. Based on the results obtained, some remarks and educational implications are discussed.",2004,23,49,1,1,1,2,3,5,5,0,1,5,4
2294cddf83c01629c138b307bffcecb19822e307,"Abstract The Chomskyan revolution in linguistics in the 1950s in essence turned linguistics into a branch of cognitive science (and ultimately biology) by both changing the linguistic landscape and forcing a radical change in cognitive science to accommodate linguistics as many of us conceive of it today. More recently Chomsky has advanced the boldest version of his naturalistic approach to language by proposing a Minimalist Program for linguistic theory. In this article, we wish to examine the foundations of the Minimalist Program and its antecedents and draw parallelisms with (meta-)methodological foundations in better-developed sciences such as physics. Once established, such parallelisms, we argue, help direct inquiry in linguistics and cognitive science/biology and unify both disciplines.",2005,150,54,1,1,0,2,4,6,5,2,1,5,2
2bfbe016c77a55119c84d93bfb1b5f3139e83771,"Interdisciplinary collaboration occurs when people with different educational and research backgrounds bring complementary skills to bear on a problem or task. The strength of interdisciplinary scientific research collaboration is its capacity to bring together diverse scientific knowledge to address complex problems and questions. However, interdisciplinary scientific research can be difficult to initiate and sustain. We do not yet fully understand factors that impact interdisciplinary scientific research collaboration. This study synthesizes empirical data from two empirical studies to provide a more comprehensive understanding of interdisciplinary scientific research collaboration within the natural sciences in ac ademia. Data analysis confirmed factors previously identified in various literatures and yielded new factors. A total of twenty factors were identified, and classified into four categories: personal, resources, motivation and common ground. These categorie s and their factors are described, and implications for academic policies and practices to facilitate and sustain interdisciplinary collaboration are discussed.",2005,17,50,3,0,1,4,2,0,0,4,5,2,3
9fff1ce502fd37eb5512b972177c4cff3b038c4b,"The global tendency is obvious: interest in science is on the decrease, the number of pupils choosing university science curriculums has been constantly declining, and scientific knowledge in society (especially among young people) is inadequate. In our opinion, humanity verges on social cataclysms owing to inadequate natural science education as well as on insufficient and often improper knowledge of nature and human. Natural sciences give us most fundamental knowledge about the world of nature. Encouragement of young people’s interest in science is the essential scientific problem. As educational paradigms are being altered we must search for new quality approaches to teaching chemistry and other science subjects. The research, which involved 350 senior pupils from Latvia and 762 from Lithuania, analyzes present-day situation in natural science education. We tried to analyze the factors that cause the interest in natural sciences to decline: inadequate content of teaching, issues related to teachers‘ competence, general attitude of society to natural sciences etc.",2004,13,17,0,2,1,1,1,0,2,0,0,2,1
9e86f2a2d795ac16288d030844b918e125658025,"From the viewpoint of biology, learning and education can be defined as the processes of forming neuronal connections in response to external environmental stimuli, and of controlling or adding appropriate stimuli, respectively. Learning and education can thus be studied as a new field of natural sciences with the entire human life span as its subject, thus including various problems such as fetal environment, childcare, language acquisition, general/special education, and rehabilitation. Non-invasive imaging of higher-order brain functions in humans will clarify the brain's developmental processes, and will provide various evidence for learning sciences. This new approach is called 'developing the brain' or 'brain science and education'. The origin of the concept and its present state are described and its future prospects are briefly analyzed.",2004,52,70,3,1,3,6,3,3,7,5,5,2,5
7b83b725196e087b2b96a6f185c93eff259c6182,,2004,0,67,0,0,0,1,0,3,1,1,0,3,3
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
9b981fa3d1559ac7f17930a25abcb592d03419e3,"Marrying the biological and social sciences: culture, social constructions and natural science possible frameworks evolution and the theory of evolution alternative theories to NeoDarwinism how good a theory is evolutionary theory? suggested readings. The evolution of intelligence: why intelligence ever evolved at all the limits of reductionism intelligence unlimited? Fodor poses a problem human intelligence as adaptation or exaptation suggested readings. The emergence of culture: broadening the picture the trouble with ""levels"" a solution to the levels problem suggested readings. Naturalizing culture the process way: the puzzle of war universal Darwinism modelling co-evolution the ""new"" science of memetics suggested readings. Causal mechanisms: a general framework for understanding psychological mechanism what those mechanisms may be concepts, schemata and other higher-order knowledge structures imitation language theory of mind social force a single magical mechanism? suggested readings. Individuals, groups and culture: the behavioural ecology of group living the units and levels of selection vehicles, interactors and the revival of group selection niche construction suggested readings. The strangeness of culture: the construction of social reality a sociological turn social representations cultural psychology a tentative conclusion suggested readings.",2002,0,77,8,0,3,1,8,7,4,4,1,6,9
78360a9b8edc5f4b2884024a39c318b484e3f3ce,"Given the growing multicultural composition of South African classrooms, educators of science and technology, like educators across the spectrum of all learning areas, are increasingly challenged to reflect how they and their learners conceive of and, as a result, construct knowledge. The reality is that in an expanding globalised world, learners can easily become alienated from what is taught in science and technology, as well as the way it is taught. Indigenous Knowledge Systems (IKS), as a broad framework of thinking about our local context, seeks to problematise the insufficient integration of the cultural-social and the canonical-academic dimensions of natural science and technology education. In this article I conceptualise and clarify IKS vis-a-vis knowledge production, particularly towards educational transformation in which educators may assume that all learners are the same in terms of identity and cultural dynamics. Natural science and technology, in particular, have assumed a definite culture of power, which has marginalized the majority of learners in the past. IKS strategically wishes to transform this view and therefore holds valuable implications for educators in the learning areas of natural science and technology.",2002,58,72,4,0,0,0,1,0,2,2,4,5,3
5d2af227cfafe9b468ac48d97658a8cc051ed08f,"Abstract We investigated the significance of risk assessment studies in the public discussion on CO 2 emissions. Politicians and representatives from the public where interviewed by using the social-science technique of qualitative in-depth interviews. Three different types of attitudes towards natural science were found among politicians. Depending on which attitude a politician holds, risk assessment studies can have an impact on his/her readiness to support environmental policy measures. Regarding lay people, key factors affecting the acceptance of environmental policy measures are knowledge of environmental problems, their impacts on ecosystems or human health as well as direct personal perception of those impacts. Since direct perception is not always possible in everyday life, natural science experiments might be a means for successfully mediating this lacking perception.",2003,36,23,0,0,0,0,1,2,2,2,0,0,3
0a56287d86658f11b233164d6aae85845b1f0676,"Abstract Hermeneutics, or interpretation, is concerned with the generation, transmission, and acceptance of meaning within the lifeworld, and was the original method of the human sciences stemming, from F. Schleiermacher and W. Dilthey. The `hermeneutic philosophy' refers mostly to Heidegger. This paper addresses natural science from the perspective of Heidegger's analysis of meaning and interpretation. Its purpose is to incorporate into the philosophy of science those aspects of historicality, culture, and tradition that are absent from the traditional analysis of theory and explanation, to re-orient the current discussion about scientific realism around the hermeneutics of meaning and truth in science, and to establish some relationship between the current philosophy of natural science and hermeneutical philosophy. The paper has particular relevance to the history and social studies of science and technology.",1998,152,69,4,1,3,0,1,20,2,1,0,0,1
cfb2e9e2841c3c19b3938f4a69b75fa1edd4fb5d,"The worldwide best-selling intermediate microeconomics textbook is distinguished by its remarkably up-to-date and rigorous yet accessible analytical approach. The seventh edition has been carefully updated and revised, adding a wealth of new applications and examples that analyse the important lessons offered by eBay, drug companies, the Yellow Pages and even Maine Lobstermen.",1987,0,2028,93,0,1,1,1,7,4,6,7,5,7
8eccb702db8fa03b35ad999c168ba41037723172,"This paper reviews some of the most popular policy evaluation methods in empirical microeconomics: social experiments, natural experiments, matching, instrumental variables, discontinuity design, and control functions. It discusses identification of traditionally used average parameters and more complex distributional parameters. The adequacy, assumptions, and data requirements of each approach are discussed drawing on empirical evidence from the education and employment policy evaluation literature. A workhorse simulation model of education returns is used throughout the paper to discuss and illustrate each approach. The full set of STATA datasets and do-files are available free online and can be used to reproduce all estimation and simulation results.",2009,134,1059,66,44,78,82,84,88,91,105,87,75,86
7bb70a28be826f93541e1f1fca4e8aee52a5dce6,"Twenty years ago, most banking courses focused on either management or monetary aspects of banking, with no connecting. Since then, a microeconomic theory of banking has developed, mainly through a switch of emphasis from the modeling of risk to the modeling of imperfect information. This asymmetric information model is based on the assumption that different economic agents possess different pieces of information on relevant economic variables, and that they will use the information for their own profit. The model has been extremely useful in explaining the role of banks in the economy. It has also been useful in pointing out structural weaknesses of the banking sector that may justify government intervention--for example, exposure to runs and panics, the persistence of rationing in the credit market, and solvency problems. Microeconomics of Banking provides a guide to the new theory. Topics include why financial intermediaries exist, the industrial organization approach to banking, optimal contracting between lenders and borrowers, the equilibrium of the credit market, macroeconomic consequences of financial imperfections, individual bank runs and systemic risk, risk management inside the banking firm, and bank regulation. Each chapter ends with a detailed problem set and solutions.",1997,166,1191,15,0,13,22,37,44,52,90,69,55,48
e7d688587abe1c7cde5be5bd52b16e81860d69cb,"The article reviews the book ""The Microeconomics of Banking,"" 2nd edn, by Xavier Freixas and Jean-Charles Rochet.",2009,6,555,36,48,42,31,40,41,36,52,36,34,39
2149d1f44b534a62f7f11ac6166ed485e1a9d8a7,"Preface ix Prologue: Economics and the Wealth of Nations and People 1 Part I: Coordination and Conflict: Generic Social Interactions 21 Chapter One: Social Interactions and Institutional Design 23 Chapter Two: Spontaneous Order: The Self-organization of Economic Life 56 Chapter Three: Preferences and Behavior 93 Chapter Four: Coordination Failures and Institutional Responses 127 Chapter Five: Dividing the Gains to Cooperation: Bargaining and Rent Seeking 167 Part II : Competition and Cooperation: The Institutions of Capitalism 203 Chapter Six: Utopian Capitalism: Decentralized Coordination 205 Chapter Seven: Exchange: Contracts, Norms, and Power 233 Chapter Eight: Employment, Unemployment, and Wages 267 Chapter Nine: Credit Markets, Wealth Constraints, and Allocative Inefficiency 299 Chapter Ten: The Institutions of a Capitalist Economy 331 Part III: Change: The Coevolution of Institutions and Preferences 363 Chapter Eleven: Institutional and Individual Evolution 365 Chapter Twelve: Chance, Collective Action, and Institutional Innovation 402 Chapter Thirteen: The Coevolution of Institutions and Preferences 437 Part IV: Conclusion 471 Chapter Fourteen: Economic Governance: Markets, States, and Communities 473 Problem Sets 502 Additional Readings 529 Works Cited 537 Index 571",2003,1,953,48,5,9,32,53,64,59,58,62,60,74
0ee9179b57c6f9f4a8ae0ad70c5f507df7d731f1,"'Personalized medicine' promises to increase the quality of clinical care and, in some cases, decrease health-care costs. Despite this, only a handful of diagnostic tests have made it to market, with mixed success. Historically, the challenges in this field were scientific. However, as discussed in this article, with the maturation of the '-omics' sciences, it now seems that the major barriers are increasingly related to economics. Overcoming the poor microeconomic alignment of incentives among key stakeholders is therefore crucial to catalysing the further development and adoption of personalized medicine, and we propose several actions that could help achieve this goal.",2009,5,170,4,4,17,23,22,20,22,15,12,9,9
5de57633474d065cabde23e6b1def5bb3a6a3d37,"This paper reviews some of the most popular policy evaluation methods in empirical microeconomics: social experiments, natural experiments, matching, instrumental variables, discontinuity design, and control functions. It discusses identification of traditionally used average parameters and more complex distributional parameters. The adequacy, assumptions, and data requirements of each approach are discussed, drawing on empirical evidence from the education and employment policy evaluation literature. A workhorse simulation model of education and earnings is used throughout the paper to discuss and illustrate each approach. The full set of STATA data sets and do-files are available free online and can be used to reproduce all estimation results.",2009,167,60,1,4,11,10,6,2,4,0,4,0,5
def040a2e825a1ecd635e5dc195c1e8e5e465e64,"How do students enrolled in online courses perform relative to those who choose a more traditional classroom environment? What student characteristics help explain differences in student academic achievement in the two modes of instruction? What factors affect the students' choice of instruction mode? The authors address these questions in relation to the teaching of introductory economics courses. They find that the two groups of students are significantly different in age, gender composition, marital status and number of children, GPA, previous economics exposure, planned major, and other important characteristics. The raw data suggested a higher mean score for the online class sections. But after considering course selection bias, the findings indicated that age and GPA positively affect students' performance in the course, whereas the online teaching mode has a narrowly insignificant, or even negative, effect. Semester effects are most important for the online subsample, and male students enjoy a premium in the traditional classroom setting.",2009,42,107,7,1,4,7,15,11,7,16,7,11,8
1644ff8ea98e753133f713c9ae39ccaf00d1e68c,Introduction Part I. The Theory of the Firm: 1. The consumer 2. The firm 3. Separation of consumer objectives and firm objectives Part II. The Entrepreneur in Equilibrium: 4. The entrepreneur 5. Competition between entrepreneurs Part III. Human Capital and Financial Capital: 6. Human capital and the organization of the firm 7. Financial capital and the organization of the firm Part IV. Intermediation by the Firm: 8. The firm as intermediary in the pure exchange economy 9. The firm versus free riding Part V. Market Making by the Firm: 10. The firm creates markets 11. The firm in the market for contracts 12. Conclusion.,2009,1,110,4,0,6,5,12,9,14,19,6,7,14
6bbc9c84d4671d15e32ded5e98cc2d69329a7afc,"Consider a group consisting of S members facing a common budget constraint p'xi=1: any demand vector belonging to the budget set can be (privately or publicly) consumed by the members. Although the intragroup decision process is not known, it is assumed to generate Pareto-efficient outcomes; neither individual consumptions nor intragroup transfers are observable. The paper analyzes when, to what extent, and under which conditions it is possible to recover the underlying structure-individual preferences and the decision process-from the group's aggregate behavior. We show that the general version of the model is not identified. However, a simple exclusion assumption (whereby each member does not consume at least one good) is sufficient to guarantee generic identifiability of the welfare-relevant structural concepts. Copyright 2009 The Econometric Society.",2009,40,118,5,6,10,13,13,4,8,8,9,11,8
5eb7a931be5d30bb3207233a6b6fc1e5d8d35dfd,,2009,0,78,9,3,6,7,9,3,5,8,12,11,9
32ef38c321142ea1975e5a2c59acd0bb4b516a6b,"We develop a framework based on microeconomic theory from which the ideal gas like market models can be addressed. A kinetic exchange model based on that framework is proposed and its distributional features have been studied by considering its moments. Next, we derive the moments of the CC model (Eur. Phys. J. B 17 (2000) 167) as well. Some precise solutions are obtained which conform with the solutions obtained earlier. Finally, an output market is introduced with global price determination in the model with some necessary modifications.",2009,19,58,5,1,4,7,8,7,4,2,4,9,3
a8e598a00d6c91fe41e8db9dd40c19171933f8be,"Over the last thirty years, a new paradigm in banking theory has overturned economists' traditional vision of the banking sector. The asymmetric information model, extremely powerful in many areas of economic theory, has proven useful in banking theory both for explaining the role of banks in the economy and for pointing out structural weaknesses in the banking sector that may justify government intervention. In the past, banking courses in most doctoral programs in economics, business, or finance focused either on management or monetary issues and their macroeconomic consequences; a microeconomic theory of banking did not exist because the Arrow-Debreu general equilibrium model of complete contingent markets (the standard reference at the time) was unable to explain the role of banks in the economy. This text provides students with a guide to the microeconomic theory of banking that has emerged since then, examining the main issues and offering the necessary tools for understanding how they have been modeled. This second edition covers the recent dramatic developments in academic research on the microeconomics of banking, with a focus on four important topics: the theory of two-sided markets and its implications for the payment card industry; ""non-price competition"" and its effect on the competition-stability tradeoff and the entry of new banks; the transmission of monetary policy and the effect on the functioning of the credit market of capital requirements for banks; and the theoretical foundations of banking regulation, which have been clarified, although recent developments in risk modeling have not yet led to a significant parallel development of economic modeling.",2008,0,228,12,2,10,25,21,23,47,35,16,15,19
a079086b15e10e3fe1b3b1aaadec897590b5cc6c,"In this paper we focus on education as a private decision to invest in ""human capital"" and the estimation of the rate of return to that private investment. While the literature is replete with studies that estimate the rate of return using regression methods where the estimated return is obtained as the coefficient on a years of education variable in a log wage equation that contains controls for work experience and other individual characteristics, the issue is surrounded with difficulties. We outline the theoretical arguments underpinning the empirical developments and show that the evidence on private returns to the individual is compelling. Despite some of these issues surrounding the estimation of the return to schooling, our evidence, based on estimates from a variety of datasets and specifications, is that there is an unambiguously positive effect on the earnings of an individual from participation in education. Moreover, the size of the effect seems large relative to the returns on other investments. Copyright Blackwell Publishing Ltd, 2003.",2003,81,541,29,5,16,20,30,36,22,26,38,27,31
703954375c1c66c27297540214cfc855f7af2788,"Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making.",2004,78,493,20,7,39,43,48,40,49,24,44,25,30
f51e9221eceeee0b77c9692a6951d45dfcc1b3df,"Abstract Introductory Microeconomics as offered by the University of South Africa (Unisa) is a compulsory module for a Bachelor of Commerce, a Bachelor of Accountancy or a Bachelor of Administration degree. Success or failure in Introductory Microeconomics directly impacts on the number of years students take to complete their degrees, and eventually also on the throughput subsidy to Unisa. A number of exceptional institutional rules and regulations impact on the teaching of Introductory Microeconomics at Unisa, as an open and distance learning (ODL) institution. Unlike many residential institutions, Unisa does not require Mathematics at school level for registration for Introductory Microeconomics. This article reports on research done at Unisa to determine how student success in Introductory Microeconomics is influenced by variables such as race, home language, whether the students passed mathematics at matriculation level, matriculation exemption1, gender and the passing of assignments. Although this research confirms previous research that home language and age do impact on student success, it finds that the successful passing of assignments has the greatest impact on student success. 1. Matriculation (“matric”): In South Africa, the final school exit certificate, which is awarded with or without university exemption (“endorsement”).",2009,22,13,1,0,2,0,2,0,1,0,1,5,0
63b8967bb550e9e57f6e644aba73fb5c5f5fd2b4,"Purpose – The past decade has witnessed a trend toward unbundling of enterprises that were once highly integrated in vertical forms or horizontally as conglomerates. The economic forces behind these changes simultaneously enabled collaborative relationships that replaced command‐control coordination. While such change has been widespread, the food industry serves as an example of an industry where such strategies have been incompletely pursued. This paper aims to provide a microeconomic explanation of three bases for the emergence of collaboration and network formation: transaction costs, interdependence in value creation processes, and shared resources.Design/methodology/approach – Within a setting of the food industry, a microeconomic theory of firm level choice of transactions is presented and extended to consider market level equilibrium in where persistent relationships are defined to compose an integrated economic network.Findings – The paper presents a framework for identification of the optimal pa...",2009,25,18,0,1,6,1,4,1,1,2,1,0,0
d10b699b81a62170458734ee252de720ea7c9d0b,"Section 1: INTRODUCTION. 1. Economics and Institutions: A Shift of Emphasis. Section 2: PREFERENCES, UTILITIES, DEMAND, AND UNCERTAINTY. 2. Consumers and Their Preferences. 3. Utilities--Indifference Curves. 4. Demand and Behavior in Markets. 5. Some Applications of Consumer, Demand, and Welfare Analysis. 6. Uncertainty and the Emergence of Insurance. 7. Uncertainty--Applications and Criticisms. Section 3: PRODUCTION AND COST. 8. The Discovery of Production and Its Technology. 9. Cost and Choice. 10. Cost Curves. Section 4: DECISIONS AND GAMES. 11. Game Theory and the Tools of Strategic Business Analysis. 12. Decision Making Over Time. 13. The Internal Organization of the Firm. Section 5: MARKETS. 14. Perfectly Competitive Markets: Short Run Analysis. 15. Competitive Markets in the Long Run. 16. Market Institutions and Auctions. 17. The Age of Entrepreneurship: Monopoly. 18. Natural Monopoly and the Economics of Regulation. 19. The World of Oligopoly: Preliminaries to Successful Entry. 20. Market Entry and the Emergency of Perfect Competition. Section 6: EXCHANGE AND GENERAL EQUILIBRIUM. 21. The Problem of Exchange. 22. General Equilibrium and the Origins of the Free-Market and Interventionist Ideologies. Section 7: BREAKDOWNS AND MARKET FAILURE. 23. Moral Hazard and Adverse Selection: Informational Market Failures. 24. Externalities: The Free Market--Interventionist Battle Continues. Section 8: INPUT MARKETS AND THE ORIGINS OF CLASS STRUGGLE. 25. Public Goods, the Consequences of Strategic Voting Behavior, and the Role of Government. 26. Input Markets and the Origins of Class Conflict.",2008,0,115,5,3,9,5,11,7,10,6,6,7,5
baa3fbdd1c56e68c1c9ea52dae7a36bd30d97d47,"Preface. I. AN INTRODUCTION TO MICROECONOMICS. 1. Microeconomics: A Working Methodology. Appendix A1: Model Building. II. INDIVIDUAL CHOICE. 2. A Theory of Preferences. 3. Demand Theory. Appendix 3A: Composite Commodities. 4. More Demand Theory. 5. Intertemporal Decision Making and Capital Values. III. PRODUCTION AND COST. 6. Production and Cost: One Variable Input. 7. Production and Cost: Many Variable Inputs. IV. MARKETS FOR GOODS. 8. The Theory of Perfect Competition. 9. Applications of the Competitive Model. 10. Monopoly. V. RESOURCE MARKETS AND GENERAL EQUILIBRIUM. 11. Input Markets and the Allocation of Resources. 12. The Distribution of Income. 13. Competitive General Equilibrium. Appendix 13A: Efficiency. VI. IMPERFECT COMPETITION. 14. Price Discrimination and Monopoly Practices. 15. Game Theory 16. Oligopoly. VII. UNCERTAINTY AND ASYMMETRIC INFORMATION. 17. Choice Making Under Uncertainty. 18. Asymmetric Information, the Rules of the Game, and Externalities. 19. The Theory of the Firm. 20. Asymmetric Information and Market Behaviour. Answers to Problems. Glossary. Index.",2009,0,8,0,0,0,2,1,0,1,0,0,1,2
191b7c32f89ab82d3b7011f7d724e0ec911874b0,,2008,14,66,4,0,4,2,4,5,8,7,4,9,7
fe156eff70ae341eba64589a1d9311c7b9debd53,"In this relatively short survey, we present the core elements of the microeconomic analysis of insurance markets at a level suitable for senior undergraduate and graduate economics students. The aim of this analysis is to understand how insurance markets work, what their fundamental economic functions are, and how efficiently they may be expected to carry these out.",2008,110,59,2,1,1,8,3,6,6,5,5,6,3
1ac2f395f072d5fc4a23ff89637c77cab7d4df36,"The proliferation of economics courses offered partly or completely online (Arnold Katz and William E. Becker, 1999) raises important questions about the effects of the new technologies on student learning. Do students enrolled in online courses learn more or less than students taught face-to-face? Can we identify any student characteristics, such as gender, race, ACT scores, or grade averages, that are associated with better outcomes in one technology or another? How would the online (or face-to-face) students fare if they had taken the course using the alternative technology? This paper addresses these questions using student data from our Principles of Microeconomics courses at Michigan State University.",2002,4,372,27,4,7,13,15,18,19,24,21,26,25
8939fd3f868aa844bc99d37e03cf50a8690f7327,"Economic geography during an era of global competition involves a paradox. It is widely recognized that changes in technology and competition have diminished many of the traditional roles of location. Yet clusters, or geographic concentrations of interconnected companies, are a striking feature of virtually every national, regional, state, and even metropolitan economy, especially in more advanced nations. The prevalence of clusters reveals important insights about the microeconomics of competition and the role of location in competitive advantage. Even as old reasons for clustering have diminished in importance with globalization, new influences of clusters on competition have taken on growing importance in an increasingly complex, knowledge-based, and dynamic economy. Clusters represent a new way of thinking about national, state, and local economies, and they necessitate new roles for companies, government, and other institutions in enhancing competitiveness.",2000,38,4057,281,10,19,39,69,81,84,89,136,166,184
d02d2eb57454778faba3c4d7eebaa95592aade84,"This book brings together in one place the work of one of our most respected economic theorists, on a field which he has played a large part in originating: the New Institutional Economics. Transaction cost economics, which studies the governance of contractual relations, is the branch of the New Institutional Economics with which Oliver Williamson is especially associated. Transaction cost economics takes issue with one of the fundamental building blocks in microeconomics: the theory of the firm. Whereas orthodox economics describes the firm in technological terms, as a production function, transaction cost economics describes the firm in organizational terms, as a governance structure. Alternative feasible forms of organization--firms, markets, hybrids, bureaus--are examined comparatively. The analytical action resides in the details of transactions and the mechanisms of governance. Transaction cost economics has had a pervasive influence on current economic thought about how and why institutions function as they do, and it has become a practical framework for research in organizations by representatives of a variety of disciplines. Through a transaction cost analysis, The Mechanisms of Governance shows how and why simple contracts give way to complex contracts and internal organization as the hazards of contracting build up. That complicates the study of economic organization, but a richer and more relevant theory of organization is the result. Many testable implications and lessons for public policy accrue to this framework. Applications of both kinds are numerous and growing. Written by one of the leading economic theorists of our time, The Mechanisms of Governance is sure to be an important work for years to come. It will be of interest to scholars and students of economics, organization, management, and law.",1997,52,4612,368,27,54,92,93,120,158,202,180,223,219
eb4b8a5afb3ea7022fdae1c4e553fe19654fb0af,"Microeconomics develops core microeconomic principles to a high level using a clear and carefully constructed learning framework. The book will give readers a solid foundation in microeconomic analysis, using mathematical techniques where appropriate, and will enable them to apply these analytical techniques to a range of economic problems. It compounds the student's understanding of principles and techniques by re-using them throughout the text after each has been covered. The book is designed to assist the student's learning in every way possible and contains comprehensive sets of problems and exercises at all stages. ONLINE RESOURCE CENTRE For lecturers: worked solutions to selected exercises in the book, figures from the book, PowerPoint presentations, solutions manual and figures to accompany the solutions manual",2007,177,49,2,1,1,1,1,7,8,6,4,5,5
648efe361ab23991f315dd95b5387514569baeb2,,2008,0,23,2,4,3,1,1,0,3,1,1,2,1
caaaf486a8f7c2618627f206e3ea8e6651cad068,"The 2008 crash has left all the established economic doctrines - equilibrium models, real business cycles, disequilibria models - in disarray. Part of the problem is due to Smith’s ""veil of ignorance"": individuals unknowingly pursue society’s interest and, as a result, have no clue as to the macroeconomic effects of their actions: witness the Keynes and Leontief multipliers, the concept of value added, fiat money, Engel’s law and technical progress, to name but a few of the macrofoundations of microeconomics. A good viewpoint to take bearings anew lies in comparing the post-Great Depression institutions with those emerging from Thatcher and Reagan’s economic policies: deregulation, exogenous vs. endoge- nous money, shadow banking vs. Volcker’s Rule. Very simply, the banks, whose lending determined deposits after Roosevelt, and were a public service became private enterprises whose deposits determine lending. These underlay the great moderation preceding 2006, and the subsequent crash.",1986,1,3057,422,0,1,0,0,0,0,0,2,0,0
4e77fd7c955f8fd70d9e1c6cb670fa1e38ad7157,"Part 1 Rise of science-related technology: introduction evolutionary theory in economics and technical change process innovations materials innovations product and system innovation paradigm change. Part 2 Innovations and the firms: the microeconomics of innovation success and failure, the role of marketing and user-producer networks innovation, size of firm, economies of scale and scope uncertainty, project evaluation and finance of innovation, management strategy and theory of the firm. Part 3 Macroeconomics of innovation - science, technology and economic growth globalization and multinational corporations underdevelopment and catching up. Part 4 Innovation and public policies: market failure and aspects of public support for innovation technical change, employment and skills environmental issues technological assessment. Appendix: measurement and definitions.",1975,0,4611,103,1,2,5,7,4,18,10,13,4,21
d205b8690cc46bcb930a51cb64cffa0ecead7bc0,"With the beginning of the new millennium it has become more and more apparent that education and human capital constitute a key element of modern economies. Despite the important role of human capital in modern societies, there are still many unknowns about the process of educational production as well as individual and collective decisions concerning how much and what kind of education to obtain. This literature review aims at providing a better understanding of the process of human capital formation and educational attainment. Although human capital plays an important role in both microeconomics and macroeconomics, we focus on the former branch of literature in order to analyze the individual incentives to acquire skills. This review is divided into six parts each of them representing an important stream of human capital literature. First, we introduce the basic concept of human capital that models individuals as investing in skills in response to the expected returns to education. After this, we investigate the different implications of investments in general and specific human capital and then provide an overview of various empirical studies measuring the rate of return to education. Because educational attainment may also be affected by other factors such as school characteristics or family background, we review the literature on educational production functions and discuss the significance of potential inputs into the process of educational production. Subsequently, we refer to models of human capital accumulation over the life-cycle that manage to replicate the empirical life-cycle patterns with respect to the age-earnings profile of individuals. Finally, we analyze the effects of taxation and education subsidies on the formation of human capital.",2007,171,37,2,0,0,0,1,3,2,3,6,5,4
315a7b0d6c6080fbbf179f6dccc99a6af2447180,"When should government intervene in market activity and when is it best to let market forces take their natural course? How does the existing empirical evidence about government performance guide our answers to these questions? In this clear, concise book, Clifford Winston offers his innovative analysis --shaped by thirty years of evidence --to assess the efficacy of government interventions. Markets fail when it is possible to make one person better off without making someone else worse off, thus indicating inefficiency. Governments fail when an intervention is unwarranted because markets are performing well or when the intervention fails to correct a market problem efficiently. Winston concludes from existing research that the cost of government failure may actually be considerably greater than the cost of market failure: ""My search of the evidence is not limited to policy failures. I will report success stories, but few of them emerged from my search."" The prevalence of market failure is due to a lack of conviction in favor of markets, the inflexibility of intervening government agencies, and political forces that enable certain interest groups to benefit at the expense of society as a whole. Winston suggests that government policy can be improved by making greater use of market-oriented solutions that have already produced benefits in certain situations.",2007,118,120,10,1,14,4,6,10,13,8,10,14,7
464c48f2dda4942e8f998707c372aba90db30033,"1. Introduction 2. Supply and Demand 3. A Consumer's Constrained Choice 4. Demand 5. Consumer Welfare and Policy Analysis 6. Firms and Production 7. Costs 8. Competitive Firms and Markets 9. Properties and Applications of the Competitive Model 10. General Equilibrium and Economic Welfare 11. Monopoly 12. Pricing and Advertising 13. Oligopoly and Monopolistic Competition 14. Game Theory 15. Factor Markets 16. Uncertainty 17. Externalities, Open Access, and Public Goods 18. Asymmetric Information 19. Contracts and Moral Hazard",2007,0,114,13,0,3,4,6,11,13,8,15,7,10
b5f18ffb038093c199cf2ccc7ae542bb00fa53ab,"Tropical forests may contribute to the well-being of local people by providing a form of “natural insurance.” We draw on microeconomic theory to conceptualize a model relating agricultural risks to collection of non-timber forest products. Forest collection trips are positively correlated with both agricultural shocks and expected agricultural risks in an event-count model of survey data from the Brazilian Amazon. This suggests that households rely on forests to mitigate agricultural risk. Forest product collection may be less important to households with other consumption-smoothing options, but its importance is not restricted to the poorest households. (JEL Q23)",2001,65,355,15,0,1,7,16,23,13,8,10,17,32
deafb4a1abac1d762cf7e6d18d979d3271852bfd,"The resource-based view can be positioned relative to at least three theoretical traditions: SCP-based theories of industry determinants of firm performance, neo-classical microeconomics, and evolutionary economics. In the 1991 article, only the first of these ways of positioning the resourcebased view is explored. This article briefly discusses some of the implications of positioning the resource-based view relative to these other two literatures; it also discusses some of the empirical implications of each of these different resource-based theories.",2001,31,2472,176,2,10,28,39,36,56,63,85,94,109
20ce660fa8ef6f027c17a2d0fda0b92e4c5bfa75,"Abstract: Interest in using classroom experiments to teach economics is increasing whereas empirical evidence on how experiments affect learning is limited and mixed. The author used a pretest-posttest control-group design to test whether classroom experiments and grade incentives that reward performance in experiments affect learning of introductory microeconomics. The author measured the partial effects of experiments independently of instructor quality and teaching methods using Test of Understanding in College Economics scores. Experiments without incentives are associated with higher posttest scores and greater improvement over pretest scores, but grade incentives may offset benefits of experiments. Controlling for student aptitude and other characteristics, limiting influence of potential outliers, or adjusting for potential selection bias from incomplete observation of test scores does not alter the conclusion that experiments increase learning whereas grade incentives do not.",2006,37,149,12,1,5,5,6,9,18,9,8,11,17
aace52e1ecc601b14b117e1a17fd081c1d468df0,,2007,0,45,2,1,4,2,5,0,2,2,2,4,2
b9f3670d36d851e3d3083c70cdd49f0b18570850,"In this article it is claimed that in order to obtain the right composition, structure, and functionality for a new product, one needs to anticipate what the demand of the product will be and take into account all costs involved (associated manufacturing and supply chain). To obtain the demand, one needs a pricing model, which in turn relies on consumer preferences that are connected to the product composition, structure, and functionality. Thus, a model, including the varying characteristics of the product, the manufacturing capacity and site, the supply chain and ultimately, the markets, is proposed. I use a very simple case of insect repellent to illustrate how the best repellent, identified as the one that consumers will prefer the most, is not the most profitable one and how one can obtain the insect repellent composition that maximizes profit. © 2007 American Institute of Chemical Engineers AIChE J, 2007",2007,69,56,1,0,5,4,3,2,4,5,4,2,5
3aeeca365fa1f59fa7a491fdd1f10eadfa535eca,"Microeconomics' notions of “market supply” and “market demand” do not exist in real-world markets. Its models give a central place to equilibria, implying that they are predictions. It distracts from more essential aspects of economic behavior and exchange and encourages inventing absurd tales, especially concerning production. We should consider society as it is organized, with different social groups, norms, and customs, and then concentrate on decision making and choice.",2008,10,11,0,0,0,1,0,1,1,1,2,0,2
ad7fa90b2b1994f4bd7ced630638656fdf95fbab,"This paper uses OLS regression analysis to examine the effect of student characteristics on performance in Introductory Microeconomics at five South African universities. No consistent race-effects were found, but Indian students performed significantly worse than Whites at historically-White universities. Male students outperformed females in general. Older students did better at the historically-White institutions only. At one university, Black students who speak English as their home language outperformed those who are non-English speakers. Students who devoted more time to study outside formal classes did better in general. Greater verbal and mathematical ability had large and significant positive effects on student achievement. Copyright (c) 2006 The Author. Journal compilation (c) 2006 Economic Society of South Africa.",2006,36,65,11,1,2,2,6,6,7,10,2,5,5
2ac66da70aa7b99760fc221f018dff28d6ba3952,"Trying to achieve higher usage efficiency for spectrum has been on the research agenda for some time now. More efficient transmission technologies are being developed, but they alone will not solve the problem of spatially and temporally underused spectrum and radio resources. Mechanisms to optimize spectrum access over space and time are required. This paper describes schemes, based on multiple agents that collaborate to find more efficient allocation patterns in a defined coverage area. Leveraging on microeconomics inspired mechanisms, the paper describes and analyses schemes based on collaborating 'agents' (hat can be either whole operator, or BS or end user terminal) that negotiate with each other to find the most optimized allocation pattern for a given area and allocation duration. The optimization strategies investigated include both bargaining as well as auction based mechanisms. Both allow the negotiation of spectrum and radio resources, based on market driven incentives. The auction types investigated support dynamic allocations on different timescales, ranging from short to medium and long term allocation scenarios. While auctions are discussed to be used for the longer term allocations, a MAC based rental protocol is evaluated for shorter term allocations when operated either at the BS or end user terminal level. Finally, the paper discusses how the MAC based rental protocol between BSs can be implemented.",2007,24,25,0,0,3,4,5,1,3,4,2,1,2
3ce5e7926aa01087d16937752f5e68d829e64333,"The Econometrics of Panel DataSpringer Handbook of Science and Technology IndicatorsPanel Data EconometricsThe Econometrics of Panel DataA Practitioner's Guide to Stochastic Frontier Analysis Using StataBenchmarking for Performance EvaluationEssays on Microeconomics and Industrial OrganisationHealth System EfficiencyInternational Journal of Production EconomicsEconometric Analysis of Model Selection and Model TestingInternational Applications of Productivity and Efficiency AnalysisAdvanced Robust and Nonparametric Methods in Efficiency AnalysisEconometrics and the Philosophy of EconomicsThe Measurement of Productive EfficiencyMeasuring Efficiency in Health CareFinancial, Macro and Micro Econometrics Using REconometric Analysis of Cross Section and Panel DataApplied EconometricsProductivity and Efficiency AnalysisEconometric Model SelectionProductivity and Efficiency AnalysisStochastic Frontier AnalysisThe Oxford Handbook of Health EconomicsThe Measurement of Productive Efficiency and Productivity GrowthNew Directions in Productivity Measurement and Efficiency AnalysisA Primer on Efficiency Measurement for Utilities and Transport RegulatorsPanel Data EconometricsProduction and Efficiency Analysis with RApplications of Modern Production TheoryThe Measurement of Productive EfficiencyNonparametric Econometric Methods and ApplicationAn Introduction to Efficiency and Productivity AnalysisHealth, the Medical Profession, and RegulationThe Analysis of Household SurveysData Envelopment AnalysisProgramming Collective IntelligenceEfficiency AnalysisProductivity and Efficiency AnalysisMeasurement of Productivity and EfficiencyProduction Frontiers",2008,241,1097,88,34,55,56,65,77,62,84,91,90,108
b8074c1ebfffe3fc1ef166497a28846f63d993e7,"This volume presents a collection of studies on the dynamics of income inequality based on micro data. Using a simple but powerful empirical methodology, the authors analyze the roles of prices, occupational choice, and educational choice in accounting for household income and its contribution to inequality. It casts doubt on the grand theories of growth and income inequality that have dominated discussions in development economics. It paves the way for a full-blown, micro-based general equilibrium theory of income determination and income inequality.",2004,118,270,10,4,11,18,14,17,25,18,21,20,14
8217443b8b4c075e60b322e39e4cd77b010c6c09,This volume proposes evolutionary microeconomics as a synthesis of the collective schools of heterodox economic thought with complex systems theory and graph theory. The text charts a research programme for evolutionary economics that encompasses various theories.,2001,0,323,15,5,6,26,26,19,22,25,20,19,21
7c646037d51fc101aba12e74c02cc1cded26c0d1,,1997,0,411,35,0,4,5,8,14,24,24,24,17,18
3c80d6d5eb6fc6ce5203900d0efff719fce95f0d,Microeconomic principles courses focus on perfectly competitive markets far more than other market structures. The authors examine five possible reasons for this but find none of them sufficiently compelling. They conclude that textbook authors should place more emphasis on how economists select appropriate models and test models' predictions against the empirical evidence. This requires a more even treatment of market structures and less emphasis on perfect competition. Greater emphasis on imperfect competition would also allow textbook authors to address the neglected topic of dynamic efficiency.,2007,41,29,1,2,0,3,3,5,0,2,2,2,2
af69f1afbdceae76ba2dc5ccf828fbbcd32ec2ad,Contents: Preface Preface to Walrasian Microeconomics 1. Introduction 2. The Theory of Demand: Utility Maximization 3. Topics in Demand Theory 4. Production and Cost 5. Models of the Firm 6. Markets in Isolation 7. Interacting Markets 8. The Fixed-Factor-Supply Economy 9. Dynamics and Equilibrium 10. Methodological Individualism and the Theory of Price Determination 11. Imperfect Competition 12. Economic Welfare 13. Capital 14. The Grand View 15. Some Alternative Assumptions and Methods of Analysis Appendices Index,2006,0,29,1,0,1,3,0,2,1,2,4,4,5
03c66b9affed9acdb378ea9bf675f4ee0dcf7769,"Despite considerable research on customer retention and word-of-mouth referrals, it has always been difficult quantifying their contributions to the bottom line. Using a metric known as ?net promoter score,? the author believes firms can now measure the dollar value of customers based on satisfaction levels.

The author administered a survey designed to assess customer relationships to thousands of customers in six industries. He determined that customers tend to cluster into one of three categories: promoters, passives and detractors. Promoters represent more than 80% of the positive referrals a company receives, while detractors represent more than 80% of the negative word-of-mouth. NPS is determined by subtracting the percentage of detractors from the percentage of promoters. Using this data, a firm can quantify the value of a customer by tracking five categories: retention rate, profit margins, spending, cost efficiencies and word-of-mouth.

The firm can then use NPS to make strategic decisions by targeting its efforts to leverage the most value for its customer service dollar. For instance, American Express targets its promoters with premium credit cards in an effort to increase profitability, and GE sends cross-functional teams to its detractors in order to prevent the spread of negative word-of-mouth.",2006,3,104,6,0,6,11,6,8,9,7,7,6,5
a8f495264b07f8d7f701ca659c5c4a643d78bf5c,"from them. The Adam Smith Address: Location, Clusters, and the New Microeconomics",1998,8,316,28,0,1,6,6,9,9,8,14,11,6
c5d706479b8b5f88bc4dfb884ae781fbfe73df85,"Previous studies have documented a gender gap in the study of economics in Canada, the UK, and the US. One important factor may be women's low expectations about their ability to succeed in economics courses. Women in our sample expect to do less well than men in an introductory microeconomics course, even after controlling for variables relating to family background, academic experience, and mathematics experience. These expectations are partly self-fulfilling, since expected grades have an important and positive effect on class performance. We also find that having taken an economics course in secondary school actually has a negative effect on performance. We observe this negative effect for women and men, but it is more pronounced for women. When we control for both expectations and secondary-school experience with economics, the independent effect of gender is small and insignificant.",2005,54,65,9,0,1,3,8,2,4,7,4,6,4
acdb9071cb24bce944909b4334ac82ccf16e6548,"Using Microsoft Excel, the market leading spreadsheet package, this book combines theory with modelling aspects and spreadsheet analysis. Microeconomics Using Excel provides students with the tools with which to better understand microeconomic analysis.A new textbook, it focuses on solving microeconomic problems by integrating economic theory, policy analysis and spreadsheet modelling. This unique approach facilitates a more comprehensive understanding of the link between theory and problem solving.It is divided into four core parts:analysis of price policiesanalysis of structural policiesmulti-market modelsbudget policy and priority settings. The theory behind each problem is explained and each model is solved using excel. Each model is also available on the accompanying CD and can be used as a prototype for analysis and specific needs.Microeconomics using Excel will be of great interest to students studying economics as well as to professionals in economic and policy analysis.",2007,39,16,1,1,0,1,1,4,1,3,1,0,0
f570fe56548b16fb69518e0dc0da6b493d7c7c59,"Few other economists have been read and cited as often as R.H. Coase has been, even though, as he admits, ""most economists have a different way of looking at economic problems and do not share my conception of the nature of our subject."" Coase's particular interest has been that part of economic theory that deals with firms, industries, and markets—what is known as price theory or microeconomics. He has always urged his fellow economists to examine the foundations on which their theory exists, and this volume collects some of his classic articles probing those very foundations. ""The Nature of the Firm"" (1937) introduced the then-revolutionary concept of transaction costs into economic theory. ""The Problem of Social Cost"" (1960) further developed this concept, emphasizing the effect of the law on the working of the economic system. The remaining papers and new introductory essay clarify and extend Coarse's arguments and address his critics. ""These essays bear rereading. Coase's careful attention to actual institutions not only offers deep insight into economics but also provides the best argument for Coase's methodological position. The clarity of the exposition and the elegance of the style also make them a pleasure to read and a model worthy of emulation.""—Lewis A. Kornhauser, Journal of Economic Literature Ronald H. Coase was awarded the Nobel Prize in Economic Science in 1991.",1990,0,1984,90,10,6,10,13,27,31,44,26,44,53
ae0eb7b44ac3cbca7dd1f8175b44fb2ce62c974a,"Abstract The relation between Thermodynamics and Economics is a paramount issue in Ecological Economics. Two different levels can be distinguished when discussing it: formal and substantive. At the formal level, a mathematical framework is used to describe both thermodynamic and economic systems. At the substantive level, thermodynamic laws are applied to economic processes. In Ecological Economics, there is a widespread claim that neoclassical economics has the same mathematical formulation as classical mechanics and is therefore fundamentally flawed because: 1) utility does not obey a conservation law as energy does; 2) an equilibrium theory cannot be used to study irreversible processes. Here, we show that neoclassical economics is based on a wrong formulation of classical mechanics, being in fact formally analogous to equilibrium thermodynamics. The similarity between both formalisms, namely that they are both cases of constrained optimisation, is easily perceived when thermodynamics is looked upon using the Tisza–Callen axiomatisation. In this paper, we take the formal analogy between equilibrium thermodynamics and economic systems far enough to answer the formal criticisms, proving that the formalism of neoclassical economics has irreversibility embedded in it. However, the formal similarity between equilibrium thermodynamics and neoclassical microeconomics does not mean that economic models are in accordance with mass, energy and entropy balance equations. In fact, neoclassical theory suffers from flaws in the substantive integration with thermodynamic laws as has already been fully demonstrated by valuable work done by ecological economists in this field.",2006,44,46,1,2,3,2,10,2,0,2,1,0,2
7a4c5e838aef0a82de5113ebe96ea531b566237d,"Part 1 Introduction: thinking like an economist supply and demand. Part 2 The theory of consumer behaviour - rational consumer choices individual and market demand applications of rational choice and demand theories the economics of information and choice under uncertainty (supplementary) explaining tastes - the importance of altruism and other non-egoistic behaviour (supplementary) cognitive limitations and consumer behaviour (supplementary). Part 3 The theory of the firm and market structure: production costs perfect competition mono[poly oligopoly and monopolistic competition. Part 4 Factor markets: labour capital (supplementary). Part 5 General equilibrium and welfare: general equilibrium and market efficiency externalities, property rights, and the Case theorem government (supplementary). Appendices: the utility function approach to the consumer budgeting problem additional topics in demand theory additional topics in supply theory search theory and the winner's curse mathematical extensions of production theory additional extensions of the theory of costs additional models of monopolistic competition a more detailed look at exhaustible resource allocation.",1991,0,430,22,0,1,1,2,6,8,4,6,8,10
a50268a11b5d6e1c21a41a0b4ec3548b5a3627dd,"Abstract: The author presents new evidence on the effects of attendance on academic performance. He used a large panel data set for introductory microeconomics students to explicitly take into account the effect of unobservable factors correlated with attendance, such as ability, effort, and motivation. He found that neither proxy variables nor instrumental variables provide a solution to the omitted variable bias. Panel estimators indicate that attendance has a smaller but significant impact on performance. Lecture and classes have a similar effect on performance individually, although their impact cannot be identified separately. Overall, the results indicate that, after controlling for unobservable student characteristics, attendance has a statistically significant and quantitatively relevant effect on student learning.",2006,45,178,15,2,2,8,6,14,14,14,10,14,9
d9ca4a5d05a398de65d60fbde9cfb3121e2e37ff,"The conventional utility-based approach to microeconomics is now nearly a century old and although frequently criticised, it has yet to be replaced. On the Reappraisal of Microeconomics offers an alternative approach that overcomes most of the objections to orthodox theory, whilst offering some unique additional advantages. 

The authors present a new approach to non-equilibrium microeconomics that applies equally to production, trade and consumption, and that is also consistent with the laws of thermodynamics. This new theory is not limited to equilibrium or near-equilibrium conditions. The core of the theory is proof that, for each agent (firm or individual), there exists an unique function of goods and money (denoted Z) that can be interpreted as subjective wealth for an individual or the owners of a firm. Exchanges may occur only when both parties enjoy an increase in subjective wealth as a consequence. On average, this Z-function will increase over time if, and only if, the agent obeys a simple decision rule in all economic transactions: namely to 'avoid avoidable losses'understood, or AAL, it being understood that some losses are unavoidable. Dynamic equations describing growth (or decline) can be derived simply by calculating time derivatives of a wealth function, without the need for constrained maximization of an integral of utility (or some surrogate) BM_1_over time. The Z-function also has a number of other interesting properties that can be used for multi-agent and multi-sectoral simulation models to explore a variety of economic situations that cannot be addressed so easily using conventional methods.",2005,0,27,0,1,0,2,0,0,4,4,3,6,0
5455fe83185e45030345dfcc6b6c7f32772e5d6b,"This paper presents new evidence on the effects of attendance on academic performance. We exploit a large panel data set for Introductory Microeconomics students to explicitly take into account the effect of unobservable factors correlated with attendance, such as ability, effort and motivation. We find that neither proxy variables nor instrumental variables provide a viable solution to the omitted variable bias. Panel estimators indicate that attendance has a positive and significant impact on performance. Lecture and classes have a similar effect on performance individually, although their impact cannot be identified separately. Overall, the results indicate that, after controlling for unobservable student characteristics, teaching has an important independent effect on learning.",2004,31,78,4,0,0,0,2,4,6,7,7,3,8
a194bb9c7ea9b48fc9ed1cc2602e7152829f0f7f,"Nutritional Physiology: Introduction, Definition, and Classification of Mineral Nutrients. Ion Uptake Mechanisms of Individual Cells and Roots: Short Distance Transport. Long-Distance Transport in the Xylem and Phloem and its Regulation. Uptake and Release of Mineral Elements by Leaves and Other Aerial Plant Parts. Yield and the Source-Sink Relationships. Mineral Nutrition and Yield Response. Nitrogen Fixation. Functions of Mineral Nutrients: Macronutrients. Function of Mineral Nutrients: Micronutrients. Beneficial Mineral Elements. Relationship between Mineral Nutrition and Plant Diseases and Pests. Diagnosis of Deficiency and Toxicity of Mineral Nutrients. Plant-Soil Relationships: Nutrient Availability in Soils. Effect of Internal and External Factors on Root Growth and Development. The Soil-Root Interface (Rhizosphere) in Relation to Mineral Nutrition. Adaptation of Plants to Adverse Chemical Soil Conditions. References. Subject Index.",1988,0,19001,1911,0,0,0,0,0,0,0,0,0,0
15f824dd4b569f6ec10f6cbed9dc9131649f7a2a,"We have used the Escherichia coli beta‐glucuronidase gene (GUS) as a gene fusion marker for analysis of gene expression in transformed plants. Higher plants tested lack intrinsic beta‐glucuronidase activity, thus enhancing the sensitivity with which measurements can be made. We have constructed gene fusions using the cauliflower mosaic virus (CaMV) 35S promoter or the promoter from a gene encoding the small subunit of ribulose bisphosphate carboxylase (rbcS) to direct the expression of beta‐glucuronidase in transformed plants. Expression of GUS can be measured accurately using fluorometric assays of very small amounts of transformed plant tissue. Plants expressing GUS are normal, healthy and fertile. GUS is very stable, and tissue extracts continue to show high levels of GUS activity after prolonged storage. Histochemical analysis has been used to demonstrate the localization of gene activity in cells and tissues of transformed plants.",1987,4,9184,727,1,17,64,119,128,148,181,176,213,182
77d574bda2cc7edd4129da81a5f1aeff0c45572a,Chapter 1 The Biosphere Chapter 2 The Anthroposphere Introduction Air Pollution Water Pollution Soil Plants Chapter 3 Soils and Soil Processes Introduction Weathering Processes Pedogenic Processes Chapter 4 Soil Constituents Introduction Trace Elements Minerals Organic Matter Organisms in Soils Chapter 5 Trace Elements in Plants Introduction Absorption Translocation Availability Essentiality and Deficiency Toxicity and Tolerance Speciation Interaction Chapter 6 Elements of Group 1 (Previously Group Ia) Introduction Lithium Rubidium Cesium Chapter 7 Elements of Group 2 (Previously Group IIa) Beryllium Strontium Barium Radium Chapter 8 Elements of Group 3 (Previously Group IIIb) Scandium Yttrium Lanthanides Actinides Chapter 9 Elements of Group 4 (Previously Group IVb) Titanium Zirconium Hafnium Chapter 10 Elements of Group 5 (Previously Group Vb) Vanadium Niobium Tantalum Chapter 11 Elements of Group 6 (Previously Group VIb) Chromium Molybdenum Tungsten Chapter 12 Elements of Group 7 (Previously Group VIIb) Manganese Technetium Rhenium Chapter 13 Elements of Group 8 (Previously Part of Group VIII) Iron Ruthenium Osmium Chapter 14 Elements of Group 9 (Previously Part of Group VIII) Cobalt Rhodium Iridium Chapter 15 Elements of Group 10 (Previously Part of Group VIII) Nickel Palladium Platinum Chapter 16 Elements of Group 11 (Previously Group Ib) Copper Silver Gold Chapter 17 Trace Elements of Group 12 (Previously of Group IIb) Zinc Cadmium Mercury Chapter 18 Elements of Group 13 (Previously Group IIIa) Boron Aluminum Gallium Indium Thallium Chapter 19 Elements of Group I4 (Previously Group IVa) Silicon Germanium Tin Lead Chapter 20 Elements of Group 15 (Previously Group Va) Arsenic Antimony Bismuth Chapter 21 Elements of Group 16 (Previously Group VIa) Selenium Tellurium Polonium Chapter 22 Elements of Group 17 (Previously Group VIIa) Fluorine Chlorine Bromine Iodine,1984,813,7166,397,0,2,10,14,13,11,19,19,21,21
544a324945198b486c58cb2ffe800fc56db07975,"Reactive oxygen species (ROS) control many different processes in plants. However, being toxic molecules, they are also capable of injuring cells. How this conflict is resolved in plants is largely unknown. Nonetheless, it is clear that the steady-state level of ROS in cells needs to be tightly regulated. In Arabidopsis, a network of at least 152 genes is involved in managing the level of ROS. This network is highly dynamic and redundant, and encodes ROS-scavenging and ROS-producing proteins. Although recent studies have unraveled some of the key players in the network, many questions related to its mode of regulation, its protective roles and its modulation of signaling networks that control growth, development and stress response remain unanswered.",2004,81,4392,303,1,32,113,118,150,190,197,225,331,311
4e98194e61ce6de2f033947ec65370ce0cf1d97d,,1979,0,7583,269,42,77,106,130,149,145,188,146,186,190
abd930550b7c9301ea366382c3628494864b5f27,"Shoots, roots, and seeds of corn (Zea mays L., cv. Michigan 500), oats (Avena sativa L., cv. Au Sable), and peas (Pisum sativum L., cv. Wando) were analyzed for their superoxide dismutase content using a photochemical assay system consisting of methionine, riboflavin, and p-nitro blue tetrazolium. The enzyme is present in the shoots, roots, and seeds of the three species. On a dry weight basis, shoots contain more enzyme than roots. In seeds, the enzyme is present in both the embryo and the storage tissue. Electrophoresis indicated a total of 10 distinct forms of the enzyme. Corn contained seven of these forms and oats three. Peas contained one of the corn and two of the oat enzymes. Nine of the enzyme activities were eliminated with cyanide treatment suggesting that they may be cupro-zinc enzymes, whereas one was cyanide-resistant and may be a manganese enzyme. Some of the leaf superoxide dismutases were found primarily in mitochondria or chloroplasts. Peroxidases at high concentrations interfere with the assay. In test tube assays of crude extracts from seedlings, the interference was negligible. On gels, however, peroxidases may account for two of the 10 superoxide dismutase forms.",1977,21,4092,488,1,3,0,2,3,8,5,2,3,7
9e61a2515807d1c7cc6a81327072f4520b47978b,"Salt and drought stress signal transduction consists of ionic and osmotic homeostasis signaling pathways, detoxification (i.e., damage control and repair) response pathways, and pathways for growth regulation. The ionic aspect of salt stress is signaled via the SOS pathway where a calcium-responsive SOS3-SOS2 protein kinase complex controls the expression and activity of ion transporters such as SOS1. Osmotic stress activates several protein kinases including mitogen-activated kinases, which may mediate osmotic homeostasis and/or detoxification responses. A number of phospholipid systems are activated by osmotic stress, generating a diverse array of messenger molecules, some of which may function upstream of the osmotic stress-activated protein kinases. Abscisic acid biosynthesis is regulated by osmotic stress at multiple steps. Both ABA-dependent and -independent osmotic stress signaling first modify constitutively expressed transcription factors, leading to the expression of early response transcriptional activators, which then activate downstream stress tolerance effector genes.",2002,129,4872,231,4,53,86,120,150,149,186,210,206,294
3bca850920c736324a97fbcb8ec57a895ceb31b4,"Fifteen or more elements present in rocks and soils normally in very small amounts are essential for plant and/or animal nutrition. By the nature of their low abundance in natural uncontaminated earth materials or plants, they are known as trace elements, minor elements or micro-nutrients. Boron, copper, iron, manganese, molybdenum, silicon, vanadium and zinc are required by plants; copper, cobalt, iodine, iron, manganese, molybdenum, selenium and zinc by animals. In addition essential roles of arsenic, fluorine, nickel, silicon, tin and vanadium have in recent years been established in animal nutrition.",1980,80,3261,595,0,1,1,0,0,0,1,2,1,2
876f3e75121b2de085d850ca8d4816f17e9c9ab7,"Medicinal plants and traditional medicine in Africa , Medicinal plants and traditional medicine in Africa , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,5116,202,0,0,0,2,4,3,1,7,7,8
e69e904ec6339303464dd2ef11a3d16f160eb73e,"Responses of plants to environmental stresses , Responses of plants to environmental stresses , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1973,0,4773,187,13,16,27,30,36,29,44,57,45,46
5855944e55f5599ac352d7da356877c467a3aa92,"Physiological and ecological constraints play key roles in the evolution of plant growth patterns, especially in relation to defenses against herbivores. Phenotypic and life history theories are unified within the growth-differentiation balance (GDB) framework, forming an integrated system of theories explaining and predicting patterns of plant defense and competitive interactions in ecological and evolutionary time. Plant activity at the cellular level can be classified as growth (cell division and enlargement) of differentiation (chemical and morphological changes leading to cell maturation and specialization). The GDB hypothesis of plant defense is premised upon a physiological trade-off between growth and differentiation processes. The trade-off between growth and defense exists because secondary metabolism and structural reinforcement are physiologically constrained in dividing and enlarging cells, and because they divert resources from the production of new leaf area. Hence the dilemma of plants: They must grow fast enough to complete, yet maintain the defenses necessary to survive in the presence of pathogens and hervivores. The physiological trade-off between growth and differentiation processes interacts with herbivory and plant-plant competition to manifest itself as a genetic trade-off between growth and defense in the evolution of plant life history strategies. Evolutionary theories of plant defense are reviewed. We also extend a standard growth rate model by separating its ecological and evolutionary components,and formalizing the role of competition in the evolution of plant defense. We conclude with a conceptual model of the evolution of plant defense in which plant physioligical trade-offs interact with the abiotic environment, competition and herbivory.",1992,661,3613,211,1,10,30,19,52,51,55,71,93,84
9739f7c963191891f04d56d1138b93a6c815d141,"Over the past 100 years, the global average temperature has increased by approximately 0.6 °C and is projected to continue to rise at a rapid rate. Although species have responded to climatic changes throughout their evolutionary history, a primary concern for wild species and their ecosystems is this rapid rate of change. We gathered information on species and global warming from 143 studies for our meta-analyses. These analyses reveal a consistent temperature-related shift, or ‘fingerprint’, in species ranging from molluscs to mammals and from grasses to trees. Indeed, more than 80% of the species that show changes are shifting in the direction expected on the basis of known physiological constraints of species. Consequently, the balance of evidence from these studies strongly suggests that a significant impact of global warming is already discernible in animal and plant populations. The synergism of rapid temperature rise and other stresses, in particular habitat destruction, could easily disrupt the connectedness among species and lead to a reformulation of species communities, reflecting differential changes in species, and to numerous extirpations and possibly extinctions.",2003,46,4262,204,56,124,172,185,236,231,234,273,269,303
1a2c56c4af02697f2b21a6b9353dd367e2d236fa,"Abstract.  Much confusion exists in the English-language literature on plant invasions concerning the terms ‘naturalized’ and ‘invasive’ and their associated concepts. Several authors have used these terms in proposing schemes for conceptualizing the sequence of events from introduction to invasion, but often imprecisely, erroneously or in contradictory ways. This greatly complicates the formulation of robust generalizations in invasion ecology. 
 
Based on an extensive and critical survey of the literature we defined a minimum set of key terms related to a graphic scheme which conceptualizes the naturalization/invasion process. Introduction means that the plant (or its propagule) has been transported by humans across a major geographical barrier. Naturalization starts when abiotic and biotic barriers to survival are surmounted and when various barriers to regular reproduction are overcome. Invasion further requires that introduced plants produce reproductive offspring in areas distant from sites of introduction (approximate scales: > 100 m over  6 m/3 years for taxa spreading by roots, rhizomes, stolons or creeping stems). Taxa that can cope with the abiotic environment and biota in the general area may invade disturbed, seminatural communities. Invasion of successionally mature, undisturbed communities usually requires that the alien taxon overcomes a different category of barriers. 
 
We propose that the term ‘invasive’ should be used without any inference to environmental or economic impact. Terms like ‘pests’ and ‘weeds’ are suitable labels for the 50–80% of invaders that have harmful effects. About 10% of invasive plants that change the character, condition, form, or nature of ecosystems over substantial areas may be termed ‘transformers’.",2000,120,3242,201,3,11,17,59,95,81,111,139,137,148
de31d3468dc6086993403b00d906fd036bc01bfb,"Plants exposed to salt stress undergo changes in their environment. The ability of plants to tolerate salt is determined by multiple biochemical pathways that facilitate retention and/or acquisition of water, protect chloroplast functions, and maintain ion homeostasis. Essential pathways include those that lead to synthesis of osmotically active metabolites, specific proteins, and certain free radical scavenging enzymes that control ion and water flux and support scavenging of oxygen radicals or chaperones. The ability of plants to detoxify radicals under conditions of salt stress is probably the most critical requirement. Many salt-tolerant species accumulate methylated metabolites, which play crucial dual roles as osmoprotectants and as radical scavengers. Their synthesis is correlated with stress-induced enhancement of photorespiration. In this paper, plant responses to salinity stress are reviewed with emphasis on physiological, biochemical, and molecular mechanisms of salt tolerance. This review may help in interdisciplinary studies to assess the ecological significance of salt stress.",2005,235,3278,137,8,24,43,70,95,113,166,171,263,270
54d1abdf4487202215024c07c355b580caf676b2,"Our understanding of plant mineral nutrition comes largely from studies of herbaceous crops that evolved from ruderal species characteristic of nutri­ ent-rich disturbed sites (52). With the development of agriculture, these ancestral species were bred for greater productivity and reproductive output at high nutrient levels where there was little selective advantage in efficient nutrient use. This paper briefly reviews the nature of crop responses to nutrient stress and compares these responses to those of species that have evolved under more natural conditions, particularly in low-nutrient envi­ ronments. I draw primarily upon nutritional studies of nitrogen and phos­ phorus because these elements most commonly limit plant growth and because their role in controlling plant growth and metabolism is most clearly understood (51). Other more specific aspects of nutritional plant ecology not discussed here include ammonium/nitrate nutrition (79), cal­ cicole/calcifuge nutrition (51,88), heavy metal tolerance (4), and serpentine ecology (133).",1980,79,4080,180,0,4,15,34,27,43,57,41,65,77
5f06f0f5202c32631c81fdda2a419b0fc113bceb,"Lignocellulosic biomass has long been recognized as a potential sustainable source of mixed sugars for fermentation to biofuels and other biomaterials. Several technologies have been developed during the past 80 years that allow this conversion process to occur, and the clear objective now is to make this process cost-competitive in today's markets. Here, we consider the natural resistance of plant cell walls to microbial and enzymatic deconstruction, collectively known as “biomass recalcitrance.” It is this property of plants that is largely responsible for the high cost of lignocellulose conversion. To achieve sustainable energy production, it will be necessary to overcome the chemical and structural properties that have evolved in biomass to prevent its disassembly.",2007,34,3694,104,18,83,102,180,249,258,339,373,324,367
2ef2ae40f89de3d996c2ba407bb39204ca3dcc55,"The volatile oils of black pepper [Piper nigrum L. (Piperaceae)], clove [Syzygium aromaticum (L.) Merr. & Perry (Myrtaceae)], geranium [Pelargonium graveolens L'Herit (Geraniaceae)], nutmeg [Myristica fragrans Houtt. (Myristicaceae), oregano [Origanum vulgare ssp. hirtum (Link) Letsw. (Lamiaceae)] and thyme [Thymus vulgaris L. (Lamiaceae)] were assessed for antibacterial activity against 25 different genera of bacteria. These included animal and plant pathogens, food poisoning and spoilage bacteria. The volatile oils exhibited considerable inhibitory effects against all the organisms under test while their major components demonstrated various degrees of growth inhibition.",2000,90,3808,123,2,10,19,30,50,54,89,118,122,158
26ca41ef0ccf8700a54f1e59c69d08874ca958f9,"Indian medicinal plants / , Indian medicinal plants / , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",2005,0,4369,54,117,131,149,150,246,350,421,429,401,390
a8d4e645455a0944323882ee1ad75899546bee35,"Severe droughts have been associated with regional-scale forest mortality worldwide. Climate change is expected to exacerbate regional mortality events; however, prediction remains difficult because the physiological mechanisms underlying drought survival and mortality are poorly understood. We developed a hydraulically based theory considering carbon balance and insect resistance that allowed development and examination of hypotheses regarding survival and mortality. Multiple mechanisms may cause mortality during drought. A common mechanism for plants with isohydric regulation of water status results from avoidance of drought-induced hydraulic failure via stomatal closure, resulting in carbon starvation and a cascade of downstream effects such as reduced resistance to biotic agents. Mortality by hydraulic failure per se may occur for isohydric seedlings or trees near their maximum height. Although anisohydric plants are relatively drought-tolerant, they are predisposed to hydraulic failure because they operate with narrower hydraulic safety margins during drought. Elevated temperatures should exacerbate carbon starvation and hydraulic failure. Biotic agents may amplify and be amplified by drought-induced plant stress. Wet multidecadal climate oscillations may increase plant susceptibility to drought-induced mortality by stimulating shifts in hydraulic architecture, effectively predisposing plants to water stress. Climate warming and increased frequency of extreme events will probably cause increased regional mortality episodes. Isohydric and anisohydric water potential regulation may partition species between survival and mortality, and, as such, incorporating this hydraulic framework may be effective for modeling plant survival and mortality under future climate conditions.",2008,295,2789,175,13,48,91,130,139,222,198,242,249,258
363468e953a9d3c16cf71c3d3fa17023840ab1eb,"Transformed petunia, tobacco, and tomato plants have been produced by means of a novel leaf disk transformation-regeneration method. Surface-sterilized leaf disks were inoculated with an Agrobacterium tumefaciens strain containing a modified tumor-inducing plasmid (in which the phytohormone biosynthetic genes from transferred DNA had been deleted and replaced with a chimeric gene for kanamycin resistance) and cultured for 2 days. The leaf disks were then transferred to selective medium containing kanamycin. Shoot regeneration occurred within 2 to 4 weeks, and transformants were confirmed by their ability to form roots in medium containing kanamycin. This method for producing transformed plants combines gene transfer, plant regeneration, and effective selection for transformants into a single process and should be applicable to plant species that can be infected by Agrobacterium and regenerated from leaf explants.",1985,0,4262,83,9,45,81,87,111,118,88,104,116,92
10d1c4b96dc8af23fa1cd89984d14c9a36efc8f2,"Heat stress due to increased temperature is an agricultural problem in many areas in the world. Transitory or constantly high temperatures cause an array of morpho-anatomical, physiological and biochemical changes in plants, which affect plant growth and development and may lead to a drastic reduction in economic yield. The adverse effects of heat stress can be mitigated by developing crop plants with improved thermotolerance using various genetic approaches. For this purpose, however, a thorough understanding of physiological responses of plants to high temperature, mechanisms of heat tolerance and possible strategies for improving crop thermotolerance is imperative. Heat stress affects plant growth throughout its ontogeny, though heat-threshold level varies considerably at different developmental stages. For instance, during seed germination, high temperature may slow down or totally inhibit germination, depending on plant species and the intensity of the stress. At later stages, high temperature may adversely affect photosynthesis, respiration, water relations and membrane stability, and also modulate levels of hormones and primary and secondary metabolites. Furthermore, throughout plant ontogeny, enhanced expression of a variety of heat shock proteins, other stress-related proteins, and production of reactive oxygen species (ROS) constitute major plant responses to heat stress. In order to cope with heat stress, plants implement various mechanisms, including maintenance of membrane stability, scavenging of ROS, production of antioxidants, accumulation and adjustment of compatible solutes, induction of mitogen-activated protein kinase (MAPK) and calcium-dependent protein kinase (CDPK) cascades, and, most importantly, chaperone signaling and transcriptional activation. All these mechanisms, which are regulated at the molecular level, enable plants to thrive under heat stress. Based on a complete understanding of such mechanisms, potential genetic strategies to improve plant heat-stress tolerance include traditional and contemporary molecular breeding protocols and transgenic approaches. While there are a few examples of plants with improved heat tolerance through the use of traditional breeding protocols, the success of genetic transformation approach has been thus far limited. The latter is due to limited knowledge and availability of genes with known effects on plant heat-stress tolerance, though these may not be insurmountable in future. In addition to genetic approaches, crop heat tolerance can be enhanced by preconditioning of plants under different environmental stresses or exogenous application of osmoprotectants such as glycinebetaine and proline. Acquiring thermotolerance is an active process by which considerable amounts of plant resources are diverted to structural and functional maintenance to escape damages caused by heat stress. Although biochemical and molecular aspects of thermotolerance in plants are relatively well understood, further studies focused on phenotypic flexibility and assimilate partitioning under heat stress and factors modulating crop heat tolerance are imperative. Such studies combined with genetic approaches to identify and map genes (or QTLs) conferring thermotolerance will not only facilitate marker-assisted breeding for heat tolerance but also pave the way for cloning and characterization of underlying genetic factors which could be useful for engineering plants with improved heat tolerance.",2007,313,2568,218,0,16,31,67,101,132,179,202,213,242
2a1f60cefc8d8981a45f1cc27bec2e9e2cf7418b,,1956,0,5897,167,0,0,1,1,0,2,2,5,5,6
257c08b0a31deee145a7ec85890f11ec96eefbcc,1 Graphical modeling using L-systems.- 1.1 Rewriting systems.- 1.2 DOL-systems.- 1.3 Turtle interpretation of strings.- 1.4 Synthesis of DOL-systems.- 1.4.1 Edge rewriting.- 1.4.2 Node rewriting.- 1.4.3 Relationship between edge and node rewriting.- 1.5 Modeling in three dimensions.- 1.6 Branching structures.- 1.6.1 Axial trees.- 1.6.2 Tree OL-systems.- 1.6.3 Bracketed OL-systems.- 1.7 Stochastic L-systems.- 1.8 Context-sensitive L-systems.- 1.9 Growth functions.- 1.10 Parametric L-systems.- 1.10.1 Parametric OL-systems.- 1.10.2 Parametric 2L-systems.- 1.10.3 Turtle interpretation of parametric words.- 2 Modeling of trees.- 3 Developmental models of herbaceous plants.- 3.1 Levels of model specification.- 3.1.1 Partial L-systems.- 3.1.2 Control mechanisms in plants.- 3.1.3 Complete models.- 3.2 Branching patterns.- 3.3 Models of inflorescences.- 3.3.1 Monopodial inflorescences.- 3.3.2 Sympodial inflorescences.- 3.3.3 Polypodial inflorescences.- 3.3.4 Modified racemes.- 4 Phyllotaxis.- 4.1 The planar model.- 4.2 The cylindrical model.- 5 Models of plant organs.- 5.1 Predefined surfaces.- 5.2 Developmental surface models.- 5.3 Models of compound leaves.- 6 Animation of plant development.- 6.1 Timed DOL-systems.- 6.2 Selection of growth functions.- 6.2.1 Development of nonbranching filaments.- 6.2.2 Development of branching structures.- 7 Modeling of cellular layers.- 7.1 Map L-systems.- 7.2 Graphical interpretation of maps.- 7.3 Microsorium linguaeforme.- 7.4 Dryopteris thelypteris.- 7.5 Modeling spherical cell layers.- 7.6 Modeling 3D cellular structures.- 8 Fractal properties of plants.- 8.1 Symmetry and self-similarity.- 8.2 Plant models and iterated function systems.- Epilogue.- Appendix A Software environment for plant modeling.- A.1 A virtual laboratory in botany.- A.2 List of laboratory programs.- Appendix B About the figures.- Turtle interpretation of symbols.,1990,207,2983,229,0,15,31,28,33,41,50,41,53,47
36e2c49a23cb62bc5f47c82603373970e975b87d,"The rhizosphere encompasses the millimeters of soil surrounding a plant root where complex biological and ecological processes occur. This review describes recent advances in elucidating the role of root exudates in interactions between plant roots and other plants, microbes, and nematodes present in the rhizosphere. Evidence indicating that root exudates may take part in the signaling events that initiate the execution of these interactions is also presented. Various positive and negative plant-plant and plant-microbe interactions are highlighted and described from the molecular to the ecosystem scale. Furthermore, methodologies to address these interactions under laboratory conditions are presented.",2006,209,3163,137,9,38,63,86,108,122,169,242,244,261
90a2296556990639faec40787fe27661f1f51fbd,"One of the least understood aspects of population biology is community evolution-the evolutionary interactions found among different kinds or organisms where exchange of genetic information among the kinds is assumed to be minimal or absent. Studies of community evolution have, in general, tended to be narrow in scope and to ignore the reciprocal aspects of these interactions. Indeed, one group of organisms is all too often viewed'as a kind of physical constant. In an extreme example a parasitologist might not consider the evolutionary history and responses of hosts, while a specialist in vertebrates might assume species of vertebrate parasites to be invariate entities. This viewpoint is one factor in the general lack of progress toward the understanding of organic diversification. One approach to what we would like to call coevolution is the examination of patterns of interaction between two major groups of organisms with a close and evident ecological relationship, such as plants and herbivores. The considerable amount of information available about butterflies and their food plants make them particularly suitable for these investigations. Further, recent detailed investigations have provided a relatively firm basis for statements about the phenetic relationships of the various higher groups of Papilionoidea (Ehrlich, 1958, and unpubl.). It should, however, be remembered that we are considering the butterflies as a model. They are only one of the many groups of herbivorous organisms coevolving with plants. In this paper, we shall investigate the relationship between butterflies and their food",1964,125,3775,116,0,1,3,5,4,3,7,7,14,11
115214635784abe693708c5746526b9f61900f90,,1987,0,3885,98,8,14,13,7,10,9,23,21,21,35
a0155c2d2d69db4e2bf5870fc9be9b0adecdd9ee,"A method is described which permits measurement of sap pressure in the xylem of vascular plants. As long predicted, sap pressures during transpiration are normally negative, ranging from -4 or -5 atmospheres in a damp forest to -80 atmospheres in the desert. Mangroves and other halophytes maintain at all times a sap pressure of -35 to -60 atmospheres. Mistletoes have greater suction than their hosts, usually by 10 to 20 atmospheres. Diurnal cycles of 10 to 20 atmospheres are common. In tall conifers there is a hydrostatic pressure gradient that closely corresponds to the height and seems surprisingly little influenced by the intensity of transpiration. Sap extruded from the xylem by gas pressure on the leaves is practically pure water. At zero turgor this procedure gives a linear relation between the intracellular concentration and the tension of the xylem.",1965,14,3829,79,6,7,10,11,8,13,18,20,25,32
f5df7a0982dffb91a033e211fc5b66d6b4fc1193,,1938,0,3866,251,0,0,1,0,0,0,0,0,2,0
bf8f9de501397315843fbb8894544c48d1aa67cf,PATHWAY OF ETHYLENE BIOSyNTHESIS 156 Methionine as an Intermediate ....... 157 S-Adenosylmethionine as an Intermediate 158 I-Aminocyclopropanecarboxylic Acid as an Intermediate . ......... 158 Methionine Cycle ........ 161 Conversion of l-Aminocyclopropanecarboxylic Acid to Ethylene 164 REGULATION OF ETHYLENE BIOSYNTHESIS 167 Regulation in Ripening Fruits and Senescing Flowers ...... 167 Auxin-Induced Ethylene Production 169 Regulation of Ethylene Biosynthesis by Ethylene 169 Stress-Induced Ethylene Production . . . . . . . . 171 Regulation by Light and Carbon Dioxide 173 Inhibitors of Ethylene Biosynthesis : 174 Conjugation of l-Aminocyclopropanecarboxylic Acid to l-(Malonylamino) cyclopropanecarboxylic Acid 178 CONCLUDING REMARKS 180,1984,56,3095,140,2,29,39,45,48,88,69,72,64,88
afca469ed6f40dfa69540f139a3b06764928dab6,"Tolerance to high soil [Na(+)] involves processes in many different parts of the plant, and is manifested in a wide range of specializations at disparate levels of organization, such as gross morphology, membrane transport, biochemistry and gene transcription. Multiple adaptations to high [Na(+)] operate concurrently within a particular plant, and mechanisms of tolerance show large taxonomic variation. These mechanisms can occur in all cells within the plant, or can occur in specific cell types, reflecting adaptations at two major levels of organization: those that confer tolerance to individual cells, and those that contribute to tolerance not of cells per se, but of the whole plant. Salt-tolerant cells can contribute to salt tolerance of plants; but we suggest that equally important in a wide range of conditions are processes involving the management of Na(+) movements within the plant. These require specific cell types in specific locations within the plant catalysing transport in a coordinated manner. For further understanding of whole plant tolerance, we require more knowledge of cell-specific transport processes and the consequences of manipulation of transporters and signalling elements in specific cell types.",2003,251,2849,167,4,31,59,72,97,108,148,165,193,197
6d9c4b121f781c46b6c9227f6889524db86f5b8c,"Inducible defense-related proteins have been described in many plant species upon infection with oomycetes, fungi, bacteria, or viruses, or insect attack. Several types of proteins are common and have been classified into 17 families of pathogenesis-related proteins (PRs). Others have so far been found to occur more specifically in some plant species. Most PRs and related proteins are induced through the action of the signaling compounds salicylic acid, jasmonic acid, or ethylene, and possess antimicrobial activities in vitro through hydrolytic activities on cell walls, contact toxicity, and perhaps an involvement in defense signaling. However, when expressed in transgenic plants, they reduce only a limited number of diseases, depending on the nature of the protein, plant species, and pathogen involved. As exemplified by the PR-1 proteins in Arabidopsis and rice, many homologous proteins belonging to the same family are regulated developmentally and may serve different functions in specific organs or tissues. Several defense-related proteins are induced during senescence, wounding or cold stress, and some possess antifreeze activity. Many defense-related proteins are present constitutively in floral tissues and a substantial number of PR-like proteins in pollen, fruits, and vegetables can provoke allergy in humans. The evolutionary conservation of similar defense-related proteins in monocots and dicots, but also their divergent occurrence in other conditions, suggest that these proteins serve essential functions in plant life, whether in defense or not.",2006,205,2579,139,9,66,110,135,146,163,200,229,220,185
030989e2e5056d3eca03e13c1006670f5d034dc9,,1962,0,3108,187,3,1,3,2,4,12,7,5,5,4
a72c494aba62e75c696566d924f7b70da9e4c3fe,"Since its publication in 2000, Biochemistry & Molecular Biology of Plants, has been hailed as a major contribution to the plant sciences literature and critical acclaim has been matched by global sales success. Maintaining the scope and focus of the first edition, the second will provide a major update, include much new material and reorganise some chapters to further improve the presentation. This book is meticulously organised and richly illustrated, having over 1,000 full-colour illustrations and 500 photographs. It is divided into five parts covering: Compartments: Cell Reproduction: Energy Flow; Metabolic and Developmental Integration; and Plant Environment and Agriculture. Specific changes to this edition include: Completely revised with over half of the chapters having a major rewrite. Includes two new chapters on signal transduction and responses to pathogens. Restructuring of section on cell reproduction for improved presentation. Dedicated website to include all illustrative material. Biochemistry & Molecular Biology of Plants holds a unique place in the plant sciences literature as it provides the only comprehensive, authoritative, integrated single volume book in this essential field of study.",2002,0,3146,64,96,112,127,150,196,227,225,200,232,261
b03c9cf546e6b18c9a4467ab555a53f995299795,"Glucosinolates (beta-thioglucoside-N-hydroxysulfates), the precursors of isothiocyanates, are present in sixteen families of dicotyledonous angiosperms including a large number of edible species. At least 120 different glucosinolates have been identified in these plants, although closely related taxonomic groups typically contain only a small number of such compounds. Glucosinolates and/or their breakdown products have long been known for their fungicidal, bacteriocidal, nematocidal and allelopathic properties and have recently attracted intense research interest because of their cancer chemoprotective attributes. Numerous reviews have addressed the occurrence of glucosinolates in vegetables, primarily the family Brassicaceae (syn. Cruciferae; including Brassica spp and Raphanus spp). The major focus of much previous research has been on the negative aspects of these compounds because of the prevalence of certain ""antinutritional"" or goitrogenic glucosinolates in the protein-rich defatted meal from widely grown oilseed crops and in some domesticated vegetable crops. There is, however, an opposite and positive side of this picture represented by the therapeutic and prophylactic properties of other ""nutritional"" or ""functional"" glucosinolates. This review addresses the complex array of these biologically active and chemically diverse compounds many of which have been identified during the past three decades in other families. In addition to the Brassica vegetables, these glucosinolates have been found in hundreds of species, many of which are edible or could provide substantial quantities of glucosinolates for isolation, for biological evaluation, and potential application as chemoprotective or other dietary or pharmacological agents.",2001,304,2471,167,12,45,43,51,73,88,89,117,103,124
70a9222687cbd9c0c24e60a56e914abe0cf8977e,"MicroRNAs (miRNAs) are small, endogenous RNAs that regulate gene expression in plants and animals. In plants, these approximately 21-nucleotide RNAs are processed from stem-loop regions of long primary transcripts by a Dicer-like enzyme and are loaded into silencing complexes, where they generally direct cleavage of complementary mRNAs. Although plant miRNAs have some conserved functions extending beyond development, the importance of miRNA-directed gene regulation during plant development is now particularly clear. Identified in plants less than four years ago, miRNAs are already known to play numerous crucial roles at each major stage of development-typically at the cores of gene regulatory networks, targeting genes that are themselves regulators, such as those encoding transcription factors and F-box proteins.",2006,195,2377,167,32,99,121,118,151,138,204,199,185,197
50d072f18f0566700dba5e1f7084d140d905215f,"SummaryThe photosynthetic capacity of leaves is related to the nitrogen content primarily bacause the proteins of the Calvin cycle and thylakoids represent the majority of leaf nitrogen. To a first approximation, thylakoid nitrogen is proportional to the chlorophyll content (50 mol thylakoid N mol-1 Chl). Within species there are strong linear relationships between nitrogen and both RuBP carboxylase and chlorophyll. With increasing nitrogen per unit leaf area, the proportion of total leaf nitrogen in the thylakoids remains the same while the proportion in soluble protein increases. In many species, growth under lower irradiance greatly increases the partitioning of nitrogen into chlorophyll and the thylakoids, while the electron transport capacity per unit of chlorophyll declines. If growth irradiance influences the relationship between photosynthetic capacity and nitrogen content, predicting nitrogen distribution between leaves in a canopy becomes more complicated. When both photosynthetic capacity and leaf nitrogen content are expressed on the basis of leaf area, considerable variation in the photosynthetic capacity for a given leaf nitrogen content is found between species. The variation reflects different strategies of nitrogen partitioning, the electron transport capacity per unit of chlorophyll and the specific activity of RuBP carboxylase. Survival in certain environments clearly does not require maximising photosynthetic capacity for a given leaf nitrogen content. Species that flourish in the shade partition relatively more nitrogen into the thylakoids, although this is associated with lower photosynthetic capacity per unit of nitrogen.",2004,88,2380,183,93,66,64,69,83,95,80,109,108,133
27c764dde11259d2fdc0ab41514eff2903913c74,"INTRODUCTION 492 ECOLOGICAL ASPECTS OF PHOTOSYNTHETIC TEMPERATURE ADAPTATION 493 Photosynthetic Temperature Dependence in Thermally Contrasting Climates ........ 493 Photosynthetic Temperature Acclimation 497 Seasonal acclimation in natural habitats ...... ....... 497 Studies in controlled environments 499 THE MECHANISTIC BASIS FOR PHOTOSYNTHETIC RESPONSE AND ADAPTATION TO TEMPERATURE 504 Reversible Temperature Respon.ses 505 Stomata! effec� o� the . temJH!.rature response 0/ photo.rynthesis ......... ....... 505 Interacttons with /lght mtenslty 507 C, photo.rynthesis (lM photorespiration 507 C, photo.rynthesis 515 Comparison 0/ plants from contrasting thermol regimes ...... ...... 517 [""eversible Temperature Respon.ses 519 Low temperature sensitMty ....... 519 High temperature sensitivity 524 Adoptive responses in the heat stability 0/ the photosynthetic apparatus 530 CONCLUDING REMARKS 532",1980,67,2662,155,0,7,18,21,34,26,27,27,27,27
d729e27861197182ff1bb89517cfa93b60786076,,1988,0,2341,224,1,4,8,11,13,16,13,12,13,21
12163e44c1388a62c833b74566e5a7e6daded4f8,"A-Z presentation of Indian medicinal plants including taxonomy, traditional and international synonyms, plant parts, applications and pharmacokinetic action.",2007,30,2149,133,0,4,22,71,117,164,193,243,242,214
a8e79ad55b64d09a09798aeaf1f77679d2d80682,"Publisher Summary In this chapter, the advances that have been made in understanding the ecology of the mineral nutrition of wild plants from terrestrial ecosystems have been reviewed. This chapter is organized along three lines. First, the issues of nutrient-limited plant growth and nutrient uptake, with special emphasis on the importance of the uptake of nutrients in organic form—both by mycorrhizal and by non-mycorrhizal plants—and the importance of symbiotic nitrogen fixation is treated. In addition, the influence of allocation patterns on mineral nutrient uptake is described. Next, a few of the nutritional aspects of leaf functioning and how nutrients are used for biomass production by the plant are explored. That is done by studying the nutrient use efficiency (NUE) of plants and the various components of NUE. Finally, the feedback of plant species to soil nutrient availability by reviewing patterns in litter decomposition and nutrient mineralization is investigated. The chapter concludes with a synthesis of the various aspects of the mineral nutrition of wild plants. The chapter ends with a conceptual description of plant strategies with respect to mineral nutrition.",1999,241,2332,124,3,6,22,41,70,77,118,71,90,98
e5cf8b5bad58d9cfe9d950d98c59c2fb37489264,"There are at least three RNA silencing pathways for silencing specific genes in plants. In these pathways, silencing signals can be amplified and transmitted between cells, and may even be self-regulated by feedback mechanisms. Diverse biological roles of these pathways have been established, including defence against viruses, regulation of gene expression and the condensation of chromatin into heterochromatin. We are now in a good position to investigate the full extent of this functional diversity in genetic and epigenetic mechanisms of genome control.",2004,94,2204,117,10,141,136,145,160,128,130,120,135,142
edb564c5673f3dcc37dd8fb1d52b50061fea733f,"DNA barcoding involves sequencing a standard region of DNA as a tool for species identification. However, there has been no agreement on which region(s) should be used for barcoding land plants. To provide a community recommendation on a standard plant barcode, we have compared the performance of 7 leading candidate plastid DNA regions (atpF–atpH spacer, matK gene, rbcL gene, rpoB gene, rpoC1 gene, psbK–psbI spacer, and trnH–psbA spacer). Based on assessments of recoverability, sequence quality, and levels of species discrimination, we recommend the 2-locus combination of rbcL+matK as the plant barcode. This core 2-locus barcode will provide a universal framework for the routine use of DNA sequence data to identify specimens and contribute toward the discovery of overlooked species of land plants.",2009,32,2012,77,9,90,130,151,148,145,189,198,179,199
16a64ffec2bddf4e48251660240b93fd3a7c7000,"Alkaloids, tannins, saponins, steroid, terpenoid, flavonoids, phlobatannin and cardic glycoside distribution in ten medicinal plants belonging to different families were assessed and compared. The medicinal plants investigated were Cleome nutidosperma, Emilia coccinea, Euphorbia heterophylla, Physalis angulata, Richardia bransitensis, Scopania dulcis, Sida acuta, Spigelia anthelmia, Stachytarpheta cayennensis and Tridax procumbens. All the plants were found to contain alkaloids, tannins and flavonoids except for the absence of tannins in S. acuta and flavonoids in S. cayennsis respectively. The significance of the plants in traditional medicine and the importance of the distribution of these chemical constituents were discussed with respect to the role of these plants in ethnomedicine in Nigeria.",2005,22,2511,90,0,5,13,20,58,85,135,188,235,282
561d6920bfe33e087fd9d411d873e83a5e71872b,,1959,0,3656,9,1,3,5,3,3,4,4,8,5,5
526efd0a489d35f3184263110a7041a201ca8601,"Despite widespread concern about declines in pollination services, little is known about the patterns of change in most pollinator assemblages. By studying bee and hoverfly assemblages in Britain and the Netherlands, we found evidence of declines (pre-versus post-1980) in local bee diversity in both countries; however, divergent trends were observed in hoverflies. Depending on the assemblage and location, pollinator declines were most frequent in habitat and flower specialists, in univoltine species, and/or in nonmigrants. In conjunction with this evidence, outcrossing plant species that are reliant on the declining pollinators have themselves declined relative to other plant species. Taken together, these findings strongly suggest a causal connection between local extinctions of functionally linked plant and pollinator species.",2006,46,2313,102,7,42,70,85,123,117,138,170,203,182
5ab7efc9d71304c10f0c1cdf0021aa293b3961ea,"Phosphorus (P) is limiting for crop yield on > 30% of the world's arable land and, by some estimates, world resources of inexpensive P may be depleted by 2050. Improvement of P acquisition and use by plants is critical for economic, humanitarian and environmental reasons. Plants have evolved a diverse array of strategies to obtain adequate P under limiting conditions, including modifications to root architecture, carbon metabolism and membrane structure, exudation of low molecular weight organic acids, protons and enzymes, and enhanced expression of the numerous genes involved in low-P adaptation. These adaptations may be less pronounced in mycorrhizal-associated plants. The formation of cluster roots under P-stress by the nonmycorrhizal species white lupin (Lupinus albus), and the accompanying biochemical changes exemplify many of the plant adaptations that enhance P acquisition and use. Physiological, biochemical, and molecular studies of white lupin and other species response to P-deficiency have identified targets that may be useful for plant improvement. Genomic approaches involving identification of expressed sequence tags (ESTs) found under low-P stress may also yield target sites for plant improvement. Interdisciplinary studies uniting plant breeding, biochemistry, soil science, and genetics under the large umbrella of genomics are prerequisite for rapid progress in improving nutrient acquisition and use in plants. Contents I. Introduction 424 II. The phosphorus conundrum 424 III. Adaptations to low P 424 IV. Uptake of P 424 V. P deficiency alters root development and function 426 VI. P deficiency modifies carbon metabolism 431 VII. Acid phosphatase 436 VIII. Genetic regulation of P responsive genes 437 IX. Improving P acquisition 439 X. Synopsis 440.",2003,292,2143,147,0,26,47,50,50,71,85,90,128,132
17ded1f9e7cd14d7632aaf936053453e9698646d,"Agricultural productivity worldwide is subject to increasing environmental constraints, particularly to drought and salinity due to their high magnitude of impact and wide distribution. Traditional breeding programs trying to improve abiotic stress tolerance have had some success, but are limited by the multigenic nature of the trait. Tolerant plants such as Craterostigma plantagenium, Mesembryanthemum crystallinum, Thellungiella halophila and other hardy plants could be valuable tools to dissect the extreme tolerance nature. In the last decade, Arabidopsis thaliana, a genetic model plant, has been extensively used for unravelling the molecular basis of stress tolerance. Arabidopsis also proved to be extremely important for assessing functions for individual stress-associated genes due to the availability of knock-out mutants and its amenability for genetic transformation. In this review, the responses of plants to salt and water stress are described, the regulatory circuits which allow plants to cope with stress are presented, and how the present knowledge can be applied to obtain tolerant plants is discussed.",2005,457,2201,93,12,28,39,80,85,109,162,155,181,169
20ffe80b1746fb63c442674ba0ba798072267291,"This paper empirically investigates the effects of trade liberalization on plant productivity in the case of Chile. Chile presents an interesting setting to study this relationship since it underwent a massive trade liberalization that significantly exposed its plants to competition from abroad during the late 1970s and early 1980s. Methodologically, I approach this question in two steps. In the first step, I estimate a production function to obtain a measure of plant productivity. I estimate the production function semiparametrically to correct for the presence of selection and simultaneity biases in the estimates of the input coefficients required to construct a productivity measure. I explicitly incorporate plant exit in the estimation to correct for the selection problem induced by liquidated plants. These methodological aspects are important in obtaining a reliable plant-level productivity measure based on consistent estimates of the input coefficients. In the second step, I identify the impact of trade liberalization on plants' productivity in a regression framework allowing variation in productivity over time and across traded- and nontraded-goods sectors. Using plant-level panel data on Chilean manufacturers, I find evidence of within plant productivity improvements that can be attributed to a liberalized trade policy, especially for the plants in the import-competing sector. In many cases, aggregate productivity improvements stem from the reshuffling of resources and output from less to more efficient producers.",2000,57,1998,146,4,10,23,47,65,72,92,84,122,131
003a49aa331f59d72b40ca9d86d665ad42f3ef4e,"Molecular studies of drought stress in plants use a variety of strategies and include different species subjected to a wide range of water deficits. Initial research has by necessity been largely descriptive, and relevant genes have been identified either by reference to physiological evidence or by differential screening. A large number of genes with a potential role in drought tolerance have been described, and major themes in the molecular response have been established. Particular areas of importance are sugar metabolism and late-embryogenesis-abundant (LEA) proteins. Studies have begun to examine mechanisms that control the gene expression, and putative regulatory pathways have been established. Recent attempts to understand gene function have utilized transgenic plants. These efforts are of clear agronomic importance.",1996,208,2237,103,6,34,52,71,66,82,119,66,105,87
451358bc1b585bc3fa95f1d78b29b055278b2bd0,"The evolutionary response of plants to herbivory is constrained by the availability of resources in the environment. Woody plants adapted to low-resource environments have intrinsically slow growth rates that limit their capacity to grow rapidly beyond the reach of most browsing mammals. Their low capacity to acquire resources limits their potential for compensatory growth which would otherwise enable them to replace tissue destroyed by browsing. Plants adapted to low-resource environments have responded to browsing by evolving strong constitutive defenses with relatively low ontogenetic plasticity. Because nutrients are often more limiting than light in boreal forests, slowly growing boreal forest trees utilize carbon-based rather than nitrogen-based defenses. More rapidly growing shade-intolerant trees that are adapted to growth in high-resource environments are selected for competitive ability and can grow rapidly beyond the range of most browsing mammals. Moreover, these plants have the carbon and nutrient reserves necessary to replace tissue lost to browsing through compensatory growth. However, because browsing of juvenile plants reduces vertical growth and thus competitive ability, these plants are selected for resistance to browsing during the juvenile growth phase. Consequently, early successional boreal forest trees have responded to browsing by evolving strong defenses during juvenility only. Because severe pruning causes woody plants to revert to a juvenile form, resistance of woody plants to hares increases after severe hare browsing as occurs during hare population outbreaks. This increase in browsing resistance may play a significant role in boreal forest plant-hare interactions. Unlike woody plants, graminoids retain large reserves of carbon and nutrients below ground in both low-resource and high-resource environments and can respond to severe grazing through compensatory growth. These fundamental differences between the response of woody plants and graminoids to vertebrate herbivory suggest that the dynamics of browsing systems and grazing systems are qualitatively different.",1983,107,2362,132,7,7,17,16,24,32,26,27,41,37
69ff421246a51fb9d8f3d10a8227221ed532960a,"Methods of analysis for soils plants and waters , Methods of analysis for soils plants and waters , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,2254,168,17,19,21,20,33,16,16,19,25,13
d4b6194ba708d6c9c0f36b9294fec65b12e16a30,"This paper reviews the plant geography, ecology, metal tolerance and phytochcmistry of terrestrial higher plants which arc able to accumulate metallic elements in their dry matter to an exceptional degree. The review is limited to the elements Co, Cu, Cr, Pb. Mn. Ni and Zn. Hyperaccumulators of Co, Cu, Cr, Pb and Ni arc here defined as plants containing over 1000 u.g/g (ppm) of any of these elements in the dry matter; for Mn and Zn, the criterion is 10,000 u.g/g (1%). A unifying feature of hypcraccumula ting plants is their general restriction to mineralized soils and specific rock types. Lists of hypcraccumula ting species arc presented for the elements considered. These suggest that the phenomenon is widespread throughout the plant kingdom. For example, 145 hyper-accumulators of nickel are reported: these arc distributed among 6 supcrordcrs, 17 orders, and 22 families and include herbs, shrubs and trees from both the temperate and tropical zones. Although some phylogcnetic relationships emerge, the evolutionary significance of metal hyperaccumulation remains obscure. Phytochemical studies however suggest that hyperaccumulation is closely linked to the mechanism of metal tolerance involved in the successful colonization of metalliferous and otherwise phytotoxic soils. The potentialities of hyperaccumula ting plants in biorccovcry and soil detoxification arc indicated.",1989,58,2313,157,1,0,5,4,5,12,7,9,20,16
fcb343d0e7a762992cb20d618fa4253040a0ff80,,1971,0,2579,178,1,5,8,15,30,14,12,17,36,18
dbe6c36aac8d85dc67c944079e02d5f6d6000657,"Publisher Summary This chapter focuses on evolutionary significance of phenotypic plasticity in plants. The expression of an individual genotype is modified by its environment. The amount by which it can be modified is termed its plasticity. This plasticity can be either morphological or physiological; these are interrelated. The plasticity of a character is related to the general pattern of its development, and apart from this, that plasticity is a general property of the whole genotype. Plasticity of a character appears to be specific for that character, specific in relation to particular environmental influences, specific in direction, under genetic control not necessarily related to heterozygosity, and able to be radically altered by selection. Because plants are static organisms, plasticity is of marked adaptive value in a great number of situations. Examples of all these situations in plant species are discussed. They indicate that adaptation by plasticity is a widespread and important phenomenon in plants and has evolved differently in different species. The mechanisms involved in plasticity are varied. At one extreme, the character shows a continuous range of modification dependent on the intensity of the environmental stimulus. At the other, the character shows only two discrete modifications. The stimulus causing these modifications may be direct or indirect. The mechanisms found can be related to the particular environmental situation involved.",1965,70,2777,113,0,1,1,5,4,7,3,4,3,5
129c03c968f58f85a9f7afeae38c2fdc1cc704a9,"We reconcile trade theory with plant-level export behavior, extending the Ricardian model to accommodate many countries, geographic barriers, and imperfect competition. Our model captures qualitatively basic facts about U.S. plants: (i) productivity dispersion, (ii) higher productivity among exporters, (iii) the small fraction who export, (iv) the small fraction earned from exports among exporting plants, and (v) the size advantage of exporters. Fitting the model to bilateral trade among the United States and 46 major trade partners, we examine the impact of globalization and dollar appreciation on productivity, plant entry and exit, and labor turnover in U.S. manufacturing. (JEL F11, F17, O33)",2004,21,1459,272,26,40,49,80,103,96,107,124,98,103
88792e3cf04d925fd6c073599a179d857e173ed6,"Sugars not only fuel cellular carbon and energy metabolism but also play pivotal roles as signaling molecules. The experimental amenability of yeast as a unicellular model system has enabled the discovery of multiple sugar sensors and signaling pathways. In plants, different sugar signals are generated by photosynthesis and carbon metabolism in source and sink tissues to modulate growth, development, and stress responses. Genetic analyses have revealed extensive interactions between sugar and plant hormone signaling, and a central role for hexokinase (HXK) as a conserved glucose sensor. Diverse sugar signals activate multiple HXK-dependent and HXK-independent pathways and use different molecular mechanisms to control transcription, translation, protein stability and enzymatic activity. Important and complex roles for Snf1-related kinases (SnRKs), extracellular sugar sensors, and trehalose metabolism in plant sugar signaling are now also emerging.",2006,158,1814,123,11,75,94,103,114,120,146,133,165,131
a14bbdbbf49575f81b460473784080de623c7534,"Toxic metal pollution of waters and soils is a major environmental problem, and most conventional remediation approaches do not provide acceptable solutions. The use of specially selected and engineered metal-accumulating plants for environmental clean-up is an emerging technology called phytoremediation. Three subsets of this technology are applicable to toxic metal remediation: (1) Phytoextraction—the use of metal-accumulating plants to remove toxic metals from soil; (2) Rhizoflltration—the use of plant roots to remove toxic metals from polluted waters; and (3) Phytostabilization—the use of plants to eliminate the bioavailability of toxic metals hi soils. Biological mechanisms of toxic metal uptake, translocation and resistance as well as strategies for improving phytoremediation are also discussed.",1995,67,2177,88,1,12,27,18,35,39,39,46,67,76
271fb86a07b21eb8060c3272d6c298bfb1d59189,"Woody plants such as trees have a significant economic and climatic influence on global economies and ecologies. This completely revised classic book is an up-to-date synthesis of the intensive research devoted to woody plants published in the second edition, with additional important aspects from the authors' previous book, ""Growth Control in Woody Plants"". Intended primarily as a reference for researchers, the interdisciplinary nature of the book makes it useful to a broad range of scientists and researchers from agroforesters, agronomists, and arborists to plant pathologists and soil scientists. This third edition provides crutial updates to many chapters, including: responses of plants to elevated CO2; the process and regulation of cambial growth; photoinhibition and photoprotection of photosynthesis; nitrogen metabolism and internal recycling, and more.Revised chapters focus on emerging discoveries of the patterns and processes of woody plant physiology. This is the only book to provide recommendations for the use of specific management practices and experimental procedures and equipment. Interdisciplinary approach will appeal to a broad range of scientists, researchers, and growers. It is thoroughly updated with the latest research devoted to woody plants.",1983,0,2528,55,17,33,30,36,36,38,29,37,38,49
807d6057c5f59e97dfe4aa288f455c2cd0badc3a,"Abstract The paper summarizes present knowledge in the field of higher plant responses to cadmium, an important environmental pollutant. The principal mechanisms reviewed here include phytochelatin-based sequestration and compartmentalization processes, as well as additional defense mechanisms, based on cell wall immobilization, plasma membrane exclusion, stress proteins, stress ethylene, peroxidases, metallothioneins, etc. An analysis of data taken from the international literature has been carried out, in order to highlight possible ‘qualitative’ and ‘quantitative’ differences in the response of wild-type (non-tolerant) plants to chronic and acute cadmium stress. The dose-response relationships indicate that plant response to low and high cadmium level exposures is a very complex phenomenon indeed: cadmium evokes a number of parallel and/or consecutive events at molecular, physiological and morphological levels. We propose that, above all in response to acute cadmium stress, various mechanisms might operate both in an additive and in a potentiating way. Thus, a holistic and integrated approach seems to be necessary in the study of the response of higher plants to cadmium. This multi-component model, which we would call ‘fan-shaped’ response, may accord with the Selyean ‘general adaptation syndrome’ hypothesis. While cadmium detoxification is a complex phenomenon, probably under polygenic control, cadmium ‘real’ tolerance—found in mine plants or in plant systems artificially grown under long-term selection pressure, exposed to high levels of cadmium—seems to be a simpler phenomenon, possibly involving only monogenic/oligogenic control. We conclude that, following a ‘pyramidal’ model, (adaptive) tolerance is supported by (constitutive) detoxification mechanisms, which in turn rely on (constitutive) homeostatic processes. The shift between homeostasis and ‘fan-shaped’ response can be rapid and involve quick changes in (poly)gene expression. Differently, the slow shift from ‘fan-shaped’ response to ‘real’ cadmium tolerance is caused and affected by long-term selection pressure, which may increase the frequency (and promote the expression) of one or a few tolerance gene(s).",1999,169,2160,82,0,10,14,27,38,63,71,74,102,112
70c0541452254eaed66b19526884bcc9f179bb82,"In this review we describe and discuss several approaches to selecting higher plants as candidates for drug development with the greatest possibility of success. We emphasize the role of information derived from various systems of traditional medicine (ethnomedicine) and its utility for drug discovery purposes. We have identified 122 compounds of defined structure, obtained from only 94 species of plants, that are used globally as drugs and demonstrate that 80% of these have had an ethnomedical use identical or related to the current use of the active elements of the plant. We identify and discuss advantages and disadvantages of using plants as starting points for drug development, specifically those used in traditional medicine.",2001,118,1986,51,0,5,5,8,19,16,30,43,43,62
b7f38626c1ffbc8acfcc6a4d4655c796debe6ea4,"MicroRNAs (miRNAs) are an extensive class of ~22-nucleotide noncoding RNAs thought to regulate gene expression in metazoans. We find that miRNAs are also present in plants, indicating that this class of noncoding RNA arose early in eukaryotic evolution. In this paper 16 Arabidopsis miRNAs are described, many of which have differential expression patterns in development. Eight are absolutely conserved in the rice genome. The plant miRNA loci potentially encode stem-loop precursors similar to those processed by Dicer (a ribonuclease III) in animals. Mutation of an Arabidopsis Dicer homolog, CARPEL FACTORY, prevents the accumulation of miRNAs, showing that similar mechanisms direct miRNA processing in plants and animals. The previously described roles of CARPEL FACTORY in the development of Arabidopsis embryos, leaves, and floral meristems suggest that the miRNAs could play regulatory roles in the development of plants as well as animals.",2002,62,1648,134,17,59,90,89,61,87,76,75,77,97
1744b271a1564af41e34302e6f51c0cc3dbd0659,"Revue bibliographique suggerant que, au moins pour la croissance vegetative les plantes fonctionnent conformement aux theoremes economiques: optimiser les profits et repartir de facon optimale les ressources",1985,99,2216,80,0,2,18,25,22,26,35,39,28,43
caf3dfcf243a415c5dd4f337c8560e4bc04d2fa2,"Multicellular eukaryotes produce small RNA molecules (approximately 21–24 nucleotides) of two general types, microRNA (miRNA) and short interfering RNA (siRNA). They collectively function as sequence-specific guides to silence or regulate genes, transposons, and viruses and to modify chromatin and genome structure. Formation or activity of small RNAs requires factors belonging to gene families that encode DICER (or DICER-LIKE [DCL]) and ARGONAUTE proteins and, in the case of some siRNAs, RNA-dependent RNA polymerase (RDR) proteins. Unlike many animals, plants encode multiple DCL and RDR proteins. Using a series of insertion mutants of Arabidopsis thaliana, unique functions for three DCL proteins in miRNA (DCL1), endogenous siRNA (DCL3), and viral siRNA (DCL2) biogenesis were identified. One RDR protein (RDR2) was required for all endogenous siRNAs analyzed. The loss of endogenous siRNA in dcl3 and rdr2 mutants was associated with loss of heterochromatic marks and increased transcript accumulation at some loci. Defects in siRNA-generation activity in response to turnip crinkle virus in dcl2 mutant plants correlated with increased virus susceptibility. We conclude that proliferation and diversification of DCL and RDR genes during evolution of plants contributed to specialization of small RNA-directed pathways for development, chromatin structure, and defense.",2004,104,1512,193,46,122,111,104,111,102,91,101,108,78
294ec060bca8f76d34862776da6aedf6cfa80849,"Various novel techniques including ultrasound-assisted extraction, microwave-assisted extraction, supercritical fluid extraction, and accelerated solvent extraction have been developed for the extraction of nutraceuticals from plants in order to shorten the extraction time, decrease the solvent consumption, increase the extraction yield, and enhance the quality of extracts. A critical review was conducted to introduce and compare the conventional Soxhlet extraction and the new alternative methods used for the extraction of nutraceuticals from plants. The practical issues of each extraction method were discussed. Potential uses of those methods for the extraction of nutraceuticals from plant materials was finally summarized.",2006,88,1620,82,3,13,22,28,39,68,90,113,144,162
379ffd011c11510b0ea4bf6b1940f7f5603af6c6,"The services of ecological systems and the natural capital stocks that produce them are critical to the functioning of the Earth's life-support system. They contribute to human welfare, both directly and indirectly, and therefore represent part of the total economic value of the planet. We have estimated the current economic value of 17 ecosystem services for 16 biomes, based on published studies and a few original calculations. For the entire biosphere, the value (most of which is outside the market) is estimated to be in the range of US$16-54 trillion (1012) per year, with an average of US$33 trillion per year. Because of the nature of the uncertainties, this must be considered a minimum estimate. Global gross national product total is around US$18 trillion per year.",1997,57,15925,796,0,0,0,0,0,0,0,0,1,0
13c3630c03bec0c3e443ad5a3dc6d1951db74c20,"Status of this Memo This memo provides information for the Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by aggregating traffic classification state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need only be implemented at network boundaries or hosts. Network resources are allocated to traffic streams by service provisioning policies which govern how traffic is marked and conditioned upon entry to a differentiated services-capable network, and how that traffic is forwarded within that network. A wide variety of services can be implemented on top of these building blocks.",1998,14,4114,339,7,121,220,322,407,404,381,357,324,254
505ae0232a6b59786ce8ed61af98573c68f69b92,"Salespeople involved in the marketing of complex services often perform the role of “relationship manager.” It is, in part, the quality of the relationship between the salesperson and the customer ...",1990,45,4378,306,2,1,9,21,29,33,54,39,39,54
d73a3014f2fbe3c02cfe4884bc6f5bbc0990e557,"The Second European Edition of Services Marketing: Integrating Customer Focus Across the Firm by Wilson, Zeithaml, Bitner and Gremler uniquely focuses on the development of customer relationships through quality service. Reflecting the increasing importance of the service economy, Services Marketing is the only text that put the customer's experience of services at the centre of its approach. The core theories, concepts and frameworks are retained, and specifically the gaps model, a popular feature of the book. The text moves from the foundations of services marketing before introducing the gaps model and demonstrating its application to services marketing. In the second edition, the book takes on more European and International contexts to reflect the needs of courses, lecturers and students. The second edition builds on the wealth of European and International examples, cases, and research in the first edition, offering more integration of European content. It has also be fully updated with the latest research to ensure that it continues to be seen as the text covering the very latest services marketing thinking. In addition, the cases section has been thoroughly examined and revised to offer a range of new case studies with a European and global focus. The online resources have also been fully revised and updated providing an excellent package of support for lecturers and students.",1996,0,4803,271,0,0,1,0,3,20,23,41,54,69
698afd7515d01f72e028927d829e1c1d22019f87,"The Semantic Web should enable greater access not only to content but also to services on the Web. Users and software agents should be able to discover, invoke, compose, and monitor Web resources offering particular services and having particular properties. As part of the DARPA Agent Markup Language program, we have begun to develop an ontology of services, called DAML-S, that will make these functionalities possible. In this paper we describe the overall structure of the ontology, the service profile for advertising services, and the process model for the detailed description of the operation of services. We also compare DAML-S with several industry efforts to define standards for characterizing services on the Web.",2001,83,3214,330,10,78,114,136,234,265,300,320,300,267
9438aa83eb8218b7e6e3891ad7bc2b388e35bc33,"In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic “virtual organizations” formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, “The Anatomy of the Grid,” by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration—within and across organizational domains. This is a DRAFT document and continues to be revised. The latest version can be found at http://www.globus.org/research/papers/ogsa.pdf. Please send comments to foster@mcs.anl.gov, carl@isi.edu, jnick@us.ibm.com, tuecke@mcs.anl.gov Physiology of the Grid 2",2002,84,3620,238,141,427,550,551,436,326,231,230,166,131
1b74223af38112ee2814688b1c4c96f597c1ff43,,1983,0,3573,477,1,2,0,1,4,3,2,5,10,3
70f5ad30aabb1de547b34d5aa4167dd9bb8d0957,"This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support real- time as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation.",1994,27,3979,194,6,39,50,78,96,141,217,279,280,326
c9253ce46a869d921a304c30ca604820089c8d9d,"processes are rarely used. The most common scenario is to use them as a template to define executable processes. Abstract processes can be used to replace sets of rules usually expressed in natural language, which is often ambiguous. In this book, we will first focus on executable processes and come back to abstract processes in Chapter 4. 21 This material is copyright and is licensed for the sole use by Encarnacion Bellido on 20th February 2006 Via Alemania, 10, bajos, , Palma de Mallorca, Baleares, 07006",2004,3,3752,193,390,523,566,483,413,283,236,172,116,97
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
1c90a7392994300df01fed1801a41fa0c9693ca2,"Relationship marketing is an old idea but a new focus now at the forefront of services marketing practice and academic research. The impetus for its development has come from the maturing of services marketing with the emphasis on quality, increased recognition of potential benefits for the firm and the customer, and technological advances. Accelerating interest and active research are extending the concept to incorporate newer, more sophisticated viewpoints. Emerging perspectives explored here include targeting profitable customers, using the strongest possible strategies for customer bonding, marketing to employees and other stakeholders, and building trust as a marketing tool. Although relationship marketing is developing, more research is needed before it reaches maturity. A baker’s dozen of researchable questions suggests some future directions.",1995,21,3161,284,1,11,21,26,41,59,56,72,71,84
1bfb01fc10cf40d307f5d9b99e9b94de3fb85685,"Processes serve a descriptive role, with more than one use case. One such use case might be to describe the observable behavior of some or all of the services offered by an Executable Process. Another use case would be to define a process template that embodies domain-specific best practices. Such a process template would capture essential process logic in a manner compatible with a design-time representation, while excluding execution details to be completed when mapping to an Executable Process. Regardless of the specific use case and purpose, all Abstract Processes share a common syntactic base. They have different requirements for the level of opacity and restrictions on which parts of a process definition may be omitted or hidden. Tailored uses of Abstract Processes have different effects on the consistency constraints and on the semantics of that process. Some of these required constraints are not enforceable by the XML Schema. A common base specifies the features that define the syntactic universe of Abstract Processes. Given this common base, a usage profile provides the necessary specializations and semantics based on Executable WS-BPEL for a particular use of an Abstract Process. As mentioned above it is possible to use WS-BPEL to define an Executable Business Process. While a WS-BPEL Abstract Process definition is not required to be fully specified, the language effectively defines a portable execution format for business processes that rely exclusively on Web Service resources and XML data. Moreover, such processes execute and interact with their partners in a consistent way regardless of the supporting platform or programming model used by the implementation of the hosting environment. The continuity of the basic conceptual model between Abstract and Executable Processes in WSBPEL makes it possible to export and import the public aspects embodied in Abstract Processes as process or role templates while maintaining the intent and structure of the observable behavior. This applies even where private implementation aspects use platform dependent functionality.",2007,50,3133,200,211,360,423,412,359,310,248,240,146,97
9ea11c0728473937644be8dda349a34d32f656a1,"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.",2004,59,2872,297,7,51,109,202,193,263,297,248,267,238
3cf9e452adceb66ead134d9377ae8155016aa259,"Companies that want to improve their service quality should take a cue from manufacturing and focus on their own kind of scrap heap: customers who won't come back. Because that scrap heap can be every bit as costly as broken parts and misfit components, service company managers should strive to reduce it. They should aim for ""zero defections""--keeping every customer they can profitably serve. As companies reduce customer defection rates, amazing things happen to their financials. Although the magnitude of the change varies by company and industry, the pattern holds: profits rise sharply. Reducing the defection rate just 5% generates 85% more profits in one bank's branch system, 50% more in an insurance brokerage, and 30% more in an auto-service chain. And when MBNA America, a Delaware-based credit card company, cut its 10% defection rate in half, profits rose a whopping 125%. But defection rates are not just a measure of service quality; they are also a guide for achieving it. By listening to the reasons why customers defect, managers learn exactly where the company is falling short and where to direct their resources. Staples, the stationery supplies retailer, uses feedback from customers to pinpoint products that are priced too high. That way, the company avoids expensive broad-brush promotions that pitch everything to everyone. Like any important change, managing for zero defections requires training and reinforcement. Great-West Life Assurance Company pays a 50% premium to group health-insurance brokers that hit customer-retention targets, and MBNA America gives bonuses to departments that hit theirs.",1990,0,5927,136,0,10,16,17,40,47,42,56,89,88
1d109c5b293c59983f388de75faeb2b49abb606e,"Life itself as well as the entire human economy depends on goods and services provided by earth's natural systems. The processes of cleansing, recycling, and renewal, along with goods such as seafood, forage, and timber, are worth many trillions of dollars annually, and nothing could live without them. Yet growing human impacts on the environment are profoundly disrupting the functioning of natural systems and imperiling the delivery of these services.Nature's Services brings together world-renowned scientists from a variety of disciplines to examine the character and value of ecosystem services, the damage that has been done to them, and the consequent implications for human society. Contributors including Paul R. Ehrlich, Donald Kennedy, Pamela A. Matson, Robert Costanza, Gary Paul Nabhan, Jane Lubchenco, Sandra Postel, and Norman Myers present a detailed synthesis of our current understanding of a suite of ecosystem services and a preliminary assessment of their economic value. Chapters consider: major services including climate regulation, soil fertility, pollination, and pest control philosophical and economic issues of valuation case studies of specific ecosystems and services implication of recent findings and steps that must be taken to address the most pressing concerns Nature's Services represents one of the first efforts by scientists to provide an overview of the many benefits and services that nature offers to people and the extent to which we are all vitally dependent on those services. The book enhances our understanding of the value of the natural systems that surround us and can play an essential role in encouraging greater efforts to protect the earth's basic life-support systems before it is too late. -- publisher's description",1998,0,3671,124,34,55,58,60,100,83,82,101,139,147
5541ab96c84e9fbe56c0be28631b682608ca9b40,"This letter is in response to your two Citizen Petitions dated November 17, 1994 and May 13, 2008, requesting that the Food and Drug Administration (FDA or the Agency) require a cancer warning on cosmetic talc products. Your 1994 Petition requests that all cosmetic talc bear labels with a warning such as ""Talcum powder causes cancer in laboratory animals. Frequent talc application in the female genital area increases the risk of ovarian cancer."" Additionally, your 2008 Petition requests that cosmetic talcum powder products bear labels with a prominent warning such as: ""Frequent talc application in the female genital area is responsible for major risks of ovarian cancer."" Further, both of your Petitions specifically request, pursuant to 21 CFR 1 0.30(h)(2), a hearing for you to present scientific evidence in support of this petition.",1999,141,9317,33,52,92,115,158,177,225,227,279,299,330
22f33f9d67edbc1ab9a8da239bfe7a464337db24,"This new edition of Ann Bowling's well-known and highly respected text has been thoroughly revised and updated to reflect key methodological developments in health research. It is a comprehensive, easy to read, guide to the range of methods used to study and evaluate health and health services. It describes the concepts and methods used by the main disciplines involved in health research, including: demography, epidemiology, health economics, psychology and sociology.The research methods described cover the assessment of health needs, morbidity and mortality trends and rates, costing health services, sampling for survey research, cross-sectional and longitudinal survey design, experimental methods and techniques of group assignment, questionnaire design, interviewing techniques, coding and analysis of quantitative data, methods and analysis of qualitative observational studies, and types of unstructured interviewing. With new material on topics such as cluster randomization, utility analyses, patients' preferences, and perception of risk, the text is aimed at students and researchers of health and health services. It has also been designed for health professionals and policy makers who have responsibility for applying research findings in practice, and who need to know how to judge the value of that research",1997,9,2685,210,0,5,11,17,33,43,55,73,100,69
4db2daec1fb9c1e8c49d0c549ea2c3af940485ba,"Abstract The concept of ecosystems services has become an important model for linking the functioning of ecosystems to human welfare. Understanding this link is critical for a wide-range of decision-making contexts. While there have been several attempts to come up with a classification scheme for ecosystem services, there has not been an agreed upon, meaningful and consistent definition for ecosystem services. In this paper we offer a definition of ecosystem services that is likely to be operational for ecosystem service research and several classification schemes. We argue that any attempt at classifying ecosystem services should be based on both the characteristics of the ecosystems of interest and a decision context for which the concept of ecosystem services is being mobilized. Because of this there is not one classification scheme that will be adequate for the many contexts in which ecosystem service research may be utilized. We discuss several examples of how classification schemes will be a function of both ecosystem and ecosystem service characteristics and the decision-making context.",2009,62,2451,155,19,80,102,157,195,237,263,275,244,231
a556ba6f4ae669b253de9a4a7cfa25f3d7b58742,"Pfam is a database of protein families that currently contains 7973 entries (release 18.0). A recent development in Pfam has enabled the grouping of related families into clans. Pfam clans are described in detail, together with the new associated web pages. Improvements to the range of Pfam web tools and the first set of Pfam web services that allow programmatic access to the database and associated tools are also presented. Pfam is available on the web in the UK (), the USA (), France () and Sweden ().",2005,17,2185,207,3,76,350,472,315,180,134,98,81,69
251b03a94b29698df843c0f5c2ffbf2d17fd213e,"The question of the existence of competition among auditors has been the subject of considerable discussion in recent years. More specifically, the ""Big Eight"" firms as a group have been accused of monopolizing the market for audits {Staff Study of the Subcommittee on Reports, Accounting and Management of the Senate Committee on Government Operations [1977]). However, evidence on the issue is scanty and typically anecdotal (e.g., Bernstein [1978]). The evidence of the Staff Study itself is limited to concentration statistics, with the allegations relying on what has come to be called the ""concentration doctrine"" (Demsetz [1973]). According to this doctrine, supplier concentration is a reliable indicator of supplier behavior and performance. In this paper, I provide evidence from a test of the hypothesis that price competition prevails throughout the market for the audits of publicly held companies, irrespective of the share of a market segment which is serviced by the Big Eight firms. The evidence is based on an examination of a sample cross-section of audit fees.",1980,11,2265,399,0,2,2,2,5,1,5,2,8,1
1f101fbd343044499ba2bab28d05d1eaa268e7dd,"Management literature is almost unanimous in suggesting to manufacturers that they should integrate services into their core product offering. The literature, however, is surprisingly sparse in describing to what extent services should be integrated, how this integration should be carried out, or in detailing the challenges inherent in the transition to services. Reports on a study of 11 capital equipment manufacturers developing service offerings for their products. Focuses on identifying the dimensions considered when creating a service organization in the context of a manufacturing firm, and successful strategies to navigate the transition. Analysis of qualitative data suggests that the transition involves a deliberate developmental process to build capabilities as firms shift the nature of the relationship with the product end‐users and the focus of the service offering. The report concludes identifying implications of our findings for further research and practitioners.",2003,40,2178,265,5,13,37,22,43,62,111,118,155,152
1065f1c73c538a8d4b017af1825967e1fab1bf52,"Advances in sensing and tracking technology enable location-based applications but they also create significant privacy risks. Anonymity can provide a high degree of privacy, save service users from dealing with service providers’ privacy policies, and reduce the service providers’ requirements for safeguarding private information. However, guaranteeing anonymous usage of location-based services requires that the precise location information transmitted by a user cannot be easily used to re-identify the subject. This paper presents a middleware architecture and algorithms that can be used by a centralized location broker service. The adaptive algorithms adjust the resolution of location information along spatial or temporal dimensions to meet specified anonymity constraints based on the entities who may be using location services within a given area. Using a model based on automotive traffic counts and cartographic material, we estimate the realistically expected spatial resolution for different anonymity constraints. The median resolution generated by our algorithms is 125 meters. Thus, anonymous location-based requests for urban areas would have the same accuracy currently needed for E-911 services; this would provide sufficient resolution for wayfinding, automated bus routing services and similar location-dependent services.",2003,47,2364,188,8,20,45,63,97,112,151,148,150,178
7d32e764672117c2d5bce2c69dee737c6e88b719,"The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that location of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL.We propose a solution based on DAML-S, a DAML-based language for service description, and we show how service capabilities are presented in the Profile section of a DAML-S description and how a semantic match between advertisements and requests is performed.",2002,20,2572,165,17,105,179,200,251,274,249,229,230,167
1741947f385ae50bbb50c879655ada49a45860ba,"Internet-delivered e-services are increasingly being made available to consumers; however, little is known about how consumers evaluate them for potential adoption. Past Technology Adoption Research has focused primarily on the positive utility gains attributable to system adoption. This research extends that approach to include measures of negative utility (potential losses) attributable to e-service adoption. Drawing from Perceived Risk Theory, specific risk facets were operationalized, integrated, and empirically tested within the Technology Acceptance Model resulting in a proposed e-services adoption model. Results indicated that e-services adoption is adversely affected primarily by performance-based risk perceptions, and perceived ease of use of the e-service reduced these risk concerns. Implications of integrating perceived risk into the proposed e-services adoption model are discussed.",2002,67,2025,186,1,8,8,18,30,43,37,65,72,96
3649fdff0b991e7a5bec42318809c3d50f660690,"Like many other incipient technologies, Web services are still surrounded by a substantial level of noise. This noise results from the always dangerous combination of wishful thinking on the part of research and industry and of a lack of clear understanding of how Web services came to be. On the one hand, multiple contradictory interpretations are created by the many attempts to realign existing technology and strategies with Web services. On the other hand, the emphasis on what could be done with Web services in the future often makes us lose track of what can be really done with Web services today and in the short term. These factors make it extremely difficult to get a coherent picture of what Web services are, what they contribute, and where they will be applied.Alonso and his co-authors deliberately take a step back. Based on their academic and industrial experience with middleware and enterprise application integration systems, they describe the fundamental concepts behind the notion of Web services and present them as the natural evolution of conventional middleware, necessary to meet the challenges of the Web and of B2B application integration. Rather than providing a reference guide or a ""how to write your first Web service"" kind of book, they discuss the main objectives of Web services, the challenges that must be faced to achieve them, and the opportunities that this novel technology provides. Established, as well as recently proposed, standards and techniques (e.g., WSDL, UDDI, SOAP, WS-Coordination, WS-Transactions, and BPEL), are then examined in the context of this discussion in order to emphasize their scope, benefits, and shortcomings. Thus, the book is ideally suited both for professionals considering the development of application integration solutions and for research and students interesting in understanding and contributing to the evolution of enterprise application technologies.",2009,0,1860,138,215,176,164,137,100,108,79,42,33,18
fbfae405e9144274063149c40566da25055dd855,The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models—Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.,1990,55,2487,127,7,9,27,34,46,44,36,28,46,53
0531bb2ecef7a42909719310b23757740ded95f1,"This research examines the benefits customers receive as a result of engaging in long-term relational exchanges with service firms. Findings from two studies indicate that consumer relational benefits can be categorized into three distinct benefit types: confidence, social, and special treatment benefits. Confidence benefits are received more and rated as more important than the other relational benefits by consumers, followed by social and special treatment benefits, respectively. Responses segmented by type of service business show a consistent pattern with respect to customer rankings of benefit importance. Management implications for relational strategies and future research implications of the findings are discussed.",1998,49,2332,168,2,9,28,37,35,48,57,60,75,99
f1b109e548e0ec8434ae5c1495553bc1ff82c4c8,"Aims of the Paper 
The principal aims of this paper are (1) to increase professional health workers’ knowledge of selected research findings and theory so that they may better understand why and under what conditions people take action to prevent, detect and diagnose disease; and (2) to increase awareness among qualified behavioral scientists about the kinds of behavioral research opportunities and needs that exist in public health. 
 
A matter of personal philosophy of the author is that the goal of understanding and predicting behavior should appropriately precede the goal of attempting to persuade people to modify their health practices, even though behavior can sometimes be changed in a planned way without clear understanding of its original causes. Efforts to modify behavior will ultimately be more successful if they grow out of an understanding of causal processes. Accordingly, primary attention will here be given to an effort to understand why people behave as they do. Only then will brief consideration be given to problems of how to persuade people to use health services.",1966,67,2711,239,0,1,1,7,7,5,12,12,17,12
681b6891fca9fab4b6c968ebe6b889fd5d60db7c,"OBJECTIVE
To provide practical strategies for conducting and evaluating analyses of qualitative data applicable for health services researchers. DATA SOURCES AND DESIGN: We draw on extant qualitative methodological literature to describe practical approaches to qualitative data analysis. Approaches to data analysis vary by discipline and analytic tradition; however, we focus on qualitative data analysis that has as a goal the generation of taxonomy, themes, and theory germane to health services research.


PRINCIPLE FINDINGS
We describe an approach to qualitative data analysis that applies the principles of inductive reasoning while also employing predetermined code types to guide data analysis and interpretation. These code types (conceptual, relationship, perspective, participant characteristics, and setting codes) define a structure that is appropriate for generation of taxonomy, themes, and theory. Conceptual codes and subcodes facilitate the development of taxonomies. Relationship and perspective codes facilitate the development of themes and theory. Intersectional analyses with data coded for participant characteristics and setting codes can facilitate comparative analyses.


CONCLUSIONS
Qualitative inquiry can improve the description and explanation of complex, real-world phenomena pertinent to health services research. Greater understanding of the processes of qualitative data analysis can be helpful for health services researchers as they use these methods themselves or collaborate with qualitative researchers from a wide range of disciplines.",2007,72,2266,52,3,23,29,52,68,99,122,146,216,242
a2bfe963ce2168cecaad21d2e66059662e996f70,"This article examines some advantages and disadvantages of conducting online survey research. It explores current features, issues, pricing, and limitations associated with products and services, such as online questionnaire features and services to facilitate the online survey process, such as those offered by web survey businesses. The review shows that current online survey products and services can vary considerably in terms of available features, consumer costs, and limitations. It is concluded that online survey researchers should conduct a careful assessment of their research goals, research timeline, and financial situation before choosing a specific product or service.",2006,48,2106,130,8,15,27,45,68,102,114,139,157,184
72e0802c917104b36a396135bcb5876494abe00e,"This document defines a notation for specifying business process behavior based on Web Services. This notation is called Business Process Execution Language for Web Services (abbreviated to BPEL4WS in the rest of this document). Processes in BPEL4WS export and import functionality by using Web Service interfaces exclusively. Business processes can be described in two ways. Executable business processes model actual behavior of a participant in a business interaction. Business protocols, in contrast, use process descriptions that specify the mutually visible message exchange behavior of each of the parties involved in the protocol, without revealing their internal behavior. The process descriptions for business protocols are called abstract processes. BPEL4WS is meant to be used to model the behavior of both executable and abstract processes. BPEL4WS provides a language for the formal specification of business processes and business interaction protocols. By doing so, it extends the Web services interaction model and enables it to support business transactions. BPEL4WS defines an interoperable integration model that should facilitate the expansion of automated process integration in both the intracorporate and the business-to-business spaces. Status of this Document This is an initial public draft release of the BPEL4WS specification. We anticipate a number of extensions to the feature set of BPEL4WS that are discussed briefly at the end of the document. BPEL4WS represents a convergence of the ideas in the XLANG and WSFL specifications. Both XLANG and WSFL are superseded by the BPEL4WS specification. BPEL4WS and related specifications are provided as-is and for review and evaluation only. BEA, IBM and Microsoft hope to solicit your contributions and suggestions in the near future. BEA, IBM and Microsoft make no warrantees or representations regarding the specifications in any manner whatsoever.",2003,0,2127,161,112,187,282,364,345,247,145,120,98,54
0bd82f5b93c8e2e52d584c3c9bd126fee243e61d,"Humanity is increasingly urban, but continues to depend on Nature for its survival. Cities are dependent on the ecosystems beyond the city limits, but also benefit from internal urban ecosystems. The aim of this paper is to analyze the ecosystem services generated by ecosystems within the urban area. ‘Ecosystem services’ refers to the benefits human populations derive from ecosystems. Seven different urban ecosystems have been identified: street trees; lawns:parks; urban forests; cultivated land; wetlands; lakes:sea; and streams. These systems generate a range of ecosystem services. In this paper, six local and direct services relevant for Stockholm are addressed: air filtration, micro climate regulation, noise reduction, rainwater drainage, sewage treatment, and recreational and cultural values. It is concluded that the locally generated ecosystem services have a substantial impact on the quality-of-life in urban areas and should be addressed in land-use planning. © 1999 Elsevier Science B.V. All rights reserved.",1999,39,2194,94,0,2,4,3,7,4,27,23,43,45
85a4155ee7d04554df0821d28a7a1fe0469beda3,"Concern is growing about the consequences of biodiversity loss for ecosystem functioning, for the provision of ecosystem services, and for human well being. Experimental evidence for a relationship between biodiversity and ecosystem process rates is compelling, but the issue remains contentious. Here, we present the first rigorous quantitative assessment of this relationship through meta-analysis of experimental work spanning 50 years to June 2004. We analysed 446 measures of biodiversity effects (252 in grasslands), 319 of which involved primary producer manipulations or measurements. Our analyses show that: biodiversity effects are weaker if biodiversity manipulations are less well controlled; effects of biodiversity change on processes are weaker at the ecosystem compared with the community level and are negative at the population level; productivity-related effects decline with increasing number of trophic links between those elements manipulated and those measured; biodiversity effects on stability measures ('insurance' effects) are not stronger than biodiversity effects on performance measures. For those ecosystem services which could be assessed here, there is clear evidence that biodiversity has positive effects on most. Whilst such patterns should be further confirmed, a precautionary approach to biodiversity management would seem prudent in the meantime.",2006,148,2169,93,4,32,83,100,108,134,171,178,205,201
1a15e7ee67b3e34fc7277341a0574daaac60af27,"Human-dominated marine ecosystems are experiencing accelerating loss of populations and species, with largely unknown consequences. We analyzed local experiments, long-term regional time series, and global fisheries data to test how biodiversity loss affects marine ecosystem services across temporal and spatial scales. Overall, rates of resource collapse increased and recovery potential, stability, and water quality decreased exponentially with declining diversity. Restoration of biodiversity, in contrast, increased productivity fourfold and decreased variability by 21%, on average. We conclude that marine biodiversity loss is increasingly impairing the ocean's capacity to provide food, maintain water quality, and recover from perturbations. Yet available data suggest that at this point, these trends are still reversible.",2006,102,2405,39,4,71,128,140,153,177,163,186,150,156
a258380b26971406230aaf017bb3e3d473d1c849,"Excellent service is the foundation for services marketing, contend Leonard Berry and A. Parasuraman in this companion volume to ""Delivering Quality Service."" Building on eight years of research, the authors develop a model for understanding the relationship between quality and marketing in services and offer dozens of practical insights into ways to improve services marketing. They argue that superior service cannot be manufactured in a factory, packaged, and delivered intact to customers. Though an innovative service concept may give a company an initial edge, superior quality is vital to sustaining success. Berry and Parasuraman show that inspired leadership, a customer-minded corporate culture, an excellent service-system design, and effective use of technology and information are crucial to superior service quality and services marketing. When a company's service is excellent, customers are more likely to perceive value in transactions, spread favorable word-of-mouth impressions, and respond positively to employee-cross-selling efforts. The authors point out that a service company that does relatively little pre-sales marketing but is truly dedicated to delivering excellent quality service will have greater marketing effectiveness, higher customer retention, and more sales to existing customers than a company that emphasizes pre-sale marketing but falls short during actual service delivery. The focus of any company, they insist, must be customer satisfaction through integration of service quality throughout the entire system. Filled with examples, stories, and insights from senior executives, Berry and Parasuraman's new framework for effective marketing servicescontains the key to high-performance services marketing.",1991,0,2280,117,0,3,13,24,35,31,34,52,41,51
0a954386b36717010e3b07391da43916afa72ad1,"Abstract. Electronic government, or e‐government, increases the convenience and accessibility of government services and information to citizens. Despite the benefits of e‐government – increased government accountability to citizens, greater public access to information and a more efficient, cost‐effective government – the success and acceptance of e‐government initiatives, such as online voting and licence renewal, are contingent upon citizens’ willingness to adopt this innovation. In order to develop ‘citizen‐centred’ e‐government services that provide participants with accessible, relevant information and quality services that are more expedient than traditional ‘brick and mortar’ transactions, government agencies must first understand the factors that influence citizen adoption of this innovation. This study integrates constructs from the Technology Acceptance Model, Diffusions of Innovation theory and web trust models to form a parsimonious yet comprehensive model of factors that influence citizen adoption of e‐government initiatives. The study was conducted by surveying a broad diversity of citizens at a community event. The findings indicate that perceived ease of use, compatibility and trustworthiness are significant predictors of citizens’ intention to use an e‐government service. Implications of this study for research and practice are presented.",2005,57,1853,201,4,20,23,58,106,103,128,146,133,142
3283f3ca81ff395643ca49722d522e3606eca006,"Nature provides a wide range of benefits to people. There is increasing consensus about the importance of incorporating these “ecosystem services” into resource management decisions, but quantifying the levels and values of these services has proven difficult. We use a spatially explicit modeling tool, Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST), to predict changes in ecosystem services, biodiversity conservation, and commodity production levels. We apply InVEST to stakeholder-defined scenarios of land-use/land-cover change in the Willamette Basin, Oregon. We found that scenarios that received high scores for a variety of ecosystem services also had high scores for biodiversity, suggesting there is little tradeoff between biodiversity conservation and ecosystem services. Scenarios involving more development had higher commodity production values, but lower levels of biodiversity conservation and ecosystem services. However, including payments for carbon sequestration alleviates this tradeoff. Quantifying ecosystem services in a spatially explicit manner, and analyzing tradeoffs between them, can help to make natural resource decisions more effective, efficient, and defensible.",2009,85,1881,75,28,71,97,136,191,196,182,185,178,189
baebed6da6c8a7ddda676fb8b0d98d5d2168f219,"The Millennium Ecosystem Assessment (MA) introduced a new framework for analyzing social–ecological systems that has had wide influence in the policy and scientific communities. Studies after the MA are taking up new challenges in the basic science needed to assess, project, and manage flows of ecosystem services and effects on human well-being. Yet, our ability to draw general conclusions remains limited by focus on discipline-bound sectors of the full social–ecological system. At the same time, some polices and practices intended to improve ecosystem services and human well-being are based on untested assumptions and sparse information. The people who are affected and those who provide resources are increasingly asking for evidence that interventions improve ecosystem services and human well-being. New research is needed that considers the full ensemble of processes and feedbacks, for a range of biophysical and social systems, to better understand and manage the dynamics of the relationship between humans and the ecosystems on which they rely. Such research will expand the capacity to address fundamental questions about complex social–ecological systems while evaluating assumptions of policies and practices intended to advance human well-being through improved ecosystem services.",2009,79,1745,75,29,75,101,119,178,189,207,167,160,154
a20cd573efefe1eac59f51faf28e0fbc18d69e35,"Over the past decade, efforts to value and protect ecosystem services have been promoted by many as the last, best hope for making conservation mainstream – attractive and commonplace worldwide. In theory, if we can help individuals and institutions to recognize the value of nature, then this should greatly increase investments in conservation, while at the same time fostering human well-being. In practice, however, we have not yet developed the scientific basis, nor the policy and finance mechanisms, for incorporating natural capital into resource- and land-use decisions on a large scale. Here, we propose a conceptual framework and sketch out a strategic plan for delivering on the promise of ecosystem services, drawing on emerging examples from Hawai‘i. We describe key advances in the science and practice of accounting for natural capital in the decisions of individuals, communities, corporations, and governments.",2009,112,1667,78,15,48,81,106,151,187,193,142,168,159
f81183ede9e5455e82bad824718540fce6f2d1a7,"Ecosystem management that attempts to maximize the production of one ecosystem service often results in substantial declines in the provision of other ecosystem services. For this reason, recent studies have called for increased attention to development of a theoretical understanding behind the relationships among ecosystem services. Here, we review the literature on ecosystem services and propose a typology of relationships between ecosystem services based on the role of drivers and the interactions between services. We use this typology to develop three propositions to help drive ecological science towards a better understanding of the relationships among multiple ecosystem services. Research which aims to understand the relationships among multiple ecosystem services and the mechanisms behind these relationships will improve our ability to sustainably manage landscapes to provide multiple ecosystem services.",2009,70,1550,100,2,20,42,70,106,113,147,177,177,158
caf04b76596bf1da40ec442edb6060dcf3927180,,2001,0,2451,15,22,34,56,56,75,74,105,108,132,167
a54438cc19171b4ba9380ba9a6443532e82357c0,"Payments for environmental services (PES) are part of a new and more direct conservation paradigm, explicitly recognizing the need to bridge the interests of landowners and outsiders. Eloquent theoretical assessments have praised the absolute advantages of PES over traditional conservation approaches. Some pilot PES exist in the tropics, but many field practitioners and prospective service buyers and sellers remain skeptical about the concept. This paper aims to help demystify PES for non-economists, starting with a simple and coherent definition of the term. It then provides practical ‘how-to' hints for PES design. It considers the likely niche for PES in the portfolio of conservation approaches. This assessment is based on a literature review, combined with field observations from research in Latin America and Asia. It concludes that service users will continue to drive PES, but their willingness to pay will only rise if schemes can demonstrate clear additionality vis-a-vis carefully established baselines, if trust-building processes with service providers are sustained, and PES recipients' livelihood dynamics is better understood. PES best suits intermediate and/or projected threat scenarios, often in marginal lands with moderate conservation opportunity costs. People facing credible but medium-sized environmental degradation are more likely to become PES recipients than those living in relative harmony with Nature. The choice between PES cash and in-kind payments is highly context-dependent. Poor PES recipients are likely to gain from participation, though their access might be constrained and non-participating landless poor could lose out. PES is a highly promising conservation approach that can benefit buyers, sellers and improve the resource base, but it is unlikely to completely outstrip other conservation instruments.",2005,83,1607,196,10,19,62,70,98,102,121,133,156,139
916743e3f5d7ac258aa4af94b7b588414f0f0dd9,"More and more corporations throughout the world are adding value to their core corporate offerings through services. The trend is pervading almost all industries, is customer demand-driven, and perceived by corporations as sharpening their competitive edges. Modern corporations are increasingly offering fuller market packages or ""bundles"" of customer-focussed combinations of goods, services, support, self-service, and knowledge. But services are beginning to dominate. This movement is termed the ""servitization of business"" by authors Sandra Vandermerwe and Juan Rada, and is clearly a powerful new feature of total market strategy being adopted by the best companies. It is leading to new relationships between them and their customers. Giving many real-life examples, the authors assess the main motives driving corporations to servitization, and point out that its cumulative effects are changing the competitive dynamics in which managers will have to operate. The special challenge for top managers is how to blend services into the overall strategies of the company.",1988,0,2006,119,0,2,1,1,1,0,5,4,1,2
bce78fc8dc981e231293be5d13348a1e03b10efa,"Differentiated services enhancements to the Internet protocol are intended to enable scalable service discrimination in the Internet without the need for per-flow state and signaling at every hop. A variety of services may be built from a small, well-defined set of building blocks which are deployed in network nodes. The services may be either end-to-end or intra-domain; they include both those that can satisfy quantitative performance requirements (e.g., peak bandwidth) and those based on relative performance (e.g., ""class"" differentiation). Services can be constructed by a combination of:",1998,10,1939,100,11,94,133,162,157,160,114,113,114,94
2f40193b0dff1a6015b8b3229cdfcbe889ad9e16,"CONTEXT
The US military has conducted population-level screening for mental health problems among all service members returning from deployment to Afghanistan, Iraq, and other locations. To date, no systematic analysis of this program has been conducted, and studies have not assessed the impact of these deployments on mental health care utilization after deployment.


OBJECTIVES
To determine the relationship between combat deployment and mental health care use during the first year after return and to assess the lessons learned from the postdeployment mental health screening effort, particularly the correlation between the screening results, actual use of mental health services, and attrition from military service.


DESIGN, SETTING, AND PARTICIPANTS
Population-based descriptive study of all Army soldiers and Marines who completed the routine postdeployment health assessment between May 1, 2003, and April 30, 2004, on return from deployment to Operation Enduring Freedom in Afghanistan (n = 16,318), Operation Iraqi Freedom (n = 222,620), and other locations (n = 64,967). Health care utilization and occupational outcomes were measured for 1 year after deployment or until leaving the service if this occurred sooner.


MAIN OUTCOME MEASURES
Screening positive for posttraumatic stress disorder, major depression, or other mental health problems; referral for a mental health reason; use of mental health care services after returning from deployment; and attrition from military service.


RESULTS
The prevalence of reporting a mental health problem was 19.1% among service members returning from Iraq compared with 11.3% after returning from Afghanistan and 8.5% after returning from other locations (P<.001). Mental health problems reported on the postdeployment assessment were significantly associated with combat experiences, mental health care referral and utilization, and attrition from military service. Thirty-five percent of Iraq war veterans accessed mental health services in the year after returning home; 12% per year were diagnosed with a mental health problem. More than 50% of those referred for a mental health reason were documented to receive follow-up care although less than 10% of all service members who received mental health treatment were referred through the screening program.


CONCLUSIONS
Combat duty in Iraq was associated with high utilization of mental health services and attrition from military service after deployment. The deployment mental health screening program provided another indicator of the mental health impact of deployment on a population level but had limited utility in predicting the level of mental health services that were needed after deployment. The high rate of using mental health services among Operation Iraqi Freedom veterans after deployment highlights challenges in ensuring that there are adequate resources to meet the mental health needs of returning veterans.",2006,32,1868,110,23,59,93,133,133,178,179,146,160,155
737337d2d4761d966b3c8b9176d51ecab6a186d1,"This paper advocates consistently defined units of account to measure the contributions of nature to human welfare. We argue that such units have to date not been defined by environmental accounting advocates and that the term ""ecosystem services"" is too ad hoc to be of practical use in welfare accounting. We propose a definition, rooted in economic principles, of ecosystem service units. A goal of these units is comparability with the definition of conventional goods and services found in GDP and the other national accounts. We illustrate our definition of ecological units of account with concrete examples. We also argue that these same units of account provide an architecture for environmental performance measurement by governments, conservancies, and environmental markets.",2006,51,1810,94,7,16,34,41,78,121,144,158,180,187
8aae732e07c9d14f08ccf31eb9b742683de19595,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity; large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior. We present an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.",2001,46,1724,117,31,86,161,177,186,177,149,122,100,112
bdcb0b64e9142fa934831a13e94375bbca0b71f6,"After initial interviews with 20,291 adults in the National Institute of Mental Health Epidemiologic Catchment Area Program, we estimated prospective 1-year prevalence and service use rates of mental and addictive disorders in the US population. An annual prevalence rate of 28.1% was found for these disorders, composed of a 1-month point prevalence of 15.7% (at wave 1) and a 1-year incidence of new or recurrent disorders identified in 12.3% of the population at wave 2. During the 1-year follow-up period, 6.6% of the total sample developed one or more new disorders after being assessed as having no previous lifetime diagnosis at wave 1. An additional 5.7% of the population, with a history of some previous disorder at wave 1, had an acute relapse or suffered from a new disorder in 1 year. Irrespective of diagnosis, 14.7% of the US population in 1 year reported use of services in one or more component sectors of the de facto US mental and addictive service system. With some overlap between sectors, specialists in mental and addictive disorders provided treatment to 5.9% of the US population, 6.4% sought such services from general medical physicians, 3.0% sought these services from other human service professionals, and 4.1% turned to the voluntary support sector for such care. Of those persons with any disorder, only 28.5% (8.0 per 100 population) sought mental health/addictive services. Persons with specific disorders varied in the proportion who used services, from a high of more than 60% for somatization, schizophrenia, and bipolar disorders to a low of less than 25% for addictive disorders and severe cognitive impairment. Applications of these descriptive data to US health care system reform options are considered in the context of other variables that will determine national health policy.",1993,34,1988,80,7,24,51,55,64,66,74,86,67,111
b756107971906fc319a94da0b247e04607b63225,"Execution Language Working Draft 01, 16 22 October 23 November 2003 Document identifier: wsbpel-specification-draft-01 (XML, HTML, PDF) Location: http://www.oasis-open.org/apps/org/workgroup/wsbpel/ Editors: Ben Bloch <ben_b54@hotmail.com> Francisco Curbera, IBM <curbera@us.ibm.com> Yaron Goland, BEA <ygoland@bea.com> Neelakantan Kartha, Sterling Commerce <N_Kartha@stercomm.com> Canyang Kevin Liu, SAP <kevin.liu@sap.com> Satish Thatte, Microsoft <satisht@microsoft.com> Prasad Yendluri, webMethods <pyendluri@webmethods.com> Editor’s Notes – KevinL – list needs to be updated to include all editors Contributors:",2009,15,1202,120,174,137,125,110,96,77,53,31,30,18
4e547d5df2ef5de3fa2f91db0f11c93f4682a2a8,"Abstract In this article we focus on the vital ecological services provided by insects. We restrict our focus to services provided by “wild” insects; we do not include services from domesticated or mass-reared insect species. The four insect services for which we provide value estimates—dung burial, pest control, pollination, and wildlife nutrition—were chosen not because of their importance but because of the availability of data and an algorithm for their estimation. We base our estimations of the value of each service on projections of losses that would accrue if insects were not functioning at their current level. We estimate the annual value of these ecological services provided in the United States to be at least $57 billion, an amount that justifies greater investment in the conservation of these services.",2006,99,1420,143,7,19,39,51,61,62,93,107,106,115
6435a4806428a18bbeb046668e6794d2bba47c34,"""Every developer working with the Web needs to read this book."" -- David Heinemeier Hansson, creator of the Rails framework ""RESTful Web Services finally provides a practical roadmap for constructing services that embrace the Web, instead of trying to route around it."" -- Adam Trachtenberg, PHP author and EBay Web Services Evangelist You've built web sites that can be used by humans. But can you also build web sites that are usable by machines? That's where the future lies, and that's what RESTful Web Services shows you how to do. The World Wide Web is the most popular distributed application in history, and Web services and mashups have turned it into a powerful distributed computing platform. But today's web service technologies have lost sight of the simplicity that made the Web successful. They don't work like the Web, and they're missing out on its advantages. This book puts the ""Web"" back into web services. It shows how you can connect to the programmable web with the technologies you already use every day. The key is REST, the architectural style that drives the Web. This book: Emphasizes the power of basic Web technologies -- the HTTP application protocol, the URI naming standard, and the XML markup languageIntroduces the Resource-Oriented Architecture (ROA), a common-sense set of rules for designing RESTful web servicesShows how a RESTful design is simpler, more versatile, and more scalable than a design based on Remote Procedure Calls (RPC)Includes real-world examples of RESTful web services, like Amazon's Simple Storage Service and the Atom Publishing ProtocolDiscusses web service clients for popular programming languagesShows how to implement RESTful services in three popular frameworks -- Ruby on Rails, Restlet (for Java), and Django (for Python)Focuses on practical issues: how to design and implement RESTful web services and clients This is the first book that applies the REST design philosophy to real web services. It sets down the best practices you need to make your design a success, and the techniques you need to turn your design into working code. You can harness the power of the Web for programmable applications: you just have to work with the Web instead of against it. This book shows you how.",2007,0,1411,113,5,61,90,132,167,137,146,172,129,109
3741254fcde051b5512143b1a01b57385ca87c44,The diversity of the service sector makes it difficult to come up with managerially useful generalizations concerning marketing practice in service organizations. This article argues for a focus on specific categories of services and proposes five schemes for classifying services in ways that transcend narrow industry boundaries. In each instance insights are offered into how the nature of the service might affect the marketing task.,1983,19,1999,81,0,3,6,6,9,8,9,17,12,17
ce2d38350ef4dad58589ac3de3cdaad1645818a8,"The U.S. Preventive Services Task Force (USPSTF/Task Force) represents one of several efforts to take a more evidence-based approach to the development of clinical practice guidelines. As methods have matured for assembling and reviewing evidence and for translating evidence into guidelines, so too have the methods of the USPSTF. This paper summarizes the current methods of the third USPSTF, supported by the Agency for Healthcare Research and Quality (AHRQ) and two of the AHRQ Evidence-based Practice Centers (EPCs). The Task Force limits the topics it reviews to those conditions that cause a large burden of suffering to society and that also have available a potentially effective preventive service. It focuses its reviews on the questions and evidence most critical to making a recommendation. It uses analytic frameworks to specify the linkages and key questions connecting the preventive service with health outcomes. These linkages, together with explicit inclusion criteria, guide the literature searches for admissible evidence. Once assembled, admissible evidence is reviewed at three strata: (1) the individual study, (2) the body of evidence concerning a single linkage in the analytic framework, and (3) the body of evidence concerning the entire preventive service. For each stratum, the Task Force uses explicit criteria as general guidelines to assign one of three grades of evidence: good, fair, or poor. Good or fair quality evidence for the entire preventive service must include studies of sufficient design and quality to provide an unbroken chain of evidence-supported linkages, generalizable to the general primary care population, that connect the preventive service with health outcomes. Poor evidence contains a formidable break in the evidence chain such that the connection between the preventive service and health outcomes is uncertain. For services supported by overall good or fair evidence, the Task Force uses outcomes tables to help categorize the magnitude of benefits, harms, and net benefit from implementation of the preventive service into one of four categories: substantial, moderate, small, or zero/negative. The Task Force uses its assessment of the evidence and magnitude of net benefit to make a recommendation, coded as a letter: from A (strongly recommended) to D (recommend against). It gives an I recommendation in situations in which the evidence is insufficient to determine net benefit. The third Task Force and the EPCs will continue to examine a variety of methodologic issues and document work group progress in future communications.",2001,47,1687,88,7,37,50,67,82,93,91,103,114,130
7ba11566fec38a35edf9626d2ac48891db32a831,The purpose of this article is to lay the foundations of a theory that can be used to interpret innovation processes in the service sector. The hypothesis underpinning this article is based on Lancaster's definition of the product (in both manufacturing and services) as a set of service characteristics. The article follows the example of those who have sought to apply Lancaster's work to technological phenomena. Various modes of innovation in the service sectors are highlighted and illustrated.,1997,49,1863,120,0,8,6,19,14,28,23,21,36,46
a14750ba068c664b213c88508037e55283427664,"Increasingly, computing addresses collaboration, data sharing, and interaction modes that involve distributed resources, resulting in an increased focus on the interconnection of systems both within and across enterprises. These evolutionary pressures have led to the development of Grid technologies. The authors' work focuses on the nature of the services that respond to protocol messages. Grid provides an extensible set of services that can be aggregated in various ways to meet the needs of virtual organizations, which themselves can be defined in part by the services they operate and share.",2002,18,1872,72,33,189,251,279,215,172,134,122,109,90
56ce6e97f2090df9e0a8ea2d581017a853b45122,This article compares problems and strategies cited in the services marketing literature with those reported by actual service suppliers in a study conducted by the authors. Discussion centers on several broad themes that emerge from this comparison and on guidelines for future work in services marketing.,1985,66,2117,39,0,5,5,7,17,17,19,13,28,17
462570ca88f5ea8da5a3d46b480b378dd4bf8382,"Assessing Ecological Restoration In the wake of the Millennium Ecosystem Assessment, the analysis of ecosystem services, and their relationship to biodiversity, has become one of the most rapidly developing research themes in environmental science. At the same time, ecological restoration is widely being implemented as a response to environmental degradation and biodiversity loss. Rey Benayas et al. (p. 1121, published online 30 July) link these themes in a meta-analysis of the impacts of ecological restoration actions on provision of ecosystem services and biodiversity conservation. The analysis of 89 published restoration projects worldwide establishes that ecological restoration does, in general, have positive impacts on both biodiversity and provision of ecosystem services. These effects are especially marked in the tropics. Thus, ecological restoration actions may indeed deliver benefits, both in terms of biodiversity conservation and supporting human livelihoods. Restoration, biodiversity, and ecosystem services are positively linked in a wide range of ecosystem types across the globe. Ecological restoration is widely used to reverse the environmental degradation caused by human activities. However, the effectiveness of restoration actions in increasing provision of both biodiversity and ecosystem services has not been evaluated systematically. A meta-analysis of 89 restoration assessments in a wide range of ecosystem types across the globe indicates that ecological restoration increased provision of biodiversity and ecosystem services by 44 and 25%, respectively. However, values of both remained lower in restored versus intact reference ecosystems. Increases in biodiversity and ecosystem service measures after restoration were positively correlated. Results indicate that restoration actions focused on enhancing biodiversity should support increased provision of ecosystem services, particularly in tropical terrestrial biomes.",2009,17,1199,46,1,35,57,74,99,113,109,93,115,135
5b3f813531dad9413f145e911c9042972ac65ce3,"The physician's degree of resourcefulness, i.e., the ability to deal skillfully and promptly with new situations, is important for changing the health behaviors of patients within the constraints of a brief office visit. This quality, however, was in short supply among 15 primary care physicians selected for their interest in preventive medicine. The physicians tended to rely on a single approach for changing specific health behaviors of patients, restricted referrals to community services and other health specialists, relied almost exclusively on fear for motivating patients and expressed considerable pessimism about changing the health behaviors of older patients. The physicians uniformly reported that their inadequate education and the lack of reimbursement influenced how they counseled their patients. A good place to begin to rectify this situation is the required reading of the Guide to Clinical Preventive Services for medical students and residents, and continuing education opportunities for practic...",1993,14,2089,0,21,11,18,36,82,113,122,169,173,165
618343bee669d361102b3f68ad3e586d308a2ed2,"This research investigated the relationship between three elements – core service quality, relational service quality‐ and perceived value – and customer satisfaction and future intentions across four services. The results revealed that core service quality (the promise) and perceived value were the most important drivers of customer satisfaction with relational service quality (the delivery) a significant but less important driver. A direct link between customer satisfaction and future intentions was established. The relative importance of the three drivers of satisfaction varied among services. Specifically, the importance of core service quality and perceived value was reversed depending on the service. A major conclusion was that both perceived value and service quality dimensions should be incorporated into customer satisfaction models to provide a more complete picture of the drivers of satisfaction.",2000,38,1742,72,0,3,11,13,19,22,36,59,64,74
d0fd9c58fb89f1c7b8ce3c66b550f05b5b751bc3,"Nanotechnology is the set of technologies that enables the manipulation, study or exploitation of very small (typically less than 100 nanometres) structures and systems. To put this into perspective, one nanometre is one-billionth of a metre, or around 80,000 times smaller than the diameter of a human hair. Nanotechnology contributes to novel materials, devices and products that demonstrate different properties.",2001,0,6415,196,47,53,76,128,208,246,314,421,556,611
2224928c84a2255da72d81cb268eea47fca29c80,"Although gold is the subject of one of the most ancient themes of investigation in science, its renaissance now leads to an exponentially increasing number of publications, especially in the context of emerging nanoscience and nanotechnology with nanoparticles and self-assembled monolayers (SAMs). We will limit the present review to gold nanoparticles (AuNPs), also called gold colloids. AuNPs are the most stable metal nanoparticles, and they present fascinating aspects such as their assembly of multiple types involving materials science, the behavior of the individual particles, size-related electronic, magnetic and optical properties (quantum size effect), and their applications to catalysis and biology. Their promises are in these fields as well as in the bottom-up approach of nanotechnology, and they will be key materials and building block in the 21st century. Whereas the extraction of gold started in the 5th millennium B.C. near Varna (Bulgaria) and reached 10 tons per year in Egypt around 1200-1300 B.C. when the marvelous statue of Touthankamon was constructed, it is probable that “soluble” gold appeared around the 5th or 4th century B.C. in Egypt and China. In antiquity, materials were used in an ecological sense for both aesthetic and curative purposes. Colloidal gold was used to make ruby glass 293 Chem. Rev. 2004, 104, 293−346",2004,709,9486,25,41,141,206,374,461,471,559,791,792,764
7ba37494ac59afa65acca09a2334a3eb1eafde03,"Nanotechnology is a multidisciplinary field, which covers a vast and diverse array of devices derived from engineering, biology, physics and chemistry. These devices include nanovectors for the targeted delivery of anticancer drugs and imaging contrast agents. Nanowires and nanocantilever arrays are among the leading approaches under development for the early detection of precancerous and malignant lesions from biological fluids. These and other nanodevices can provide essential breakthroughs in the fight against cancer.",2005,170,3925,48,19,85,110,137,185,245,280,316,374,363
48c581b5531239d3a519aef99c8be0b125717d1a,"In this work we briefly describe the most relevant features of WSXM, a freeware scanning probe microscopy software based on MS-Windows. The article is structured in three different sections: The introduction is a perspective on the importance of software on scanning probe microscopy. The second section is devoted to describe the general structure of the application; in this section the capabilities of WSXM to read third party files are stressed. Finally, a detailed discussion of some relevant procedures of the software is carried out.",2007,20,6005,94,58,232,320,340,447,475,509,472,507,492
9eff1caf86b7dd0ba2f3e8c189f7e90b1d7a4e2a,,2002,31,665,24,0,1,7,9,16,25,39,40,36,48
653f0008135664fc9a6d07f5a37420a3e4f0fb88,,2005,621,6548,89,26,135,225,316,337,407,515,553,577,543
26e83f1836da0963587e449a07aad1110fc55e4e,"Abstract In the large field of nanotechnology, polymer matrix based nanocomposites have become a prominent area of current research and development. Exfoliated clay-based nanocomposites have dominated the polymer literature but there are a large number of other significant areas of current and emerging interest. This review will detail the technology involved with exfoliated clay-based nanocomposites and also include other important areas including barrier properties, flammability resistance, biomedical applications, electrical/electronic/optoelectronic applications and fuel cell interests. The important question of the “nano-effect” of nanoparticle or fiber inclusion relative to their larger scale counterparts is addressed relative to crystallization and glass transition behavior. Of course, other polymer (and composite)-based properties derive benefits from nanoscale filler or fiber addition and these are addressed.",2008,285,2698,46,6,45,108,188,211,217,238,277,266,283
e7f7108c95d937344e3a265591a653d745832fa2,"Nanotechnology is the engineering and manufacturing of materials at the atomic and molecular scale. In its strictest definition from the National Nanotechnology Initiative, nanotechnology refers to structures roughly in the 1-100 nm size regime in at least one dimension. Despite this size restriction, nanotechnology commonly refers to structures that are up to several hundred nanometers in size and that are developed by top-down or bottom-up engineering of individual components. Herein, we focus on the application of nanotechnology to drug delivery and highlight several areas of opportunity where current and emerging nanotechnologies could enable entirely novel classes of therapeutics.",2009,37,2415,12,23,66,147,163,206,233,216,260,270,193
e90ea03e84087480384e8e7fa3a65182cfd6e9c0,"A leading physicist delves into relativity and experimental applications Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity offers a Nobel laureate's perspectives on the wealth of data technological developments have brought to expand upon Einstein's theory. Unique in basing relativity on the Principle of Equivalence of Gravitation and Inertia over Riemannian geometry, this book explores relativity experiments and observational cosmology to provide a sound foundation upon which analyses can be made. Covering special and general relativity, tensor analysis, gravitation, curvature, and more, this book provides an engaging, insightful introduction to the forces that shape the universe.",1973,81,4113,456,1,6,3,6,6,10,11,10,10,10
0665b56ad2271286f2a71cfef5c6ab310ea4de50,,1980,0,6692,615,0,1,2,14,10,14,8,16,17,29
f51351e4b07ff1cd5ee4238663a06bb255802c8f,Manifold Theory. Tensors. Semi-Riemannian Manifolds. Semi-Riemannian Submanifolds. Riemannian and Lorenz Geometry. Special Relativity. Constructions. Symmetry and Constant Curvature. Isometries. Calculus of Variations. Homogeneous and Symmetric Spaces. General Relativity. Cosmology. Schwarzschild Geometry. Causality in Lorentz Manifolds. Fundamental Groups and Covering Manifolds. Lie Groups. Newtonian Gravitation.,1983,0,3284,256,0,0,1,2,6,4,7,12,11,11
daf937b2c82ad95e991596036b8d023d43eb4460,"Preface Notation Important formulae and physical constants 1. Introduction 2. Special relativity, non-inertial effects and electromagnetism 3. Differential geometry I: vectors, forms and absolute differentiation 4. Differential geometry II: geodesics and curvature 5. Einstein field equations, the Schwarzschild solution and experimental test of general relativity 6. Gravitomagnetic effects: gyroscopes and clocks 7. Gravitational collapse and black holes 8. Action principles, conservation laws and the Cauchy problem 9. Gravitational radiation 10. Cosmology 11. Gravitation and field theory References Index.",2009,115,77,12,3,3,7,5,3,7,3,5,7,12
673def1212cf6dc2d3026951e46a6904fb293117,"The status of experimental tests of general relativity and of theoretical frameworks for analyzing them is reviewed and updated. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of local Lorentz invariance and clock experiments. Ongoing tests of EEP and of the inverse square law are searching for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light deflection, the Shapiro time delay, the perihelion advance of Mercury, the Nordtvedt effect in lunar motion, and frame-dragging. Gravitational wave damping has been detected in an amount that agrees with general relativity to better than half a percent using the Hulse-Taylor binary pulsar, and a growing family of other binary pulsar systems is yielding new tests, especially of strong-field effects. Current and future tests of relativity will center on strong gravity and gravitational waves.",2001,671,1296,26,0,1,0,0,0,0,0,0,2,2
30f921ce45573aa7e7d0864f1cfe2abf36230cce,"This paper summarizes the author's recently published findings about differences in people's work-related values among 50 countries. In view of these differences, ethnocentric management theories (those based on the value system of one particular country) have become untenable. This concept is illustrated for the fields of leadership, organization, and motivation.",1983,5,3007,269,0,0,3,6,5,6,6,10,8,15
e4636b2555401d5079417aa80ffaf4490cb91ba3,"This article—summarizing the authors’ then novel formulation of General Relativity—appeared as Chap. 7, pp. 227–264, in Gravitation: an introduction to current research, L. Witten, ed. (Wiley, New York, 1962), now long out of print. Intentionally unretouched, this republication as Golden Oldie is intended to provide contemporary accessibility to the flavor of the original ideas. Some typographical corrections have been made: footnote and page numbering have changed–but not section nor equation numbering, etc. Current institutional affiliations are encoded in: arnowitt@physics.tamu.edu, deser@brandeis.edu, misner@umd.edu.",2004,14,1710,90,39,44,63,51,61,64,91,111,106,112
db55ed6b64d35509854623f6d57ca3529bc57c85,1. Special Relativity and Flat Spacetime. 2. Manifolds. 3. Curvature. 4. Gravitation. 5. The Schwarzchild Solution. 6. More General Black Holes. 7. Perturbation Theory and Gravitational Radiation. 8. Cosmology. 9. Quantum Field Theory in Curved Spacetime. 10. Appendicies.,2003,0,1508,173,2,8,15,25,34,60,64,73,83,91
145f08ebcfbb1a5417d77fa7a5c7494bdcf1a95f,"List of contributors Preface 1. An introductory survey S. W. Hawking and W. Israel 2. The confrontation between gravitation theory and experiment C. M. Will 3. Gravitational-radiation experiments D. H. Douglass and V. B. Braginsky 4. The initial value problem and the dynamical formulation of general relativity A. E. Fischer and J. E. Marsden 5. Global structure of spacetimes R. Geroch and G. T. Horowitz 6. The general theory of the mechanical, electromagnetic and thermodynamic properties of black holes B. Carter 7. An introduction to the theory of the Kerr metric and its peturbations S. Chandrasekhar 8. Black hole astrophysics R. D. Blandford and K. S. Thorne 9. The big bang cosmology - enigmas and nostrums R. H. Dicke and P. J. E. Peebles 10. Cosmology and the early universe Ya B. Zel'dovitch 11. Anisotropic and inhomogeneous relativistic cosmologies M. A. H. MacCallum 12. Singularities and time-asymmetry R. Penrose 13. Quantum field theory in curved spacetime G. W. Gibbons 14. Quantum gravity: the new synthesis B. S. DeWitt 15. The path-integral approach to quantum gravity S. W. Hawking 16. Ultraviolet divergences in quantum theories of gravitation S. Weinberg References Index.",1979,0,2119,84,4,9,23,22,26,33,32,26,19,19
78037360d244887bce8e9dd16d451744d91dd03e,,1960,0,2053,171,0,3,4,11,8,13,11,8,25,16
4b58b298114ee0a77b471ee2268072ee63e87c3b,,1969,0,2218,59,1,3,2,3,5,5,9,4,4,9
ef8395964a6b17068d49b6c71344e30ae0c019c1,"According to general relativity, photons are deflected and delayed by the curvature of space-time produced by any mass. The bending and delay are proportional to γ + 1, where the parameter γ is unity in general relativity but zero in the newtonian model of gravity. The quantity γ - 1 measures the degree to which gravity is not a purely geometric effect and is affected by other fields; such fields may have strongly influenced the early Universe, but would have now weakened so as to produce tiny—but still detectable—effects. Several experiments have confirmed to an accuracy of ∼0.1% the predictions for the deflection and delay of photons produced by the Sun. Here we report a measurement of the frequency shift of radio photons to and from the Cassini spacecraft as they passed near the Sun. Our result, γ = 1 + (2.1 ± 2.3) × 10-5, agrees with the predictions of standard general relativity with a sensitivity that approaches the level at which, theoretically, deviations are expected in some cosmological models.",2003,22,1361,89,6,35,59,69,57,56,89,73,78,74
f883cb14548fd2a87b9853628c0a8d08baae982f,"This is an introduction to the by now fifteen years old research field of canonical quantum general relativity, sometimes called ""loop quantum gravity"". The term ""modern"" in the title refers to the fact that the quantum theory is based on formulating classical general relativity as a theory of connections rather than metrics as compared to in original version due to Arnowitt, Deser and Misner. Canonical quantum general relativity is an attempt to define a mathematically rigorous, non-perturbative, background independent theory of Lorentzian quantum gravity in four spacetime dimensions in the continuum. The approach is minimal in that one simply analyzes the logical consequences of combining the principles of general relativity with the principles of quantum mechanics. The requirement to preserve background independence has lead to new, fascinating mathematical structures which one does not see in perturbative approaches, e.g. a fundamental discreteness of spacetime seems to be a prediction of the theory providing a first substantial evidence for a theory in which the gravitational field acts as a natural UV cut-off. An effort has been made to provide a self-contained exposition of a restricted amount of material at the appropriate level of rigour which at the same time is accessible to graduate students with only basic knowledge of general relativity and quantum field theory on Minkowski space.",2007,3,1161,158,19,44,64,89,115,100,98,93,83,89
287dd15fd2862b28db56f81b91a94b82c09cdaad,"AbstractThe status of experimental tests of general relativity and of theoretical frameworks for analysing them are reviewed. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of special relativity, and the gravitational redshift experiment. Future tests of EEP and of the inverse square law will search for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light defl
ection the Shapiro time delay, the perihelion advance of Mercury, and the Nordtvedt effect in lunar motion. Gravitational wave damping has been detected in an amount that agrees with general relativity to half a percent using the Hulse-Taylor binary pulsar, and new binary pulsar systems may yield further improvements. When direct observation of gravitational radiation from astrophysical sources begins, new tests of general relativity will be possible.",2001,346,1195,128,9,7,13,9,15,22,29,41,46,46
61225179ce90adce138d842ada42c2c18ce621f4,"Rapid interstellar travel by means of spacetime wormholes is described in a way that is useful for teaching elementary general relativity. The description touches base with Carl Sagan’s novel Contact, which, unlike most science fiction novels, treats such travel in a manner that accords with the best 1986 knowledge of the laws of physics. Many objections are given against the use of black holes or Schwarzschild wormholes for rapid interstellar travel. A new class of solutions of the Einstein field equations is presented, which describe wormholes that, in principle, could be traversed by human beings. It is essential in these solutions that the wormhole possess a throat at which there is no horizon; and this property, together with the Einstein field equations, places an extreme constraint on the material that generates the wormhole’s spacetime curvature: In the wormhole’s throat that material must possess a radial tension τ0 with the enormous magnitude τ0∼ (pressure at the center of the most massive of ne...",1988,0,1474,100,1,2,6,7,6,6,8,6,6,17
383eedafaec868d6168b13a956c7fc41d8bf2087,"A generalization of Einstein's gravitational theory is discussed in which the spin of matter as well as its mass plays a dynamical role. The spin of matter couples to a non-Riemannian structure in space-time, Cartan's torsion tensor. The theory which emerges from taking this coupling into account, the ${U}_{4}$ theory of gravitation, predicts, in addition to the usual infinite-range gravitational interaction medicated by the metric field, a new, very weak, spin contact interaction of gravitational origin. We summarize here all the available theoretical evidence that argues for admitting spin and torsion into a relativistic gravitational theory. Not least among this evidence is the demonstration that the ${U}_{4}$ theory arises as a local gauge theory for the Poincar\'e group in space-time. The deviations of the ${U}_{4}$ theory from standard general relativity are estimated, and the prospects for further theoretical development are assessed.",1976,146,1860,34,3,12,27,20,46,28,21,35,28,35
bf387e0f57f6f3b878ae09fed558a21b57ff35cb,"SummaryAn approach to shock waves, boundary surfaces and thin shells in general relativity is developed in which their histories are characterized in a purely geometrical way by the extrinsic curvatures of their imbeddings in space-time. There is some gain in simplicity and ease of application over previous treatments in that no mention of « admissible » or, indeed, any space-time co-ordinates is needed. The formalism is applied to a study of the dynamics of thin shells of dust.RiassuntoSi sviluppa un’approssimazione alle onde d’urto, superfici di contorno e strati sottili in relatività generale in cui le loro storie sono caratterizzate in modo puramente geometrico dalle curvature estrinseche delle loro giaciture nello spazio-tempo. Si ha un qualche guadagno in semplicità e facilità di applicazione rispetto alle trattazioni precedenti in quanto non è necessaria alcuna menzione di « ammissibilità » o, nella fattispecie di alcuna coordinata spazio-temporale. Si applica questo formalismo allo studio della dinamica degli strati sottili di polveri.",1966,3,1640,42,0,1,4,2,1,0,2,0,4,3
9284029d7c39a83d285d8a7cb859c5715228e1f3,,1984,0,1418,77,1,4,5,5,7,4,5,5,13,8
3561dcbcfcd9fe7b58a926d1da8515b1c88bb72c,Introduction 1. Elementary principles 2. The tensor calculus 3. The law of gravitation 4. Relativity mechanics 5. Curvature of space and time 6. Electricity 7. World geometry Supplementary notes Bibliography Index.,1924,0,1206,71,4,1,1,2,1,3,0,0,1,3
927285079badad9009fb579f1539cca0db4edba4,"In 1921, five years after the appearance of his comprehensive paper on general relativity and twelve years before he left Europe permanently to join the Institute for Advanced Study, Albert Einstein visited Princeton University, where he delivered the Stafford Little Lectures for that year. These four lectures constituted an overview of his then-controversial theory of relativity. Princeton University Press made the lectures available under the title ""The Meaning of Relativity,"" the first book by Einstein to be produced by an American publisher. As subsequent editions were brought out by the Press, Einstein included new material amplifying the theory. A revised version of the appendix ""Relativistic Theory of the Non-Symmetric Field,"" added to the posthumous edition of 1956, was Einstein's last scientific paper.",1946,0,1650,102,0,0,0,1,1,1,0,1,3,2
07ecafbabe5cb83c52725d554f8c42583f2bebfb,Research data on dominant work-related values patterns in 53 countries and regions are used to suggest how definitions of the quality of life are affected by national culture patterns.,1984,9,1241,102,0,1,5,2,2,6,4,4,6,8
5d9aacb0eea230539e9ea476e87c42e2fcf031e6,"FOREWORD ACKNOWLEDGEMENTS 1. Lorentzian Geometry 2. Special Relativity 3. General Relativity and the Einstein Equations 4. Schwarzschild Space-time and Black Holes 5. Cosmology 6. Local Cauchy Problem 7. Constraints 8. Other Hyperbolic-Elliptic systems 9. Relativistic Fluids 10. Kinetic Theory 11. Progressive Waves 12. Global Hyperbolicity and Causality 13. Singularities 14. Stationary Space-times and Black Holes 15. Global Existence Theorems, Asymptotically Euclidean Data 16. Global existence theorems, cosmological case APPENDICES I. Sobolev Spaces II. Elliptic Systems III. Second Order Quasidiagonal Systems IV. General Hyperbolic Systems V. Cauchy Kovalevski and Fuchs theorems VI. Conformal Methods VII. Kaluza Klein Formulas",2009,0,413,64,9,25,24,24,34,37,48,41,48,27
eef500f59b5d007955e2cf24ad797c8f8116845c,"This paper is divided into four parts. In part A, some general considerations about gravitational radiation are followed by a treatment of the scalar wave equation in the manner later to be applied to Einstein’s field equations. In part B, a co-ordinate system is specified which is suitable for investigation of outgoing gravitational waves from an isolated axi-symmetric reflexion-symmetric system . The metric is expanded in negative powers of a suitably defined radial co-ordinate r, and the vacuum field equations are investigated in detail. It is shown that the flow of information to infinity is controlled by a single function of two variables called the news function. Together with initial conditions specified on a light cone, this function fully defines the behaviour of the system . No constraints of any kind are encountered. In part C, the transformations leaving the metric in the chosen form are determined. An investigation of the corresponding transformations in Minkowski space suggests that no generality is lost by assuming that the transformations, like the metric, may be expanded in negative powers of r. In part D, the mass of the system is defined in a way which in static metrics agrees with the usual definition. The principal result of the paper is then deduced, namely, that the mass of a system is constant if and only if there is no news; if there is news, the mass decreases monotonically so long as it continues. The linear approximation is next discussed, chiefly for its heuristic value, and employed in the analysis of a receiver for gravitational waves. Sandwich waves are constructed, and certain non-radiative but non-static solutions are discussed. This part concludes with a tentative classification of time-dependent solutions of the types considered.",1962,6,1459,50,0,4,1,6,6,3,12,3,8,6
3c5c9c3cf20e888d30673b8b73cbedf9dc0906db,"I show that it is possible to formulate the Relativity postulates in a way that does not lead to inconsistencies in the case of spacetimes whose short-distance structure is governed by an observer-independent length scale. The consistency of these postulates proves incorrect the expectation that modifications of the rules of kinematics involving the Planck length would necessarily require the introduction of a preferred class of inertial observers. In particular, it is possible for every inertial observer to agree on physical laws supporting deformed dispersion relations of the type E2-c2 p2-c4m2 + f(E, p, m; Lp) =0, at least for certain types of f.",2000,51,910,23,0,5,32,39,39,46,38,39,43,49
7562d6af07c5449a681939842f99809e22a4b53e,"This is a textbook on the structural analysis and design of highway pavements. It presents the theory of pavement design and reviews the methods developed by several organizations, such as the American Association of State Highway and Transportation Officials (AASHTO), the Asphalt Institute (AI), and the Portland Cement Association (PCA). It can be used for an undergraduate course by skipping the appendices or as an advanced graduate course by including them. The book is organized in 13 chapters. Chapter 1 introduces the historical development of pavement design, the major road tests, the various design factors, and the differences in design concepts among highway pavements, airport pavements, and railroad trackbeds. Chapter 2 discusses stresses and strains in flexible pavements. Chapter 3 presents the KENLAYER computer program, based on Burmister's layered theory, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis on the effect of various factors on pavement responses. Chapter 4 discusses stresses and deflections in rigid pavements due to curling, loading, and friction, as well as the design of dowels and joints. Influence charts for determining stresses and deflections are also presented. Chapter 5 presents the KENSLABS computer program, based on the finite element method, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis. Chapter 6 discusses the concept of equivalent single-wheel and single-axle loads and the prediction of traffic. Chapter 7 describes the material characterization for mechanistic-empirical methods of pavement design including the determination of resilient modulus, fatigue and permanent deformation properties, and the modulus of subgrade reaction. Chapter 8 outlines the subdrainage design including general principles, drainage materials, and design procedures. Chapter 9 discusses pavement performance including distress, serviceability, skid resistance, nondestructive testing, and the evaluation of pavement performance. Chapter 10 illustrates the reliability concept of pavement design in which the variabilities of traffic, material, and geometric parameters are all taken into consideration. A probabilistic procedure, developed by Rosenblueth, is described and two probabilistic computer programs including VESYS for flexible pavements and PMRPD for rigid pavements are discussed. Chapter 11 outlines an idealistic mechanistic method of flexible pavement design and presents in detail the AI method and the AASHTO method, as well as the design of flexible pavement shoulders. Chapter 12 outlines an idealistic mechanistic method of rigid pavement design and presents in detail the PCA method and the AASHTO method. The design of continuous reinforced concrete pavements and rigid pavement shoulders is also included. Chapter 13 outlines the design of overlay on both flexible and rigid pavements including the AASHTO, AI, and PCA procedures. An Author Index and a Subject Index are provided.",1992,0,2550,357,0,0,4,7,11,10,22,17,29,31
a898919a0c07eae8e6f309421b1aa5815952b191,"List of Symbols.Introduction.Atomic Structure and Interatomic Bonding.The Structure of Crystalline Solids.Imperfections in Solids.Diffusion.Mechanical Properties of Metals.Dislocations and Strengthening Mechanisms.Failure.Phase Diagrams.Phase Transformations in Metals: Development of Microstructure and Alteration of Mechanical Properties.Thermal Processing of Metal Alloys.Metals Alloys.Structures and Properties of Ceramics.Applications and Processing of Ceramics.Polymer Structures.Characteristics, Applications, and Processing of Polymers.Composites.Corrosion and Degradation of Materials.Electrical Properties.Thermal Properties.Magnetic Properties.Optical Properties.Materials Selection and Design Considerations.Economic, Environmental, and Societal Issues in Materials Science and Engineering.Appendix A: The International System of Units (SI).Appendix B: Properties of Selected Engineering Materials.Appendix C: Costs and Relative Costs for Selected Engineering Materials.Appendix D: Mer Structures for Common Polymers.Appendix E: Glass Transition and Melting Temperatues for Common Polymeric Materials.Glossary.Answers to Selected Problems.Index.",1985,0,6238,294,0,0,0,1,0,3,1,2,2,4
d2e70a35b896bc9a46f3d9625f27e361a700e2bb,"Metadynamics is a powerful algorithm that can be used both for reconstructing the free energy and for accelerating rare events in systems described by complex Hamiltonians, at the classical or at the quantum level. In the algorithm the normal evolution of the system is biased by a history-dependent potential constructed as a sum of Gaussians centered along the trajectory followed by a suitably chosen set of collective variables. The sum of Gaussians is exploited for reconstructing iteratively an estimator of the free energy and forcing the system to escape from local minima. This review is intended to provide a comprehensive description of the algorithm, with a focus on the practical aspects that need to be addressed when one attempts to apply metadynamics to a new system: (i) the choice of the appropriate set of collective variables; (ii) the optimal choice of the metadynamics parameters and (iii) how to control the error and ensure convergence of the algorithm.",2008,138,1129,15,0,16,35,51,73,81,89,104,131,132
a549d9228337cbabcfc6709a0110740c1963b782,This comprehensive book provides state-of-the-art scientific and technical information in a clear format and consistent structure making it suitable for formal course work or self-instruction. T ...,2001,0,1284,20,5,11,20,28,43,66,86,73,84,110
05df5cd581b1029d0e8cd5004fb4605bccd0f24f,"Abstract The paper addresses current needs in Natural Gas (NG) treating. Basic principles of Pressure Swing Adsorption (PSA) separation processes are described. A state of the art of microporous adsorbents in the frame of NG treating is given. It includes reference and advanced zeolites, carbon based materials and Metal-Organic Frameworks (MOFs). The pros and cons of each material category are discussed. Guidelines to develop on-purpose materials are given from thermodynamics and material state of the art. Finally, PSA applicability to inert (nitrogen and carbon dioxide) rejection from NG is discussed.",2009,96,334,6,0,3,16,21,22,34,27,35,33,29
a7599db290a008d910e973e9444b0c490e51b127,"The enzyme-modified electrode is the fundamental component of amperometric biosensors and biofuel cells. The selection of appropriate combinations of materials, such as: enzyme, electron transport mediator, binding and encapsulation materials, conductive support matrix and solid support, for construction of enzyme-modified electrodes governs the efficiency of the electrodes in terms of electron transfer kinetics, mass transport, stability, and reproducibility. This review investigates the varieties of materials that can be used for these purposes. Recent innovation in conductive electro-active polymers, functionalized polymers, biocompatible composite materials, composites of transition metal-based complexes and organometallic compounds, sol-gel and hydro-gel materials, nanomaterials, other nano-metal composites, and nano-metal oxides are reviewed and discussed here. In addition, the critical issues related to the construction of enzyme electrodes and their application for biosensor and biofuel cell applications are also highlighted in this article. Effort has been made to cover the recent literature on the advancement of materials sciences to develop enzyme electrodes and their potential applications for the construction of biosensors and biofuel cells.",2009,83,240,5,8,22,24,20,33,27,20,15,21,15
447a916ec26d769230d202e02424ba2a904db4fb,"The metal catalyzed azide/alkyne ‘click’ reaction (a variation of the Huisgen 1,3-dipolar cycloaddition reaction between terminal acetylenes and azides) has vastly increased in broadness and application in the field of polymer science. Thus, this reaction represents one of the few universal, highly efficient functionalization reactions, which combines both high efficiency with an enormously high tolerance of functional groups and solvents under highly moderate reaction temperatures (25–70 °C). The present review assembles an update of this reaction in the field of polymer science (linear polymers, surfaces) with a focus on the synthesis of functionalized polymeric architectures and surfaces.",2008,241,646,1,8,42,61,78,75,65,62,56,49,38
99af52a4b42684ba0507aa251f0692fb9ab162bf,"Answer FIVE questions, taking ANY TWO from Group A, ANY TWO from Group B and ALL from Group C. All parts of a question (a, b, etc) should be answered atone place, Answer should be brief and to-the-point and be supplemented with neat sketches. Unnecessary long answers may result in loss of marks. Any missing data ,or wrong data may be assumed suitably giving proper justification. Figures on the right-hand side margin indicate full marks. 1. (a) What is a Burger vector? Show it by drawing a Burger circuit? What is Frank-Read source? State its importance in plastic deformation. 2+2+2 (b) Distinguish between: (2x2)+ (2x2) (i) Slip and Cross slip (ii) Sessile dislocation and Glissile dislocation. (c) What is Critical Resolved Shear Stress? Derive its formulae. 2+2 (d) Calculate the degree of freedom of ice and water kept in a beaker at 1 atmosphere pressure. 2 2. (a) State Fick's laws of Diffusion. How can it help you m the problems of Case Carburising? Given an activation energy, Q of 142 kJ/mol, for the diffusion of carbon in FCC iron and an initial temperature of 1000 K, find the temperature that will increase the diffusion coefficient by a factor 10. [R =8.314 J/(mol.K)]: Will you use a very high temperature? 2+2+(3+1) (b) What is a Phase? What is the difference between α-iron and ferrite? Define an invariant reaction with an example. 2+2 (c) Differentiate between: (2x2)+ (2x2) (i) Phase Rule and Phase Diagram, (ii) Solvus Line and Solidus Line. 3. (a) Explain Lever Rule with a Tie Line. Find the weight percentage of pro-eutectoid ferrite just above, the eutectoid temperature of a 0 3%C-steel. 2+2 (b) Derive the relationship between True Strain and Engineering Strain. What is Resilience? Why is it important for spring material? 2+(1 +1) (c) Describe Yield Point Phenomenon. Draw the engineering stress-strain diagram of Glass. Why does necking occur during tension test of a ductile material 2+2+2 (d) Justify: 2x3 (i) Zinc is not as ductile as copper (ii) Cold working increases hardness of materials (iii) Steel is a brittle material at sub-zero atmosphere. 4. (a) Suggest one suitable material for each of the following purpose with justifications: 2x5 (i) File Cabinet (ii) WaterTap (iii) Manhole Cover (iv) Garden Chair (v) Glass Cutter.",2007,0,567,10,21,32,25,33,67,70,59,69,41,27
a8d88e305089129fb2e306d324d8cd482252abb7,"Recent examples on the functionalization of the ferrocene core by means of cross-coupling reactions are reported in this re- view. Several methods are discussed including Negishi, Suzuki and Stille couplings for ferrocene-aryl bond formation and Sonogashira reaction for ferrocene-alkyne coupling. The properties in material science and asymmetric catalysis of the prepared ferrocenyl com- pounds are briefly discussed.",2008,1,26,0,0,1,1,1,2,8,5,1,1,3
887d2eb4c920a11c83294e5d9565c3c70bb76bc0,"This book introduces the principles of electrochemistry with a special emphasis on materials science. This book is clearly organized around the main topic areas comprising electrolytes, electrodes, development of the potential differences in combining electrolytes with electrodes, the electrochemical double layer, mass transport, and charge transfer, making the subject matter more accessible. In the second part, several important areas for materials science are described in more detail. These chapters bridge the gap between the introductory textbooks and the more specialized literature. They feature the electrodeposition of metals and alloys, electrochemistry of oxides and semiconductors, intrinsically conducting polymers, and aspects of nanotechnology with an emphasis on the codeposition of nanoparticles.This book provides a good introduction into electrochemistry for the graduate student. For the research student as well as for the advanced reader there is sufficient information on the basic problems in special chapters. The book is suitable for students and researchers in chemistry, physics, engineering, as well as materials science. It includes: introduction into electrochemistry; metal and alloy electrodeposition; oxides and semiconductors, corrosion; intrinsically conducting polymers; and, codeposition of nanoparticles, multilayers.",2008,0,51,1,0,3,2,4,9,6,6,6,7,4
79c66d7379ee590a935fb554d78518106491d29e,"This review introduces the basic material science concepts and principles behind some common topics in the development of pharmaceutical solid formulations. The physiochemical properties of small organic pharmaceutical materials are summarized. Common phases, differences in phases, phase transitions, and their relation to pharmaceutical development are reviewed. The characteristics and physical nature of solid phases, including crystalline and amorphous solids, are presented in conjunction with some pharmaceutically relevant phenomena, such as polymorphism, phase transition kinetics, and relaxation. Mesophases, including liquid crystals and condis crystals, are introduced. The potential energy states of different phases are highlighted as the key connection between the physical nature of the materials and their pharmaceutical behavior, and energy landscape is employed to enhance the understanding of this relation.",2007,48,72,3,0,6,9,8,3,7,8,9,4,4
3a88e844e0e17651c2c03568b7b74e9ffd2a35e2,"We present the initial architecture and implementation of VLab, a Grid and Web‐Service‐based system for enabling distributed and collaborative computational chemistry and material science applications for the study of planetary materials. The requirements of VLab include job preparation and submission, job monitoring, data storage and analysis, and distributed collaboration. These components are divided into client entry (input file creation, visualization of data, task requests) and back‐end services (storage, analysis, computation). Clients and services communicate through NaradaBrokering, a publish/subscribe Grid middleware system that identifies specific hardware information with topics rather than IP addresses. We describe three aspects of VLab in this paper: (1) managing user interfaces and input data with JavaBeans and Java Server Faces; (2) integrating Java Server Faces with the Java CoG Kit; and (3) designing a middleware framework that supports collaboration. To prototype our collaboration and visualization infrastructure, we have developed a service that transforms a scalar data set into its wavelet representation. General adaptors are placed between the endpoints and NaradaBrokering, which serve to isolate the clients/services from the middleware. This permits client and service development independently of potential changes to the middleware. Copyright © 2007 John Wiley & Sons, Ltd.",2007,42,31,0,10,1,2,1,0,1,0,0,0,0
90aeafe24a1d7bd5d0c5db9c56718b1ea024b8dc,"In recent years, non-volatile solid state memories have in many applications replaced magnetic hard disk drives. While the most popular and successful non-volatile memory is the “FLASH” random access memory, several contenders have entered the stage that might be viable alternatives in the near future. One of the most promising storage concepts is based on phase change materials which are already successfully employed in rewritable optical data storage. In this paper we will review the present understanding of phase change materials as well as open questions.",2007,26,127,0,1,9,5,11,17,13,9,12,14,8
18bbd2ddc3d478ab6db6cc0a9eabb36a4173da84,"Nanoparticles are the subject of numerous papers and reports and are full of promises for electronic, optical, magnetic and biomedical applications. Although metallic nanoparticles have been functionalized with peptides, proteins and DNA during the last 20 years, carbohydrates have not been used with this purpose until 2001. Since the first synthesis of gold nanoparticles functionalized with carbohydrates (glyconanoparticles) was reported, the number of published articles has considerably increased. This article reviews progress in the development of nanoparticles functionalized with biological relevant oligosaccharides. The glyconanoparticles constitute a good bio-mimetic model of carbohydrate presentation at the cell surface, and maybe, excellent tools for Glycobiology, Biomedicine and Material Science investigations.",2006,132,241,3,0,10,21,14,35,21,25,17,24,22
b6ca80a2fba34dc39850712a37cb9d890ea62cf7,,2006,0,230,0,2,24,22,14,26,17,21,19,14,12
b6d1c6da4132a99a2d167f4c78e03b7aa4df1091,"Objectives There are several methods in the literature to quantify powder flow, such as Carr’s index, critical orifice diameter or powder rheometry. A disadvantage of these methods is that they require large sample sizes to perform a measurement. Novel, investigational and lab scale powder processing techniques, such as SEDS or freeze drying, may not produce sufficient sample sizes to allow the use of these techniques to quantify flow, an important consideration for pharmaceutical manufacturing processes. Methods In the Xcelodose system, powder is retained in a dispense head (hopper) with a known number, diameter and surface area of holes. Tapping the dispense head via a solenoid will break powder bridges, dislodging the powder, causing flow onto an eight place balance below. For a given powder and dispense head the amount of powder dispensed is proportional to the number and frequency of taps. A variety of heterogeneous powders, as well as nine spray dried lactose:lactose monohydrate blends, with different flow properties, were tested using the Xcelodose system. The powders were tested in three hoppers with the same hole surface area but different hole diameters and the amount of powder dispensed recorded. From this a ‘Flow Gradient’ was calculated. These powders were then tested using Carr’s index and Basic Flow Energy (BFE, Freeman Powder Rheometer) and these results compared. Results For the group of heterogeneous powders there is a clear correlation between Carr’s index and the Flow Gradient (R = 0.8376, P < 0.001), allowing flow categorisation, similar to Carr’s of the powders. With the lactose powders the BFE was also compared. A correlation between Flow Gradient and BFE of 0.8793 (P < 0.01) was observed for these 9 blends. This correlation improves to 0.9417 (P < 0.001) if only the 7 poorly flowing powders are included, suggesting the value of this technique for poorly flowing powders (Figure 1). Conclusions A novel method for the measurement of flow properties with very small quantities of material has been developed and validated against different powder types and different established powder characterisation methodologies. The flow gradient method produces an excellent correlation with larger scale measurement methods and would be suitable to categorise powder flow when the amount of material is limited. 41 Forming amorphous and nanocrystalline APIs through solid-state interactions with cross-linked polyvinylpyrrolidone",2007,0,3,0,0,0,0,0,0,1,0,0,1,1
b18edfa0ea92f539a929d9a44e356f20cb023b06,"This paper illustrates the merits of convergence in nanobiology of two seemingly disparate fields, material science and computational biology. Traditionally, material science has been a discipline involving design and fabrication of synthetic polymers consisting of repeating units. Collaboration with synthetic organic chemists allowed design of new polymers, with a range of altered conformations. Yet, naturally occurring proteins are also materials. Their varied sequences and structures should enrich material science providing more complex shapes, scaffolds and chemical properties. For material scientists, the enhanced coverage of chemical space obtained by integrating proteins and synthetic organic chemistry through the introduction of non-natural residues allows a range of new useful potential applications.",2006,64,20,0,0,2,7,2,0,5,1,1,0,1
388f82d66bcf1b16b9569a93667224af02f90a03,,2006,2,91,5,1,4,5,10,12,5,8,5,13,3
56280b5dc973d1a8ba220e7fd7cf90266883e60a,"Piezoelectric actuators are at an important stage of their development into a large component market. This market pull is for dynamically driven actuators for Diesel injector valves in automobiles. Cost, yield, and reliability are important concerns for the automobile industry. A number of these concerns relate back to basic material science issues in the manufacture of the piezoelectric actuators. This paper discusses material development of the piezoelectric ceramic and new opportunities for higher temperature materials. An important consideration in developing low-fire ceramics is the flux selection for a given system, and these must be selected to limit electrode-ceramic interface reactions in both Ag/Pd and copper-metallized electrode actuators.",2005,40,197,1,0,1,4,1,4,12,12,13,20,17
e4cbedde6ef293b7c112215731129fcd5b30753e,SYNTHESIS.Addition of Terminal Acetylides to C=O and C=N Electrophiles.Synthesis of Heterocycles and Carbocycles via Electrophilic Cyclization of Alkynes.Transition Metal Acetylides.ADVANCED MATERIALS-ORIENTED.Semiconducting Poly(arylene ethylene)s.Polyynes via Alkylidene Carbenes and Carbenoids.PROPERTIES AND THEORY.Theoretical Studies on Acetylenic Scaffolds.Macrocycles Based on Phenyl-Acetylene Scaffolding.Carbon-Rich Compounds: Acetylene-Based Carbon Allotropes.Chiral Acetylenic Macromolecules.Shape-Persistent Acetylenic Macrocycles for Ordered Systems.BIOLOGY-ORIENTED.Acetylenosaccharides.,2004,0,185,3,0,2,3,10,10,15,13,15,19,14
36eab53578807c9bf4ecaef321c003659c2076c9,"This chapter describes the development of actor-network theory and feminist material semiotics by exploring case studies within STS (science and technology studies). It notes that STS (and so material semiotics) develops its theoretical approaches through empirical case studies, and notes that unless this is understood it is difficult to understand the significance of 'actor network theory' or any other STS theory or approach.",2009,70,1357,132,10,26,66,65,94,129,142,168,165,150
9f5cf2bdf56fa5959bb86c4ae30b6b679e1b6f49,,2004,0,90,0,8,7,5,7,9,7,11,6,10,5
c313d1942e9b50cc836bf5da3e323c46b5cae0a1,"I. Fundamental Physics and Materials Technology of Ice.- 1, General Concepts.- 1. Introduction.- 2. Equations of Balance.- 3. Material Response.- (a) General constitutive relations, simple materials.- (b) The rule of material objectivity.- (c) Material symmetry.- (d) Constitutive response for isotropic bodies.- (e) Materials with bounded memory - some constitutive representations.- (f) Incompressibility.- (g) Some representations of isotropic functions.- 4. The Entropy Principle.- (a) The viscous heat-conducting compressible fluid.- (b) The viscous heat-conducting incompressible fluid.- (c) Pressure and extra stress as independent variables.- (d) Thermoelastic solid.- (e) Final remarks.- 5. Phase Changes.- (a) Phase changes for a viscous compressible heat-conducting fluid.- (b) Phase changes for a viscous incompressible heat-conducting fluid.- References.- 2. A Brief Summary of Constitutive Relations for Ice.- 1. Preliminary Remarks.- 2. The Mechanical Properties of Hexagonal Ice.- (a) The crystal structure of ordinary ice.- (b) The elastic behavior of hexagonal ice.- (c) The inelastic behavior of single-crystal ice.- 3. The Mechanical Properties of Polycrystalline Ice.- (a) The elastic behavior of polycrystalline ice.- (b) Linear viscoelastic properties of polycrystalline ice.- (?) General theory.- (?) Experimental results.- (c) Non-linear viscous deformation and creep.- (?) Results of creep tests.- (?) Generalization to a three-dimensional flow law.- (?) Other flow laws.- 4. The Mechanical Properties of Sea Ice.- (a) The phase diagram of standard sea ice and its brine content.- (b) Elastic properties.- (c) Other material properties.- References.- II. The Deformation of an Ice Mass Under Its Own Weight.- 3. A Mathematical Ice-flow Model and its Application to Parallel-sided Ice Slabs.- 1. Motivation and Physical Description.- 2. The Basic Model - Its Field Equations and Boundary Conditions.- (a) The field equations.- (?) Cold ice region.- (?) Temperate ice region.- (b) Boundary conditions.- (?) At the free surface.- (?) Along the ice-water interface.- (?) Along the bedrock surface.- (?) Along the melting surface.- 3. The Response of a Parallel-sided Ice Slab to Steady Conditions.- (a) Dimensionless forms of the field equations.- (b) Parallel-sided ice slab, a first approximation to glacier and ice-shelf flow dynamics.- (?) Velocity and temperature fields.7V-independent.- (?) Extending and compressing flow.- (?) Floating ice shelves.- 4. Concluding Remarks.- References.- 4. Thermo-mechanical Response of Nearly Parallel-sided Ice Slabs Sliding over their Bed.- 1. Motivation.- 2. The Basic Boundary-value Problem and its Reduction to Linear Form.- 3. The Solution of the Boundary-value Problems.- (a) Zeroth-order problem.- (b) First-order problem.- (?) Harmonic perturbation from uniform flow for a zero accumulation rate.- (?) Analytic solution for a Newtonian fluid.- (?) Numerical solution for non-linear rheology.- (?) Effect of a steady accumulation rate.- (?) A historical note on a previous approach.- (?) The first-order temperature problem.- (c) Numerical results for steady state.- (?) Transfer of bottom protuberances to the surface.- (?) Basal stresses.- (?) Surface velocities.- (?) Effect of a steady accumulation rate.- 4. Remarks on Response to a Time-dependent Accumulation Rate.- 5. Surface-wave Stability Analysis.- (a) The eigenvalue problem.- (b) Discussion of results.- 6. Final Remarks.- References.- 5, The Application of the Shallow-ice Approximation.- 1. Background and Previous Work.- 2. Derivation of the Basal Shear-stress Formula by Integrating the Momentum Equations over Ice Thickness.- (a) Derivation.- (b) The use of the basal shear-stress formula in applied glaciology.- 3. Solution of the Ice-flow Problem using the Shallow-ice Approximation.- (a) Governing equations.- (b) Shallow-ice approximation.- (c) Construction of the perturbation solution.- (d) Results.- (e) Temperature field.- 4. Theoretical Steady-state Profiles.- (a) Earlier theories and their limitations.- (b) Surface profiles determined by using the shallow-ice approximation.- 5. An Alternative Scaling - a Proper Analysis of Dynamics of Ice Sheets with Ice Divides.- (a) Finite-bed inclination.- (b) Small-bed inclination.- (c) Illustrations.- References.- 6. The Response of a Glacier or an Ice Sheet to Seasonal and Climatic Changes.- 1. Statement of the Problem.- 2. Development of the Kinematic Wave Theory.- (a) Full non-linear theory.- (b) Perturbation expansion - linear theory.- (c) An estimate for the coefficients C and D.- (d) Boundary and initial conditions.- 3. Theoretical Solutions for a Model Glacier.- (a) Solutions neglecting diffusion.- (b) Theoretical solutions for a diffusive model.- (?) Coefficient functions for the special model.- (?) Solution for a step function.- (?) General solution for uniform accumulation rate.- (?) The inverse problem - calculation of climate from variations of the snout.- 4. General Treatment for an Arbitrary Valley Glacier.- (a) Fourier analysis in time.- (?) Low-frequency response.- (?) High-frequency response.- (?) Use of the results.- (b) Direct integration methods.- 5. Derivation of the Surface-wave Equation from First Principles - Non-linear Theory.- (a) Surface waves in the shallow-ice approximation.- (?) Integration by the methods of characteristics.- (?) An illustrative example.- (?) A remark on linearization.- (?) Effects of diffusion.- (b) Remarks regarding time-dependent surface profiles in ice sheets.- (c) Long waves in an infinite ice slab - Is accounting for diffusion enough?.- (?) Basic equations.- (?) Construction of perturbation solutions.- (?) Numerical results.- 6. Concluding Remarks.- References.- 7. Three-dimensional and Local Flow Effects in Glaciers and Ice Sheets.- 1. Introduction.- 2. Effect of Valley Sides on the Motion of a Glacier.- (a) Solutions in special cases.- (?) Exact solutions for the limiting cases.- (?) Solution for a slightly off-circular channel.- (?) A note on very deep and wide channels.- (b) A useful result for symmetrical channels with no boundary slip.- (c) Numerical solution - discussion of results.- 3. Three-dimensional Flow Effects in Ice Sheets.- (a) Basic equations.- (b) Decoupling of the stress-velocity problem from the problem of surface profile.- (c) The equation describing the surface geometry.- (d) The margin conditions.- 4. Variational Principles.- (a) Fundamental variational theorem.- (b) Variational principle for velocities.- (c) Reciprocal variational theorem.- (d) Maximum and minimum principles.- (e) Adoption of the variational principles to ice problems.- 5. Discussion of Some Finite-element Solutions.- References.- Appendix. Detailed Calculations Pertaining to Higher-order Stresses in the Shallow-ice Approximation.- Author Index.",1983,0,581,70,0,0,4,4,2,3,5,2,1,0
889949974fd50e8111e93397663039188887d622,"The critical role of materials science and engineering in the development of fuel cell technology is surveyed. The inability to fabricate reliable triple-phase-boundary (tbp) structures involving electrolytes, electronic conductors, and gaseous reactants, severely restricted the progress of fuel cells until about four decades ago (∼1960). However at the start of the new millennium, commercialisation of four fuel cell types: polymeric electrolyte membrane (PEMFC), phosphoric acid (PAFC), molten carbonate (MCFC), and solid oxide (SOFC), is now being very energetically pursued. Materials selection for each type of fuel cell is briefly examined, and the predominant engineering issues related to the development of commercial products are summarised. The fabrication, reliability, and cost of the relevant materials is of paramount importance to ensure rapid market penetration. The choice of fuel and relevant infrastructure is also considered, and the crucial role of materials for energy storage (particularly hydrogen) and fuel processing, is emphasised.",2001,35,419,2,1,4,9,24,22,22,25,23,19,29
6dc1739bd1e7d8bda913799038a6acf43583b0b0,"An ultrasonic levitation device operable in both ordinary ground‐based as well as in potential space‐borne laboratories is described together with its various applications in the fields of fluid dynamics, material science, and light scattering. Some of the phenomena which can be studied by this instrument include surface waves on freely suspended liquids, the variations of the surface tension with temperature and contamination, the deep undercooling of materials with the temperature variations of their density and viscosity, and finally some of the optical diffraction properties of transparent substances.",1985,18,201,1,0,3,1,2,9,2,3,3,4,6
d0c0dd34cbf2de1aa266a13fc08d6bb463056799,"Abstract The material science beamline is a multi-purpose beamline intended for experiments in the photon energy range 2.4–20 keV. The beamline will be located at port I811 of the MAX-II ring. According to our planning the first experiments will be conducted in the spring 2001. In order to overcome the relatively low energy of the electron-beam in the MAX II ring (1.5 GeV) we will use a super-conducting multi-pole wiggler insertion device. The disadvantage with this solution is the high K value ( K =20) of the device, which leads to a divergent X-ray beam. The optical solutions for the beamline therefore emphasizes on solving the divergence problem of the photon beam and the high heat load on the optical elements.",2001,6,20,1,0,0,0,0,0,0,0,0,0,2
f67d87c2ba785779fde532970026b0e3ab7f062a,Abstract The development of the Material Science EXAFS line which covers the energy range of 4 keV to > 25 keV is described. The general design parameters of the line have been developed specifically for the application of X-ray absorption spectroscopy to problems in material science. The optics systems consisting of a vertically collimating SiC mirror and a unique four crystal monochromator are described along with the results of extensive ray tracing studies. The two backend configurations to be used on this line will also be discussed.,1983,4,29,0,0,1,0,2,1,1,2,0,1,1
90320d358c7d58996ada4a5f89a8279c24c068b2,"UNLABELLED
Reprocessing (repeated cleaning, disinfection, and sterilization) and reuse of single-use medical devices has been performed safely with some devices. The aim of our study was to analyze whether reprocessing of the Combitubes (Kendall-Sheridan, Argyll, NY) airway device, used for emergency endotracheal intubation and difficult airway management, is possible and can be performed appropriately and safely. Microbiological, microstructure, and material science examinations were performed with unused, as well as multiple reused and reprocessed Combitubes. The reprocessing procedure consisted of a cleaning, a disinfection, a final inspection, and a sterilization. Microbiological examinations of multiple reused and reprocessed Combitubes found no test organisms in quantitative cultures. A microbial reduction between four and five log levels compared with nonreprocessed tubes was found. Microstructure analysis for the examination of topographical alterations and changes in the chemical composition of the surface demonstrated nonsignificant alterations between new and reprocessed medical devices. In material science examinations, cuff burst pressures were not different between unused and multiple reprocessed Combitubes. The results of all examinations proved that the decontamination process is adequately effective, and that no significant superficial alterations are generated by the multiple reuse and reprocessing of the Combitubes. To assure uniformly good results, a quality management system must be established and only validated methods should be used.


IMPLICATIONS
Reprocessing of single-use medical devices offers the opportunity of significant savings and is already performed with some devices. Microbiological, microstructure, and material science examinations proved that reprocessing of multiple reused Combitubes (Kendall-Sheridan, Argyll, NY), mainly used for emergency airway management, is possible and safe.",2000,37,20,0,0,3,5,1,0,2,0,2,0,1
a9d015e4c2ac2ebcf0bc170ad42eea73d2d48ceb,Partial table of contents: Enantioselective Addition of Dialkyzincs to Aldehydes Catalyzed by Chiral Ferrocenyl Aminoalcohols. Ferrocene Compounds Containing Heteroelements. Electrochemical and X-ray Structural Aspects of Transition Metal Complexes Containing Redox-Active Ferrocene Ligands. Ferrocene-Containing Thermotropic Liquid Crystals. Synthesis and Characterization of Ferrocene Containing Polymers.,1994,0,407,3,0,5,6,10,9,14,12,19,13,12
c0ee5787154e15cc1d3ed0cf38c186b88c7fe1ad,"Coordination metal complexes with one-dimensional polymeric structures have long been investigated as materials with unusual properties. Molecular-based ferromagnets, synthetic metallic conductors, non-linear optical materials, and ferroelectrics represent several applications of low-dimensional coordination polymers. It is possible to modify the bulk magnetic, electrical, and optical properties of such materials by tailoring the constituent molecules. This review presents recent examples of each of these applications and focuses on the correlation between the molecular structure and the bulk properties of the materials.",1993,179,425,2,0,1,5,7,11,3,12,8,19,22
f5d8a14f2c5110e0f193009e531902a11f2473e9,,2000,0,273,10,0,0,4,6,2,16,14,17,16,12
5a70679ac5fe8725bebd57ca26bab3baacc0012c,"Abstract Many texture studies have been published on crispness because of the great interest of consumers towards crispy foods. This work reviews the existing literature on the topic, and especially the different approaches, instrumental and sensory, applied to study crispness. These studies result in a wide range of data but, because crispness is not a clearly defined sensory attribute, the conclusions that can be drawn from these studies should be carefully examined. The physical basis for crispness are discussed and the role of structure, hydration and ingredients on crispness and its stability are presented.",2002,100,229,10,0,2,4,8,9,11,14,17,22,16
e1eab0c1d033852ae041d86d31287d7ff7756bbf,Abstract The use of electrostatic forces in the design of a positioning system and acoustic forces in the implementation of a mixing system for material science experiments on Spacelab are described. The electrostatic positioning of samples is described with special reference to its advantages and disadvantages with regard to other positioning methods. The design of such a positioner is described including the considerations relating to the processing of both high and low vapour pressure materials in a positioner compatible with both the isothermal heating facility (IHF) and the mirror heating facility (MHF) of Spacelab under microgravity (10 −4 –10 −3 g) conditions. The application of acoustic and ultrasonic forces to the problem of sample mixing in material science experiments is explained. The design of a mixer compatible with existing furnace hardware for Spacelab and capable of effectively mixing samples at temperatures up to 1200°C is described. Tests of the mixer show that a 15 μm displacement adequate for good mixing can be achieved with a d.c. power input of 23 W and a conversion efficiency of 70%. Tests on alumina particles and carbon fibres in various alloy matrices show that complete wetting can be achieved.,1980,0,16,0,0,0,1,0,1,0,0,0,0,0
01c23f080f47379e915d31cd734448f1b6c7b9cc,"Abstract Since their first observation nearly a decade ago by Iijima (Iijima S. Helical microtubules of graphitic carbon Nature. 1991; 354:56–8), carbon nanotubes have been the focus of considerable research. Numerous investigators have since reported remarkable physical and mechanical properties for this new form of carbon. From unique electronic properties and a thermal conductivity higher than diamond to mechanical properties where the stiffness, strength and resilience exceeds any current material, carbon nanotubes offer tremendous opportunities for the development of fundamentally new material systems. In particular, the exceptional mechanical properties of carbon nanotubes, combined with their low density, offer scope for the development of nanotube-reinforced composite materials. The potential for nanocomposites reinforced with carbon tubes having extraordinary specific stiffness and strength represent tremendous opportunity for application in the 21st century. This paper provides a concise review of recent advances in carbon nanotubes and their composites. We examine the research work reported in the literature on the structure and processing of carbon nanotubes, as well as characterization and property modeling of carbon nanotubes and their composites.",2001,101,4407,84,0,18,75,115,128,152,210,188,240,219
818d63bcec8c028109c4d50d905061e6beeb5f9e,,1978,0,224,3,5,9,14,7,9,6,3,6,4,2
c409669f26e30f2016273f9cf5fcb40ac3eaa973,,1984,0,191,1,1,4,8,8,7,13,9,11,17,6
0d69ed8ecce3ab31950c26e98a42a0e67a71671f,"Tricalcium, tetracalcium phosphate and hydroxyapatite ceramics exhibit distinct differences in their chemical and structural composition. Only hydroxyapatite ceramic is identical with the original bone mineral. Different preparation methods lead to compact hydroxyapatite ceramic or to porous material with interconnecting macropores as structural equivalents of the spatial structure of cancellous bone. Concerning the behaviour in a biological environment, high crystallinity and large material density result in resistance to dissolution and long lasting stability. Amorphous ultrastructure and porous formation enhance interface activity and bone ingrowth, but also biological degradation of the ceramic implant material.",1980,5,193,0,0,2,1,1,0,0,0,2,2,2
c79245d20facedb5aa8ec27b43b72e9fd107847e,,1972,0,105,6,0,0,0,0,0,0,0,0,0,0
3c69bf381dd00605c74b01751dd62b271d1d0dfc,,1975,0,129,11,0,0,0,0,0,0,0,0,0,1
b3a5b7f4f14ce6112a1327b216839c6f9a3fd787,,1971,0,76,0,0,0,1,3,1,2,1,2,0,3
e9f7649b8da9fae259cfd34789121eb341a3ebdf,"Software for calculation of phase diagrams and thermodynamic properties have been developed since the 1970's. Software and computers have now developed to a level where such calculations can be used as tools for material and process development. In the present paper some of the latest software developments at Thermo-Calc Software are presented together with application examples. It is shown how advanced thermodynamic calculations have become more accessible since: - A more user-friendly windows version of Thermo-Calc, TCW, has been developed. - There is an increasing amount of thermodynamic databases for different materials available. - Thermo-Calc can be accessed from user-written software through several different programming interfaces are available which enables access to the thermodynamic software from a user-written software. Accurate data for thermodynamic properties and phase equilibria can then easily be incorporated into software written in e.g. C++, Matlab and FORTRAN. Thermo-Calc Software also produces DICTRA, a software for simulation of diffusion controlled phase transformations. Using DICTRA it is possible to simulate processes such as homogenization, carburising, microsegregation and coarsening in multicomponent alloys. The different models in the DICTRA software are briefly presented in the present paper together with some application examples.",2002,18,2557,53,1,13,40,45,59,56,69,58,71,65
bf5a13c1a5096e86eaa316b97ff43df0078a2042,"This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. Now results are discussed, along with the application of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book is not restricted to one particular discipline but rather will be of interest to scientists working in any area where inference from incomplete information is necessary.",2004,3,1631,202,38,48,50,87,91,94,105,113,123,123
20974e0e65b2831648cee5e589fbbdc37f1ef5b2,"Abstract The study of tin oxide is motivated by its applications as a solid state gas sensor material, oxidation catalyst, and transparent conductor. This review describes the physical and chemical properties that make tin oxide a suitable material for these purposes. The emphasis is on surface science studies of single crystal surfaces, but selected studies on powder and polycrystalline films are also incorporated in order to provide connecting points between surface science studies with the broader field of materials science of tin oxide. The key for understanding many aspects of SnO 2 surface properties is the dual valency of Sn. The dual valency facilitates a reversible transformation of the surface composition from stoichiometric surfaces with Sn 4+ surface cations into a reduced surface with Sn 2+ surface cations depending on the oxygen chemical potential of the system. Reduction of the surface modifies the surface electronic structure by formation of Sn 5s derived surface states that lie deep within the band gap and also cause a lowering of the work function. The gas sensing mechanism appears, however, only to be indirectly influenced by the surface composition of SnO 2 . Critical for triggering a gas response are not the lattice oxygen concentration but chemisorbed (or ionosorbed) oxygen and other molecules with a net electric charge. Band bending induced by charged molecules cause the increase or decrease in surface conductivity responsible for the gas response signal. In most applications tin oxide is modified by additives to either increase the charge carrier concentration by donor atoms, or to increase the gas sensitivity or the catalytic activity by metal additives. Some of the basic concepts by which additives modify the gas sensing and catalytic properties of SnO 2 are discussed and the few surface science studies of doped SnO 2 are reviewed. Epitaxial SnO 2 films may facilitate the surface science studies of doped films in the future. To this end film growth on titania, alumina, and Pt(1 1 1) is reviewed. Thin films on alumina also make promising test systems for probing gas sensing behavior. Molecular adsorption and reaction studies on SnO 2 surfaces have been hampered by the challenges of preparing well-characterized surfaces. Nevertheless some experimental and theoretical studies have been performed and are reviewed. Of particular interest in these studies was the influence of the surface composition on its chemical properties. Finally, the variety of recently synthesized tin oxide nanoscopic materials is summarized.",2005,455,2028,25,0,18,51,74,78,87,103,160,177,164
9f267904f7e36571f7001813eef4e5fffd5ef993,"The extended and generalized finite element methods are reviewed with an emphasis on their applications to problems in material science: (1) fracture, (2) dislocations, (3) grain boundaries and (4) phases interfaces. These methods facilitate the modeling of complicated geometries and the evolution of such geometries, particularly when combined with level set methods, as for example in the simulation growing cracks or moving phase interfaces. The state of the art for these problems is described along with the history of developments.",2009,183,674,16,2,15,47,50,46,50,69,74,68,69
d8ced5336468ef31fd62c91476be9146805720f0,"The aerodynamics of various types of wind power machines and advantages and disadvantages of various schemes for obtaining power from the wind are discussed. Simple, one-dimensional models for various power producing machines are given along with their performance characteristics and presented as a function of their elementary aerodynamic and kinematic characteristics. Propeller type wind turbine theory is reviewed to level of strip theory including both induced axial and tangential velocities. (auth, from Introduction)",1974,2,323,15,0,0,2,2,5,3,5,4,2,6
b0eaa24d9302e85bcc532bea3c3626e2deb92a40,"TABLE OF CONTENTS Preface to the Fifth Edition Part 1: Fundamental Principles 1. Aerodynamics: Some Introductory Thoughts 2. Aerodynamics: Some Fundamental Principles and Equations Part 2: Inviscid, Incompressible Flow 3. Fundamentals of Inviscid, Incompressible Flow 4. Incompressible Flow Over Airfoils 5. Incompressible Flow Over Finite Wings 6. Three-Dimensional Incompressible Flow Part 3: Inviscid, Compressible Flow 7. Compressible Flow: Some Preliminary Aspects 8. Normal Shock Waves and Related Topics 9. Oblique Shock and Expansion Waves 10. Compressible Flow Through Nozzles, Diffusers and Wind Tunnels 11. Subsonic Compressible Flow Over Airfoils: Linear Theory 12. Linearized Supersonic Flow 13. Introduction to Numerical Techniques for Nonlinear Supersonic Flow 14. Elements of Hypersonic Flow Part 4: Viscous Flow 15. Introduction to the Fundamental Principles and Equations of Viscous Flow 16. A Special Case: Couette Flow 17. Introduction to Boundary Layers 18. Laminar Boundary Layers 19. Turbulent Boundary Layers 20. Navier-Stokes Solutions: Some Examples Appendix A: Isentropic Flow Properties Appendix B: Normal Shock Properties Appendix C: Prandtl-Meyer Function and Mach Angle Appendix D: Standard Atmosphere Bibliography Index",1984,7,3072,256,0,0,2,2,3,1,1,8,12,8
3c21e68088b176d521d127a4dc358f6fa6d41fca,Preface to the second edition Preface to the first edition Acknowledgements List of main symbols 1. Introduction: a history of helicopter flight 2. Fundamentals of rotor aerodynamics 3. Blade element analysis 4. Rotating blade motion 5. Helicopter performance 6. Aerodynamics design of helicopters 7. Aerodynamics of rotor airfoils 8. Unsteady airfoil behavior 9. Dynamic stall 10. Rotor wakes and blade tip vortices 11. Rotor-airframe interaction aerodynamics 12. Autogiros and gyroplanes 13. Aerodynamics of wind turbines 14. Computational methods for helicopter aerodynamics Appendix Index.,2000,4,2084,255,1,9,21,30,35,48,55,67,64,83
d35ec67ae8367863e1004e810ae6319d7faa7b68,,1977,0,1333,181,0,5,2,5,4,5,4,5,3,4
bf30ff4b4569719b8663c728184c440de36bcebf,"SUMMARY The flight of insects has fascinated physicists and biologists for more than a century. Yet, until recently, researchers were unable to rigorously quantify the complex wing motions of flapping insects or measure the forces and flows around their wings. However, recent developments in high-speed videography and tools for computational and mechanical modeling have allowed researchers to make rapid progress in advancing our understanding of insect flight. These mechanical and computational fluid dynamic models, combined with modern flow visualization techniques, have revealed that the fluid dynamic phenomena underlying flapping flight are different from those of non-flapping, 2-D wings on which most previous models were based. In particular, even at high angles of attack, a prominent leading edge vortex remains stably attached on the insect wing and does not shed into an unsteady wake, as would be expected from non-flapping 2-D wings. Its presence greatly enhances the forces generated by the wing, thus enabling insects to hover or maneuver. In addition, flight forces are further enhanced by other mechanisms acting during changes in angle of attack, especially at stroke reversal, the mutual interaction of the two wings at dorsal stroke reversal or wing–wake interactions following stroke reversal. This progress has enabled the development of simple analytical and empirical models that allow us to calculate the instantaneous forces on flapping insect wings more accurately than was previously possible. It also promises to foster new and exciting multi-disciplinary collaborations between physicists who seek to explain the phenomenology, biologists who seek to understand its relevance to insect physiology and evolution, and engineers who are inspired to build micro-robotic insects using these principles. This review covers the basic physical principles underlying flapping flight in insects, results of recent experiments concerning the aerodynamics of insect flight, as well as the different approaches used to model these phenomena.",2003,124,1059,56,0,6,22,29,29,41,59,68,60,69
bb92e666b524f0a50e13c35e165cce8c28ceb9e5,"A new technique of free-space simulation has been developed for solving unbounded electromagnetic problems with the finite-difference time-domain method. Referred to as PML, the new technique is based on the use of an absorbing layer especially designed to absorb without reflection the electromagnetic waves. The first part of the paper presents the theory of the PML technique. The second part is devoted to numerical experiments and to numerical comparisons with the previously used techniques of free-space simulation. These comparisons show that the PML technique works better than the others in all cases; using it allows us to obtain a higher accuracy in some problems and a release of computational requirements in some others.",1994,10,9341,609,5,45,127,170,187,195,216,193,217,277
f6c711b66fa0a060f8ed604af8a2a47c10daef48,Introduction.- The Helmholtz Equation.- Direct Acoustic Obstacle Scattering.- III-Posed Problems.- Inverse Acoustic Obstacle Scattering.- The Maxwell Equations.- Inverse Electromagnetic Obstacle Scattering.- Acoustic Waves in an Inhomogeneous Medium.- Electromagnetic Waves in an Inhomogeneous Medium.- The Inverse Medium Problem.-References.- Index,1992,29,4514,450,0,8,10,19,24,46,70,69,80,74
22132078b8dc885bb3cef2434b5476b6dc566cf7,"The dependence of the dielectric constant, at frequencies between 1 MHz and 1 GHz, on the volumetric water content is determined empirically in the laboratory. The effect of varying the texture, bulk density, temperature, and soluble salt content on this relationship was also determined. Time-domain reflectometry (TDR) was used to measure the dielectric constant of a wide range of granular specimens placed in a coaxial transmission line. The water or salt solution was cycled continuously to or from the specimen, with minimal disturbance, through porous disks placed along the sides of the coaxial tube. 
 
Four mineral soils with a range of texture from sandy loam to clay were tested. An empirical relationship between the apparent dielectric constant Ka and the volumetric water content θv, which is independent of soil type, soil density, soil temperature, and soluble salt content, can be used to determine θv, from air dry to water saturated, with an error of estimate of 0.013. Precision of θv to within ±0.01 from Ka can be obtained with a calibration for the particular granular material of interest. An organic soil, vermiculite, and two sizes of glass beads were also tested successfully. The empirical relationship determined here agrees very well with other experimenters' results, which use a wide range of electrical techniques over the frequency range of 20 MHz and 1 GHz and widely varying soil types. The results of applying the TDR technique on parallel transmission lines in the field to measure θv versus depth are encouraging.",1980,19,4524,402,1,3,2,3,6,6,6,4,10,8
395b44480e3cc49f5b536d37ab11a819698c83e3,"The electric field integral equation (EFIE) is used with the moment method to develop a simple and efficient numerical procedure for treating problems of scattering by arbitrarily shaped objects. For numerical purposes, the objects are modeled using planar triangular surfaces patches. Because the EFIE formulation is used, the procedure is applicable to both open and closed surfaces. Crucial to the numerical formulation is the development of a set of special subdomain-type basis functions which are defined on pairs of adjacent triangular patches and yield a current representation free of line or point charges at subdomain boundaries. The method is applied to the scattering problems of a plane wave illuminated flat square plate, bent square plate, circular disk, and sphere. Excellent correspondence between the surface current computed via the present method and that obtained via earlier approaches or exact formulations is demonstrated in each case.",1980,21,4867,392,0,1,1,3,8,9,7,10,15,9
da3872f2955bf01550bca2f5c199a99765be6087,"Using the freedom of design that metamaterials provide, we show how electromagnetic fields can be redirected at will and propose a design strategy. The conserved fields—electric displacement field D, magnetic induction field B, and Poynting vector B—are all displaced in a consistent manner. A simple illustration is given of the cloaking of a proscribed volume of space to exclude completely all electromagnetic fields. Our work has relevance to exotic lens design and to the cloaking of objects from electromagnetic fields.",2006,61,6777,146,34,145,307,363,459,509,544,560,573,572
68ad7ddfeb078558aedf5f69600ba799baec3b2b,"A new type of metallic electromagnetic structure has been developed that is characterized by having high surface impedance. Although it is made of continuous metal, and conducts dc currents, it does not conduct ac currents within a forbidden frequency band. Unlike normal conductors, this new surface does not support propagating surface waves, and its image currents are not phase reversed. The geometry is analogous to a corrugated metal surface in which the corrugations have been folded up into lumped-circuit elements, and distributed in a two-dimensional lattice. The surface can be described using solid-state band theory concepts, even though the periodicity is much less than the free-space wavelength. This unique material is applicable to a variety of electromagnetic problems, including new kinds of low-profile antennas.",1999,67,3836,194,3,18,27,51,91,92,153,130,159,212
370a6f60b2ae303d0a6d7cdf5e3ab9ccd6769120,Foreword to the Revised Edition. Preface. Fundamental Concepts. Introduction to Waves. Some Theorems and Concepts. Plane Wave Functions. Cylindrical Wave Functions. Spherical Wave Functions. Perturbational and Variational Techniques. Microwave Networks. Appendix A: Vector Analysis. Appendix B: Complex Permittivities. Appendix C: Fourier Series and Integrals. Appendix D: Bessel Functions. Appendix E: Legendre Functions. Bibliography. Index.,1961,0,4960,307,0,2,2,6,10,7,13,19,16,7
0ea9c798317f23df8bda748e7bdb2608aa4967bd,"In this paper, we discuss some interesting properties of the electromagnetic potentials in the quantum domain. We shall show that, contrary to the conclusions of classical mechanics, there exist effects of potentials on charged particles, even in the region where all the fields (and therefore the forces on the particles) vanish. We shall then discuss possible experiments to test these conclusions; and, finally, we shall suggest further possible developments in the interpretation of the potentials.",1959,0,4612,127,0,1,4,3,1,2,2,3,11,6
9880699907547b7f4bcbad05a3c466c425d5395f,"A recently published theory has suggested that a cloak of invisibility is in principle possible, at least over a narrow frequency band. We describe here the first practical realization of such a cloak; in our demonstration, a copper cylinder was “hidden” inside a cloak constructed according to the previous theoretical prescription. The cloak was constructed with the use of artificially structured metamaterials, designed for operation over a band of microwave frequencies. The cloak decreased scattering from the hidden object while at the same time reducing its shadow, so that the cloak and object combined began to resemble empty space.",2006,56,5927,106,11,122,303,308,367,389,456,457,466,467
d741a75c8b6a9046477d2c9b935de66040a0a463,"THE SCATTERING OF ELECTROMAGNETIC WAVES FROM ROUGH SURFACES PDF Are you looking for the scattering of electromagnetic waves from rough surfaces Books? Now, you will be happy that at this time the scattering of electromagnetic waves from rough surfaces PDF is available at our online library. With our complete resources, you could find the scattering of electromagnetic waves from rough surfaces PDF or just found any kind of Books for your readings everyday.",1963,0,3478,162,1,3,5,12,7,10,14,18,17,22
6908a6e13a1aff03007d8478129403dbcbd6f348,"Scalp electric potentials (electroencephalograms) and extracranial magnetic fields (magnetoencephalograms) are due to the primary (impressed) current density distribution that arises from neuronal postsynaptic processes. A solution to the inverse problem--the computation of images of electric neuronal activity based on extracranial measurements--would provide important information on the time-course and localization of brain function. In general, there is no unique solution to this problem. In particular, an instantaneous, distributed, discrete, linear solution capable of exact localization of point sources is of great interest, since the principles of linearity and superposition would guarantee its trustworthiness as a functional imaging method, given that brain activity occurs in the form of a finite number of distributed hot spots. Despite all previous efforts, linear solutions, at best, produced images with systematic nonzero localization errors. A solution reported here yields images of standardized current density with zero localization error. The purpose of this paper is to present the technical details of the method, allowing researchers to test, check, reproduce and validate the new method.",2002,28,2763,205,1,3,9,24,35,51,70,113,127,157
52290de9c21c139bf8edc000e3c9e0eb6814d7d0,"Preface. Acknowledgments. Acronyms. 1 Introduction. 1.1 Definition of Metamaterials (MTMs) and Left-Handed (LH) MTMs. 1.2 Theoretical Speculation by Viktor Veselago. 1.3 Experimental Demonstration of Left-Handedness. 1.4 Further Numerical and Experimental Confirmations. 1.5 ""Conventional"" Backward Waves and Novelty of LH MTMs. 1.6 Terminology. 1.7 Transmission Line (TL) Approach. 1.8 Composite Right/Left-Handed (CRLH) MTMs. 1.9 MTMs and Photonic Band-Gap (PBG) Structures. 1.10 Historical ""Germs"" of MTMs. References. 2 Fundamentals of LH MTMs. 2.1 Left-Handedness from Maxwell's Equations. 2.2 Entropy Conditions in Dispersive Media. 2.3 Boundary Conditions. 2.4 Reversal of Doppler Effect. 2.5 Reversal of Vavilov- Cerenkov Radiation. 2.6 Reversal of Snell's Law: Negative Refraction. 2.7 Focusing by a ""Flat LH Lens"". 2.8 Fresnel Coefficients. 2.9 Reversal of Goos-H anchen Effect. 2.10 Reversal of Convergence and Divergence in Convex and Concave Lenses. 2.11 Subwavelength Diffraction. References. 3 TLTheoryofMTMs. 3.1 Ideal Homogeneous CRLH TLs. 3.1.1 Fundamental TL Characteristics. 3.1.2 Equivalent MTM Constitutive Parameters. 3.1.3 Balanced and Unbalanced Resonances. 3.1.4 Lossy Case. 3.2 LC Network Implementation. 3.2.1 Principle. 3.2.2 Difference with Conventional Filters. 3.2.3 Transmission Matrix Analysis. 3.2.4 Input Impedance. 3.2.5 Cutoff Frequencies. 3.2.6 Analytical Dispersion Relation. 3.2.7 Bloch Impedance. 3.2.8 Effect of Finite Size in the Presence of Imperfect Matching. 3.3 Real Distributed 1D CRLH Structures. 3.3.1 General Design Guidelines. 3.3.2 Microstrip Implementation. 3.3.3 Parameters Extraction. 3.4 Experimental Transmission Characteristics. 3.5 Conversion from Transmission Line to Constitutive Parameters. References. 4 Two-Dimensional MTMs. 4.1 Eigenvalue Problem. 4.1.1 General Matrix System. 4.1.2 CRLH Particularization. 4.1.3 Lattice Choice, Symmetry Points, Brillouin Zone, and 2D Dispersion Representations. 4.2 Driven Problem by the Transmission Matrix Method (TMM). 4.2.1 Principle of the TMM. 4.2.2 Scattering Parameters. 4.2.3 Voltage and Current Distributions. 4.2.4 Interest and Limitations of the TMM. 4.3 Transmission Line Matrix (TLM) Modeling Method. 4.3.1 TLM Modeling of the Unloaded TL Host Network. 4.3.2 TLM Modeling of the Loaded TL Host Network (CRLH). 4.3.3 Relationship between Material Properties and the TLM Model Parameters. 4.3.4 Suitability of the TLM Approach for MTMs. 4.4 Negative Refractive Index (NRI) Effects. 4.4.1 Negative Phase Velocity. 4.4.2 Negative Refraction. 4.4.3 Negative Focusing. 4.4.4 RH-LH Interface Surface Plasmons. 4.4.5 Reflectors with Unusual Properties. 4.5 Distributed 2D Structures. 4.5.1 Description of Possible Structures. 4.5.2 Dispersion and Propagation Characteristics. 4.5.3 Parameter Extraction. 4.5.4 Distributed Implementation of the NRI Slab. References. 5 Guided-Wave Applications. 5.1 Dual-Band Components. 5.1.1 Dual-Band Property of CRLH TLs. 5.1.2 Quarter-Wavelength TL and Stubs. 5.1.3 Passive Component Examples: Quadrature Hybrid and Wilkinson Power Divider. 5.1.3.1 Quadrature Hybrid. 5.1.3.2 Wilkinson Power Divider. 5.1.4 Nonlinear Component Example: Quadrature Subharmonically Pumped Mixer. 5.2 Enhanced-Bandwidth Components. 5.2.1 Principle of Bandwidth Enhancement. 5.2.2 Rat-Race Coupler Example. 5.3 Super-compact Multilayer ""Vertical"" TL. 5.3.1 ""Vertical"" TL Architecture. 5.3.2 TL Performances. 5.3.3 Diplexer Example. 5.4 Tight Edge-Coupled Coupled-Line Couplers (CLCs). 5.4.1 Generalities on Coupled-Line Couplers. 5.4.1.1 TEM and Quasi-TEM Symmetric Coupled-Line Structures with Small Interspacing: Impedance Coupling (IC). 5.4.1.2 Non-TEM Symmetric Coupled-Line Structures with Relatively Large Spacing: Phase Coupling (PC). 5.4.1.3 Summary on Symmetric Coupled-Line Structures. 5.4.1.4 Asymmetric Coupled-Line Structures. 5.4.1.5 Advantages of MTM Couplers. 5.4.2 Symmetric Impedance Coupler. 5.4.3 Asymmetric Phase Coupler. 5.5 Negative and Zeroth-Order Resonator. 5.5.1 Principle. 5.5.2 LC Network Implementation. 5.5.3 Zeroth-Order Resonator Characteristics. 5.5.4 Circuit Theory Verification. 5.5.5 Microstrip Realization. References. 6 Radiated-Wave Applications. 6.1 Fundamental Aspects of Leaky-Wave Structures. 6.1.1 Principle of Leakage Radiation. 6.1.2 Uniform and Periodic Leaky-Wave Structures. 6.1.2.1 Uniform LW Structures. 6.1.2.2 Periodic LW Structures. 6.1.3 Metamaterial Leaky-Wave Structures. 6.2 Backfire-to-Endfire (BE) Leaky-Wave (LW) Antenna. 6.3 Electronically Scanned BE LW Antenna. 6.3.1 Electronic Scanning Principle. 6.3.2 Electronic Beamwidth Control Principle. 6.3.3 Analysis of the Structure and Results. 6.4 Reflecto-Directive Systems. 6.4.1 Passive Retro-Directive Reflector. 6.4.2 Arbitrary-Angle Frequency Tuned Reflector. 6.4.3 Arbitrary-Angle Electronically Tuned Reflector. 6.5 Two-Dimensional Structures. 6.5.1 Two-Dimensional LW Radiation. 6.5.2 Conical-Beam Antenna. 6.5.3 Full-Space Scanning Antenna. 6.6 Zeroth Order Resonating Antenna. 6.7 Dual-Band CRLH-TL Resonating Ring Antenna. 6.8 Focusing Radiative ""Meta-Interfaces"". 6.8.1 Heterodyne Phased Array. 6.8.2 Nonuniform Leaky-Wave Radiator. References. 7 The Future of MTMs. 7.1 ""Real-Artificial"" Materials: the Challenge of Homogenization. 7.2 Quasi-Optical NRI Lenses and Devices. 7.3 Three-Dimensional Isotropic LH MTMs. 7.4 Optical MTMs. 7.5 ""Magnetless"" Magnetic MTMs. 7.6 Terahertz Magnetic MTMs. 7.7 Surface Plasmonic MTMs. 7.8 Antenna Radomes and Frequency Selective Surfaces. 7.9 Nonlinear MTMs. 7.10 Active MTMs. 7.11 Other Topics of Interest. References. Index.",2005,0,2590,214,8,57,101,134,223,215,224,243,211,182
d11f1ac25a813dda546cd2cb7a6cd157afa194dd,"This paper presents a new method for localizing the electric activity in the brain based on multichannel surface EEG recordings. In contrast to the models presented up to now the new method does not assume a limited number of dipolar point sources nor a distribution on a given known surface, but directly computes a current distribution throughout the full brain volume. In order to find a unique solution for the 3-dimensional distribution among the infinite set of different possible solutions, the method assumes that neighboring neurons are simultaneously and synchronously activated. The basic assumption rests on evidence from single cell recordings in the brain that demonstrates strong synchronization of adjacent neurons. In view of this physiological consideration the computational task is to select the smoothest of all possible 3-dimensional current distributions, a task that is a common procedure in generalized signal processing. The result is a true 3-dimensional tomography with the characteristic that localization is preserved with a certain amount of dispersion, i.e., it has a relatively low spatial resolution. The new method, which we call Low Resolution Electromagnetic Tomography (LORETA) is illustrated with two different sets of evoked potential data, the first showing the tomography of the P100 component to checkerboard stimulation of the left, right, upper and lower hemiretina, and the second showing the results for the auditory N100 component and the two cognitive components CNV and P300. A direct comparison of the tomography results with those obtained from fitting one and two dipoles illustrates that the new method provides physiologically meaningful results while dipolar solutions fail in many situations. In the case of the cognitive components, the method offers new hypotheses on the location of higher cognitive functions in the brain.",1994,50,2599,220,0,1,12,16,35,31,28,70,64,68
f576d363baecb17d5b1d1a4ab03a68da016f1fc2,Historical introduction 1. Basic properties of the electromagnetic field 2. Electromagnetic potentials and polarization 3. Foundations of geometrical optics 4. Geometrical theory of optical imaging 5. Geometrical theory of aberrations 6. Image-forming instruments 7. Elements of the theory of interference and interferometers 8. Elements of the theory of diffraction 9. The diffraction theory of aberrations 10. Interference and diffraction with partially coherent light 11. Rigorous diffraction theory 12. Diffraction of light by ultrasonic waves 13. Scattering from inhomogeneous media 14. Optics of metals 15. Optics of crystals 16. Appendices Author index Subject index.,1999,0,3100,57,14,25,34,35,51,57,69,106,106,139
af526b9350f53fa150c2582a2cb5b2cc263330d8,,1977,0,2913,157,0,7,0,3,5,5,4,0,8,10
b2167e98e19ba19a61e88acc773ee1f3e8b39a91,,1969,0,3008,92,0,8,18,35,23,27,33,31,39,39
dcc529acbebd28d2951dbda122f8274e0e52bb0b,"We discuss the validity of standard retrieval methods that assign bulk electromagnetic properties, such as the electric permittivity epsilon and the magnetic permeability mu, from calculations of the scattering (S) parameters for finite-thickness samples. S-parameter retrieval methods have recently become the principal means of characterizing artificially structured metamaterials, which, by nature, are inherently inhomogeneous. While the unit cell of a metamaterial can be made considerably smaller than the free space wavelength, there remains a significant variation of the phase across the unit cell at operational frequencies in nearly all metamaterial structures reported to date. In this respect, metamaterials do not rigorously satisfy an effective medium limit and are closer conceptually to photonic crystals. Nevertheless, we show here that a modification of the standard S-parameter retrieval procedure yields physically reasonable values for the retrieved electromagnetic parameters, even when there is significant inhomogeneity within the unit cell of the structure. We thus distinguish a metamaterial regime, as opposed to the effective medium or photonic crystal regimes, in which a refractive index can be rigorously established but where the wave impedance can only be approximately defined. We present numerical simulations on typical metamaterial structures to illustrate the modified retrieval algorithm and the impact on the retrieved material parameters. We find that no changes to the standard retrieval procedures are necessary when the inhomogeneous unit cell is symmetric along the propagation axis; however, when the unit cell does not possess this symmetry, a modified procedure--in which a periodic structure is assumed--is required to obtain meaningful electromagnetic material parameters.",2005,2,2111,50,1,23,42,48,87,86,127,152,148,169
008d140ab2aad3a95380207f47f2c8a08497f7a2,"When time-domain electromagnetic-field equations are solved using finite-difference techniques in unbounded space, there must be a method limiting the domain in which the field is computed. This is achieved by truncating the mesh and using absorbing boundary conditions at its artificial boundaries to simulate the unbounded surroundings. This paper presents highly absorbing boundary conditions for electromagnetic-field equations that can be used for both two-and three-dimensional configurations. Numerical results are given that clearly exhibit the accuracy and limits of applicability of highly absorbing boundary conditions. A simplified, but equally accurate, absorbing condition is derived for two- dimensional time-domain electromagnetic-field problems.",1981,20,2441,94,0,2,1,1,6,3,10,12,19,37
51d8bcee2a5b1648bf2087f99a1d6e8b7d43097c,"Achieving control of light-material interactions for photonic device applications at nanoscale dimensions will require structures that guide electromagnetic energy with a lateral mode confinement below the diffraction limit of light. This cannot be achieved by using conventional waveguides1 or photonic crystals2. It has been suggested that electromagnetic energy can be guided below the diffraction limit along chains of closely spaced metal nanoparticles3,4 that convert the optical mode into non-radiating surface plasmons5. A variety of methods such as electron beam lithography6 and self-assembly7 have been used to construct metal nanoparticle plasmon waveguides. However, all investigations of the optical properties of these waveguides have so far been confined to collective excitations8,9,10, and direct experimental evidence for energy transport along plasmon waveguides has proved elusive. Here we present observations of electromagnetic energy transport from a localized subwavelength source to a localized detector over distances of about 0.5 μm in plasmon waveguides consisting of closely spaced silver rods. The waveguides are excited by the tip of a near-field scanning optical microscope, and energy transport is probed by using fluorescent nanospheres.",2003,17,2027,11,18,46,65,94,142,111,145,132,139,170
67f6126040381d0b5c89376892a65a7df4fff918,"One of the most striking phenomena in condensed-matter physics is the quantum Hall effect, which arises in two-dimensional electron systems subject to a large magnetic field applied perpendicular to the plane in which the electrons reside. In such circumstances, current is carried by electrons along the edges of the system, in so-called chiral edge states (CESs). These are states that, as a consequence of nontrivial topological properties of the bulk electronic band structure, have a unique directionality and are robust against scattering from disorder. Recently, it was theoretically predicted that electromagnetic analogues of such electronic edge states could be observed in photonic crystals, which are materials having refractive-index variations with a periodicity comparable to the wavelength of the light passing through them. Here we report the experimental realization and observation of such electromagnetic CESs in a magneto-optical photonic crystal fabricated in the microwave regime. We demonstrate that, like their electronic counterparts, electromagnetic CESs can travel in only one direction and are very robust against scattering from disorder; we find that even large metallic scatterers placed in the path of the propagating edge modes do not induce reflections. These modes may enable the production of new classes of electromagnetic device and experiments that would be impossible using conventional reciprocal photonic states alone. Furthermore, our experimental demonstration and study of photonic CESs provides strong support for the generalization and application of topological band theories to classical and bosonic systems, and may lead to the realization and observation of topological phenomena in a generally much more controlled and customizable fashion than is typically possible with electronic systems.",2009,37,1553,14,1,15,43,58,55,67,102,117,143,235
462770e4b0b8f794a40185c7e99273bec9b7ab22,"A first year graduate text on electromagnetic field theory emphasizing mathematical approaches, problem solving and physical interpretation. Examples deal with guidance propagation, radiation, and scattering of electromagnetic waves; metallic and dielectric wave guides, resonators, antennas and radiating structures, Cerenkov radiation, moving media, plasmas, crystals, integrated optics, lasers and fibers, remote sensing, geophysical probing, dipole antennas and stratified media.",1986,9,1943,84,2,2,5,14,14,23,25,31,33,26
4424ae3af143d7d7b3dec6669e95efff825a7e95,"We review the basic physics of surface-plasmon excitations occurring at metal/dielectric interfaces with special emphasis on the possibility of using such excitations for the localization of electromagnetic energy in one, two, and three dimensions, in a context of applications in sensing and waveguiding for functional photonic devices. Localized plasmon resonances occurring in metallic nanoparticles are discussed both for single particles and particle ensembles, focusing on the generation of confined light fields enabling enhancement of Raman-scattering and nonlinear processes. We then survey the basic properties of interface plasmons propagating along flat boundaries of thin metallic films, with applications for waveguiding along patterned films, stripes, and nanowires. Interactions between plasmonic structures and optically active media are also discussed.",2005,185,1669,13,3,48,68,78,96,110,134,147,135,122
1423c21de05c4345a5907ad55f9f6a0e0bdc148f,"The fast multipole method (FMM) and multilevel fast multipole algorithm (MLFMA) are reviewed. The number of modes required, block-diagonal preconditioner, near singularity extraction, and the choice of initial guesses are discussed to apply the MLFMA to calculating electromagnetic scattering by large complex objects. Using these techniques, we can solve the problem of electromagnetic scattering by large complex three-dimensional (3-D) objects such as an aircraft (VFY218) on a small computer.",1997,40,1469,172,0,20,24,30,30,27,55,46,45,53
e3083dcadcc1a9c4294d6f83b40d4fb27b8c4666,"There has been tremendous advances in our ability to produce images of human brain function. Applications of functional brain imaging extend from improving our understanding of the basic mechanisms of cognitive processes to better characterization of pathologies that impair normal function. Magnetoencephalography (MEG) and electroencephalography (EEG) (MEG/EEG) localize neural electrical activity using noninvasive measurements of external electromagnetic signals. Among the available functional imaging techniques, MEG and EEG uniquely have temporal resolutions below 100 ms. This temporal precision allows us to explore the timing of basic neural processes at the level of cell assemblies. MEG/EEG source localization draws on a wide range of signal processing techniques including digital filtering, three-dimensional image analysis, array signal processing, image modeling and reconstruction, and, blind source separation and phase synchrony estimation. We describe the underlying models currently used in MEG/EEG source estimation and describe the various signal processing steps required to compute these sources. In particular we describe methods for computing the forward fields for known source distributions and parametric and imaging-based approaches to the inverse problem.",2001,75,1531,119,0,11,21,50,51,56,64,61,83,75
c26c55df0cfd4f81f70942cd8cc7965cee2d32ff,"We use the discrete dipole approximation to investigate the electromagnetic fields induced by optical excitation of localized surface plasmon resonances of silver nanoparticles, including monomers and dimers, with emphasis on what size, shape, and arrangement leads to the largest local electric field (E-field) enhancement near the particle surfaces. The results are used to determine what conditions are most favorable for producing enhancements large enough to observe single molecule surface enhanced Raman spectroscopy. Most of the calculations refer to triangular prisms, which exhibit distinct dipole and quadrupole resonances that can easily be controlled by varying particle size. In addition, for the dimer calculations we study the influence of dimer separation and orientation, especially for dimers that are separated by a few nanometers. We find that the largest /E/2 values for dimers are about a factor of 10 larger than those for all the monomers examined. For all particles and particle orientations, the plasmon resonances which lead to the largest E-fields are those with the longest wavelength dipolar excitation. The spacing of the particles in the dimer plays a crucial role, and we find that the spacing needed to achieve a given /E/2 is proportional to nanoparticle size for particles below 100 nm in size. Particle shape and curvature are of lesser importance, with a head to tail configuration of two triangles giving enhanced fields comparable to head to head, or rounded head to tail. The largest /E/2 values we have calculated for spacings of 2 nm or more is approximately 10(5).",2004,52,1562,23,9,19,37,48,77,74,93,121,145,145
af418f2a017b505fb6cd3bac87d491b704790c30,"An investigation is made of the structure of the electromagnetic field near the focus of an aplanatic system which images a point source. First the case of a linearly polarized incident field is examined and expressions are derived for the electric and magnetic vectors in the image space. Some general consequences of the formulae are then discussed. In particular the symmetry properties of the field with respect to the focal plane are noted and the state of polarization of the image region is investigated. The distribution of the time-averaged electric and magnetic energy densities and of the energy flow (Poynting vector) in the focal plane is studied in detail, and the results are illustrated by diagrams and in a tabulated form based on data obtained by extensive calculations on an electronic computor. The case of an unpolarized field is also investigated. The solution is riot restricted to systems of low aperture, and the computational results cover, in fact, selected values of the angular semi-aperture a on the image side, in the whole range 0 ≤ α ≤ 90°. The limiting case α → 0 is examined in detail and it is shown that the field is then completely characterized by a single, generally complex, scalar function, which turns out to be identical with that of the classical scalar theory of Airy, Lommel and Struve. The results have an immediate bearing on the resolving power of image forming systems; they also help our understanding of the significance of the scalar diffraction theory, which is customarily employed, without a proper justification, in the analysis of images in lowaperture systems.",1959,7,2454,81,0,0,1,0,0,1,1,1,1,0
45d875db775c224c225311e496fab93d1fb6a51d,"In this paper basic mathematical and physical concepts of the biomagnetic inverse problem are reviewed with some new approaches. The forward problem is discussed for both homogeneous and inhomogeneous media. Geselowitz' formulae and a surface integral equation are presented to handle a piecewise homogeneous conductor. The special cases of a spherically symmetric conductor and a horizontally layered medium are discussed in detail. The non-uniqueness of the solution of the magnetic inverse problem is discussed and the difficulty caused by the contribution of the electric potential to the magnetic field outside the conductor is studied. As practical methods of solving the inverse problem, a weighted least-squares search with confidence limits and the method of minimum norm estimate are discussed.",1987,13,1773,104,4,2,18,8,13,18,20,25,26,35
f1ccc56e5f128d935d35fc26de184f21913738be,"The electromagnetic interference (EMI) shielding mechanisms of multi-walled carbon nanotube (MWCNT)/polymer composites were analyzed experimentally and theoretically. For the experimental analysis, EMI shielding effectiveness (SE) of MWCNT/polypropylene (PP) composite plates made in three different thicknesses and at four different concentrations were studied. A model based on the shielding of electromagnetic plane wave was used to theoretically study the EMI shielding mechanisms. The experimental results showed that absorption is the major shielding mechanism and reflection is the secondary shielding mechanism. The modeling results demonstrated that multiple-reflection within MWCNT internal surfaces and between MWCNT external surfaces decrease the overall EMI SE. The EMI SE of MWCNT/PP composites increased with increase in MWCNT content and shielding plate thickness.",2009,37,1024,12,1,9,37,39,61,84,83,115,111,120
f666190e624adbd530e3635624eb193c94639865,"This book concerns the use of concepts from statistical physics in the description of financial systems. The authors illustrate the scaling concepts used in probability theory, critical phenomena, and fully developed turbulent fluids. These concepts are then applied to financial time series. The authors also present a stochastic model that displays several of the statistical properties observed in empirical data. Statistical physics concepts such as stochastic dynamics, short- and long-range correlations, self-similarity and scaling permit an understanding of the global behaviour of economic systems without first having to work out a detailed microscopic description of the system. Physicists will find the application of statistical physics concepts to economic systems interesting. Economists and workers in the financial world will find useful the presentation of empirical analysis methods and well-formulated theoretical tools that might help describe systems composed of a huge number of interacting subsystems.",1999,234,2885,140,18,53,88,82,78,118,115,123,149,139
6eddc19efa13f7e70301908d98e85a19d6f32a02,"Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed.",2002,46,30869,4468,0,0,0,0,0,0,0,0,0,0
5c8fe9a0412a078e30eb7e5eeb0068655b673e86,"Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLARANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.",1996,19,16999,2425,0,0,0,0,0,0,0,0,0,0
3c718363c22221fd16771672da3bfd5f67d2c34c,"We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.",2009,42,8802,1393,71,136,273,396,568,732,879,973,941,1108
d36efb9ad91e00faa334b549ce989bfae7e2907a,"Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed.",1977,136,47881,2148,0,0,0,0,0,0,0,0,0,1
023f6fc69fe1f6498e35dbf85932ecb549d36ca4,"This paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints. This problem may be understood as the convex relaxation of a rank minimization problem and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix problem). Off-the-shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries. This paper develops a simple first-order and easy-to-implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank. The algorithm is iterative, produces a sequence of matrices $\{\boldsymbol{X}^k,\boldsymbol{Y}^k\}$, and at each step mainly performs a soft-thresholding operation on the singular values of the matrix $\boldsymbol{Y}^k$. There are two remarkable features making this attractive for low-rank matrix completion problems. The first is that the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of the iterates $\{\boldsymbol{X}^k\}$ is empirically nondecreasing. Both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low. On the theoretical side, we provide a convergence analysis showing that the sequence of iterates converges. On the practical side, we provide numerical examples in which $1,000\times1,000$ matrices are recovered in less than a minute on a modest desktop computer. We also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are connected with the recent literature on linearized Bregman iterations for $\ell_1$ minimization, and we develop a framework in which one can understand these algorithms in terms of well-known Lagrange multiplier algorithms.",2008,113,4463,396,3,49,89,139,190,257,371,413,480,551
d260b5c495daec9149319da796b9710f53deb8f3,"The increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. We describe a new approach, based on the maximum- likelihood principle, which clearly satisfies these requirements. The core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. This algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. Due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. We used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. The reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. For example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 rbcL sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. This new method is implemented in the PHYML program, which is freely available on our web page: http://www.lirmm.fr/w3ifa/MAAS/.",2003,70,15596,3668,0,0,0,0,0,0,1,3,233,1355
8978cf7574ceb35f4c3096be768c7547b28a35d0,"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",2006,33,12844,1058,0,0,0,0,0,1,1,6,29,664
034b2b97e6b23061f6f71a5e19c1b03bf4c19ec8,"In recent years, various heuristic optimization methods have been developed. Many of these methods are inspired by swarm behaviors in nature. In this paper, a new optimization algorithm based on the law of gravity and mass interactions is introduced. In the proposed algorithm, the searcher agents are a collection of masses which interact with each other based on the Newtonian gravity and the laws of motion. The proposed method has been compared with some well-known heuristic search methods. The obtained results confirm the high performance of the proposed method in solving various nonlinear functions.",2009,41,4294,389,2,20,64,127,211,299,373,398,453,512
288f41a655a178bf28d5883f68aa95807edbc950,,1963,0,26729,779,0,0,0,0,0,0,0,0,0,0
7bb9bab74df4d2939bbdf41fc33027b59e0f229e,"We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from t...",1986,15,10250,1068,0,0,0,0,0,0,84,108,164,191
d92f735b0773b4e697e7e72798eccae2f647acd6,"We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.",1987,36,11951,518,0,0,2,3,2,2,7,4,3,4
2599131a4bc2fa957338732a37c744cfe3e17b24,"A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.",1992,41,10457,814,0,0,0,1,0,2,1,2,0,3
730543ef418a898cb94b5854b79ea35b082abf89,"Despite recent advances achieved by application of high-performance computing methods and novel algorithmic techniques to maximum likelihood (ML)-based inference programs, the major computational bottleneck still consists in the computation of bootstrap support values. Conducting a probably insufficient number of 100 bootstrap (BS) analyses with current ML programs on large datasets-either with respect to the number of taxa or base pairs-can easily require a month of run time. Therefore, we have developed, implemented, and thoroughly tested rapid bootstrap heuristics in RAxML (Randomized Axelerated Maximum Likelihood) that are more than an order of magnitude faster than current algorithms. These new heuristics can contribute to resolving the computational bottleneck and improve current methodology in phylogenetic analyses. Computational experiments to assess the performance and relative accuracy of these heuristics were conducted on 22 diverse DNA and AA (amino acid), single gene as well as multigene, real-world alignments containing 125 up to 7764 sequences. The standard BS (SBS) and rapid BS (RBS) values drawn on the best-scoring ML tree are highly correlated and show almost identical average support values. The weighted RF (Robinson-Foulds) distance between SBS- and RBS-based consensus trees was smaller than 6% in all cases (average 4%). More importantly, RBS inferences are between 8 and 20 times faster (average 14.73) than SBS analyses with RAxML and between 18 and 495 times faster than BS analyses with competing programs, such as PHYML or GARLI. Moreover, this performance improvement increases with alignment size. Finally, we have set up two freely accessible Web servers for this significantly improved version of RAxML that provide access to the 200-CPU cluster of the Vital-IT unit at the Swiss Institute of Bioinformatics and the 128-CPU cluster of the CIPRES project at the San Diego Supercomputer Center. These Web servers offer the possibility to conduct large-scale phylogenetic inferences to a large part of the community that does not have access to, or the expertise to use, high-performance computing resources.",2008,44,6294,1446,17,185,416,510,570,655,675,582,568,500
0e6beb95b5150ce99b108acdefabf70ccd3fee30,"An efficient method for the calculation of the interactions of a 2' factorial ex- periment was introduced by Yates and is widely known by his name. The generaliza- tion to 3' was given by Box et al. (1). Good (2) generalized these methods and gave elegant algorithms for which one class of applications is the calculation of Fourier series. In their full generality, Good's methods are applicable to certain problems in which one must multiply an N-vector by an N X N matrix which can be factored into m sparse matrices, where m is proportional to log N. This results inma procedure requiring a number of operations proportional to N log N rather than N2. These methods are applied here to the calculation of complex Fourier series. They are useful in situations where the number of data points is, or can be chosen to be, a highly composite number. The algorithm is here derived and presented in a rather different form. Attention is given to the choice of N. It is also shown how special advantage can be obtained in the use of a binary computer with N = 2' and how the entire calculation can be performed within the array of N data storage locations used for the given Fourier coefficients. Consider the problem of calculating the complex Fourier series N-1 (1) X(j) = EA(k)-Wjk, j = 0 1, * ,N- 1, k=0",1965,8,11146,601,0,0,3,1,0,2,3,4,1,3
b76be6707fa5858cc5378bc11f10ec6f6a97d85c,"Decomposition is a basic strategy in traditional multiobjective optimization. However, it has not yet been widely used in multiobjective evolutionary optimization. This paper proposes a multiobjective evolutionary algorithm based on decomposition (MOEA/D). It decomposes a multiobjective optimization problem into a number of scalar optimization subproblems and optimizes them simultaneously. Each subproblem is optimized by only using information from its several neighboring subproblems, which makes MOEA/D have lower computational complexity at each generation than MOGLS and nondominated sorting genetic algorithm II (NSGA-II). Experimental results have demonstrated that MOEA/D with simple decomposition methods outperforms or performs similarly to MOGLS and NSGA-II on multiobjective 0-1 knapsack problems and continuous multiobjective optimization problems. It has been shown that MOEA/D using objective normalization can deal with disparately-scaled objectives, and MOEA/D with an advanced decomposition method can generate a set of very evenly distributed solutions for 3-objective test instances. The ability of MOEA/D with small population, the scalability and sensitivity of MOEA/D have also been experimentally investigated in this paper.",2007,48,4806,772,4,17,40,65,82,137,190,265,348,404
321759af5a7d74afeb515719d8da25fba4759764,"Abstract.We present a primal-dual interior-point algorithm with a filter line-search method for nonlinear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the feasibility restoration phase for the filter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlinear programming.",2006,35,5805,565,42,71,129,129,153,208,274,301,392,417
21b65bbf11bc246a80b7f08043db3ff5c0844e91,"This paper describes DARTEL, which is an algorithm for diffeomorphic image registration. It is implemented for both 2D and 3D image registration and has been formulated to include an option for estimating inverse consistent deformations. Nonlinear registration is considered as a local optimisation problem, which is solved using a Levenberg-Marquardt strategy. The necessary matrix solutions are obtained in reasonable time using a multigrid method. A constant Eulerian velocity framework is used, which allows a rapid scaling and squaring method to be used in the computations. DARTEL has been applied to intersubject registration of 471 whole brain images, and the resulting deformations were evaluated in terms of how well they encode the shape information necessary to separate male and female subjects and to predict the ages of the subjects.",2007,45,5926,409,1,26,65,107,213,323,411,549,568,546
83b522f4bfa5db7f7d34f839475af7d078107634,"In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data",2006,55,7245,448,10,24,58,104,153,245,377,558,732,838
4cb5dd388ea707bd421226d8c1ef61e7f56982e2,"Swarm intelligence is a research branch that models the population of interacting agents or swarms that are able to self-organize. An ant colony, a flock of birds or an immune system is a typical example of a swarm system. Bees’ swarming around their hive is another example of swarm intelligence. Artificial Bee Colony (ABC) Algorithm is an optimization algorithm based on the intelligent behaviour of honey bee swarm. In this work, ABC algorithm is used for optimizing multivariable functions and the results produced by ABC, Genetic Algorithm (GA), Particle Swarm Algorithm (PSO) and Particle Swarm Inspired Evolutionary Algorithm (PS-EA) have been compared. The results showed that ABC outperforms the other algorithms.",2007,30,5511,398,4,7,43,96,210,288,395,434,513,524
68c1bfe375dde46777fe1ac8f3636fb651e3f0f8,"In an earlier paper, we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's ""bagging"" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",1996,34,8451,718,16,46,81,115,118,128,172,230,305,320
c02dfd94b11933093c797c362e2f8f6a3b9b8012,"Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.",2001,14,8195,661,0,19,69,112,159,189,217,269,322,391
f6e0fb4c77906bc23fe59a8f848ce62ba9687181,"In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a pre-specified set of linear transforms, or by adapting the dictionary to a set of training signals. Both these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method – the K-SVD algorithm – generalizing the K-Means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results on both synthetic tests and in applications on real image data.",2005,49,5949,1135,4,9,26,53,92,133,226,346,489,637
f813ebf28ae4bd689a8f263b909a2f847b0fcf99,"Phylogenetic trees have a multitude of applications in biology, epidemiology, conservation and even forensics. However, the inference of phylogenetic trees can be extremely computationally intensive. The computational burden of such analyses becomes even greater when model-based methods are used. Model-based methods have been repeatedly shown to be the most accurate choice for the reconstruction of phylogenetic trees, and thus are an attractive choice despite their high computational demands. Using the Maximum Likelihood (ML) criterion to choose among phylogenetic trees is one commonly used model-based technique. Until recently, software for performing ML analyses of biological sequence data was largely intractable for more vi than about one hundred sequences. Because advances in sequencing technology now make the assembly of datasets consisting of thousands of sequences common, ML search algorithms that are able to quickly and accurately analyze such data must be developed if ML techniques are to remain a viable option in the future. I have developed a fast and accurate algorithm that allows ML phylogenetic searches to be performed on datasets consisting of thousands of sequences. My software uses a genetic algorithm approach, and is named GARLI (Genetic Algorithm for Rapid Likelihood Inference). The speed of this new algorithm results primarily from its novel technique for partial optimization of branch-length parameters following topological rearrangements. Experiments performed with GARLI show that it is able to analyze large datasets in a small fraction of the time required by the previous generation of search algorithms. The program also performs well relative to two other recently introduced fast ML search programs. Large parallel computer clusters have become common at academic institutions in recent years, presenting a new resource to be used for phylogenetic analyses. The P-GARLI algorithm extends the approach of GARLI to allow simultaneous use of many computer processors. The processors may be instructed to work together on a phylogenetic search in either a highly coordinated or largely independent fashion.",2006,79,3334,1291,2,53,184,236,321,321,333,337,338,313
9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d,"We propose a new measure, the method noise, to evaluate and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local smoothing filters. Second, we propose a new algorithm, the nonlocal means (NL-means), based on a nonlocal averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local smoothing filters.",2005,22,5574,518,5,19,43,62,96,149,211,272,360,422
29d4dfae2807a67a2c66c720b4985cb599c4e245,"Many networks display community structure--groups of vertices within which connections are dense but between which they are sparser--and sensitive computer algorithms have in recent years been developed for detecting this structure. These algorithms, however, are computationally demanding, which limits their application to small networks. Here we describe an algorithm which gives excellent results when tested on both computer-generated and real-world networks and is much faster, typically thousands of times faster, than previous algorithms. We give several example applications, including one to a collaboration network of more than 50,000 physicists.",2003,45,4515,416,3,15,51,80,117,151,233,207,244,287
a2ec66b68bdc119443a3b07448f4fab3258f314a,"In reporting Implicit Association Test (IAT) results, researchers have most often used scoring conventions described in the first publication of the IAT (A.G. Greenwald, D.E. McGhee, & J.L.K. Schwartz, 1998). Demonstration IATs available on the Internet have produced large data sets that were used in the current article to evaluate alternative scoring procedures. Candidate new algorithms were examined in terms of their (a) correlations with parallel self-report measures, (b) resistance to an artifact associated with speed of responding, (c) internal consistency, (d) sensitivity to known influences on IAT measures, and (e) resistance to known procedural influences. The best-performing measure incorporates data from the IAT's practice trials, uses a metric that is calibrated by each respondent's latency variability, and includes a latency penalty for errors. This new algorithm strongly outperforms the earlier (conventional) procedure.",2003,37,4684,370,9,28,53,91,154,150,212,243,280,320
4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a,"We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted p-penalties on the coefficients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. Use of such p-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. © 2004 Wiley Periodicals, Inc.",2003,55,4189,371,2,8,28,31,100,159,215,212,254,250
b13724cb54ae4171916f3f969d304b9e9752a57f,"The Strength Pareto Evolutionary Algorithm (SPEA) (Zitzler and Thiele 1999) is a relatively recent technique for finding or approximating the Pareto-optimal set for multiobjective optimization problems. In different studies (Zitzler and Thiele 1999; Zitzler, Deb, and Thiele 2000) SPEA has shown very good performance in comparison to other multiobjective evolutionary algorithms, and therefore it has been a point of reference in various recent investigations, e.g., (Corne, Knowles, and Oates 2000). Furthermore, it has been used in different applications, e.g., (Lahanas, Milickovic, Baltas, and Zamboglou 2001). In this paper, an improved version, namely SPEA2, is proposed, which incorporates in contrast to its predecessor a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method. The comparison of SPEA2 with SPEA and two other modern elitist methods, PESA and NSGA-II, on different test problems yields promising results.",2001,48,4842,716,4,26,61,68,100,109,202,195,262,270
7c46799502bebfe6a9ae0f457b7b8b92248ec260,"An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",1980,47,7945,298,3,14,27,9,28,44,61,54,102,110
aa7bd176c83cef1ccad8cfcfb10e73713bfa8e8d,"We have developed a real-time algorithm for detection of the QRS complexes of ECG signals. It reliably recognizes QRS complexes based upon digital analyses of slope, amplitude, and width. A special digital bandpass filter reduces false detections caused by the various types of interference present in ECG signals. This filtering permits use of low thresholds, thereby increasing detection sensitivity. The algorithm automatically adjusts thresholds and parameters periodically to adapt to such ECG changes as QRS morphology and heart rate. For the standard 24 h MIT/BIH arrhythmia database, this algorithm correctly detects 99.3 percent of the QRS complexes.",1985,14,5809,399,1,3,2,8,7,15,12,12,14,10
18f355d7ef4aa9f82bf5c00f84e46714efa5fd77,"This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm.",1978,11,5293,507,2,7,15,15,33,32,24,23,36,25
070eb63031af260d840534b33fd8385b7152fda6,"For aligning DNA sequences that differ only by sequencing errors, or by equivalent errors from other sources, a greedy algorithm can be much faster than traditional dynamic programming approaches and yet produce an alignment that is guaranteed to be theoretically optimal. We introduce a new greedy alignment algorithm with particularly good performance and show that it computes the same alignment as does a certain dynamic programming algorithm, while executing over 10 times faster on appropriate data. An implementation of this algorithm is currently used in a program that assembles the UniGene database at the National Center for Biotechnology Information.",2000,38,4172,523,2,13,24,41,59,54,86,99,85,158
7d50991b693fc23edda316fb1487f114f6cc6706,"The first unified account of the theory, methodology, and applications of the EM algorithm and its extensionsSince its inception in 1977, the Expectation-Maximization (EM) algorithm has been the subject of intense scrutiny, dozens of applications, numerous extensions, and thousands of publications. The algorithm and its extensions are now standard tools applied to incomplete data problems in virtually every field in which statistical methods are used. Until now, however, no single source offered a complete and unified treatment of the subject.The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. Employing numerous examples, Geoffrey McLachlan and Thriyambakam Krishnan examine applications both in evidently incomplete data situations-where data are missing, distributions are truncated, or observations are censored or grouped-and in a broad variety of situations in which incompleteness is neither natural nor evident. They point out the algorithm's shortcomings and explain how these are addressed in the various extensions.Areas of application discussed include: Regression Medical imaging Categorical data analysis Finite mixture analysis Factor analysis Robust statistical modeling Variance-components estimation Survival analysis Repeated-measures designs For theoreticians, practitioners, and graduate students in statistics as well as researchers in the social and physical sciences, The EM Algorithm and Extensions opens the door to the tremendous potential of this remarkably versatile statistical tool.",1996,1,6018,352,2,19,47,70,117,134,180,219,234,294
688384fc5e643445e835435e96b9dfcfb6598d36,"An algorithm is presented for the rapid evaluation of the potential and force fields in systems involving large numbers of particles whose interactions are Coulombic or gravitational in nature. For a system ofNparticles, an amount of work of the orderO(N2) has traditionally been required to evaluate all pairwise interactions, unless some approximation or truncation method is used. The algorithm of the present paper requires an amount of work proportional toNto evaluate all interactions to within roundoff error, making it considerably more practical for large-scale problems encountered in plasma physics, fluid dynamics, molecular dynamics, and celestial mechanics.",1987,19,4714,380,2,12,10,25,27,27,40,49,84,101
27e2d4a36ce20138cfb05c1ea60d7c6dceb20392,"Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information. However, as recent experience shows, the anonymous, open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files.We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value, based on the peer's history of uploads. We present a distributed and secure method to compute global trust values, based on Power iteration. By having peers use these global trust values to choose the peers from whom they download, the network effectively identifies malicious peers and isolates them from the network.In simulations, this reputation system, called EigenTrust, has been shown to significantly decrease the number of inauthentic files on the network, even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system.",2003,23,3806,369,27,118,174,273,294,294,306,322,304,298
ba293ef8bfac8c0483521e325c1578586ccb3f13,"The particle swarm algorithm adjusts the trajectories of a population of ""particles"" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.",1997,7,4176,434,1,4,5,1,3,9,8,38,58,93
59c9f2036e673d8bc9713eed851d12c6c9fe53cb,A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.,1977,16,5426,352,0,5,2,1,2,3,2,3,12,6
298d799da82395a64a3bda38ef9d2a4646828ccb,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",1996,26,5311,317,14,35,75,66,77,76,93,130,121,120
08c370eb9ba13bfb836349e7f3ea428be4697818,"Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of ""local"" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative ""turbo"" decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms.",2001,65,3956,294,33,56,96,84,162,200,224,197,246,234
229a5963aa3649b5a9ad714c5764b07ddd8918d5,"Prediction of small molecule binding modes to macromolecules of known three-dimensional structure is a problem of paramount importance in rational drug design (the ""docking"" problem). We report the development and validation of the program GOLD (Genetic Optimisation for Ligand Docking). GOLD is an automated ligand docking program that uses a genetic algorithm to explore the full range of ligand conformational flexibility with partial flexibility of the protein, and satisfies the fundamental requirement that the ligand must displace loosely bound water on binding. Numerous enhancements and modifications have been applied to the original technique resulting in a substantial increase in the reliability and the applicability of the algorithm. The advanced algorithm has been tested on a dataset of 100 complexes extracted from the Brookhaven Protein DataBank. When used to dock the ligand back into the binding site, GOLD achieved a 71% success rate in identifying the experimental binding mode.",1997,125,4990,328,2,21,31,34,38,54,79,120,126,159
64a877d135db3acbc23c295367927176f332595f,"Abstract This paper transmits a FORTRAN-IV coding of the fuzzy c -means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.",1984,28,4547,358,1,1,4,1,4,1,6,2,13,5
916ceefae4b11dadc3ee754ce590381c568c90de,"A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques.<<ETX>>",1993,6,4378,309,5,14,20,38,60,70,88,77,107,109
435c91f9103f7e2314f7e1298ed18ff4cd602f0b,"This document describes the MD5 message-digest algorithm. The
algorithm takes as input a message of arbitrary length and produces as
output a 128-bit ""fingerprint"" or ""message digest"" of the input. This
memo provides information for the Internet community. It does not
specify an Internet standard.",1992,0,4032,328,11,29,32,62,81,94,103,111,111,127
30cadff20998ea7bea6da42fa0eed48334fdde1e,"An iterative method is given for solving Ax ~ffi b and minU Ax b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned.",1982,38,3834,315,3,2,4,5,10,12,20,19,31,26
e99603b5b524485bcf1afb2f01acadc34dfb033c,,1979,2,8898,212,0,1,4,1,1,1,3,0,3,4
a651bb7cc7fc68ece0cc66ab921486d163373385,"Purpose – The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach – An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings – Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value – The piece provides a useful historical document on information retrieval.",1980,13,6360,248,1,1,1,0,1,1,2,5,1,3
ae0ae3baf338f5454d58074d58386246eb5009e5,"A convolution-backprojection formula is deduced for direct reconstruction of a three-dimensional density function from a set of two-dimensional projections. The formula is approximate but has useful properties, including errors that are relatively small in many practical instances and a form that leads to convenient computation. It reduces to the standard fan-beam formula in the plane that is perpendicular to the axis of rotation and contains the point source. The algorithm is applied to a mathematical phantom as an example of its performance.",1984,17,5591,246,1,1,3,5,13,14,27,20,23,24
78e3847fde0b618a0702606323ccebeda3ecb77d,"The k_t and Cambridge/Aachen inclusive jet finding algorithms for hadron-hadron collisions can be seen as belonging to a broader class of sequential recombination jet algorithms, parametrised by the power of the energy scale in the distance measure. We examine some properties of a new member of this class, for which the power is negative. This ``anti-k_t'' algorithm essentially behaves like an idealised cone algorithm, in that jets with only soft fragmentation are conical, active and passive areas are equal, the area anomalous dimensions are zero, the non-global logarithms are those of a rigid boundary and the Milan factor is universal. None of these properties hold for existing sequential recombination algorithms, nor for cone algorithms with split--merge steps, such as SISCone. They are however the identifying characteristics of the collinear unsafe plain ``iterative cone'' algorithm, for which the anti-k_t algorithm provides a natural, fast, infrared and collinear safe replacement.",2008,61,5004,161,12,27,91,231,311,374,408,488,542,556
e2b9dcd4e75379a59bf7e761ac0710f42bde2f14,"Many optimization problems in various fields have been solved using diverse optimization al gorithms. Traditional optimization techniques such as linear programming (LP), non-linear programming (NLP), and dynamic program ming (DP) have had major roles in solving these problems. However, their drawbacks generate demand for other types of algorithms, such as heuristic optimization approaches (simulated annealing, tabu search, and evolutionary algo rithms). However, there are still some possibili ties of devising new heuristic algorithms based on analogies with natural or artificial phenom ena. A new heuristic algorithm, mimicking the improvisation of music players, has been devel oped and named Harmony Search (HS). The performance of the algorithm is illustrated with a traveling salesman problem (TSP), a specific academic optimization problem, and a least-cost pipe network design problem.",2001,17,4691,210,1,1,0,4,9,13,20,36,104,158
19d4da2841588fc75ccedcfb443d73772413d98e,"The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.",2001,32,3823,272,2,28,67,81,98,132,168,143,176,186
a3819dda9a5f00dbb8cd3413ca7422e37a0d5794,"A fast and flexible algorithm for computing watersheds in digital gray-scale images is introduced. A review of watersheds and related motion is first presented, and the major methods to determine watersheds are discussed. The algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using of queue of pixel. It is described in detail provided in a pseudo C language. The accuracy of this algorithm is proven to be superior to that of the existing implementations, and it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images (and even to graphs) are straightforward. The algorithm is reported to be faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for magnetic resonance (MR) imagery and for digital elevation models. An example of 3-D watershed is also provided. >",1991,69,5696,221,2,13,21,18,37,63,68,71,76,90
ce9a21b93ba29d4145a8ef6bf401e77f261848de,"The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.",1989,27,3695,262,21,68,62,154,113,104,86,89,101,93
e085f9a107c562a59b28c1136aee7a438387ac0f,"Abstract We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.",1995,76,3798,230,5,15,22,46,63,75,84,77,90,106
d45f2b8f6819fc4c524794b3cfc0fa4c4f7d43f4,"Detection of protein families in large databases is one of the principal research objectives in structural and functional genomics. Protein family classification can significantly contribute to the delineation of functional diversity of homologous proteins, the prediction of function based on domain architecture or the presence of sequence motifs as well as comparative genomics, providing valuable evolutionary insights. We present a novel approach called TRIBE-MCL for rapid and accurate clustering of protein sequences into families. The method relies on the Markov cluster (MCL) algorithm for the assignment of proteins into families based on precomputed sequence similarity information. This novel approach does not suffer from the problems that normally hinder other protein sequence clustering algorithms, such as the presence of multi-domain proteins, promiscuous domains and fragmented proteins. The method has been rigorously tested and validated on a number of very large databases, including SwissProt, InterPro, SCOP and the draft human genome. Our results indicate that the method is ideally suited to the rapid and accurate detection of protein families on a large scale. The method has been used to detect and categorise protein families within the draft human genome and the resulting families have been used to annotate a large proportion of human proteins.",2002,60,3169,401,1,28,48,55,79,113,117,148,146,169
b49eee1b10ac493c1428a80753436fb4c28f6701,"Motivated by the fast‐growing need to compute centrality indices on large, yet very sparse, networks, new algorithms for betweenness are introduced in this paper. They require O(n + m) space and run in O(nm) and O(nm + n2 log n) time on unweighted and weighted networks, respectively, where m is the number of links. Experimental evidence is provided that this substantially increases the range of networks for which centrality analysis is feasible. The betweenness centrality index is essential in the analysis of social networks, but costly to compute. Currently, the fastest known algorithms require ?(n 3) time and ?(n 2) space, where n is the number of actors in the network.",2001,31,3575,254,3,10,14,28,41,49,66,86,112,151
8e617b8c63dd35d9913bbc104d0666ffd10e9e6a,,2002,1,3023,399,40,63,93,126,143,172,169,224,190,202
145c0b53514b02bdc3dadfb2e1cea124f2abd99b,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.",1967,8,4995,176,3,4,6,8,20,5,14,12,14,8
848a25a41cba56ca180ca79c6ba3470cc3b8f143,"In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.",2002,121,4680,139,4,13,34,53,79,108,112,152,183,238
35d81066cb1369acf4b6c5117fcbb862be2af350,"For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.",2009,21,2777,214,40,93,153,202,293,336,324,335,286,251
af56e6d4901dcd0f589bf969e604663d40f1be5d,"The charter of SRC is to advance both the state of knowledge and the state of the art in computer systems. From our establishment in 1984, we have performed basic and applied research to support Digital's business objectives. Our current work includes exploring distributed personal computing on multiple platforms, networking , programming technology, system modelling and management techniques, and selected applications. Our strategy is to test the technical and practical value of our ideas by building hardware and software prototypes and using them as daily tools. Interesting systems are too complex to be evaluated solely in the abstract; extended use allows us to investigate their properties in depth. This experience is useful in the short term in refining our designs, and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this strategy, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical flavor. Some of it is in established fields of theoretical computer science, such as the analysis of algorithms, computational geometry, and logics of programming. Other work explores new ground motivated by problems that arise in our systems research. We have a strong commitment to communicating our results; exposing and testing our ideas in the research and development communities leads to improved understanding. Our research report series supplements publication in professional journals and conferences. We seek users for our prototype systems among those with whom we have common interests, and we encourage collaboration with university researchers. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission an acknowledgment of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Systems Research Center. All rights reserved. Authors' abstract We describe a block-sorting, lossless data compression algorithm, and our implementation of that algorithm. We compare the performance of our implementation with widely available data compressors running on the same hardware. The algorithm works by applying a reversible transformation to a block of input …",1994,16,2743,397,1,5,13,18,31,27,33,49,38,51
47a9ab6f97ab05fcd49aaf2864c97538b55e6268,An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported.,1995,40,3416,191,0,10,9,4,7,11,14,27,33,37
7f4901362e8f8fe603ad90d6aba5f5192ef8ea8b,"Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. 
We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. SNOPT is a particular implementation that makes use of a semidefinite QP solver. It is based on a limited-memory quasi-Newton approximation to the Hessian of the Lagrangian and uses a reduced-Hessian algorithm (SQOPT) for solving the QP subproblems. It is designed for problems with many thousands of constraints and variables but a moderate number of degrees of freedom (say, up to 2000). An important application is to trajectory optimization in the aerospace industry. Numerical results are given for most problems in the CUTE and COPS test collections (about 900 examples).",2002,152,2848,294,20,40,58,85,128,91,121,126,152,164
c1d959b3c549c0bbfea22f9f5afa6f11c8eb74fd,"The Marquardt algorithm for nonlinear least squares is presented and is incorporated into the backpropagation algorithm for training feedforward neural networks. The algorithm is tested on several function approximation problems, and is compared with a conjugate gradient algorithm and a variable learning rate algorithm. It is found that the Marquardt algorithm is much more efficient than either of the other techniques when the network contains no more than a few hundred weights.",1994,18,5574,158,2,5,15,22,31,52,78,94,123,106
240c2cb549d0ad3ca8e6d5d17ca61e95831bbe6d,"For the problem of minimizing a lower semicontinuous proper convex function f on a Hilbert space, the proximal point algorithm in exact form generates a sequence $\{ z^k \} $ by taking $z^{k + 1} $ to be the minimizes of $f(z) + ({1 / {2c_k }})\| {z - z^k } \|^2 $, where $c_k > 0$. This algorithm is of interest for several reasons, but especially because of its role in certain computational methods based on duality, such as the Hestenes-Powell method of multipliers in nonlinear programming. It is investigated here in a more general form where the requirement for exact minimization at each iteration is weakened, and the subdifferential $\partial f$ is replaced by an arbitrary maximal monotone operator T. Convergence is established under several criteria amenable to implementation. The rate of convergence is shown to be “typically” linear with an arbitrarily good modulus if $c_k $ stays large enough, in fact superlinear if $c_k \to \infty $. The case of $T = \partial f$ is treated in extra detail. Applicati...",1976,35,3177,406,2,0,8,8,1,2,2,9,6,4
adb7c53c6928789cd5accf9ab5d0cec3fd32590d,"The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.",1996,74,4524,162,10,25,28,33,44,33,61,90,107,108
d87a423334afb20747c367b2d907069d7f3b4ed2,The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.,1973,36,4250,149,5,12,14,9,12,12,11,11,28,20
5a114d3050a0a33f8cc6d28d55fa048a5a7ab6f2,An algorithm is presented for the rapid solution of the phase of the complete wave function whose intensity in the diffraction and imaging planes of an imaging system are known. A proof is given showing that a defined error between the estimated function and the correct function must decrease as the algorithm iterates. The problem of uniqueness is discussed and results are presented demonstrating the power of the method.,1972,0,4511,144,0,5,8,7,2,1,5,5,12,15
69dc367c30240fcbbde0a2be04f16b02f2f7e73e,Community detection and analysis is an important methodology for understanding the organization of various real-world networks and has applications in problems as diverse as consensus formation in social communities or the identification of functional modules in biochemical networks. Currently used algorithms that identify the community structures in large-scale real-world networks require a priori information such as the number and sizes of communities or are computationally expensive. In this paper we investigate a simple label propagation algorithm that uses the network structure alone as its guide and requires neither optimization of a predefined objective function nor prior information about the communities. In our algorithm every node is initialized with a unique label and at every step each node adopts the label that most of its neighbors currently have. In this iterative process densely connected groups of nodes form a consensus on a unique label to form communities. We validate the algorithm by applying it to networks whose community structures are known. We also demonstrate that the algorithm takes an almost linear time and hence it is computationally less expensive than what was possible so far.,2007,61,2569,258,0,11,24,45,77,95,167,194,211,281
53fcc056f79e04daf11eb798a7238e93699665aa,"This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On realworld sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.",1998,23,2785,273,5,13,12,18,43,47,69,70,96,92
3ffa807548d96e2e76d53065c7143906bc2ff4bf,"A computational method for partitioning a charge density grid into Bader volumes is presented which is efficient, robust, and scales linearly with the number of grid points. The partitioning algorithm follows the steepest ascent paths along the charge density gradient from grid point to grid point until a charge density maximum is reached. In this paper, we describe how accurate off-lattice ascent paths can be represented with respect to the grid points. This improvement maintains the efficient linear scaling of an earlier version of the algorithm, and eliminates a tendency for the Bader surfaces to be aligned along the grid directions. As the algorithm assigns grid points to charge density maxima, subsequent paths are terminated when they reach previously assigned grid points. It is this grid-based approach which gives the algorithm its efficiency, and allows for the analysis of the large grids generated from plane-wave-based density functional theory calculations.",2009,29,3976,15,16,44,114,158,170,243,313,373,339,436
eb0209d172b3d0fb9432b91809bc506c9bdd33a1,"A new distributed Euler trail algorithm is proposed to run on an Euler diagraph G(V,E) where each node knows only its adjacent edges, converting it into a new state that each node knows how an existent Euler trail routes through its incoming and outgoing edges. The communication requires only 2middot;|E| one-bit messages. The algorithm can be used as a building block for solving other distributed graph problems, and can be slightly modified to run on a strongly-connected diagraph for generating the existent Euler trail or to report that no Euler trails exist.",1993,5,13970,0,0,1,0,1,1,10,421,307,339,343
56c2779a0cfd3052192558ee3bec8fc66e1a4303,"The Moderate Resolution Imaging Spectroradiometer (MODIS) aboard both NASA’s Terra and Aqua satellites is making near-global daily observations of the earth in a wide spectral range (0.41–15 m). These measurements are used to derive spectral aerosol optical thickness and aerosol size parameters over both land and ocean. The aerosol products available over land include aerosol optical thickness at three visible wavelengths, a measure of the fraction of aerosol optical thickness attributed to the fine mode, and several derived parameters including reflected spectral solar flux at the top of the atmosphere. Over the ocean, the aerosol optical thickness is provided in seven wavelengths from 0.47 to 2.13 m. In addition, quantitative aerosol size information includes effective radius of the aerosol and quantitative fraction of optical thickness attributed to the fine mode. Spectral irradiance contributed by the aerosol, mass concentration, and number of cloud condensation nuclei round out the list of available aerosol products over the ocean. The spectral optical thickness and effective radius of the aerosol over the ocean are validated by comparison with two years of Aerosol Robotic Network (AERONET) data gleaned from 132 AERONET stations. Eight thousand MODIS aerosol retrievals collocated with AERONET measurements confirm that one standard deviation of MODIS optical thickness retrievals fall within the predicted uncertainty of 0.03 0.05 over ocean and 0.05 0.15 over land. Two hundred and seventy-one MODIS aerosol retrievals collocated with AERONET inversions at island and coastal sites suggest that one standard deviation of MODIS effective radius retrievals falls within reff 0.11 m. The accuracy of the MODIS retrievals suggests that the product can be used to help narrow the uncertainties associated with aerosol radiative forcing of global climate.",2005,76,2696,287,43,68,110,138,148,164,183,197,196,206
acd7d08468e6dd7234d474860ab64d5a33cacfdc,"Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incompletedata) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maximum likelihood estimate. A list of key properties of the algorithm is included.",1983,112,3238,149,2,8,13,15,11,17,25,18,29,37
5a6dd91b96a9b09ccb1bab27220dbcb1e1df4d39,"An analytical algorithm, called SETTLE, for resetting the positions and velocities to satisfy the holonomic constraints on the rigid water model is presented. This method is still based on the Cartesian coordinate system and can be used in place of SHAKE and RATTLE. We implemented this algorithm in the SPASMS package of molecular mechanics and dynamics. Several series of molecular dynamics simulations were carried out to examine the performance of the new algorithm in comparison with the original RATTLE method. It was found that SETTLE is of higher accuracy and is faster than RATTLE with reasonable tolerances by three to nine times on a scalar machine. Furthermore, the performance improvement ranged from factors of 26 to 98 on a vector machine since the method presented is not iterative. © 1992 by John Wiley & Sons, Inc.",1992,14,4868,83,0,3,3,7,1,9,8,4,2,10
dbf8aa1e547c863f509a5a4c03a39fa9c92c9651,"We present a new polynomial-time algorithm for linear programming. In the worst case, the algorithm requiresO(n3.5L) arithmetic operations onO(L) bit numbers, wheren is the number of variables andL is the number of bits in the input. The running-time of this algorithm is better than the ellipsoid algorithm by a factor ofO(n2.5). We prove that given a polytopeP and a strictly interior point a εP, there is a projective transformation of the space that mapsP, a toP′, a′ having the following property. The ratio of the radius of the smallest sphere with center a′, containingP′ to the radius of the largest sphere with center a′ contained inP′ isO(n). The algorithm consists of repeated application of such projective transformations each followed by optimization over an inscribed sphere to create a sequence of points which converges to the optimal solution in polynomial time.",1984,5,3853,101,2,12,38,47,69,81,94,102,127,102
b4129e11e3cf2fba5d955cf881b5623388a347f4,"Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms. ABC simulates the intelligent foraging behaviour of a honeybee swarm. In this work, ABC is used for optimizing a large set of numerical test functions and the results produced by ABC algorithm are compared with the results obtained by genetic algorithm, particle swarm optimization algorithm, differential evolution algorithm and evolution strategies. Results show that the performance of the ABC is better than or similar to those of other population-based algorithms with the advantage of employing fewer control parameters.",2009,66,2653,161,8,32,84,171,234,290,323,288,273,277
a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657,"The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.",1985,46,3317,163,27,73,90,115,74,101,102,88,81,69
c14976e9afb69a78e682c32f900b96b019878df8,"This paper proposes an algorithm for optimization inspired by the imperialistic competition. Like other evolutionary ones, the proposed algorithm starts with an initial population. Population individuals called country are in two types: colonies and imperialists that all together form some empires. Imperialistic competition among these empires forms the basis of the proposed evolutionary algorithm. During this competition, weak empires collapse and powerful ones take possession of their colonies. Imperialistic competition hopefully converges to a state in which there exist only one empire and its colonies are in the same position and have the same cost as the imperialist. Applying the proposed algorithm to some of benchmark cost functions, shows its ability in dealing with different types of optimization problems.",2007,15,2065,291,1,6,4,27,67,116,165,213,207,217
80fd362544b593bd2250e8f5f3799882fa133ca1,"Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.",1998,22,2828,205,4,36,45,57,95,101,122,144,140,137
20ae8aebf1f0f7e85f2548784a9543803a4db199,"Artificial bee colony (ABC) algorithm is an optimization algorithm based on a particular intelligent behaviour of honeybee swarms. This work compares the performance of ABC algorithm with that of differential evolution (DE), particle swarm optimization (PSO) and evolutionary algorithm (EA) for multi-dimensional numeric problems. The simulation results show that the performance of ABC algorithm is comparable to those of the mentioned algorithms and can be efficiently employed to solve engineering problems with high dimensionality.",2008,41,3016,159,6,47,95,169,246,304,312,335,296,292
287c5eb5857102bff5f566ed9ed723d25a960328,"Objective. To establish a clinically relevant list with explicit criteria for pharmacologically inappropriate prescriptions in general practice for elderly people ≥70 years. Design. A three-round Delphi process for validating the clinical relevance of suggested criteria (n = 37) for inappropriate prescriptions to elderly patients. Setting. A postal consensus process undertaken by a panel of specialists in general practice, clinical pharmacology, and geriatrics. Main outcome measures. The Norwegian General Practice (NORGEP) criteria, a relevance-validated list of drugs, drug dosages, and drug combinations to be avoided in the elderly (≤70 years) patients. Results. Of the 140 invited panellists, 57 accepted to participate and 47 completed all three rounds of the Delphi process. The panellists reached consensus that 36 of the 37 suggested criteria were clinically relevant for general practice. Relevance of three of the criteria was rated significantly higher in Round 3 than in Round 1. At the end of the Delphi process, a significant difference between the different specialist groups’ scores was seen for only one of the 36 criteria. Conclusion. The NORGEP criteria may serve as rules of thumb for general practitioners (GPs) related to their prescribing practice for elderly patients, and as a tool for evaluating the quality of GPs’ prescribing in settings where access to clinical information for individual patients is limited, e.g. in prescription databases and quality improvement interventions.",2009,48,216,18,0,7,13,25,18,23,21,24,18,25
71de3ae53b4ad8557f6b203b3d9e555a66bbb651,"The epidemiology of immune thrombocytopenic purpura (ITP) is not well‐characterised in the general population. This study described the incidence and survival of ITP using the UK population‐based General Practice Research Database (GPRD). ITP patients first diagnosed in 1990–2005 were identified in the GPRD. Overall incidence rates (per 100 000 person‐years) and rates by age, sex, and calendar periods were calculated. Survival analysis was conducted using the Kaplan‐Meier and proportional hazard methods. A total of 1145 incident ITP patients were identified. The crude incidence was 3·9 (95% confidence interval [CI]: 3·7–4·1). Overall average incidence was statistically significantly higher in women (4·4, 95% CI: 4·1–4·7) compared to men (3·4; 95% CI: 3·1–3·7). Among men, incidence was bimodal with peaks among ages under 18 and between 75–84 years. The hazard ratio for death among ITP patients was 1·6 (95% CI: 1·3–1·9) compared to age‐ and sex‐matched comparisons. During follow‐up 139 cases died, of whom 75 had a computerised plausible cause of death. Death was related to bleeding in 13% and infection in 19% of these 75. In conclusion, ITP incidence varies with age and is higher in women than men. This potentially serious medical condition is associated with increased mortality in the UK.",2009,49,195,11,2,12,14,17,16,18,22,15,22,14
f8e5bbe7e6f67ad4f829e21c90a1d288a13e9825,"Ageing of the population in western societies and the rising costs of health and social care are refocusing health policy on health promotion and disability prevention among older people. However, efforts to identify at-risk groups of older people and to alter the trajectory of avoidable problems associated with ageing by early intervention or multidisciplinary case management have been largely unsuccessful. This paper argues that this failure arises from the dominance in primary care of a managerial perspective on health care for older people, and proposes instead the adoption of a clinical paradigm based on the concept of frailty. Frailty, in its simplest definition, is vulnerability to adverse outcomes. It is a dynamic concept that is different from disability and easy to overlook, but also easy to identify using heuristics (rules of thumb) and to measure using simple scales. Conceptually, frailty fits well with the biopsychosocial model of general practice, offers practitioners useful tools for patient care, and provides commissioners of health care with a clinical focus for targeting resources at an ageing population.",2009,72,198,3,3,10,16,19,20,10,22,17,20,14
9397e3d9a5556de8be5ed16bedfaf3d2e78d054c,"BACKGROUND
The extent to which a fear of needles influences health decisions remains largely unexplored. This study investigated the prevalence of fear of needles in a southeast Queensland community, described associated symptoms, and highlighted health care avoidance tendencies of affected individuals.


METHODS
One hundred and seventy-seven participants attending an outer urban general practice responded to a questionnaire on fear of needles, symptoms associated with needles and its influence on their use of medical care.


RESULTS
Twenty-two percent of participants reported a fear of needles. Affected participants were more likely than participants with no fear to report vasovagal symptoms, have had a previous traumatic needle experience (46.2 vs. 16.4%, p<0.001) and avoid medical treatment involving needles (20.5 vs. 2.3%, p<0.001).


DISCUSSION
Fear of needles is common and is associated with health care avoidance. Health professionals could better identify and manage patients who have a fear of needles by recognising associated vasovagal symptoms and past traumatic experiences.",2009,16,116,5,1,1,3,5,5,16,11,18,15,15
91e0c86e47e15bee8a3b6203c7ffd650a3afc75f,"BACKGROUND
Although studies are available on patients' ideas, concerns, and expectations in primary care, there is a scarcity of studies that explore the triad of ideas, concerns, and expectations (ICE) in general practice consultations and the impact on medication prescribing.


AIM
To evaluate the presence of ICE and its relation to medication prescription.


DESIGN OF STUDY
Cross-sectional study.


SETTING
Thirty-six GP teaching practices affiliated with the University of Ghent, in Flanders, Belgium.


METHOD
Participants were all patients consulting on 30 May 2005, and their doctors. Reasons for an encounter (consultation or home visit) with new and follow-up contacts, the identification of ICE, and the prescription of medication were recorded by 36 trainee GPs undergoing observational training. The study included 613 consultations.


RESULTS
One, two, or three of the ICE components were expressed in 38.5%, 24.4%, and 20.1% (n = 236, 150, 123) of contacts respectively. On the other hand, in 17.0% (104/613) of all contacts, and in 22% (77/350) of the new contact reasons, no ICE was voiced, and the GPs operated without knowing this information about the patients. Mean number of ICE components per doctor and per contact was 1.54 (standard deviation = 0.54). A logistic regression analysis of the 350 new contacts showed that the presence of concerns (P = 0.037, odds ratio [OR] 1.73, 95% confidence interval [CI] = 1.03 to 2.9), and expectations (P = 0.009, OR = 2.0, 95% CI = 1.2 to 3.4) was associated with not prescribing new medication (dichotomised into the categories present/absent); however, other patient, doctor, and student variables were not significantly associated with medication prescription.


CONCLUSION
An association was found between the presence of concerns and/or expectations, and less medication prescribing. The data suggest that exploring ICE components may lead to fewer new medication prescriptions.",2009,45,93,1,6,4,3,3,6,9,8,5,8,9
e3283da7acdf880647c34b48e2951058abeba8f3,Objective: To evaluate the management of cardiovascular disease (CVD) risk in Australian general practice.,2009,31,116,2,2,10,4,14,10,11,16,6,9,11
ade4d85d0937c9445fc88a96feab1af55e785f19,"BACKGROUND
Health literacy is the ability to understand and interpret the meaning of health information in written, spoken or digital form and how this motivates people to embrace or disregard actions relating to health.


OBJECTIVE
This article aims to describe the concept of health literacy, its importance and its applications in the general practice setting.


DISCUSSION
Australia trails behind other western countries in practical applications of health literacy. Health literacy underpins the efficiency of consultations, health promotion efforts, and self management programs. Recognition of the health literacy status of individuals allows use of appropriate communication tools. This can save time and effort and improve patient satisfaction and health outcomes.",2009,23,119,5,2,4,11,12,8,11,12,7,6,10
f7d7a2c03932683dfc56baf3627f37c6c165aef3,"BackgroundGeneral practitioners sometimes base clinical decisions on gut feelings alone, even though there is little evidence of their diagnostic and prognostic value in daily practice. Research to validate the determinants and to assess the test properties of gut feelings requires precise and valid descriptions of gut feelings in general practice which can be used as a reliable measuring instrument. Research question: Can we obtain consensus on descriptions of two types of gut feelings: a sense of alarm and a sense of reassurance?MethodsQualitative research including a Delphi consensus procedure with a heterogeneous sample of 27 Dutch and Belgian GPs or ex-GPs involved in academic educational or research programmes.ResultsAfter four rounds, we found 70% or greater agreement on seven of the eleven proposed statements. A ""sense of alarm"" is defined as an uneasy feeling perceived by a GP as he/she is concerned about a possible adverse outcome, even though specific indications are lacking: There's something wrong here. This activates the diagnostic process by stimulating the GP to formulate and weigh up working hypotheses that might involve a serious outcome. A ""sense of alarm"" means that, if possible, the GP needs to initiate specific management to prevent serious health problems. A ""sense of reassurance"" is defined as a secure feeling perceived by a GP about the further management and course of a patient's problem, even though the doctor may not be certain about the diagnosis: Everything fits in.ConclusionThe sense of alarm and the sense of reassurance are well-defined concepts. These descriptions enable us to operationalise the concept of gut feelings in further research.",2009,21,77,4,0,11,5,3,5,4,4,5,9,9
f66a0d9cfd4f5eb5d586f53732cfe2bb38b471cb,"BACKGROUND
In Australia, most medical students graduate without a firm career choice, with this decision being made during their early postgraduate years. Strategies addressing the current lack of meaningful exposure to general practice during these formative prevocational years are likely to be the most effective in increasing the proportion and number of entrants to general practice.


OBJECTIVE
This review summarises the influences of medical student selection criteria, curriculum, geographical location, timing and duration of general practice exposure and experience, prevocational experience, and vocational training, on an eventual choice of general practice as a career.


DISCUSSION
These are important influences on the complex process of career choice. Much research has focused on isolated interventions at one point along the pipeline. Varied and conflicting conclusions emerge from individual studies. In complex systems it is hard to understand the influence of an isolated intervention without looking at the system as a whole.",2009,28,60,2,0,2,12,13,4,6,4,8,2,3
d29c2d9d76def6f0789b16d7fc9227dc0e08c3e2,"Over the past 5 years, general practice in the UK has undergone major change. Starting with the introduction of the new GMS contract in 2004, it has continued apace with the establishment of Postgraduate Medical Education Training Board, a GP training curriculum, and nMRCGP. The NHS is developing very differently in the four countries of the UK. Regulation of the profession is under review, and a system of relicensing, recertification, and revalidation is being introduced. The Essence project, initiated by RCGP Scotland in conjunction with International Futures Forum 4 years ago is a constructive response to these changes. It has included learning journeys, a discussion day for GPs, and commissioned short pieces of 100 words from GPs and patients. From an analysis of these, some characteristics of the essence of general practice have been defined. These include key roles and core personal qualities for GPs. It is argued that general practice has important and unique advantages - trust, coordination, continuity, flexibility, universal coverage, and leadership - which mean that it should continue to be central to the development of primary care throughout the UK.",2009,55,53,1,9,8,7,7,2,7,3,2,4,2
b612a64c54e09baa3f094940bd5aa9870fd42df9,"BackgroundSchizophrenia patients frequently develop somatic co-morbidity. Core tasks for GPs are the prevention and diagnosis of somatic diseases and the provision of care for patients with chronic diseases. Schizophrenia patients experience difficulties in recognizing and coping with their physical problems; however GPs have neither specific management policies nor guidelines for the diagnosis and treatment of somatic co-morbidity in schizophrenia patients. This paper systematically reviews the prevalence and treatment of somatic co-morbidity in schizophrenia patients in general practice.MethodsThe MEDLINE, EMBASE, PsycINFO data-bases and the Cochrane Library were searched and original research articles on somatic diseases of schizophrenia patients and their treatment in the primary care setting were selected.ResultsThe results of this search show that the incidence of a wide range of diseases, such as diabetes mellitus, the metabolic syndrome, coronary heart diseases, and COPD is significantly higher in schizophrenia patients than in the normal population. The health of schizophrenic patients is less than optimal in several areas, partly due to their inadequate help-seeking behaviour. Current GP management of such patients appears not to take this fact into account. However, when schizophrenic patients seek the GP's help, they value the care provided.ConclusionSchizophrenia patients are at risk of undetected somatic co-morbidity. They present physical complaints at a late, more serious stage. GPs should take this into account by adopting proactive behaviour. The development of a set of guidelines with a clear description of the GP's responsibilities would facilitate the desired changes in the management of somatic diseases in these patients.",2009,50,115,0,4,8,14,9,13,9,9,9,10,10
6b25ca281526bf0a5fb955ad2617662d4e52718b,"BackgroundWith increasing rates of chronic disease associated with lifestyle behavioural risk factors, there is urgent need for intervention strategies in primary health care. Currently there is a gap in the knowledge of factors that influence the delivery of preventive strategies by General Practitioners (GPs) around interventions for smoking, nutrition, alcohol consumption and physical activity (SNAP). This qualitative study explores the delivery of lifestyle behavioural risk factor screening and management by GPs within a 45–49 year old health check consultation. The aims of this research are to identify the influences affecting GPs' choosing to screen and choosing to manage SNAP lifestyle risk factors, as well as identify influences on screening and management when multiple SNAP factors exist.MethodsA total of 29 audio-taped interviews were conducted with 15 GPs and one practice nurse over two stages. Transcripts from the interviews were thematically analysed, and a model of influencing factors on preventive care behaviour was developed using the Theory of Planned Behaviour as a structural framework.ResultsGPs felt that assessing smoking status was straightforward, however some found assessing alcohol intake only possible during a formal health check. Diet and physical activity were often inferred from appearance, only being assessed if the patient was overweight. The frequency and thoroughness of assessment were influenced by the GPs' personal interests and perceived congruence with their role, the level of risk to the patient, the capacity of the practice and availability of time. All GPs considered advising and educating patients part of their professional responsibility. However their attempts to motivate patients were influenced by perceptions of their own effectiveness, with smoking causing the most frustration. Active follow-up and referral of patients appeared to depend on the GPs' orientation to preventive care, the patient's motivation, and cost and accessibility of services to patients.ConclusionGeneral practitioner attitudes, normative influences from both patients and the profession, and perceived external control factors (time, cost, availability and practice capacity) all influence management of behavioural risk factors. Provider education, community awareness raising, support and capacity building may improve the uptake of lifestyle modification interventions.",2009,22,180,13,0,11,10,23,30,20,24,12,9,15
6624a88676463a96f3b67c7731d0ea8695f5a8df,"Traditionally, professional expertise has been judged by length of experience, reputation, and perceived mastery of knowledge and skill. Unfortunately, recent research demonstrates only a weak relationship between these indicators of expertise and actual, observed performance. In fact, observed performance does not necessarily correlate with greater professional experience. Expert performance can, however, be traced to active engagement in deliberate practice (DP), where training (often designed and arranged by their teachers and coaches) is focused on improving particular tasks. DP also involves the provision of immediate feedback, time for problem-solving and evaluation, and opportunities for repeated performance to refine behavior. In this article, we draw upon the principles of DP established in other domains, such as chess, music, typing, and sports to provide insight into developing expert performance in medicine.",2008,41,1185,52,2,9,30,46,66,99,99,109,110,107
7cfad0fc9e5413aa34324b776c027e5c1aa89b39,"Outline of a Theory of Practice is recognized as a major theoretical text on the foundations of anthropology and sociology. Pierre Bourdieu, a distinguished French anthropologist, develops a theory of practice which is simultaneously a critique of the methods and postures of social science and a general account of how human action should be understood. With his central concept of the habitus, the principle which negotiates between objective structures and practices, Bourdieu is able to transcend the dichotomies which have shaped theoretical thinking about the social world. The author draws on his fieldwork in Kabylia (Algeria) to illustrate his theoretical propositions. With detailed study of matrimonial strategies and the role of rite and myth, he analyses the dialectical process of the 'incorporation of structures' and the objectification of habitus, whereby social formations tend to reproduce themselves. A rigorous consistent materialist approach lays the foundations for a theory of symbolic capital and, through analysis of the different modes of domination, a theory of symbolic power.",1972,0,22475,1640,0,0,0,0,0,0,0,0,0,0
fe001eb4659b6fefa8974780bcfb8afc0eaa0e42,,1899,0,99,0,0,0,0,0,0,0,0,0,0,0
2f3d7e9a29b58ec04a0d52257984011b1154962d,"AbstractBackgroundThe impact of high physician workload and job stress on quality and outcomes of healthcare delivery is not clear. Our study explored whether high workload and job stress were associated with lower performance in general practices in the Netherlands.MethodsSecondary analysis of data from 239 general practices, collected in practice visits between 2003 to 2006 in the Netherlands using a comprehensive set of measures of practice management. Data were collected by a practice visitor, a trained non-physician observer using patients questionnaires, doctors and staff. For this study we selected five measures of practice performance as outcomes and six measures of GP workload and job stress as predictors. A total of 79 indicators were used out of the 303 available indicators. Random coefficient regression models were applied to examine associations.Results and discussionWorkload and job stress are associated with practice performance.
Workload: Working more hours as a GP was associated with more positive patient experiences of accessibility and availability (b = 0.16). After list size adjustment, practices with more GP-time per patient scored higher on GP care (b = 0.45). When GPs provided more than 20 hours per week per 1000 patients, patients scored over 80% on the Europep questionnaire for quality of GP care.
Job stress: High GP job stress was associated with lower accessibility and availability (b = 0.21) and insufficient practice management (b = 0.25). Higher GP commitment and more satisfaction with the job was associated with more prevention and disease management (b = 0.35).ConclusionProviding more time in the practice, and more time per patient and experiencing less job stress are all associated with perceptions by patients of better care and better practice performance. Workload and job stress should be assessed by using list size adjusted data in order to realise better quality of care. Organisational development using this kind of data feedback could benefit both patients and GP.",2009,48,92,1,0,4,4,7,7,6,8,9,9,10
5a9847dc44598142d4552f6f4e4d6b30b31c43c0,"Primary care spirometry services can be provided by trained primary care staff, peripatetic specialist services, or through referral to hospital-based or laboratory spirometry. The first of these options is the focus of this Standards Document. It aims to provide detailed information for clinicians, managers and healthcare commissioners on the key areas of quality required for diagnostic spirometry in primary care--including training requirements and quality assurance. These proposals and recommendations are designed to raise the standard of spirometry and respiratory diagnosis in primary care and to provide the impetus for debate, improvement and maintenance of quality for diagnostic (rather than screening) spirometry performed in primary care. This document should therefore challenge current performance and should constitute an aspirational guide for delivery of this service.",2009,170,185,4,5,17,21,24,19,14,15,19,11,9
c2f2e32233712d0fc3ca57a401464600ca6e28e0,"BACKGROUND AND AIMS
Clinical inertia is considered a major barrier to better care. We assessed its prevalence, predictors and associations with the intermediate outcomes of diabetes care.


MATERIALS AND METHODS
Baseline and follow-up data of a Dutch randomized controlled trial on the implementation of a locally adapted guideline were used. The study involved 30 general practices and 1283 patients. Treatment targets differed between study groups [HbA1c <or= 8.0% and blood pressure (BP) < 140/85% versus HbA1c <or= 8.5% and BP < 150/85]. Clinical inertia was defined as the failure to intensify therapy when indicated. A complete medication profile of all participating patients was obtained.


RESULTS
In the intervention and control group, the percentages of patients with poor diabetes or lipid control who did not receive treatment intensification were 45% and 90%, approximately. More control group patients with BP levels above target were confronted with inertia (72.7% versus 63.3%, P < 0.05). In poorly controlled hypertensive patients, inertia was associated with the height of systolic BP at baseline [adjusted odds ratio (OR) 0.98, 95% confidence interval (CI) 0.98-0.99] and the frequency of BP control (adjusted OR 0.89, 95% CI 0.81-0.99). If a practice nurse managed these patients, clinical inertia was less common (adjusted OR 0.12, 95% CI 0.02-0.91). In both study groups, cholesterol decreased significantly more in patients who received proper treatment intensification.


CONCLUSION
GPs were more inclined to control blood glucose levels than BP or cholesterol levels. Inertia in response to poorly controlled high BP was less common if nurses assisted GPs.",2009,68,91,3,1,2,13,4,22,7,9,4,9,4
3641e66e6e0bc275ccf1ad277880a9611df92fdc,"‘There are few things we should keenly desire if we really knew what we wanted.’ Francois de la Rochefoucauld (French writer 1613–1680) Social prescribing is about expanding the range of options available to GP and patient as they grapple with a problem. Where that problem has its origins in socioeconomic deprivation or long-term psychosocial issues, it is easy for both patient and GP to feel overwhelmed and reluctant to open what could turn out to be a can of worms. Settling for a short-term medical fix may be pragmatic but can easily become a conspiracy of silence which confirms the underlying sense of defeat. Can or should we try to do more during the precious minutes of a GP consultation?

Where there are psychosocial issues GPs do suggest social avenues, such as visiting a Citizens Advice Bureau for financial problems, or a dance class for exercise and loneliness, but without a supportive framework this tends to be a token action. The big picture difficulty with leaving underlying psychosocial problems largely hidden in the consulting room is the medicalisation of society's ills. This ranges from using antidepressants for the misery of a difficult life, to the complex pharmaceutical regimes prescribed to patients with obesity and type 2 diabetes. This sort of medicalisation may help immediate problems (including driving the economy through jobs in the healthcare industries) but it is not enough if our society is to have a sustainable future.

Another way of looking at this is in terms of choice. The consumerist type of choice of provider beloved of the government, is what Canadian philosopher Charles Taylor calls ‘weak evaluation’.1 By this he means a utilitarian ‘weighing-up’ of generally short-term consequences of a choice. These choices represent ‘second-order desires’, such as to feel more cheerful, or to relieve a …",2009,31,118,1,0,0,1,2,6,3,12,8,8,19
e26b2804e4d12e0391893c496d46fdbd832dd1a2,"1. Introduction Designing PCR and sequencing primers are essential activities for molecular biologists around the world. This chapter assumes acquaintance with the principles and practice of PCR, as outlined in, for example, refs. 1–4. Primer3 is a computer program that suggests PCR primers for a variety of applications, for example to create STSs (sequence tagged sites) for radiation hybrid mapping (5), or to amplify sequences for single nucleotide polymor-phism discovery (6). Primer3 can also select single primers for sequencing reactions and can design oligonucleotide hybridization probes. In selecting oligos for primers or hybridization probes, Primer3 can consider many factors. These include oligo melting temperature, length, GC content , 3′ stability, estimated secondary structure, the likelihood of annealing to or amplifying undesirable sequences (for example interspersed repeats), the likelihood of primer–dimer formation between two copies of the same primer, and the accuracy of the source sequence. In the design of primer pairs Primer3 can consider product size and melting temperature, the likelihood of primer– dimer formation between the two primers in the pair, the difference between primer melting temperatures, and primer location relative to particular regions of interest or to be avoided.",2000,21,16090,1332,0,0,1,0,0,0,2,1,2,8
e0ed44b682c1ec1684909eb9c7c06fd659aa7c74,"Objective: To investigate and compare the prevalence, comorbidities and management of gout in practice in the UK and Germany. Methods: A retrospective analysis of patients with gout, identified through the records of 2.5 million patients in UK general practices and 2.4 million patients attending GPs or internists in Germany, using the IMS Disease Analyzer. Results: The prevalence of gout was 1.4% in the UK and Germany. Obesity was the most common comorbidity in the UK (27.7%), but in Germany the most common comorbidity was diabetes (25.9%). The prevalence of comorbidities tended to increase with serum uric acid (sUA) levels. There was a positive correlation between sUA level and the frequency of gout flares. Compared with those in whom sUA was <360 μmol/l (<6 mg/dl), odds ratios for a gout flare were 1.33 and 1.37 at sUA 360–420 μmol/l (6–7 mg/dl), and 2.15 and 2.48 at sUA >530 μmol/l ( >9 mg/dl) in the UK and Germany, respectively (p<0.01). Conclusions: The prevalence of gout in practice in the UK and Germany in the years 2000–5 was 1.4%, consistent with previous UK data for 1990–9. Chronic comorbidities were common among patients with gout and included conditions associated with an increased risk for cardiovascular disease, such as obesity, diabetes and hypertension. The importance of regular monitoring of sUA in order to tailor gout treatment was highlighted by data from this study showing that patients with sUA levels ⩾360 μmol/l (⩾6 mg/dl) had an increased risk of gout flares.",2007,47,453,26,2,4,21,23,26,43,50,49,46,47
036d7b6b85f93a2b071c0354cbc0f811a1919829,"CONTEXT
People with severe mental illness (SMI) appear to have an elevated risk of death from cardiovascular disease, but results regarding cancer mortality are conflicting.


OBJECTIVE
To estimate this excess mortality and the contribution of antipsychotic medication, smoking, and social deprivation.


DESIGN
Retrospective cohort study.


SETTING
United Kingdom's General Practice Research Database. Patients Two cohorts were compared: people with SMI diagnoses and people without such diagnoses. Main Outcome Measure Mortality rates for coronary heart disease (CHD), stroke, and the 7 most common cancers in the United Kingdom.


RESULTS
A total of 46 136 people with SMI and 300 426 without SMI were selected for the study. Hazard ratios (HRs) for CHD mortality in people with SMI compared with controls were 3.22 (95% confidence interval [CI], 1.99-5.21) for people 18 through 49 years old, 1.86 (95% CI, 1.63-2.12) for those 50 through 75 years old, and 1.05 (95% CI, 0.92-1.19) for those older than 75 years. For stroke deaths, the HRs were 2.53 (95% CI, 0.99-6.47) for those younger than 50 years, 1.89 (95% CI, 1.50-2.38) for those 50 through 75 years old, and 1.34 (95% CI, 1.17-1.54) for those older than 75 years. The only significant result for cancer deaths was an unadjusted HR for respiratory tumors of 1.32 (95% CI, 1.04-1.68) for those 50 to 75 years old, which lost statistical significance after controlling for smoking and social deprivation. Increased HRs for CHD mortality occurred irrespective of sex, SMI diagnosis, or prescription of antipsychotic medication during follow-up. However, a higher prescribed dose of antipsychotics predicted greater risk of mortality from CHD and stroke.


CONCLUSIONS
This large community sample demonstrates that people with SMI have an increased risk of death from CHD and stroke that is not wholly explained by antipsychotic medication, smoking, or social deprivation scores. Rates of nonrespiratory cancer mortality were not raised. Further research is required concerning prevention of this mortality, including cardiovascular risk assessment, monitoring of antipsychotic medication, and attention to diet and exercise.",2007,36,566,31,12,32,36,45,63,46,46,37,53,36
0c2d5f55d7b9e566f3c2e2784e191b49d59fdcff,"Background There is evidence that the prevalence of common mental disorders varies across Europe. Aims To compare prevalence of common mental disorders in general practice attendees in six European countries. Method Unselected attendees to general practices in the UK, Spain, Portugal, Slovenia, Estonia and The Netherlands were assessed for major depression, panic syndrome and other anxiety syndrome. Prevalence of DSM–IV major depression, other anxiety syndrome and panic syndrome was compared between the UK and other countries after taking account of differences in demographic factors and practice consultation rates. Results Prevalence was estimated in 2344 men and 4865 women. The highest prevalence for all disorders occurred in the UK and Spain, and lowest in Slovenia and The Netherlands. Men aged 30–50 and women aged 18–30 had the highest prevalence of major depression; men aged 40–60 had the highest prevalence of anxiety, and men and women aged 40–50 had the highest prevalence of panic syndrome. Demographic factors accounted for the variance between the UK and Spain but otherwise had little impact on the significance of observed country differences. Conclusions These results add to the evidence for real differences between European countries in prevalence of psychological disorders and show that the burden of care on general practitioners varies markedly between countries.",2008,33,261,11,7,13,14,27,21,20,22,22,28,32
b0a15b2a986f058ec01e7b43eea7b1b5b35a9441,"CONTEXT
Strategies for prevention of depression are hindered by lack of evidence about the combined predictive effect of known risk factors.


OBJECTIVES
To develop a risk algorithm for onset of major depression.


DESIGN
Cohort of adult general practice attendees followed up at 6 and 12 months. We measured 39 known risk factors to construct a risk model for onset of major depression using stepwise logistic regression. We corrected the model for overfitting and tested it in an external population.


SETTING
General practices in 6 European countries and in Chile.


PARTICIPANTS
In Europe and Chile, 10 045 attendees were recruited April 2003 to February 2005. The algorithm was developed in 5216 European attendees who were not depressed at recruitment and had follow-up data on depression status. It was tested in 1732 patients in Chile who were not depressed at recruitment. Main Outcome Measure DSM-IV major depression.


RESULTS
Sixty-six percent of people approached participated, of whom 89.5% participated again at 6 months and 85.9%, at 12 months. Nine of the 10 factors in the risk algorithm were age, sex, educational level achieved, results of lifetime screen for depression, family history of psychological difficulties, physical health and mental health subscale scores on the Short Form 12, unsupported difficulties in paid or unpaid work, and experiences of discrimination. Country was the tenth factor. The algorithm's average C index across countries was 0.790 (95% confidence interval [CI], 0.767-0.813). Effect size for difference in predicted log odds of depression between European attendees who became depressed and those who did not was 1.28 (95% CI, 1.17-1.40). Application of the algorithm in Chilean attendees resulted in a C index of 0.710 (95% CI, 0.670-0.749).


CONCLUSION
This first risk algorithm for onset of major depression functions as well as similar risk algorithms for cardiovascular events and may be useful in prevention of depression.",2008,71,147,5,1,6,8,16,9,15,12,12,11,7
8cdc8ed40279a869624c0796e9319ea1850af3b6,"AIMS
The purpose of this study was to describe the demographic and employment characteristics of Australian practice nurses and explore the relationship between these characteristics and the nurses' role.


BACKGROUND
Nursing in general practice is an integral component of primary care and chronic disease management in the United Kingdom and New Zealand, but in Australia it is an emerging specialty and there is limited data on the workforce and role.


DESIGN
National postal survey embedded in a sequential mixed method design.


METHODS
284 practice nurses completed a postal survey during 2003-2004. Descriptive statistics and factor analysis were utilized to analyse the data.


RESULTS
Most participants were female (99%), Registered Nurses (86%), employed part-time in a group practice, with a mean age of 45.8 years, and had a hospital nursing certificate as their highest qualification (63%). The tasks currently undertaken by participants and those requiring further education were inversely related (R2 = -0.779). Conversely, tasks perceived to be appropriate for a practice nurse and those currently undertaken by participants were positively related (R2 = 0.8996). There was a mismatch between the number of participants who perceived that a particular task was appropriate and those who undertook the task. This disparity was not completely explained by demographic or employment characteristics. Extrinsic factors such as legal and funding issues, lack of space and general practitioner attitudes were identified as barriers to role expansion.


CONCLUSION
Practice nurses are a clinically experienced workforce whose skills are not optimally harnessed to improve the care of the growing number of people with chronic and complex conditions. Relevance to clinical practice. Study data reveal a need to overcome the funding, regulatory and interprofessional barriers that currently constrain the practice nurse role. Expansion of the practice nurse role is clearly a useful adjunct to specialist management of chronic and complex disease, particularly within the context of contemporary policy initiatives.",2008,28,133,7,2,12,4,3,8,10,11,9,4,14
3be75b6ed1658b431ccd7d95ebcb4a3321942449,"BackgroundThis study was carried out to compare the HRQoL of patients in general practice with differing chronic diseases with the HRQoL of patients without chronic conditions, to evaluate the HRQoL of general practice patients in Germany compared with the HRQoL of the general population, and to explore the influence of different chronic diseases on patients' HRQoL, independently of the effects of multiple confounding variables.MethodsA cross-sectional questionnaire survey including the SF-36, the EQ-5D and demographic questions was conducted in 20 general practices in Germany. 1009 consecutive patients aged 15–89 participated. The SF-36 scale scores of general practice patients with differing chronic diseases were compared with those of patients without chronic conditions. Differences in the SF-36 scale/summary scores and proportions in the EQ-5D dimensions between patients and the general population were analyzed. Independent effects of chronic conditions and demographic variables on the HRQoL were analyzed using multivariable linear regression and polynomial regression models.ResultsThe HRQoL for general practice patients with differing chronic diseases tended to show more physical than mental health impairments compared with the reference group of patients without. Patients in general practice in Germany had considerably lower SF-36 scores than the general population (P < 0.001 for all) and showed significantly higher proportions of problems in all EQ-5D dimensions except for the self-care dimension (P < 0.001 for all). The mean EQ VAS for general practice patients was lower than that for the general population (69.2 versus 77.4, P < 0.001). The HRQoL for general practice patients in Germany seemed to be more strongly affected by diseases like depression, back pain, OA of the knee, and cancer than by hypertension and diabetes.ConclusionGeneral practice patients with differing chronic diseases in Germany had impaired quality of life, especially in terms of physical health. The independent impacts on the HRQoL were different depending on the type of chronic disease. Findings from this study might help health professionals to concern more influential diseases in primary care from the patient's perspective.",2008,55,122,7,0,3,9,17,7,11,14,12,12,4
44655b06c43cd1a05cec8c15dd83c3a7b4f08f30,"The emergence of healthcare assistants (HCAs) in general practice raises questions about roles and responsibilities, patients' acceptance, cost-effectiveness, patient safety and delegation, training and competence, workforce development, and professional identity. There has been minimal research into the role of HCAs and their experiences, as well as those of other staff working with HCAs in general practice. Lessons may be learned from their role and evidence of their effectiveness in hospital settings. Such research highlights blurred and contested role boundaries and threats to professional identity, which have implications for teamwork, quality of patient care, and patient safety. In this paper it is argued that transferability of evidence from hospital settings to the context of general practice cannot be assumed. Drawing on the limited research in general practice, the challenges and benefits of developing the HCA role in general practice are discussed. It is suggested that in the context of changing skill-mix models, viewing roles as fluid and dynamic is more helpful and reflective of individuals' experiences than endeavouring to impose fixed role boundaries. It is concluded that HCAs can make an increasingly useful contribution to the skill mix in general practice, but that more research and evaluation are needed to inform their training and development within the general practice team.",2008,52,85,4,2,4,4,9,8,7,6,12,6,6
fbff227595f056558eca8c081f11838ece184113,"BackgroundContinual quality improvement in primary care is an international priority. In the United Kingdom, the major initiative for improving quality of care is the Quality and Outcomes Framework (QoF) of the 2004 GP contract. Although the primary focus of the QoF is on clinical care, it is acknowledged that a comprehensive assessment of quality also requires valid and reliable measurement of the patient perspective, so financial incentives are included in the contract for general practices to survey patients' views. One questionnaire specified for use in the QoF is the General Practice Assessment Questionnaire (GPAQ). This paper describes the development of the GPAQ (with post-consultation and postal versions) and presents a preliminary examination of the psychometric properties of the questionnaire.MethodsDescription of scale development and preliminary analysis of psychometric characteristics (internal reliability, factor structure), based on a large dataset of routinely collected GPAQ surveys (n = 190,038 responses to the consultation version of GPAQ and 20,309 responses to the postal version) from practices in the United Kingdom during the 2005–6 contract year.ResultsRespondents tend to report generally favourable ratings. Responses were particularly skewed on the GP communication scale, though no more so than for other questionnaires in current use in the UK for which data were available. Factor analysis identified 2 factors that clearly relate to core concepts in primary care quality ('access' and 'interpersonal care') that were common to both version of the GPAQ. The other factors related to 'enablement' in the post-consultation version and 'nursing care' in the postal version.ConclusionThis preliminary evaluation indicates that the scales of the GPAQ are internally reliable and that the items demonstrate an interpretable factor structure. Issues concerning the distributions of GPAQ responses are discussed. Potential further developments of the item content for the GPAQ are also outlined.",2008,53,97,1,2,10,5,4,12,8,7,11,7,10
424f859cb27e3c10d1c5dd17d5da6196668e2327,"Abstract Objective: To assess the effectiveness of a home exercise programme of strength and balance retraining exercises in reducing falls and injuries in elderly women. Design: Randomised controlled trial of an individually tailored programme of physical therapy in the home (exercise group, n=116) compared with the usual care and an equal number of social visits (control group, n=117). Setting: 17 general practices in Dunedin, New Zealand. Subjects: Women aged 80 years and older living in the community and registered with a general practice in Dunedin. Main outcome measures: Number of falls and injuries related to falls and time between falls during one year of follow up; changes in muscle strength and balance measures after six months. Results: After one year there were 152 falls in the control group and 88 falls in the exercise group. The mean (SD) rate of falls was lower in the exercise than the control group (0.87 (1.29) v 1.34 (1.93) falls per year respectively; difference 0.47; 95% confidence interval 0.04 to 0.90). The relative hazard for the first four falls in the exercise group compared with the control group was 0.68 (0.52 to 0.90). The relative hazard for a first fall with injury in the exercise group compared with the control group was 0.61 (0.39 to 0.97). After six months, balance had improved in the exercise group (difference between groups in change in balance score 0.43 (0.21 to 0.65). Conclusions: An individual programme of strength and balance retraining exercises improved physical function and was effective in reducing falls and injuries in women 80 years and older. Key messages Modifiable risk factors for falls in elderly people have been well defined; they include loss of muscle strength and impaired balance A programme to improve strength and balance in women aged 80 years and older can be set up safely with four home visits from a physiotherapist This programme reduced falls and moderate injuries appreciably over the subsequent year in Dunedin, New Zealand The benefit was most noticeable in elderly people who fell often",1997,23,1076,54,2,5,20,29,44,50,59,58,60,49
b8b96b418ca0302bbc1a50f7e1bd8f8ca5ca37b4,"BackgroundObesity has become a global pandemic, considered the sixth leading cause of mortality by the WHO. As gatekeepers to the health system, General Practitioners are placed in an ideal position to manage obesity. Yet, very few consultations address weight management. This study aims to explore reasons why patients attending General Practice appointments are not engaging with their General Practitioner (GP) for weight management and their perception of the role of the GP in managing their weight.MethodsIn February 2006, 367 participants aged between 17 and 64 were recruited from three General Practices in Melbourne to complete a waiting room self – administered questionnaire. Questions included basic demographics, the role of the GP in weight management, the likelihood of bringing up weight management with their GP and reasons why they would not, and their nominated ideal person to consult for weight management. Physical measurements to determine weight status were then completed. The statistical methods included means and standard deviations to summarise continuous variables such as weight and height. Sub groups of weight and questionnaire answers were analysed using the χ2 test of significant differences taking p as < 0.05.ResultsThe population sample had similar obesity co-morbidity rates to the National Heart Foundation data. 74% of patients were not likely to bring up weight management when they visit their GP. Negative reasons were time limitation on both the patient's and doctor's part and the doctor lacking experience. The GP was the least likely person to tell a patient to lose weight after partner, family and friends. Of the 14% that had been told by their GP to lose weight, 90% had cardiovascular obesity related co-morbidities. GPs (15%) were 4th in the list of ideal persons to manage weight after personal trainerConclusionPatients do not have confidence in their GPs for weight management, preferring other health professionals who may lack evidence based training. Concurrently, GPs target only those with obesity related co-morbidities. Further studies evaluating GPs' opinions about weight management, effective strategies that can be implemented in primary care and the co-ordination of the team approach need to be done.",2008,28,67,5,0,0,1,11,4,11,9,8,2,4
625f20bafb88207c9b7ab998c44b3e000003e720,"BACKGROUND
In general practice, depression is often not recognized. As treatment of depression is effective, screening has been proposed as one solution to combat this 'hidden morbidity'. The results of screening programmes for depression, however, are inconsistent and most studies do not show a positive effect on patient outcomes. Patients do not always accept this diagnosis and hence do not receive proper treatment. Nothing is known about the tendency of those patients who screen positive for depression to accept treatment for their 'disclosed' disorder.


OBJECTIVE
In this study, we aimed to better understand the views of patients who screened positive in a screening programme for depression.


METHODS
We performed a qualitative study with semi-structured in-depth interviews with 17 patients. These adult patients (nine females), all suffering from major depressive disorder, were disclosed by a screening programme for depression performed within 11 Dutch general practices. The transcripts were independently analysed by two researchers using MAXqda2.


RESULTS
All patients appreciated the active way in which they were approached for screening. Fifteen of the 17 patients recognized the depressive symptoms but nine of them did not accept the diagnosis. The first explanation for resistance to the diagnosis of depression is fear of stigmatization and scepticism about the usefulness of labelling. Secondly, patients experienced their depressive symptoms as a normal and transitory reaction to adversity. Thirdly, patients had doubts about the necessity and effectiveness of treatment. Depressive symptoms, such as feelings of guilt, self-depreciation and fatigue, hamper help-seeking behaviour.


CONCLUSIONS
We conclude that some patients with undisclosed depression, who took the trouble of going through a complete screening programme, felt aversion to being diagnosed as having depression. In the context of screening for depression, we recommend that the patients' view on depression be elicited before diagnosing and offering treatment.",2008,43,46,2,1,2,2,8,4,4,1,7,2,5
abf37c9298831051761ae9c23d300aa185d725cc,"BACKGROUND
There has been little research into poetry-based medical education. Few studies consider learners' perceptions in depth.


OBJECTIVE
To explore general practice registrars' (GPRs) perceptions of two poetry-based sessions.


METHODS
GPRs in one general practice vocational training scheme experienced two poetry sessions. In one, the facilitator selected poems; in the other, poems were chosen by registrars. Poems were read and discussed, with emphasis on personal response. Data were obtained through in-depth semi-structured interviews with six registrars. Interviews were audiotaped, transcribed and analysed using interpretative phenomenological analysis. Identification of individual ideas and shared themes enabled exploration of the registrars' experiences.


RESULTS
Registrars described how poetry helped them explore emotional territory. They recognized a broadening of education, describing how poems helped them consider different points of view, increasing their understanding of others. Vicarious experience, development of empathy and self-discovery were also reported. Participants speculated on how this might impact on patient care and professional practice. Facilitator-selected poems provided variety and ambiguity, provoking discussions with clinical relevance. Learner-selected poems enabled involvement, self-revelation and understanding of peers and developed emotional expression.


CONCLUSIONS
These registrars reported difficulties expressing feelings in the culture of science-based medical training. Poetry sessions may provide an environment for emotional exploration, which could broaden understanding of self and others. Poetry-based education may develop emotional competence. The participants recognized development of key skills including close reading, attentive listening and interpretation of meaning. These skills may help doctors to understand individual patient's unique experience of illness, encouraging personalized care that respects patients' perspectives.",2008,52,41,1,1,1,4,3,1,7,0,4,2,4
cf9ecfbbd0095687c4cfbbbfa0546914e651b109,"PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",1998,8,2938,181,0,4,15,9,17,26,28,35,38,46
7c72b917a38b09e6d3ab19d28a4344ba54edb6ae,"Part I. Exact String Matching: The Fundamental String Problem: 1. Exact matching: fundamental preprocessing and first algorithms 2. Exact matching: classical comparison-based methods 3. Exact matching: a deeper look at classical methods 4. Semi-numerical string matching Part II. Suffix Trees and their Uses: 5. Introduction to suffix trees 6. Linear time construction of suffix trees 7. First applications of suffix trees 8. Constant time lowest common ancestor retrieval 9. More applications of suffix trees Part III. Inexact Matching, Sequence Alignment and Dynamic Programming: 10. The importance of (sub)sequence comparison in molecular biology 11. Core string edits, alignments and dynamic programming 12. Refining core string edits and alignments 13. Extending the core problems 14. Multiple string comparison: the Holy Grail 15. Sequence database and their uses: the motherlode Part IV. Currents, Cousins and Cameos: 16. Maps, mapping, sequencing and superstrings 17. Strings and evolutionary trees 18. Three short topics 19. Models of genome-level mutations.",1997,0,3106,176,4,21,32,49,77,110,130,160,175,204
05913f4b504aa1fb1e638cdd0848d94bf5eb43da,"Probability and Statistics with Reliability, Queuing and Computer Science Applications, Second Edition, offers a comprehensive introduction to probabiliby, stochastic processes, and statistics for students of computer science, electrical and computer engineering, and applied mathematics. Its wealth of practical examples and up-to-date information makes it an excellent resource for practitioners as well.",1984,6,2913,113,2,4,21,19,21,30,50,35,49,54
b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",1990,0,2877,2,18,46,73,112,105,146,181,165,161,130
9c42e606b18c992b91908626abc46ed92f39848e,"Ubiquitous computing is the method of enhancing computer use by making many computers available throughout the physical environment, but making them effectively invisible to the user. Since we started this work at Xerox PARC in 1988, a number of researchers around the world have begun to work in the ubiquitous computing framework. This paper explains what is new and different about the computer science in ubiquitous computing. It starts with a brief overview of ubiquitous computing, and then elaborates through a series of examples drawn from various subdisciplines of computer science: hardware components (e.g. chips), network protocols, interaction substrates (e.g. software for screens and pens), applications, privacy, and computational methods. Ubiquitous computing offers a framework for new and exciting research across the spectrum of computer science.",1993,37,2553,93,10,19,42,44,35,57,52,68,98,98
d62594672d6f765b44827b1066328e96e1e58f7e,"The aim of this study was to assess the learning effectiveness and motivational appeal of a computer game for learning computer memory concepts, which was designed according to the curricular objectives and the subject matter of the Greek high school Computer Science (CS) curriculum, as compared to a similar application, encompassing identical learning objectives and content but lacking the gaming aspect. The study also investigated potential gender differences in the game's learning effectiveness and motivational appeal. The sample was 88 students, who were randomly assigned to two groups, one of which used the gaming application (Group A, N=47) and the other one the non-gaming one (Group B, N=41). A Computer Memory Knowledge Test (CMKT) was used as the pretest and posttest. Students were also observed during the interventions. Furthermore, after the interventions, students' views on the application they had used were elicited through a feedback questionnaire. Data analyses showed that the gaming approach was both more effective in promoting students' knowledge of computer memory concepts and more motivational than the non-gaming approach. Despite boys' greater involvement with, liking of and experience in computer gaming, and their greater initial computer memory knowledge, the learning gains that boys and girls achieved through the use of the game did not differ significantly, and the game was found to be equally motivational for boys and girls. The results suggest that within high school CS, educational computer games can be exploited as effective and motivational learning environments, regardless of students' gender.",2009,33,1340,89,12,31,61,85,121,136,130,135,153,133
86b05bc7e953e683fa839ad01d6100a8f99558df,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",1991,0,2501,113,5,15,20,26,27,27,24,43,37,42
af465996da89a302fae95c2fe22e54d2b79e4ac3,"Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine—not just the hardware, but the programmed, living machine—is the organism we study.",1976,14,2420,71,1,8,7,4,6,5,10,5,9,8
300f6e755d2e1fd1e3c42804c724f988b7b0e1a1,,2000,340,3951,260,44,55,107,116,130,168,171,237,240,224
2fc911242f20333452d13ab39d678d017ef74852,,2006,0,1291,47,12,43,65,91,127,121,141,144,86,103
e9abcc403b216184bb3ba92989f56f70691408c1,"People can make decisions to join a group based solely on exposure to that group's physical environment. Four studies demonstrate that the gender difference in interest in computer science is influenced by exposure to environments associated with computer scientists. In Study 1, simply changing the objects in a computer science classroom from those considered stereotypical of computer science (e.g., Star Trek poster, video games) to objects not considered stereotypical of computer science (e.g., nature poster, phone books) was sufficient to boost female undergraduates' interest in computer science to the level of their male peers. Further investigation revealed that the stereotypical broadcast a masculine stereotype that discouraged women's sense of ambient belonging and subsequent interest in the environment (Studies 2, 3, and 4) but had no similar effect on men (Studies 3, 4). This masculine stereotype prevented women's interest from developing even in environments entirely populated by other women (Study 2). Objects can thus come to broadcast stereotypes of a group, which in turn can deter people who do not identify with these stereotypes from joining that group.",2009,88,848,48,1,8,27,27,44,54,72,52,81,107
88e642a5918e09480a2a59af4acc07c328368dd8,"For courses in Computer/Network Security. In recent years, the need for education in computer security and related topics has grown dramatically -- and is essential for anyone studying Computer Science or Computer Engineering. This is the only text available to provide integrated, comprehensive, up-to-date coverage of the broad range of topics in this subject. In addition to an extensive pedagogical program, the book provides unparalleled support for both research and modeling projects, giving students a broader perspective. The Text and Academic Authors Association have named Computer Security: Principles and Practice the winner of the Textbook Excellence Award for the best Computer Science textbook of 2008. Visit Stallings Companion Website at http://williamstallings.com/CompSec/CompSec1e.html for student and instructor resources and his Computer Science Student Resource site http://williamstallings.com/StudentSupport.html Password protected instructor resources can be accessed here by clicking on the Resources Tab to view downloadable files. (Registration required) Supplements Include: *Power Point Lecture Slides*Instructor's Manual*Author maintained website .",2007,0,191,14,0,0,0,0,0,0,0,0,0,1
39aa55cb38ad5711718692493dd5e77f3b79ff74,Algorithms on strings trees and sequences puter. suffix tree. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences by dan gusfield. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences guide books. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences. algorithms on strings trees and sequences ??. pdf download algorithms on strings trees and sequences. dan gusfield algorithms on strings trees and sequences pdf. algorithms on strings trees and sequences puter. 9780521585194 algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. pdf algorithms on strings trees and sequences puter. dan gusfield. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences by gusfield. 26os algorithms on strings trees and sequences. algorithms on strings trees and sequences 97 edition. algorithms on strings trees and sequences puter. algorithms on strings coursera. algorithms on strings trees and sequences puter. 0521585198 algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. algorithms on string trees and sequences libre. algorithms on strings trees and sequences puter science. algorithms on strings 9781107670990 puter science. pdf download algorithms on strings trees and sequences. rebinatorics the mit press. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. linear time construction of suffix trees chapter 6. algorithms on strings trees and sequences puter. buy algorithms on strings trees and sequences puter. seminumerical string matching chapter 4 algorithms on. substring. read free algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. customer reviews algorithms on strings trees. longest mon substring problem,1997,0,1311,164,3,5,7,22,32,40,54,50,48,60
f861d8e4bcda72cbd821454ffed2d20be8ff5e85,"Examples are discussed to show the differences among discriminant analysis, logistic regression, and multiple regression. Chapter 6, “Multivariate Analysis of Variance,” presents advantages of multivariate analysis of variance (MANOVA) over univariate analysis of variance (ANOVA), discusses assumptions of MANOVA, and assesses validations of MANOVA assumptions and model estimation. The authors also discuss post hoc tests of MANOVA and multivariate analysis of covariance. Chapter 7, “Conjoint Analysis,” explains what conjoint analysis does and how it is different from other multivariate techniques. Guidelines of selecting attributes, models, and methods of data collection are presented. Chapter 8, “Cluster Analysis,” studies objectives, roles, and limitations of cluster analysis. Two basic concepts: similarity and distance are discussed. The authors also discuss details of five most popular hierarchical algorithms (singlelinkage, complete-linkage, average-linkage, centroid method, Ward’s method) and three nonhierarchical algorithms (the sequential threshold method, the parallel threshold method, and the optimizing procedure). Profiles of clusters and guidelines for cluster validation are studied as well. Chapter 9, “Multidimensional Scaling and Correspondence Analysis,” introduces two interdependence techniques to display the relationships in the data. The book describes clearly and intuitively the differences between the two techniques and how these two techniques are performed. Chapters 10–12 cover topics in SEM. Chapter 10, “Structural Equation Modeling: An Introduction,” introduces SEM and related concepts such as exogenous, endogenous constructs, and so on, points out the differences between SEM and other multivariate techniques, overviews the decision process of SEM. Chapter 11, “Confirmatory Factor Analysis,” explains the differences between exploratory and confirmatory factor analysis, discusses how to construct, validate, and assess the goodness of fit of a measurement model in SEM by confirmatory factor analysis. Chapter 12, “Testing a Structural Model,” presents some methods of SEM in examining the relationships between latent constructs. The book is an excellent book for people in management and marketing. For the Technometrics audience, this book does not have much flavor of physical, chemical, and engineering sciences. For example, partial least squares, a very popular method in Chemometrics, is discussed but not as detailed as other techniques in the book. Furthermore, due to the amount of materials covered in the book, it might be inappropriate for someone who is new to multivariate analysis.",2007,0,886,94,40,43,56,50,52,67,70,61,60,60
42a709536d5c1f8540cb221de1d869129f8a0b2c,mathematical logic for computer science 2nd edition PDF logic in computer science solution manual PDF logic in computer science huth ryan solutions PDF handbook of logic in computer science volume 2 background computational structures PDF symbolic rewriting techniques progress in computer science and applied logic PDF logic mathematics and computer science modern foundations with practical applications PDF automated reasoning and mathematics essays in memory of william w mccune lecture notes in computer science PDF,2007,58,793,99,55,49,73,65,58,59,56,55,57,42
f1553e2e546a430e401afdfc1a2e5fc7a5a009f9,"The collection of TCS issues is about 1 meter high, 17,000 pages long and it contains 1100 papers. When in 1974 Einar Fredriksson and myself started talking about the creation of a journal dedicated to Theoretical Computer Science we were very far from even dreaming that it could take such an extension within twelve years. We were also a bit shy: what could such a journal, very theoretical indeed and hard to read, be useful to, and who would read it? Fortunately, some people encouraged us and indeed helped us a lot, Mike Paterson who was at that time President of EATCS and who accepted to become Associate Editor, Albert Meyer who was a very active editor at the beginning, Arto Salomaa, who was to become President of EATCS shortly afterwards. Indeed, I should mention all the first members of the Editorial Board, for TCS would never have come to existence without them. Theoretical Computer Science is not a clearly defined discipline with neat borderlines: it is more a state of mind, the conviction that the observed computation phenomena can be formally described and analysed as any physical phenomenon; the conviction that such a formal description helps to understand these phenomena and to master them in order to design better algorithms, better computers, better systems. Our fundamental activity is not to prove theorems in strange mathematical theories, it is to model a complicated reality and in this respect it has to be compared with theoretical physics or what we call in French “Mecanique rationnelle”. This comparison can be pursued rather far, for we also use all possible mathematical concepts and methods and when we do not find appropriate ones in traditional mathematics we create them. The aim is quite clear: using the compact and unambiguous language of mathematics brings to life concepts and methods which will be useful to all designers, builders and users of computer systems, exactly in the same way as matrix calculus or Fourier series and transforms are useful to all engineers and technicians in the electric and electronic industry. And when one thinks about the amount of time it took to build the mathematical theory of matrices and to polish and simplify it up to the state in which it could be taught to all future engineers and become a tool in daily use, we can be extremely satisfied by the development of Theoretical Computer Science. It is true that concepts and methods which were still vague and unclear when TCS was created became essential tools for all industrial designers and manufacturers, in algorithmics, in semantics, in automata theory and control, etc. . . . Certainly, TCS can be proud to have contributed to this development. Coming back to what I was saying a few minutes ago, this contribution was made possible only by the miraculous fact that the first members of the Editorial Board were sharing the same conviction about the necessity of Theoretical Computer Science",1988,0,1324,0,5,8,12,21,44,40,41,31,56,43
9d370d05a3f159ef58655124be6355a6a9bfa59b,"AbstractEmbeddedsystemsareofgrowingimportanceinindustry. Forexample,inatoday'svehicleahugenumberofembeddedandcommunicating systems can be found. Exhaustive testing of such systems is a requirement, because changes after deliveryand use are expensive and sometimes even impossible. In this paper we propose the use of qualitative models, which are anabstraction of quantitative physical models, for test case generation and test execution. In particular, we show how Simulinkmodels from which control programs are automatically extracted can be tested with respect to qualitative models. SinceSimulink models are heavily used in industry, the approach is of practical interest.Keywords: conformance testing, hybrid systems, qualitative reasoning, qrioconf, Garp3 1 Introduction In industry and especially in the automotive industry Simulink is often used to implementcontrol programs for various purposes. One reason is that those models can be directlyconverted into C code, which runs on the vehicle's electronic control units (ECUs). As aconsequence Simulink models have to be tested thoroughly. This holds in particular forsafety critical systems. In order to meet the safety and quality criteria of such modelsautomated test case generation and more specically model-based testing is of specicinterest but has hardly been explored. In order to ll this gap we present an approach thatmakes use of qualitative models for model-based test cases generation.Qualitative models represent basically cause-effect relationships and constraints onmodel variables. They can be seen as an abstraction of the usually implemented quan-titative differential equation models when using Simulink or other modeling languages.Hence, Simulink models are a renement of qualitative models. This is in contrast to theuse of other means for representing models in this domain like hybrid automata, whichshares basically the same abstraction level with Simulink models.Inordertoallowforusingqualitativemodelsformodel-basedtestingwehavetospecifythe equality relation between the specication and the implementation. For this purpose",2009,117,461,11,89,37,46,22,26,24,24,10,11,5
a5e8fdef0bfb5b41138fb79e611781cfb7a0b305,"The Computer Science Unplugged project provides ways to expose students to ideas from Computer Science without having to use computers. This has a number of applications, including outreach, school curriculum support, and clubs. The “Unplugged” project, based at Canterbury University, uses activities, games, magic tricks and competitions to show children the kind of thinking that is expected of a computer scientist. All of the activities are available free of charge at csunplugged.org. The project has recently enjoyed widespread adoption internationally, and substantial industry support. It is recommended in the ACM K-12 curriculum, and has been translated into 12 languages. As well as simply providing teaching resources, there is a very active program developing and evaluating new formats and activities. This includes adaptations of the kinaesthetic activities in virtual worlds; integration with other outreach tools such as the Alice language, adaptation for use by students in large classrooms, and videos to help teachers and presenters understand how to use the material. This paper will explore why this approach has become popular, and describe developments and adaptations that are being used for outreach and teaching around New Zealand, as well as internationally.",2009,17,270,14,0,0,6,6,12,10,18,18,21,35
e597f4c9997255e9e42205cfa5073316697bb07f,This paper describes an exploratory study to identify which environmental and student factors best predict intention to persist in the computer science major. The findings can be used to make decisions about initiatives for increasing retention. Eight indices of student characteristics and perceptions were developed using the research-based Student Experience of the Major Survey: student-student interaction; student-faculty interaction; collaborative learning opportunities; pace/workload/prior experience with programming; teaching assistants; classroom climate/pedagogy; meaningful assignments; and racism/sexism. A linear regression revealed that student-student interaction was the most powerful predictor of students' intention to persist in the major beyond the introductory course. Other factors predicting intention to persist were pace/workload/prior experience and male gender. The findings suggest that computer science departments interested in increasing retention of students set structured expectations for student-student interaction in ways that integrate peer involvement as a mainstream activity rather than making it optional or extracurricular. They also suggest departments find ways to manage programming experience gaps in CS1.,2009,21,167,14,0,5,13,17,16,12,13,14,14,17
500f99128f09df3000154cc28d729bf5304c136b,"Materials engineers easily recognize that the conduction of heat within solids is fundamental to understanding and controlling many processes. We could cite numerous examples to emphasize the importance of this topic. Some important applications that fall in this category include estimating heat losses from process equipment, quenching, or cooling operations where the cooling rate of a part actually controls its microstructure and hence its application, and solidification.",1952,2,19965,591,0,0,0,0,0,0,0,0,0,0
b23d66adc185de9f785b51967995f09ae8f2f97a,"VOLUME ONE: Determination of Optical Constants: E.D. Palik, Introductory Remarks. R.F. Potter, Basic Parameters for Measuring Optical Properties. D.Y. Smith, Dispersion Theory, Sum Rules, and Their Application to the Analysis of Optical Data. W.R. Hunter, Measurement of Optical Constants in the Vacuum Ultraviolet Spectral Region. D.E. Aspnes, The Accurate Determination of Optical Properties by Ellipsometry. J. Shamir, Interferometric Methods for the Determination of Thin-Film Parameters. P.A. Temple, Thin-Film Absorplance Measurements Using Laser Colorimetry. G.J. Simonis, Complex Index of Refraction Measurements of Near-Millimeter Wavelengths. B. Jensen, The Quantum Extension of the Drude--Zener Theory in Polar Semiconductors. D.W. Lynch, Interband Absorption--Mechanisms and Interpretation. S.S. Mitra, Optical Properties of Nonmetallic Solids for Photon Energies below the Fundamental Band Gap. Critiques--Metals: D.W. Lynch and W.R. Hunter, Comments of the Optical Constants of Metals and an Introduction to the Data for Several Metals. D.Y. Smith, E. Shiles, and M. Inokuti, The Optical Properties of Metallic Aluminum. Critiques--Semiconductors: E.D. Palik, Cadium Telluride (CdTe). E.D. Palik, Gallium Arsenide (GaAs). A. Borghesi and G. Guizzetti, Gallium Phosphide (GaP). R.F. Potter, Germanium (Ge). E.D. Palik and R.T. Holm, Indium Arsenide (InAs). R.T. Holm, Indium Antimonide (InSb). O.J. Glembocki and H. Piller, Indium Phosphide (InP). G. Bauer and H. Krenn, Lead Selenide (PbSe). G. Guizzetti and A. Borghesi, Lead Sulfide (PbS). G. Bauer and H. Krenn, Lead Telluride (PbTe). D.F. Edwards, Silicon (Si). H. Piller, Silicon (Amorphous) (-Si). W.J. Choyke and E.D. Palik, Silicon Carbide (SiC). E.D. Palik and A. Addamiano, Zinc Sulfide (ZnS). Critiques--Insulators: D.J. Treacy, Arsenic Selenide (As 2 gt Se 3 gt ). D.J. Treacy, Arsenic Sulfide (As 2 gt S 3 gt ). D.F. Edwards and H.R. Philipp, Cubic Carbon (Diamond). E.D. Palik and W.R. Hunter, Litium Fluoride (LiF). E.D. Palik, Lithium Niobote (LiNbO 3 gt ). E.D. Palik, Potassium Chloride (KCl). H.R. Philipp, Silicon Dioxide (SiO 2 gt ), Type ( (Crystalline). H.R. Philipp, Silicon Dioxide (SiO 2 gt ) (Glass). gt H.R. Philipp, Silicon Monoxide (SiO) (Noncrystalline). H.R. Philipp, Silicon Nitride (Si 3 gt N 4 gt ) (Noncrystalline). J.E. Eldridge and E.D. Palik, Sodium Chloride (NaCl). M.W. Ribarsky, Titanium Dioxide (TiO 2 gt ) (Rutile).",1997,0,12880,533,0,0,0,0,0,1,0,0,44,476
1d96e67cd65361ef4ff600e186d9c4fca60e474c,"1. Introduction 2. The structure of cellular solids 3. Material properties 4. The mechanics of honeycombs 5. The mechanics of foams: basic results 6. The mechanics of foams refinements 7. Thermal, electrical and acoustic properties of foams 8. Energy absorption in cellular materials 9. The design of sandwich panels with foam cores 10. Wood 11. Cancellous bone 12. Cork 13. Sources, suppliers and property data Appendix: the linear-elasticity of anisotropic cellular solids.",1988,0,7964,234,0,7,17,13,18,15,28,35,35,46
223e7be6f5c6d0213701cd59c5160275f0c0316c,This paper discusses the influence of surface energy on the contact between elastic solids. Equations are derived for its effect upon the contact size and the force of adhesion between two lightly loaded spherical solid surfaces. The theory is supported by experiments carried out on the contact of rubber and gelatine spheres.,1971,8,6124,223,0,2,3,6,9,6,11,10,5,6
bcfe89165d431b1939fb308bcc79afeaf9552162,"This review presents a wide-ranging broad-brush picture of dielectric relaxation in solids, making use of the existence of a `universality' of dielectric response regardless of a wide diversity of materials and structures, with dipolar as well as charge-carrier polarization. The review of the experimental evidence includes extreme examples of highly conducting materials showing strongly dispersive behaviour, low-loss materials with a `flat', frequency-independent susceptibility, dipolar loss peaks etc. The surprising conclusion is that despite the evident complexity of the relaxation processes certain very simple relations prevail and this leads to a better insight into the nature of these processes.",1983,5,4465,199,1,5,9,21,27,39,37,34,45,41
38bd6dba0071176f6369088fbd0df175de9b0145,Preface Numerical simulation of intergranular and transgranular crack propagation in ferroelectric polycrystals Microstructure and stray electric fields at surface cracks in ferroelectrics Double kink mechanisms for discrete dislocations in BCC crystals The expanding spherical inhomogeneity with transformation strain A new model of damage: a moving thick layer approach On configurational forces at boundaries in fracture mechanics HotQC simulation of nanovoid growth under tension in copper Coupled phase transformations and plasticity as a field theory of deformation incompatibility Continuum strain-gradient elasticity from discrete valence force field theory for diamond-like crystals,1982,0,4821,319,5,11,10,21,18,27,17,31,49,61
f35aab7f41f62a4faa2121e7ae89a671c441584f,"This work, part of a two-volume set, applies the material developed in the Volume One to various boundary value problems (reflection and refraction at plane surfaces, composite media, waveguides and resonators). The text also covers topics such as perturbation and variational methods.",1973,19,5087,342,2,12,18,18,29,22,44,36,30,45
1cfb61f9ff9f3800342142670cc37a15648792c9,Popular modern generalized gradient approximations are biased toward the description of free-atom energies. Restoration of the first-principles gradient expansion for exchange over a wide range of density gradients eliminates this bias. We introduce a revised Perdew-Burke-Ernzerhof generalized gradient approximation that improves equilibrium properties of densely packed solids and their surfaces.,2007,0,4584,82,0,19,38,54,82,152,205,267,376,444
bec402e6f41f2e5e0b27b3a7642ad92d10cc2fdc,"Generalized gradient approximations (GGA's) seek to improve upon the accuracy of the local-spin-density (LSD) approximation in electronic-structure calculations. Perdew and Wang have developed a GGA based on real-space cutoff of the spurious long-range components of the second-order gradient expansion for the exchange-correlation hole. We have found that this density functional performs well in numerical tests for a variety of systems: (1) Total energies of 30 atoms are highly accurate. (2) Ionization energies and electron affinities are improved in a statistical sense, although significant interconfigurational and interterm errors remain. (3) Accurate atomization energies are found for seven hydrocarbon molecules, with a rms error per bond of 0.1 eV, compared with 0.7 eV for the LSD approximation and 2.4 eV for the Hartree-Fock approximation. (4) For atoms and molecules, there is a cancellation of error between density functionals for exchange and correlation, which is most striking whenever the Hartree-Fock result is furthest from experiment. (5) The surprising LSD underestimation of the lattice constants of Li and Na by 3--4 % is corrected, and the magnetic ground state of solid Fe is restored. (6) The work function, surface energy (neglecting the long-range contribution), and curvature energy of a metallic surface are all slightly reduced in comparison with LSD. Taking account of the positive long-range contribution, we find surface and curvature energies in good agreement with experimental or exact values. Finally, a way is found to visualize and understand the nonlocality of exchange and correlation, its origins, and its physical effects.",1992,0,15014,112,0,0,0,0,0,0,0,0,0,0
0ae5eb644252eed477d8ebe50281f9d7f8ddf977,,1985,0,5115,212,1,4,10,15,14,21,28,25,28,24
85fbf4706fe98315d8ccbc4f391168738c386208,"The stopping and range of ions in matter is physically very complex, and there are few simple approximations which are accurate. However, if modern calculations are performed, the ion distributions can be calculated with good accuracy, typically better than 10%. This review will be in several sections: 
 
a) 
 
A brief exposition of what can be determined by modern calculations. 
 
 
 
 
b) 
 
A review of existing widely-cited tables of ion stopping and ranges. 
 
 
 
 
c) 
 
A review of the calculation of accurate ion stopping powers.",1985,80,9336,111,2,12,41,104,129,159,167,192,203,212
3e286005c6e0cc9077c711ab1b0ca1de097ec386,"Preface. Introduction. 1. One-dimensional motion of an elastic continuum. 2. The linearized theory of elasticity. 3. Elastodynamic theory. 4. Elastic waves in an unbound medium. 5. Plane harmonic waves in elastic half-spaces. 6. Harmonic waves in waveguides. 7. Forced motions of a half-space. 8. Transient waves in layers and rods. 9. Diffraction of waves by a slit. 10. Thermal and viscoelastic effects, and effects of anisotrophy and non-linearity. Author Index. Subject Index.",1962,0,3996,290,0,0,0,0,0,1,0,2,3,0
ff1faf4cc2ae43513e7632e295e43f8366f62a46,,1990,0,3033,548,0,2,2,1,2,3,6,9,10,12
d1bef0717c54dc85b15c2316fdf32a1cb6df6936,"This critical review will be of interest to the experts in porous solids (including catalysis), but also solid state chemists and physicists. It presents the state-of-the-art on hybrid porous solids, their advantages, their new routes of synthesis, the structural concepts useful for their 'design', aiming at reaching very large pores. Their dynamic properties and the possibility of predicting their structure are described. The large tunability of the pore size leads to unprecedented properties and applications. They concern adsorption of species, storage and delivery and the physical properties of the dense phases. (323 references)",2008,288,4286,5,47,210,298,403,495,458,407,401,400,335
b2d4b4465269e7c650430f18b240c6c228dbb3a4,"Recent extensions of the DMol3 local orbital density functional method for band structure calculations of insulating and metallic solids are described. Furthermore the method for calculating semilocal pseudopotential matrix elements and basis functions are detailed together with other unpublished parts of the methodology pertaining to gradient functionals and local orbital basis sets. The method is applied to calculations of the enthalpy of formation of a set of molecules and solids. We find that the present numerical localized basis sets yield improved results as compared to previous results for the same functionals. Enthalpies for the formation of H, N, O, F, Cl, and C, Si, S atoms from the thermodynamic reference states are calculated at the same level of theory. It is found that the performance in predicting molecular enthalpies of formation is markedly improved for the Perdew–Burke–Ernzerhof [Phys. Rev. Lett. 77, 3865 (1996)] functional.",2000,33,6786,56,1,4,19,38,73,76,119,160,200,217
abf125b006f862d74ff0b37258cd0fcf82f7054f,"The electron density, its gradient, and the Kohn-Sham orbital kinetic energy density are the local ingredients of a meta-generalized gradient approximation (meta-GGA). We construct a meta-GGA density functional for the exchange-correlation energy that satisfies exact constraints without empirical parameters. The exchange and correlation terms respect two paradigms: one- or two-electron densities and slowly varying densities, and so describe both molecules and solids with high accuracy, as shown by extensive numerical tests. This functional completes the third rung of ""Jacob's ladder"" of approximations, above the local spin density and GGA rungs.",2003,0,4038,46,4,40,48,70,85,71,131,124,190,247
5660a6ea2ac83b1b984d8c4afbe797f2243c5b94,"Originally published in 1950, this classic book was a landmark in the development of the subject of tribology. For this edition, David Tabor has written a new preface, reviewing the many advances made in this field during the past 36 years and outlining the achievements of Frank Philip Bowden. The book covers the behavior of non-metals, especially elastomers; elastohydrodynamic lubrication; and the wear of sliding surfaces, which has gradually replaced the earlier concentration on the mechanism of friction. It remains one of the most interesting and comprehensive works available on a single branch of physics.",1964,0,5097,183,35,39,36,47,44,44,40,40,50,59
b589bbacd348592e27ae9464383d72b24ec6dc95,Bond-valence parameters which relate bond valences and bond lengths have been derived for a large number of bonds. It is shown that there is a strong linear correlation between the parameters for bonds from cations to pairs of anions. This correlation is used to develop an interpolation scheme that allows the estimation of bond-valence parameters for 969 pairs of atoms. A complete listing of these parameters is given.,1991,10,5014,165,3,13,23,30,26,23,42,45,54,88
ccd50d5764b4508274b7e65420cd8a754a874a96,"Abstract The title problem concerns two isotropic phases firmly bonded together to form a mixture with any concentrations. An elementary account of several theoretical methods of attack is given, among them the derivation of inequalities between various moduli. The approach is completely general and exact. Additionally, the problem is fully solved when the phases have equal rigidities but different compressibilities, the geometry being entirely arbitrary.",1963,3,3698,233,2,2,12,6,6,8,6,9,10,11
80fd8b366a25977d44a23efc75f20222b4e46ee9,"The declared objective of this book is to provide an introductory review of the various theoretical and practical aspects of adsorption by powders and porous solids with particular reference to materials of technological importance. The primary aim is to meet the needs of students and non-specialists, who are new to surface science or who wish to use the advanced techniques now available for the determination of surface area, pore size and surface characterization. In addition, a critical account is given of recent work on the adsorptive properties of activated carbons, oxides, clays and zeolites. Key Features * Provides a comprehensive treatment of adsorption at both the gas/solid interface and the liquid/solid interface * Includes chapters dealing with experimental methodology and the interpretation of adsorption data obtained with porous oxides, carbons and zeolites * Techniques capture the importance of heterogeneous catalysis, chemical engineering and the production of pigments, cements, agrochemicals, and pharmaceuticals",1998,0,2930,197,0,3,10,24,29,36,42,50,92,62
958ffbefe66e7351d17ecf858cb03376fc4911e4,,1947,0,3696,306,0,1,2,2,2,0,1,2,2,1
f6fd0b5a9d4ff2bce8f477b2a044933fc32787ce,"Before the 1960s, all anti-Stokes emissions, which were known to exist, involved emission energies in excess of excitation energies by only a few kT. They were linked to thermal population of energy states above excitation states by such an energy amount. It was the well-known case of anti-Stokes emission for the so-called thermal bands or in the Raman effect for the well-known anti-Stokes sidebands. Thermoluminescence, where traps are emptied by excitation energies of the order of kT, also constituted a field of anti-Stokes emission of its own. Superexcitation, i.e., raising an already excited electron to an even higher level by excited-state absorption (ESA), was also known but with very weak emissions. These types of well-known anti-Stokes processes have been reviewed in classical textbooks on luminescence.1 All fluorescence light emitters usually follow the well-known principle of the Stokes law which simply states that excitation photons are at a higher energy than emitted ones or, in other words, that output photon energy is weaker than input photon energy. This, in a sense, is an indirect statement that efficiency cannot be larger than 1. This principle is",2004,12,3753,60,7,27,29,61,77,100,117,183,219,306
c1d100996090aae0cb689a6bfddfead1b6eacf3a,,2001,0,5151,9,10,108,177,173,326,316,334,356,391,355
4935df568e622737793386afcf86464bb8469846,"The term ``sensitized luminescence'' in crystalline phosphors refers to the phenomenon whereby an impurity (activator, or emitter) is enabled to luminesce upon the absorption of light in a different type of center (sensitizer, or absorber) and upon the subsequent radiationless transfer of energy from the sensitizer to the activator. The resonance theory of Forster, which involves only allowed transitions, is extended to include transfer by means of forbidden transitions which, it is concluded, are responsible for the transfer in all inorganic systems yet investigated. The transfer mechanisms of importance are, in order of decreasing strength, the overlapping of the electric dipole fields of the sensitizer and the activator, the overlapping of the dipole field of the sensitizer with the quadrupole field of the activator, and exchange effects. These mechanisms will give rise to ``sensitization'' of about 103−104, 102, and 30 lattice sites surrounding each sensitizer in typical systems. The dependence of tra...",1953,20,6651,48,0,1,6,3,4,7,6,5,5,6
d319fca1c8ab0a0399f25c8672e92bccc3d21634,1. Periodic structure 2. Lattice waves 3. Electron states 4. Static properties of solids 5. Electron-electron interaction 6. Dynamics of electrons 7. Transport properties 8. Optical properties 9. The fermi surface 10. Magnetism 11. Superconductivity Bibliography Index.,1965,0,3402,93,3,7,11,12,19,27,28,41,46,37
7087000e48e269a86af6b5afb709a72cb569a882,,1972,0,3604,68,10,16,17,23,28,35,24,34,36,37
05cf48fb520d63c7a3164d27fc926ed5eb946525,"These recommendations aim to be a tool for the selection and appraisal of the methods of characterization of porous solids, and to also give the warnings and guidelines on which the experts generally agree. For this purpose, they successively consider the description of a porous solid (definitions, terminology), the principal methods available (stereology , radiation scattering, pycnometry, adsorption, intrusion, suction, maximum buble pressure, fluid flow, immersion or adsorption calorimetry, thermoporometry , size exclusion chromatography, Xenon NMR and ultrasonic methods) and finally the general principles which are worth being followed in the selection of the appropriate method.",1994,5,2737,56,2,2,17,16,22,22,22,19,24,31
b06d33e714fc3882710692b66672095cd81c69ff,"Analysis of the shape of the curve of reflected x-ray intensity vs glancing angle in the region of total reflection provides a new method of studying certain structural properties of the mirror surface about 10 to several hundred angstroms deep. Dispersion theory, extended to treat any (small) number of stratified homogeneous media, is used as a basis of interpretation.",1954,0,3798,48,0,0,0,1,2,1,2,1,0,0
cd3c9d25fdc09ea93bf189f21468e9f4425d40e3,,1953,0,3779,22,0,0,0,2,0,1,2,3,4,2
23fff8d38a2c6566c35950412894bfc6fe5e82e3,Preface 1. Introduction 2. Classical propagation 3. Interband absorption 4. Excitons 5. Luminescence 6. Semiconductor quantum wells 7. Free electrons 8. Molecular materials 9. Luminescence centres 10. Phonons 11. Nonlinear optics Appendix A: Electromagnetism in dielectrics Appendix B: Quantum theory of radiative absorption and emission Appendix C: Band theory Appendix D: Semiconductor p-i-n diodes,2002,5,2369,133,6,19,29,42,38,49,71,70,76,101
ef04581940ad80d5b846ecf2fe3f1ea0eed735ec,"New numerical simulations of the formation of the giant of the second phase. planets are presented, in which for the first time both the gas and The actual rates at which the giant planets accreted small planetesimal accretion rates are calculated in a self-consistent, planetesimals is probably intermediate between the constant interactive fashion. The simulations combine three elements: rates assumed in most previous studies and the highly variable (1) three-body accretion cross sections of solids onto an isolated rates used here. Within the context of the adopted model of planetary embryo, (2) a stellar evolution code for the planet’s planetesimal accretion, the joint constraints of the time scale gaseous envelope, and (3) a planetesimal dissolution code for dissipation of the solar nebula and the current high-Z masses within the envelope, used to evaluate the planet’s effective of the giant planets lead to estimates of the initial surface capture radius and the energy deposition profile of accreted density (sinit) of planetesimals in the outer region of the solar material. Major assumptions include: The planet is embedded nebula. The results show that sinit P 10 g cm 22 near Jupiter’s in a disk of gas and small planetesimals with locally uniform orbit and that sinit ~ a 22 , where a is the distance from the Sun. initial surface mass density, and planetesimals are not allowed These values are a factor of 3 to 4 times as high as that of to migrate into or out of the planet’s feeding zone. the ‘‘minimum-mass’’ solar nebula at Jupiter’s distance and a All simulations are characterized by three major phases. Dur- factor of 2 to 3 times as high at Saturn’s distance. The estimates ing the first phase, the planet’s mass consists primarily of solid for the formation time of Jupiter and Saturn are 1 to 10 million material. The planetesimal accretion rate, which dominates years, whereas those for Uranus fall in the range 2 to 16 million that of gas, rapidly increases owing to runaway accretion, then years. These estimates follow from the properties of our Solar decreases as the planet’s feeding zone is depleted. During the System and do not necessarily apply to giant planets in other second phase, both solid and gas accretion rates are small planetary systems. © 1996 Academic Press, Inc. and nearly independent of time. The third phase, marked by runaway gas accretion, starts when the solid and gas masses are about equal. It is engendered by a strong positive feedback",1995,40,2108,186,0,2,6,19,29,30,24,50,63,77
2a410d3081efe44c6549afa0c03e78f72f7cb8c5,"Built upon the two original books by Mike Crisfield and their own lecture notes, renowned scientist Rene de Borst and his team offer a thoroughly updated yet condensed edition that retains and builds upon the excellent reputation and appeal amongst students and engineers alike for which Crisfield's first edition is acclaimed. Together with numerous additions and updates, the new authors have retained the core content of the original publication, while bringing an improved focus on new developments and ideas. This edition offers the latest insights in non-linear finite element technology, including non-linear solution strategies, computational plasticity, damage mechanics, time-dependent effects, hyperelasticity and large-strain elasto-plasticity. The authors' integrated and consistent style and unrivalled engineering approach assures this book's unique position within the computational mechanics literature.",1991,173,2586,102,1,1,6,7,15,23,44,31,46,60
82540fb1ff2318fd9f8975e3fac8c380d515c7f8,"This is an advanced text for higher degree materials science students and researchers concerned with the strength of highly brittle covalent–ionic solids, principally ceramics. It is a reconstructed and greatly expanded edition of a book first published in 1975. The book presents a unified continuum, microstructural and atomistic treatment of modern day fracture mechanics from a materials perspective. Particular attention is directed to the basic elements of bonding and microstructure that govern the intrinsic toughness of ceramics. These elements hold the key to the future of ceramics as high-technology materials - to make brittle solids strong, we must first understand what makes them weak. The underlying theme of the book is the fundamental Griffith energy-balance concept of crack propagation. The early chapters develop fracture mechanics from the traditional continuum perspective, with attention to linear and nonlinear crack-tip fields, equilibrium and non-equilibrium crack states. It then describes the atomic structure of sharp cracks, the topical subject of crack-microstructure interactions in ceramics, with special focus on the concepts of crack-tip shielding and crack-resistance curves, and finally deals with indentation fracture, flaws, and structural reliability.",1993,0,2373,78,32,49,63,73,71,81,71,84,71,79
d76e80eae8cb3aabd8723f73a065157709b08728,,1970,0,3031,78,3,8,26,33,24,31,38,22,23,30
a50fba74795241d9077b9ff97161fe57d6096fc5,,2000,156,2372,34,0,3,16,29,40,57,86,95,96,110
d957453c3b7a61b2d037b22a8de32b32ba726133,,1963,0,3024,28,2,11,10,19,28,32,28,42,31,50
06df74c90898b00aeb262befc914536e205acf4b,"Algorithms for the symmetry-adapted energy minimisation of solids using analytical first and second derivatives have been devised and implemented in a new computer program GULP. These new methods are found to lead to an improvement in computational efficiency of up to an order of magnitude over the standard algorithm, which takes no account of symmetry, the largest improvement being obtained from the use of symmetry in the generation of the hessian. Accelerated convergence techniques for the dispersion energy are found to be beneficial in improving the precision at little extra computational cost, particularly when a one centre decomposition is possible or the Ewald sum weighting towards real-space is increased.",1997,31,2032,53,9,20,36,56,54,57,72,79,97,66
ee19d478eb4ab5d6d4c4a70ef2a1198196752573,Laser ablation of solid targets by 0.2–5000 ps Ti: Sapphire laser pulses is studied. Theoretical models and qualitative explanations of experimental results are presented. Advantages of femtosecond lasers for precise material processing are discussed and demonstrated.,1996,16,2231,24,0,9,11,18,18,22,28,42,37,59
34685c14584dc833a5bafd8a4c0a01c46b4e6f69,"Preface. Electrical noise associated with dislocations and plastic flow in metals (G. Bertotti, A. Ferro, F. Fiorillo, P. Mazetti). Mechanisms of dislocation drag (V.I. Alshits, V.L. Indenbom). Dislocations in covalent crystals (H. Alexander). Formation and evolution of dislocation structures during irradiation (B.O. Hall). Dislocation theory of martensitic transformations (G.B. Olsen, M. Cohen). Author index. Subject index. Cumulative index.",1979,0,2631,4,3,6,12,17,30,22,33,42,24,47
5328929e4a7323203dcd9d3bec5d048987f1a775,,1916,0,5298,65,0,3,1,0,1,2,1,1,3,3
5aa0b4eab0104f96b1dbb571546394c7f02e67c5,We consider the change in polarization \ensuremath{\Delta}P which occurs upon making an adiabatic change in the Kohn-Sham Hamiltonian of the solid. A simple expression for \ensuremath{\Delta}P is derived in terms of the valence-band wave functions of the initial and final Hamiltonians. We show that physically \ensuremath{\Delta}P can be interpreted as a displacement of the center of charge of the Wannier functions. The formulation is successfully applied to compute the piezoelectric tensor of GaAs in a first-principles pseudopotential calculation.,1993,0,2251,43,1,2,4,4,2,11,14,13,17,30
9c71ccf2708e7550092adaf326916569d37b1c1a,,1972,0,2518,88,1,15,16,19,33,19,36,30,24,30
e8272dcc10b02fe549b02deb9c7696008bdb2f0e,"Theory of structural transformations in solids , Theory of structural transformations in solids , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1983,0,1970,114,0,5,6,12,7,19,27,21,28,32
7bacbaef39b33388c2365a59eae6b86097658070,"Dynamic crack growth is analysed numerically for a plane strain block with an initial central crack subject to tensile loading. The continuum is characterized by a material constitutive law that relates stress and strain, and by a relation between the tractions and displacement jumps across a specified set of cohesive surfaces. The material constitutive relation is that of an isotropic hyperelastic solid. The cohesive surface constitutive relation allows for the creation of new free surface and dimensional considerations introduce a characteristic length into the formulation. Full transient analyses are carried out. Crack branching emerges as a natural outcome of the initial-boundary value problem solution, without any ad hoc assumption regarding branching criteria. Coarse mesh calculations are used to explore various qualitative features such as the effect of impact velocity on crack branching, and the effect of an inhomogeneity in strength, as in crack growth along or up to an interface. The effect of cohesive surface orientation on crack path is also explored, and for a range of orientations zigzag crack growth precedes crack branching. Finer mesh calculations are carried out where crack growth is confined to the initial crack plane. The crack accelerates and then grows at a constant speed that, for high impact velocities, can exceed the Rayleigh wave speed. This is due to the finite strength of the cohesive surfaces. A fine mesh calculation is also carried out where the path of crack growth is not constrained. The crack speed reaches about 45% of the Rayleigh wave speed, then the crack speed begins to oscillate and crack branching at an angle of about 29° from the initial crack plane occurs. The numerical results are at least qualitatively in accord with a wide variety of experimental observations on fast crack growth in brittle solids.",1994,39,2103,103,2,3,9,12,15,28,26,35,28,48
13b39c3e1e064e1344f4d487bba222dafa1bc90d,,1980,0,2284,42,3,17,10,22,24,41,41,49,51,68
e9eb6cc511709071ff6c05ffa9a09671a742612a,An empirical model and an ab initio calculation of the bulk moduli for covalent solids are used to suggest possible new hard materials. The empirical model indicates that hypothetical covalent solids formed between carbon and nitrogen are good candidates for extreme hardness. A prototype system is chosen and a first principles pseudopotential total energy calculation on the system is performed. The results are consistent with the empirical model and show that materials like the prototype can have bulk moduli comparable to or greater than diamond. It may be possible to synthesize such materials in the laboratory.,1989,11,2169,11,0,6,7,5,11,27,49,85,83,99
544d74292be943b2ed18ee6dc68030a2af28d363,"The aim of this paper is to advocate the usefulness of the spin-density-functional (SDF) formalism. The generalization of the Hohenberg-Kohn-Sham scheme to and SDF formalism is presented in its thermodynamic version. The ground-state formalism is extended to more general Hamiltonians and to the lowest excited state of each symmetry. A relation between the exchange-correlation functional and the pair correlation function is derived. It is used for the interpretation of approximate versions of the theory, in particular the local-spin-density (LSD) approximation, which is formally valid only in the limit of slow and weak spatial variation in the density. It is shown, however, to give good account for the exchange-correlation energy also in rather inhomogeneous situations, because only the spherical average of the exchange-correlation hole influences this energy, and because it fulfills the sum rule stating that this hole should contain only one charge unit. A further advantage of the LSD approximation is that it can be systematically improved. Calculations on the homogeneous spin-polarized electron liquid are reported on. These calculations provide data in the form of interpolation formulas for the exchange-correlation energy and potentials, to be used in the LSD approximation. The ground-state properties are obtained from the Galitskii-Migdal formula, which relates the total energy to the one-electron spectrum, obtained with a dynamical self-energy. The self-energy is calculated in an electron-plasmon model where the electron is assumed to couple to one single mode. The potential for excited states is obtained by identifying the quasiparticle peak in the spectrum. Correlation is found to significantly weaken the spin dependence of the potentials, compared with the result in the Hartree-Fock approximation. Charge and spin response functions are calculated in the long-wavelength limit. Correlation is found to be very important for properties which involve a change in the spinpolarization. For atoms, molecules, and solids the usefulness of the SDF formalism is discussed. In order to explore the range of applicability, a few applications of the LSD approximation are made on systems for which accurate solutions exist. The calculated ionization potentials, affinities, and excitation energies for atoms propose that the valence electrons are fairly well described, a typical error in the ionization energy being 1/2 eV. The exchange-correlation holes of two-electron ions are discussed. An application to the hydrogen molecule, using a minimum basis set, shows that the LSD approximation gives good results for the energy curve for all separations studied, in contrast to the spin-independent local approximation. In particular, the error in the binding energy is only 0.1 eV, and bond breaking is properly described. For solids, the SDF formalism provides a framework for band models of magnetism. An estimate of the splitting between spin-up and spin-down energy bands of a ferromagnetic transition metal shows that the LSD approximation gives a correction of the correct sign and order of magnitude to published $X\ensuremath{\alpha}$ results. To stimulate further use of the SDF formalism in the LSD approximation, the paper is self-contained and describes the necessary formulas and input data for the potentials.",1976,0,2416,14,2,16,23,33,43,28,47,39,43,47
58f5dedd4320730ef39189e99b2fb21a6a82f73e,1. Crystal lattices. General theory 2. . Crystal lattices. Applications 3. Interaction of light with non-conducting crystals 4. Electrons in a perfect lattice 5. Cohesive forces in metals 6. Transport phenomena 7. Magnetic properties of metals 8. Ferromagnetism 9. Interaction of light with electrons in solids 10. Semi-conductors and luminescence 11. Superconductivity,1956,0,2799,21,6,8,6,8,5,11,11,13,18,26
8af729d71fcd38e735a62b09d29b14ce370c5c57,,1991,0,2073,52,34,38,50,72,50,66,51,53,77,93
949831004e239a7bdcc4b4107695795bc68e13b2,"The book presents a comprehensive study of elastic wave propagation in solids. Topics covered range from the theory of waves and vibrations in strings to the three-dimensional theory of waves in thick plates. The subject is covered in the following chapters: (1) waves and vibrations in strings, (2) longitudinal waves in thin rods, (3) flexural waves in thin rods, (4) waves in membranes, thin plates and shells, (5) waves in infinite media, (6) waves in semi-infinite media, (7) scattering and diffraction of elastic waves, and (8) wave propagation in plates and rods. Appendices contain introductory information on elasticity, transforms and experimental techniques. /TRRL/",1975,0,2236,67,0,2,2,3,6,4,6,10,10,10
e13f0217348b231d805fef95a9f2e89dad8804fd,"Granular materials are ubiquitous in the world around us. They have properties that are different from those commonly associated with either solids, liquids, or gases. In this review the authors select some of the special properties of granular materials and describe recent research developments.[S0034-6861(96)00204-8]",1996,14,1904,39,3,12,54,31,38,54,53,66,72,100
8441533661bec9ee265ec695d8e7686b478b28d0,"Measurements of the transient photocurrent I(t) in an increasing number of inorganic and organic amorphous materials display anomalous transport properties, The long tail of I(t) indicates a dispersion of carrier transit times. However, the shape invariance of I(t) to electric field and sample thickness (designated as universality for the classes of materials here considered) is incompatible with traditional concepts of statistical spreading, i.e., a Gaussian carrier packet. %e have developed a stochastic transport model for I(t) which describes the dynamics of a carrier packet executing a time-dependent random walk in the presence of a field-dependent spatial bias and an absorbing barrier at the sample surface. The time dependence of the random walk is governed by hopping time distribution Q(t), A packet, generated with a f(t) characteristic of hopping in a",1975,4,2287,35,0,6,16,20,32,39,35,30,46,37
e8b2f6ce1701a9418412798f031255a6735578c2,"Successful modern generalized gradient approximations (GGA's) are biased toward atomic energies. Restoration of the first-principles gradient expansion for the exchange energy over a wide range of density gradients eliminates this bias. With many collaborators, I introduce PBEsol, a revised Perdew-Burke-Ernzerhof GGA that improves equilibrium properties of densely-packed solids and their surfaces.",2007,0,1346,1,1,0,0,0,0,0,0,0,1,2
e9b7c47d761943580610bec96fe67b41b8a0e293,"1 Overview of Solid Mechanics DEFINING A PROBLEM IN SOLID MECHANICS 2 Governing Equations MATHEMATICAL DESCRIPTION OF SHAPE CHANGES IN SOLIDS MATHEMATICAL DESCRIPTION OF INTERNAL FORCES IN SOLIDS EQUATIONS OF MOTION AND EQUILIBRIUM FOR DEFORMABLE SOLIDS WORK DONE BY STRESSES: PRINCIPLE OF VIRTUAL WORK 3 Constitutive Models: Relations between Stress and Strain GENERAL REQUIREMENTS FOR CONSTITUTIVE EQUATIONS LINEAR ELASTIC MATERIAL BEHAVIORSY HYPOELASTICITY: ELASTIC MATERIALS WITH A NONLINEAR STRESS-STRAIN RELATION UNDER SMALL DEFORMATION GENERALIZED HOOKE'S LAW: ELASTIC MATERIALS SUBJECTED TO SMALL STRETCHES BUT LARGE ROTATIONS HYPERELASTICITY: TIME-INDEPENDENT BEHAVIOR OF RUBBERS AND FOAMS SUBJECTED TO LARGE STRAINS LINEAR VISCOELASTIC MATERIALS: TIME-DEPENDENT BEHAVIOR OF POLYMERS AT SMALL STRAINS SMALL STRAIN, RATE-INDEPENDENT PLASTICITY: METALS LOADED BEYOND YIELD SMALL-STRAIN VISCOPLASTICITY: CREEP AND HIGH STRAIN RATE DEFORMATION OF CRYSTALLINE SOLIDS LARGE STRAIN, RATE-DEPENDENT PLASTICITY LARGE STRAIN VISCOELASTICITY CRITICAL STATE MODELS FOR SOILS CONSTITUTIVE MODELS FOR METAL SINGLE CRYSTALS CONSTITUTIVE MODELS FOR CONTACTING SURFACES AND INTERFACES IN SOLIDS 4 Solutions to Simple Boundary and Initial Value Problems AXIALLY AND SPHERICALLY SYMMETRIC SOLUTIONS TO QUASI-STATIC LINEAR ELASTIC PROBLEMS AXIALLY AND SPHERICALLY SYMMETRIC SOLUTIONS TO QUASI-STATIC ELASTIC-PLASTIC PROBLEMS SPHERICALLY SYMMETRIC SOLUTION TO QUASI-STATIC LARGE STRAIN ELASTICITY PROBLEMS SIMPLE DYNAMIC SOLUTIONS FOR LINEAR ELASTIC MATERIALS 5 Solutions for Linear Elastic Solids GENERAL PRINCIPLES AIRY FUNCTION SOLUTION TO PLANE STRESS AND STRAIN STATIC LINEAR ELASTIC PROBLEMS COMPLEX VARIABLE SOLUTION TO PLANE STRAIN STATIC LINEAR ELASTIC PROBLEMS SOLUTIONS TO 3D STATIC PROBLEMS IN LINEAR ELASTICITY SOLUTIONS TO GENERALIZED PLANE PROBLEMS FOR ANISOTROPIC LINEAR ELASTIC SOLIDS SOLUTIONS TO DYNAMIC PROBLEMS FOR ISOTROPIC LINEAR ELASTIC SOLIDS ENERGY METHODS FOR SOLVING STATIC LINEAR ELASTICITY PROBLEMS THE RECIPROCAL THEOREM AND APPLICATIONS ENERGETICS OF DISLOCATIONS IN ELASTIC SOLIDS RAYLEIGH-RITZ METHOD FOR ESTIMATING NATURAL FREQUENCY OF AN ELASTIC SOLID 6 Solutions for Plastic Solids SLIP-LINE FIELD THEORY BOUNDING THEOREMS IN PLASTICITY AND THEIR APPLICATIONS 7 Finite Element Analysis: An Introduction A GUIDE TO USING FINITE ELEMENT SOFTWARE A SIMPLE FINITE ELEMENT PROGRAM 8 Finite Element Analysis: Theory and Implementation GENERALIZED FEM FOR STATIC LINEAR ELASTICITY THE FEM FOR DYNAMIC LINEAR ELASTICITY FEM FOR NONLINEAR (HYPOELASTIC) MATERIALS FEM FOR LARGE DEFORMATIONS: HYPERELASTIC MATERIALS THE FEM FOR VISCOPLASTICITY ADVANCED ELEMENT FORMULATIONS: INCOMPATIBLE MODES, REDUCED INTEGRATION, AND HYBRID ELEMENTS LIST OF EXAMPLE FEA PROGRAMS AND INPUT FILES 9 Modeling Material Failure SUMMARY OF MECHANISMS OF FRACTURE AND FATIGUE UNDER STATIC AND CYCLIC LOADING STRESS- AND STRAIN-BASED FRACTURE AND FATIGUE CRITERIA MODELING FAILURE BY CRACK GROWTH: LINEAR ELASTIC FRACTURE MECHANICS ENERGY METHODS IN FRACTURE MECHANICS PLASTIC FRACTURE MECHANICS LINEAR ELASTIC FRACTURE MECHANICS OF INTERFACES 10 Solutions for Rods, Beams, Membranes, Plates, and Shells PRELIMINARIES: DYADIC NOTATION FOR VECTORS AND TENSORS MOTION AND DEFORMATION OF SLENDER RODS SIMPLIFIED VERSIONS OF THE GENERAL THEORY OF DEFORMABLE ROD EXACT SOLUTIONS TO SIMPLE PROBLEMS INVOLVING ELASTIC RODS MOTION AND DEFORMATION OF THIN SHELLS: GENERAL THEORY SIMPLIFIED VERSIONS OF GENERAL SHELL THEORY: FLAT PLATES AND MEMBRANES SOLUTIONS TO SIMPLE PROBLEMS INVOLVING MEMBRANES, PLATES, AND SHELLS Appendix A: Review of Vectors and Matrices A.1. VECTORS A.2. VECTOR FIELDS AND VECTOR CALCULUS A.3. MATRICES Appendix B: Introduction to Tensors and Their Properties B.1. BASIC PROPERTIES OF TENSORS B.2. OPERATIONS ON SECOND-ORDER TENSORS B.3. SPECIAL TENSORS Appendix C: Index Notation for Vector and Tensor Operations C.1. VECTOR AND TENSOR COMPONENTS C.2. CONVENTIONS AND SPECIAL SYMBOLS FOR INDEX NOTATION C.3. RULES OF INDEX NOTATION C.4. VECTOR OPERATIONS EXPRESSED USING INDEX NOTATION C.5. TENSOR OPERATIONS EXPRESSED USING INDEX NOTATION C.6. CALCULUS USING INDEX NOTATION C.7. EXAMPLES OF ALGEBRAIC MANIPULATIONS USING INDEX NOTATION Appendix D: Vectors and Tensor Operations in Polar Coordinates D.1. SPHERICAL-POLAR COORDINATES D.2. CYLINDRICAL-POLAR COORDINATES Appendix E: Miscellaneous Derivations E.1. RELATION BETWEEN THE AREAS OF THE FACES OF A TETRAHEDRON E.2. RELATION BETWEEN AREA ELEMENTS BEFORE AND AFTER DEFORMATION E.3. TIME DERIVATIVES OF INTEGRALS OVER VOLUMES WITHIN A DEFORMING SOLID E.4. TIME DERIVATIVES OF THE CURVATURE VECTOR FOR A DEFORMING ROD References",2009,0,1088,71,14,19,38,61,73,113,111,122,121,112
efcb8f7293255dabf4ac2aea4a2515d977847c28,"After giving a concise overview of the current knowledge in the field of quantum mechanical bonding indicators for molecules and solids, we show how to obtain energy-resolved visualization of chemical bonding in solids by means of density-functional electronic structure calculations. On the basis of a band structure energy partitioning scheme, i.e., rewriting the band structure energy as a sum of orbital pair contributions, we derive what is to be defined as crystal orbital Hamilton populations (COHP). In particular, a COHP(E) diagram indicates bonding, nonbonding, and antibonding energy regions within a specified energy range while an energy integral of a COHP gives access to the contribution of an atom or a chemical bond to the distribution of one-particle energies",1993,0,1678,11,0,0,0,0,1,0,10,8,12,14
e17791492c49aaab8ceb81af41cf135cec02fb25,"Hybrid Fock exchange/density functional theory functionals have shown to be very successful in describing a wide range of molecular properties. For periodic systems, however, the long-range nature of the Fock exchange interaction and the resultant large computational requirements present a major drawback. This is especially true for metallic systems, which require a dense Brillouin zone sampling. Recently, a new hybrid functional [HSE03, J. Heyd, G. E. Scuseria, and M. Ernzerhof, J. Chem. Phys. 118, 8207 (2003)] that addresses this problem within the context of methods that evaluate the Fock exchange in real space was introduced. We discuss the advantages the HSE03 functional brings to methods that rely on a reciprocal space description of the Fock exchange interaction, e.g., all methods that use plane wave basis sets. Furthermore, we present a detailed comparison of the performance of the HSE03 and PBE0 functionals for a set of archetypical solid state systems by calculating lattice parameters, bulk moduli, heats of formation, and band gaps. The results indicate that the hybrid functionals indeed often improve the description of these properties, but in several cases the results are not yet on par with standard gradient corrected functionals. This concerns in particular metallic systems for which the bandwidth and exchange splitting are seriously overestimated.",2006,45,1470,21,8,25,19,32,27,68,73,86,104,133
dafdc4454985809ebd7cc05b8510266b1fc99f4d,"The field of viscous liquid and glassy solid dynamics is reviewed by a process of posing the key questions that need to be answered, and then providing the best answers available to the authors and their advisors at this time. The subject is divided into four parts, three of them dealing with behavior in different domains of temperature with respect to the glass transition temperature, Tg , and a fourth dealing with ‘‘short time processes.’’ The first part tackles the high temperature regime T.Tg ,i n which the system is ergodic and the evolution of the viscous liquid toward the condition at Tg is in focus. The second part deals with the regime T;Tg , where the system is nonergodic except for very long annealing times, hence has time-dependent properties ~aging and annealing!. The third part discusses behavior when the system is completely frozen with respect to the primary relaxation process but in which secondary processes, particularly those responsible for ‘‘superionic’’ conductivity, and dopart mobility in amorphous silicon, remain active. In the fourth part we focus on the behavior of the system at the crossover between the low frequency vibrational components of the molecular motion and its high frequency relaxational components, paying particular attention to very recent developments in the short time dielectric response and the high Q mechanical response. © 2000 American Institute of Physics.@S0021-8979~00!02213-1#",2000,239,1649,17,0,19,46,53,59,72,88,85,84,63
76eeeed493454a0f34af822db779c932b3db074a,"A simple two pulse phase modulation (TPPM) scheme greatly reduces the residual linewidths arising from insufficient proton decoupling power in double resonance magic angle spinning (MAS) experiments. Optimization of pulse lengths and phases in the sequence produces substantial improvements in both the resolution and sensitivity of dilute spins (e.g., 13C) over a broad range of spinning speeds at high magnetic field. The theoretical complications introduced by large homo‐ and heteronuclear interactions among the spins, as well as the amplitude modulation imposed by MAS, are explored analytically and numerically. To our knowledge, this method is the first phase‐switched sequence to exhibit improvement over continuous‐wave (cw) decoupling in a strongly coupled homogeneous spin system undergoing sample spinning.",1995,26,1761,40,0,8,9,12,19,36,43,56,47,69
3ebf01f223a6b830669197903ac0e802e72dc4c0,"We present a new nonempirical density functional generalized gradient approximation (GGA) that gives significant improvements for lattice constants, crystal structures, and metal surface energies over the most popular Perdew-Burke-Ernzerhof (PBE) GGA. The new functional is based on a diffuse radial cutoff for the exchange-hole in real space, and the analytic gradient expansion of the exchange energy for small gradients. There are no adjustable parameters, the constraining conditions of PBE are maintained, and the functional is easily implemented in existing codes.",2005,30,1445,31,0,2,12,35,38,71,69,86,98,114
00671c4c337cd0aa9fb585daee1e0a56ab5141fa,"Recently we developed an efficient broadband decoupling sequence called SPARC-16 for liquid crystals ¿J. Magn. Reson. 130, 317 (1998). The sequence is based upon a 16-step phase cycling of the 2-step TPPM decoupling method for solids ¿J. Chem. Phys. 103, 6951 (1995). Since then, we have found that a stepwise variation of the phase angle in the TPPM sequence offers even better results. The application of this new method to a liquid crystalline compound, 4-n-pentyl-4'-cyanobiphenyl, and a solid, L-tyrosine hydrochloride, is reported. The reason for the improvement is explained by an analysis of the problem in the rotating frame.",2000,28,1474,46,0,6,4,11,10,17,20,57,59,61
85de15f1af6793fef9a9ab9d21610bcc330dd56f,"CKA~K growth initiation and subsequent resistance is computed for an elastic-plastic solid with an idealized traction separation law specified on the crack plane to characterize the fracture process. The solid is specified by its Young’s modulus, E, Poisson’s ratio, v, initial tensile yield stress, (or, and strain hardening exponent, N. The primary parameters specifying the traction-separation law of the fracture process are the work of separation per unit area, To. and the peak traction, 6. Highly refined calculations have been carried out for resistance curves. K,(Arr), for plane strain, mode I growth in small-scale yielding as dependent on the parameters characterizing the elastic-plastic properties of the solid and its fracture process. With K,, = [El-,/( I ~ v’)] ’ 2 as the intensity needed to advance the crack in the absence ofplasticity, K,J& is presented in terms of its dependence on the two most important parameters, d/nr and N, with special emphasis on initiation toughness and steady-state toughness, Three applications of the results are made : to predict toughnesss when the fracture process is void growth and coalescence, to predict the role of plasticity on interface toughness for similar materials bonded together, and to illuminate the role of plasticity in enhancing toughness in dual-phase solids. The regime of applicability of the present model to ductile fracture due to void growth and coalescence, wherein multiple voids interact within the fracture process zone, is complementary to the regime of applicability of models describing the interaction between a single void and the crack tip. The two mechanism regimes are delineated and the consequence of a transition between them is discussed.",1992,11,1558,81,1,5,7,14,12,22,18,21,27,28
55a95350e3bf5ea85867ef932a86c775bbb50329,"The present work introduces an efficient screening technique to take advantage of the fast spatial decay of the short range Hartree-Fock (HF) exchange used in the Heyd-Scuseria-Ernzerhof (HSE) screened Coulomb hybrid density functional. The screened HF exchange decay properties and screening efficiency are compared with traditional hybrid functional calculations on solids. The HSE functional is then assessed using 21 metallic, semiconducting, and insulating solids. The examined properties include lattice constants, bulk moduli, and band gaps. The results obtained with HSE exhibit significantly smaller errors than pure density functional theory (DFT) calculations. For structural properties, the errors produced by HSE are up to 50% smaller than the errors of the local density approximation, PBE, and TPSS functionals used for comparison. When predicting band gaps of semiconductors, we found smaller errors with HSE, resulting in a mean absolute error of 0.2 eV (1.3 eV error for all pure DFT functionals). In addition, we present timing results which show the computational time requirements of HSE to be only a factor of 2-4 higher than pure DFT functionals. These results make HSE an attractive choice for calculations of all types of solids.",2004,36,1380,11,2,8,21,11,24,20,18,49,72,79
c9ec00a9e5c94c71c314c3baffeb7e6438306a14,"We propose a dynamical theory of low-temperature shear deformation in amorphous solids. Our analysis is based on molecular-dynamics simulations of a two-dimensional, two-component noncrystalline system. These numerical simulations reveal behavior typical of metallic glasses and other viscoplastic materials, specifically, reversible elastic deformation at small applied stresses, irreversible plastic deformation at larger stresses, a stress threshold above which unbounded plastic flow occurs, and a strong dependence of the state of the system on the history of past deformations. Microscopic observations suggest that a dynamically complete description of the macroscopic state of this deforming body requires specifying, in addition to stress and strain, certain average features of a population of two-state shear transformation zones. Our introduction of these state variables into the constitutive equations for this system is an extension of earlier models of creep in metallic glasses. In the treatment presented here, we specialize to temperatures far below the glass transition and postulate that irreversible motions are governed by local entropic fluctuations in the volumes of the transformation zones. In most respects, our theory is in good quantitative agreement with the rich variety of phenomena seen in the simulations. {copyright} {ital 1998} {ital The American Physical Society}",1997,0,1370,57,0,1,1,3,8,7,19,10,32,38
f0669eb9296f28677450d44f36ca9aafc899e809,"The NMR signals of isotopically or chemically dilute nuclear spins S in solids can be enhanced by repeatedly transferring polarization from a more abundant species I of high abundance (usually protons) to which they are coupled. The gain in power sensitivity as compared with conventional observation of the rare spins approaches NII(I+1)γI2/NSS(S+1)γS2, or ∼ 103 for S = 13C, I = 1H in organic solids. The transfer of polarization is accomplished by any of a number of double resonance methods. High‐frequency resolution of the S ‐spin signal is obtained by decoupling of the abundant spins. The experimental requirements of the technique are discussed and a brief comparison of its sensitivity with other procedures is made. Representative applications and experimental results are mentioned.",1973,69,2012,13,1,9,11,17,21,30,37,36,49,51
548af6be21fed61a54ab3e2285ace76004094642,"This article describes the variational and fixed-node diffusion quantum Monte Carlo methods and how they may be used to calculate the properties of many-electron systems. These stochastic wave-function-based approaches provide a very direct treatment of quantum many-body effects and serve as benchmarks against which other techniques may be compared. They complement the less demanding density-functional approach by providing more accurate results and a deeper understanding of the physics of electronic correlation in real materials. The algorithms are intrinsically parallel, and currently available high-performance computers allow applications to systems containing a thousand or more electrons. With these tools one can study complicated problems such as the properties of surfaces and defects, while including electron correlation effects with high precision. The authors provide a pedagogical overview of the techniques and describe a selection of applications to ground and excited states of solids and clusters.",2001,213,1373,40,5,24,36,45,48,45,52,50,45,71
c52b1b1308f122245761e05ba64f7aaa0ac8952c,The Formation of Amorphous Solids Amorphous Morphology: The Geometry and Topology of Disorder Chalcogenide Glasses and Organic Polymers The Percolation Model Localization Delocalization Transitions Optical and Electrical Properties Index.,1983,0,1843,70,1,5,26,23,25,42,37,48,50,46
404dccc6f1ebb4b39869e1a80a1938428bf3cc59,"When chopped light impinges on a solid in an enclosed cell, an acoustic signal is produced within the cell. This effect is the basis of a new spectroscopic technique for the study of solid and semisolid matter. A quantitative derivation is presented for the acoustic signal in a photoacoustic cell in terms of the optical, thermal, and geometric parameters of the system. The theory predicts the dependence of the signal on the absorption coefficient of the solid, thereby giving a theoretical foundation for the technique of photoacoustic spectroscopy. In particular, the theory accounts for the experimental observation that with this technique optical absorption spectra can be obtained for materials that are optically opaque.",1976,11,2070,8,2,22,27,38,66,54,55,39,55,49
9eb18fbbfb909cb450cfc96a57dbdfe528a71d79,"Many attempts have been made to reproduce theoretically the stress–strain curves obtained from experiments on the isothermal deformation of highly elastic ‘rubberlike' materials. The existence of a strain-energy function has usually been postulated, and the simplifications appropriate to the assumptions of isotropy and incompressibility have been exploited. However, the usual practice of writing the strain energy as a function of two independent strain invariants has, in general, the effect of complicating the associated mathematical analysis (this is particularly evident in relation to the calculation of instantaneous moduli of elasticity) and, consequently, the basic elegance and simplicity of isotropic elasticity is sacrificed. Furthermore, recently proposed special forms of the strain-energy function are rather complicated functions of two invariants. The purpose of this paper is, while making full use of the inherent simplicity of isotropic elasticity, to construct a strain-energy function which: (i) provides an adequate representation of the mechanical response of rubberlike solids, and (ii) is simple enough to be amenable to mathematical analysis. A strain-energy function which is a linear combination of strain invariants defined by ϕ(α) = (a1α+a2α+a3α–3)/α is proposed; and the principal stretches a1, a2 and a3 are used as independent variables subject to the incompressibility constraint a1a2 a3 = 1. Principal axes techniques are used where appropriate. An excellent agreement between this theory and the experimental data from simple tension, pure shear and equibiaxial tension tests is demonstrated. It is also shown that the present theory has certain repercussions in respect of the constitutive inequality proposed by Hill (1968a, 1970b).",1972,15,1957,83,0,3,2,2,6,3,6,6,6,5
f628fb70cbb073f6526f5291a4c359d43811245f,,1988,0,1511,178,3,1,1,8,3,4,5,7,21,21
1c3cec66c7eb580e8784c03d0dff9ac7913b26f2,"We discuss evolution of the Fermi surface (FS) topology with doping in electron-doped cuprates within the framework of a one-band Hubbard Hamiltonian, where antiferromagnetism and superconductivity are assumed to coexist in a uniform phase. In the lightly doped insulator, the FS consists of electron pockets around the ðp;0Þ points. The first change in the FS topology occurs in the optimally doped region when an additional hole pocket appears at the nodal point. The second change in topology takes place in the overdoped regime ð 18%Þwhere antiferromagnetism disappears and a large ðp;pÞ-centered metallic FS is formed. Evidence for these two topological transitions is found in recent Hall effect and penetration depth experiments on Pr2 xCexCuO4 d (PCCO) and with a number of spectroscopic measurements on Nd2 xCexCuO4 d (NCCO). & 2008 Elsevier Ltd. All rights reserved.",2008,14,1121,97,25,27,44,44,64,76,82,84,94,100
5b5ee79def11444b2e35d0d54aa1f010840bb78a,"We develop a method which permits the analysis of problems requiring the simultaneous resolution of continuum and atomistic length scales-and associated deformation processes-in a unified manner. A finite element methodology furnishes a continuum statement of the problem of interest and provides the requisite multiple-scale analysis capability by adaptively refining the mesh near lattice defects and other highly energetic regions. The method differs from conventional finite element analyses in that interatomic interactions are incorporated into the model through a crystal calculation based on the local state of deformation. This procedure endows the model with crucial properties, such as slip invariance, which enable the emergence of dislocations and other lattice defects. We assess the accuracy of the theory in the atomistic limit by way of three examples: a stacking fault on the (111) plane, and edge dislocations residing on (111) and (100) planes of an aluminium single crystal. The method correctly predicts the splitting of the (111) edge dislocation into Shockley partials. The computed separation of these partials is consistent with results obtained by direct atomistic simulations. The method predicts no splitting of the Al Lomer dislocation, in keeping with observation and the results of direct atomistic simulation. In both cases, the core structures are found to be in good agreement with direct lattice statics calculations, which attests to the accuracy of the method at the atomistic scale.",1996,24,1396,64,2,6,20,15,17,18,25,38,55,73
918643b95bf818415eb13794068a4f32ea8855c2,"Mathematical Introduction. Acoustic Phonons. Plasmons, Optical Phonons, and Polarization Waves. Magnons. Fermion Fields and the Hartree--Forck Approximation. Many--Body Techniques and the Electron Gas. Polarons and the Electron--Phonon Interaction. Superconductivity. Bloch Funcations----General Properties. Brillouin Zones and Crystal Symmetry. Dynamics of Electronics in a Magnetic Field: de Hass--van Alphen Effect and Cyclotron Resonance. Magnetoresistance. Calculation of Energy Bands and Fermi Surfaces. Semiconductor Crystals: I. Energy Bands, Cyclotron Resonance and Impurity States. Semiconductor Crystals: II. Optical Absorption and Excitons. Electrodynamics of Metals. Acoustic Attenuation in Metals. Theory of Alloys. Correlation Functions and Neutron Diffraction by Crystals. Recoilless Emission. Greena s Functions----Application to Solid State Physics. Appendixes.",1963,0,1940,63,0,2,4,19,16,13,32,19,25,35
0d242782af0568f2cb9c9c22d2f23a96f3e5326f,"This study of the theory of electrical and thermal conduction in metals, semiconductors, and insulators is written at a level appropriate to graduate students and research workers and assumes some knowledge of wave mechanics in its reader. The basic ideas of crystal lattice dynamics, electron zone structure, and transport theory are developed from first principles, and formulae for the macroscopic coefficients are deduced by self-contained mathematical arguments. Interpretation in terms of electronic structure, chemical purity, crystal structure, and crystal perfection is emphasized as a tool for further investigation but detailed discussion of individual means or alloys is avoided.",2001,0,1265,85,10,17,12,10,17,17,26,27,28,41
41a88a490d7ba9e383ecb16c4290083413a08258,"This book is a rigorous exposition of formal languages and models of computation, with an introduction to computational complexity. The authors present the theory in a concise and straightforward manner, with an eye out for the practical applications. Exercises at the end of each chapter, including some that have been solved, help readers confirm and enhance their understanding of the material. This book is appropriate for upper-level computer science undergraduates who are comfortable with mathematical arguments.",1979,1,14120,881,0,0,6,3,2,5,3,3,7,6
3c6f487f79b23dcf2149b994d2eed3fc9f29795b,"This special issue of Mathematical Structures in Computer Science contains several contributions related to the modern field of Quantum Information and Quantum Computing. The first two papers deal with entanglement. The paper by R. Mosseri and P. Ribeiro presents a detailed description of the two- and three-qubit geometry in Hilbert space, dealing with the geometry of fibrations and discrete geometry. The paper by J.-G.Luque et al. is more algebraic and considers invariants of pure k-qubit states and their application to entanglement measurement.",2007,0,15207,2015,3,3,13,65,236,907,1102,1054,1100,1035
71b2135b1298939385fb85ccd061e57789287975,"Abstract The paper reviews the problem of making numerical predictions of turbulent flow. It advocates that computational economy, range of applicability and physical realism are best served at present by turbulence models in which the magnitudes of two turbulence quantities, the turbulence kinetic energy k and its dissipation rate ϵ, are calculated from transport equations solved simultaneously with those governing the mean flow behaviour. The width of applicability of the model is demonstrated by reference to numerical computations of nine substantially different kinds of turbulent flow.",1990,27,10418,424,0,0,109,154,136,155,180,162,178,139
961e2156d523e3901c491cc2a1f65764c976fc44,"Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.",1995,55,5841,949,1,7,48,77,90,99,134,142,158,178
6012f4ad15ba4eb7888e43daf69f11041ab56dbd,"From the Publisher: 
""An IEEE reprinting of this classic 1968 edition, FIELD COMPUTATION BY MOMENT METHODS is the first book to explore the computation of electromagnetic fields by the most popular method for the numerical solution to electromagnetic field problems. It presents a unified approach to moment methods by employing the concepts of linear spaces and functional analysis. Written especially for those who have a minimal amount of experience in electromagnetic theory, this book illustrates theoretical and mathematical concepts to prepare all readers with the skills they need to apply the method of moments to new, engineering-related problems.Written especially for those who have a minimal amount of experience in electromagnetic theory, theoretical and mathematical concepts are illustrated by examples that prepare all readers with the skills they need to apply the method of moments to new, engineering-related problems.""",1968,0,6324,348,0,10,9,12,26,33,23,24,32,28
8665c9b459e4161825baf1f25b5141f41a5085ff,"The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.",1990,33,4017,330,4,24,48,55,63,72,146,149,133,140
638df1b831feb3647a9bf5496780b38890573d4d,"gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear",1989,0,5464,490,18,24,46,47,84,81,81,85,82,52
2a193b9417a4aaf35bcad9152cf35f78dc5906a9,"The tools of molecular biology were used to solve an instance of the directed Hamiltonian path problem. A small graph was encoded in molecules of DNA, and the ""operations"" of the computation were performed with standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computations at the molecular level.",1994,37,4010,272,3,36,52,77,69,95,92,105,120,133
74e92100fbe99fbe2de8243ff489ec7d53d8c3ae,"In a recent paper, Bai and Perron (1998) considered theoretical issues related to the limiting distribution of estimators and test statistics in the linear model with multiple structural changes. In this companion paper, we consider practical issues for the empirical applications of the procedures. We first address the problem of estimation of the break dates and present an efficient algorithm to obtain global minimizers of the sum of squared residuals. This algorithm is based on the principle of dynamic programming and requires at most least-squares operations of order O(T 2) for any number of breaks. Our method can be applied to both pure and partial structural-change models. Secondly, we consider the problem of forming confidence intervals for the break dates under various hypotheses about the structure of the data and the errors across segments. Third, we address the issue of testing for structural changes under very general conditions on the data and the errors. Fourth, we address the issue of estimating the number of breaks. We present simulation results pertaining to the behavior of the estimators and tests in finite samples. Finally, a few empirical applications are presented to illustrate the usefulness of the procedures. All methods discussed are implemented in a GAUSS program available upon request for non-profit academic use.",1998,26,4438,628,1,1,2,11,18,29,48,85,90,126
5a8d0d7094c356e3b851fd66bd929ed0e56aabfd,,1977,0,5328,362,16,18,16,35,37,50,41,58,58,53
2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea,"A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.<<ETX>>",1994,39,5367,192,7,33,58,57,77,65,89,93,117,115
650003ab3505441cd8e828ca26d2aa2faf24d2ff,,1988,0,4012,250,0,0,3,4,21,30,38,57,104,109
c86900fc9f326e9861adc395c7e4be93dcdb8c2c,"Recent advances in cDNA and oligonucleotide DNA arrays have made it possible to measure the abundance of mRNA transcripts for many genes simultaneously. The analysis of such experiments is nontrivial because of large data size and many levels of variation introduced at different stages of the experiments. The analysis is further complicated by the large differences that may exist among different probes used to interrogate the same gene. However, an attractive feature of high-density oligonucleotide arrays such as those produced by photolithography and inkjet technology is the standardization of chip manufacturing and hybridization process. As a result, probe-specific biases, although significant, are highly reproducible and predictable, and their adverse effect can be reduced by proper modeling and analysis methods. Here, we propose a statistical model for the probe-level data, and develop model-based estimates for gene expression indexes. We also present model-based methods for identifying and handling cross-hybridizing probes and contaminating array regions. Applications of these results will be presented elsewhere.",2001,13,3357,237,28,77,159,248,301,314,333,287,281,244
054b680165a7325569ca6e63028ca9cee7f3ac9a,"Quantum computers promise to increase greatly the efficiency of solving problems such as factoring large integers, combinatorial optimization and quantum physics simulation. One of the greatest challenges now is to implement the basic quantum-computational elements in a physical system and to demonstrate that they can be reliably and scalably controlled. One of the earliest proposals for quantum computation is based on implementing a quantum bit with two optical modes containing one photon. The proposal is appealing because of the ease with which photon interference can be observed. Until now, it suffered from the requirement for non-linear couplings between optical modes containing few photons. Here we show that efficient quantum computation is possible using only beam splitters, phase shifters, single photon sources and photo-detectors. Our methods exploit feedback from photo-detectors and are robust against errors from photon loss and detector inefficiency. The basic elements are accessible to experimental investigation with current technology.",2001,56,4188,110,23,56,121,134,174,155,185,194,180,195
a6e1b6ed82a5286baf684f538dab54c8235b05d4,We propose an implementation of a universal set of one- and two-quantum-bit gates for quantum computation using the spin states of coupled single-electron quantum dots. Desired operations are effected by the gating of the tunneling barrier between neighboring dots. Several measures of the gate quality are computed within a recently derived spin master equation incorporating decoherence caused by a prototypical magnetic environment. Dot-array experiments that would provide an initial demonstration of the desired nonequilibrium spin dynamics are proposed.,1997,33,4647,89,4,10,29,55,86,161,179,195,208,274
0b7cd3a0975a9fb228dacf1c63615c98cf07591e,"Topological quantum computation has emerged as one of the most exciting approaches to constructing a fault-tolerant quantum computer. The proposal relies on the existence of topological states of matter whose quasiparticle excitations are neither bosons nor fermions, but are particles known as non-Abelian anyons, meaning that they obey non-Abelian braiding statistics. Quantum information is stored in states with multiple quasiparticles, which have a topological degeneracy. The unitary gate operations that are necessary for quantum computation are carried out by braiding quasiparticles and then measuring the multiquasiparticle states. The fault tolerance of a topological quantum computer arises from the nonlocal encoding of the quasiparticle states, which makes them immune to errors caused by local perturbations. To date, the only such topological states thought to have been found in nature are fractional quantum Hall states, most prominently the $\ensuremath{\nu}=5∕2$ state, although several other prospective candidates have been proposed in systems as disparate as ultracold atoms in optical lattices and thin-film superconductors. In this review article, current research in this field is described, focusing on the general theoretical concepts of non-Abelian statistics as it relates to topological quantum computation, on understanding non-Abelian quantum Hall states, on proposed experiments to detect non-Abelian anyons, and on proposed architectures for a topological quantum computer. Both the mathematical underpinnings of topological quantum computation and the physics of the subject are addressed, using the $\ensuremath{\nu}=5∕2$ fractional quantum Hall state as the archetype of a non-Abelian topological state enabling fault-tolerant quantum computation.",2007,577,3299,71,9,47,83,97,142,139,181,211,230,254
68db0fc648885fa43fd8a055f45a4c4debfe1f3d,"Abstract : A foundational model of concurrency is developed in this thesis. It examines issues in the design of parallel systems and show why the actor model is suitable for exploiting large-scale parallelism. Concurrency in actors is constrained only by the availability of hardware resources and by the logical dependence inherent in the computation. Unlike dataflow and functional programming, however, actors are dynamically reconfigurable and can model shared resources with changing local state. Concurrency is spawned in actors using asynchronous message-passing, pipelining, and the dynamic creation of actors. The author defines an abstract actor machine and provide a minimal programming language for it. A more expressive language, which includes higher level constructs such as delayed and eager evaluation, can be defined in terms of the primitives. Examples are given to illustrate the ease with which concurrent data and control structures can be programmed. This thesis deals with some central issues in distributed computing. Specifically, problems of divergence and deadlock are addressed. Additional keywords: Object oriented programming; Semantics.",1990,44,3165,198,56,88,103,117,102,121,125,99,108,99
4c7671550671deba9ec318d867522897f20e19ba,"The usual general-purpose computing automaton (e.g.. a Turing machine) is logically irreversible- its transition function lacks a single-valued inverse. Here it is shown that such machines may he made logically reversible at every step, while retainillg their simplicity and their ability to do general computations. This result is of great physical interest because it makes plausible the existence of thermodynamically reversible computers which could perform useful computations at useful speed while dissipating considerably less than kT of energy per logical step. In the first stage of its computation the logically reversible automaton parallels the corresponding irreversible automaton, except that it saves all intermediate results, there by avoiding the irreversible operation of erasure. The second stage consists of printing out the desired output. The third stage then reversibly disposes of all the undesired intermediate results by retracing the steps of the first stage in backward order (a process which is only possible because the first stage has been carried out reversibly), there by restoring the machine (except for the now-written output tape) to its original condition. The final machine configuration thus contains the desired output and a reconstructed copy of the input, but no other undesired data. The foregoing results are demonstrated explicitly using a type of three-tape Turing machine. The biosynthesis of messenger RNA is discussed as a physical example of reversible computation.",1973,5,3397,175,1,2,2,5,2,3,2,2,3,12
ac145c160754421f43463a1d2826693f9d2422cb,"A general procedure for the solution of problems in structural dynamics is described herein. The method is capable of application to structures of any degree of complication, with any relationship ...",1959,0,4352,207,0,0,0,1,0,0,1,1,0,0
99303872c16d47af2042c5303cd5da863a1e5cfb,We implement the efficient line-of-sight method to calculate the anisotropy and polarization of the cosmic microwave background for scalar and tensor modes in almost Friedmann-Robertson-Walker models with positive spatial curvature. We present new results for the polarization power spectra in such models.,1999,33,2992,257,0,6,5,19,27,20,27,52,70,93
1d41d6ec4805f80b84a1ccd17f6753ba71e107f7,"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.",2005,0,2291,486,1,1,14,26,33,43,59,77,122,128
4cf4429f11acb8a51a362cbcf3713c06bba5aec7,"We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.",2002,47,2553,325,1,12,15,21,25,31,56,79,139,137
7a23da2c14f9355dd63a434b62cf5b28aeebc305,"Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.",1985,36,3054,157,0,5,5,29,22,45,44,30,30,31
e0535dedb8607d83cd2614317c99913378e89e26,"A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.",2002,43,2755,196,15,24,37,66,77,85,83,132,111,111
84205f6b64cf84f52defc13d22d19d8bb5814503,"A two-dimensional quantum system with anyonic excitations can be considered as a quantum 
computer. Unitary transformations can be performed by moving the excitations around 
each other. Measurements can be performed by joining excitations in pairs and observing the 
result of fusion. Such computation is fault-tolerant by its physical nature.",1997,23,2710,173,0,0,1,5,16,10,24,30,50,56
9ff69c38a1b618f5806d08cfba9a4a0f8c5d5fb4,"To understand the economic value of computers, one must broaden the traditional definition of both the technology and its effects. Case studies and firm-level econometric evidence suggest that: 1) organizational ""investments"" have a large influence on the value of IT investments; and 2) the benefits of IT investment are often intangible and disproportionately difficult to measure. Our analysis suggests that the link between IT and increased productivity emerged well before the recent surge in the aggregate productivity statistics and that the current macroeconomic productivity revival may in part reflect the contributions of intangible capital accumulated in the past.",2000,161,2953,152,18,78,114,120,118,117,136,154,146,131
59b447f58246fbbdffd5e896f83a3a142eca5cf1,"We show that a set of gates that consists of all one-bit quantum gates (U(2)) and the two-bit exclusive-or gate (that maps Boolean values (x,y) to (x,x ⊕y)) is universal in the sense that all unitary operations on arbitrarily many bits n (U(2 n )) can be expressed as compositions of these gates. We investigate the number of the above gates required to implement other gates, such as generalized Deutsch-Toffoli gates, that apply a specific U(2) transformation to one input bit if and only if the logical AND of all remaining input bits is satisfied. These gates play a central role in many proposed constructions of quantum computational networks. We derive upper and lower bounds on the exact number of elementary gates required to build up a variety of two- and three-bit quantum gates, the asymptotic number required for n-bit Deutsch-Toffoli gates, and make some observations about the number required for arbitrary n-bit unitary operations.",1995,78,2919,168,11,41,27,62,57,84,79,109,122,102
48e992a734ef6ecbc9d5aeb3fc9135bbee531e07,"Preface 1. Introduction to competitive analysis: the list accessing problem 2. Introduction to randomized algorithms: the list accessing problem 3. Paging: deterministic algorithms 4. Paging: randomized algorithms 5. Alternative models for paging: beyond pure competitive analysis 6. Game theoretic foundations 7. Request - answer games 8. Competitive analysis and zero-sum games 9. Metrical task systems 10. The k-server problem 11. Randomized k-server algorithms 12. Load-balancing 13. Call admission and circuit-routing 14. Search, trading and portfolio selection 15. Competitive analysis and decision making under uncertainty Appendices Bibliography Index.",1998,0,2505,167,12,34,58,70,87,94,119,110,121,101
12f852849fe813eddc17208c30cf97000736da87,"From the Publisher: 
Michael Sipser's philosophy in writing this book is simple: make the subject interesting and relevant, and the students will learn. His emphasis on unifying computer science theory - rather than offering a collection of low-level details - sets the book apart, as do his intuitive explanations. Throughout the book, Sipser - a noted authority on the theory of computation - builds students' knowledge of conceptual tools used in computer science, the aesthetic sense they need to create elegant systems, and the ability to think through problems on their own. INTRODUCTION TO THE THEORY OF COMPUTATION provides a mathematical treatment of computation theory grounded in theorems and proofs. Proofs are presented with a ""proof idea"" component to reveal the concepts underpinning the formalism. Algorithms are presented using prose instead of pseudocode to focus attention on the algorithms themselves, rather than on specific computational models. Topic coverage, terminology, and order of presentation are traditional for an upper-level course in computer science theory. Users of the Preliminary Edition (now out of print) will be interested to note several new chapters on complexity theory: Chapter 8 on space complexity; Chapter 9 on provable intractability, and Chapter 10 on advanced topics, including approximation algorithms, alternation, interactive proof systems, cryptography, and parallel computing.",1996,4,2233,156,3,3,7,11,34,21,33,45,59,48
4b6fb8bbff6cc0cd84f8cc36625c8b6f47874135,"List of Figures. List of Tables. Preface. Contributing Authors. Series Foreword. Part I: Foundations. 1. An Introduction to Evolutionary Algorithms J.A. Lozano. 2. An Introduction to Probabilistic Graphical Models P. Larranaga. 3. A Review on Estimation of Distribution Algorithms P. Larranaga. 4. Benefits of Data Clustering in Multimodal Function Optimization via EDAs J.M. Pena, et al. 5. Parallel Estimation of Distribution Algorithms J.A. Lozano, et al. 6. Mathematical Modeling of Discrete Estimation of Distribution Algorithms C. Gonzalez, et al. Part II: Optimization. 7. An Empiricial Comparison of Discrete Estimation of Distribution Algorithms R. Blanco., J.A. Lozano. 8. Results in Function Optimization with EDAs in Continuous Domain E. Bengoetxea, et al. 9. Solving the 0-1 Knapsack Problem with EDAs R. Sagarna, P. Larranaga. 10. Solving the Traveling Salesman Problem with EDAs V. Robles, et al. 11. EDAs Applied to the Job Shop Scheduling Problem J.A. Lozano, A. Mendiburu. 12. Solving Graph Matching with EDAs Using a Permutation-Based Representation E. Bengoetxea, et al. Part III: Machine Learning. 13. Feature Subset Selection by Estimation of Distribution Algorithms I. Inza, et al. 14. Feature Weighting for Nearest Neighbor by EDAs I. Inza, et al. 15. Rule Induction by Estimation of Distribution Algorithms B. Sierra, et al. 16. Partial Abductive Inference in Bayesian Networks: An Empirical Comparison Between GAs and EDAs L.M. de Campos, et al.17. Comparing K-Means, GAs and EDAs in Partitional Clustering J. Roure, et al. 18. Adjusting Weights in Artificial Neural Networks using Evolutionary Algorithms C. Cotta, et al. Index.",2001,32,2083,197,6,25,43,60,85,105,112,153,151,145
f54ba433ce93b894a4a30c994ae5602898448c9c,"Mathematical Background Topics from Linear Algebra Single Objective Linear Programming Determining all Alternative Optima Comments about Objective Row Parametric Programming Utility Functions, Nondominated Criterion Vectors and Efficient Points Point Estimate Weighted-sums Approach Optimal Weighting Vectors, Scaling and Reduced Feasible Region Methods Vector-Maximum Algorithms Goal Programming Filtering and Set Discretization Multiple Objective Linear Fractional Programming Interactive Procedures Interactive Weighted Tchebycheff Procedure Tchebycheff/Weighted-Sums Implementation Applications Future Directions Index.",1989,0,2815,59,39,30,42,51,55,49,50,53,81,59
889de2dc91109b767e71a9b3cffa6e0a22f8a79c,"Every function of <italic>n</italic> inputs can be efficiently computed by a complete network of <italic>n</italic> processors in such a way that:<list><item>If no faults occur, no set of size <italic>t</italic> < <italic>n</italic>/2 of players gets any additional information (other than the function value),
</item><item>Even if Byzantine faults are allowed, no set of size <italic>t</italic> < <italic>n</italic>/3 can either disrupt the computation or get additional information.
</item></list>
Furthermore, the above bounds on <italic>t</italic> are tight!",1988,18,2330,155,8,15,20,18,12,12,21,15,27,25
3371d7f50540dc8e157df672c71bd0317047c3a2,"A thorough and elegant treatment of the theory of matrix functions and numerical methods for computing them, including an overview of applications, new and unpublished research results, and improved algorithms. Key features include a detailed treatment of the matrix sign function and matrix roots; a development of the theory of conditioning and properties of the Frechet derivative; Schur decomposition; block Parlett recurrence; a thorough analysis of the accuracy, stability, and computational cost of numerical methods; general results on convergence and stability of matrix iterations; and a chapter devoted to the f(A)b problem. Ideal for advanced courses and for self-study, its broad content, references and appendix also make this book a convenient general reference. Contains an extensive collection of problems with solutions and MATLAB implementations of key algorithms.",2008,22,1764,175,23,37,62,99,82,132,160,145,165,169
77d202f90a7af3b0a42c548f614eb03819cce5b3,"The density functional theory (DFT) computation of electronic structure, total energy and other properties of materials, is a field in constant progress. In order to stay at the forefront of knowledge, a DFT software project can benefit enormously from widespread collaboration, if handled properly. Also, modern software engineering concepts can considerably ease its development. The ABINIT project relies upon these ideas: freedom of sources, reliability, portability, and self-documentation are emphasised, in the development of a sophisticated plane-wave pseudopotential code. We describe ABINITv3.0, distributed under the GNU General Public License. The list of ABINITv3.0 capabilities is presented, as well as the different software techniques that have been used until now: PERL scripts and CPP directives treat a unique set of FORTRAN90 source files to generate sequential (or parallel) object code for many different platforms; more than 200 automated tests secure existing capabilities; strict coding rules are followed; the documentation is extensive, including online help files, tutorials, and HTML-formatted sources. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,57,2254,32,5,35,55,96,136,137,189,177,150,140
a146d6514cb7db6eb511047abbc6983b04a077bb,"Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a “high-input regime” in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50–100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10–50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons—that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources.",1998,161,2094,136,6,71,56,71,63,95,80,80,73,93
bc271b77fca08772b916c59ee4d9ecdf5380d4c6,"In information processing, as in physics, our classical world view provides an incomplete approximation to an underlying quantum reality. Quantum effects like interference and entanglement play no direct role in conventional information processing, but they can—in principle now, but probably eventually in practice—be harnessed to break codes, create unbreakable codes, and speed up otherwise intractable computations.",2000,203,2458,30,36,67,93,115,115,101,133,141,126,144
37d15f157f8e8a80a5ae0b23d28e6b7b0a8b8b26,"From the Publisher: 
Many scientists and engineers now use the paradigms of evolutionary computation (genetic agorithms, evolution strategies, evolutionary programming, genetic programming, classifier systems, and combinations or hybrids thereof) to tackle problems that are either intractable or unrealistically time consuming to solve through traditional computational strategies. Recently there have been vigorous initiatives to promote cross-fertilization between the EC paradigms, and also to combine these paradigms with other approaches such as neural networks to create hybrid systems with enhanced capabilities. To address the need for speedy dissemination of new ideas in these fields, and also to assist in cross-disciplinary communications and understanding, Oxford University Press and the Institute of Physics have joined forces to create a major reference publication devoted to EC fundamentals, models, algorithms and applications. This work is intended to become the standard reference resource for the evolutionary computation community. The Handbook of Evolutionary Computation will be available in loose-leaf print form, as well as in an electronic version that combines both CD-ROM and on-line (World Wide Web) acess to its contents. Regularly published supplements will be available on a subscription basis.",1997,0,2053,99,13,19,39,64,51,57,104,90,117,113
57dc98cfb48247b400cc8decb93380e022864905,,1994,0,2472,45,121,134,190,166,144,148,115,106,113,104
2bbf413f36f366fa73da4dc028a32131b5d205d6,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this �eld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.",2005,398,1911,121,16,67,147,188,202,206,228,165,158,136
ea31017b8dcbd399368fd1c2c27c2dce114e7311,"Neuronal gamma-band synchronization is found in many cortical areas, is induced by different stimuli or tasks, and is related to several cognitive capacities. Thus, it appears as if many different gamma-band synchronization phenomena subserve many different functions. I argue that gamma-band synchronization is a fundamental process that subserves an elemental operation of cortical computation. Cortical computation unfolds in the interplay between neuronal dynamics and structural neuronal connectivity. A core motif of neuronal connectivity is convergence, which brings about both selectivity and invariance of neuronal responses. However, those core functions can be achieved simultaneously only if converging neuronal inputs are functionally segmented and if only one segment is selected at a time. This segmentation and selection can be elegantly achieved if structural connectivity interacts with neuronal synchronization. I propose that this process is at least one of the fundamental functions of gamma-band synchronization, which then subserves numerous higher cognitive functions.",2009,97,1444,69,21,55,94,118,129,115,154,152,131,123
a2b5abb2e8ef4935c5e651fde5e3f3f007c5d9f9,"From the Publisher: 
In this revised and significantly expanded second edition, distinguished scientist David B. Fogel presents the latest advances in both the theory and practice of evolutionary computation to help you keep pace with developments in this fast-changing field.. ""In-depth and updated, Evolutionary Computation shows you how to use simulated evolution to achieve machine intelligence. You will gain current insights into the history of evolutionary computation and the newest theories shaping research. Fogel carefully reviews the ""no free lunch theorem"" and discusses new theoretical findings that challenge some of the mathematical foundations of simulated evolution. This second edition also presents the latest game-playing techniques that combine evolutionary algorithms with neural networks, including their success in playing competitive checkers. Chapter by chapter, this comprehensive book highlights the relationship between learning and intelligence.. ""Evolutionary Computation features an unparalleled integration of history with state-of-the-art theory and practice for engineers, professors, and graduate students of evolutionary computation and computer science who need to keep up-to-date in this developing field.",1995,0,2114,72,21,36,68,72,71,63,92,128,87,93
692dceed6973b708ff6b2032da9a1f35963aa634,A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.,1992,5,2096,72,4,8,20,24,32,33,50,48,48,44
1a9c2fa2d12609af7a489648fe68a416075cac00,"The method of characteristics used for numerical computation of solutions of fluid dynamical equations is characterized by a large degree of non standardness and therefore is not suitable for automatic computation on electronic computing machines, especially for problems with a large number of shock waves and contact discontinuities. In 1950 v. Neumann and Richtmyer proposed to use, for the solution of fluid dynamics equations, difference equations into which viscosity was introduced artificially; this has the effect of smearing out the shock wave over several mesh points. Then, it was proposed to proceed with the computations across the shock waves in the ordinary manner. In 1954, Lax published the ""triangle'' scheme suitable for computation across the shock"" waves. A deficiency of this scheme is that it does not allow computation with arbitrarily fine time steps (as compared with the space steps divided by the sound speed) because it then transforms any initial data into linear functions. In addition, this scheme smears out contact discontinuities. The purpose of this paper is to choose a scheme which is in some sense best and which still allows computation across the shock waves. This choice is made for linear equations and then by analogy the scheme is applied to the general equations of fluid dynamics. Following this scheme we carried out a large number of computations on Soviet electronic computers. For a check, some of these computations were compared with the computations carried out by the method of characteristics. The agreement of results was fully satisfactory.",1959,2,2761,73,0,0,0,3,0,1,2,1,1,3
6bc70aec3415944b461bef65b8444cd84d11667c,"A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5.",1993,49,1815,129,22,70,65,109,100,91,91,80,75,54
7f2210ff39ef9669f2a84db611c80c4b28f9fffc,"Abstract The λ-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with λ-terms. However, if one goes further and uses βη-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values ) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations , that provide a correct basis for proving equivalence of programs for a wide range of notions of computation .",1991,61,1839,166,8,14,32,21,28,36,52,57,50,51
26b7c4232872cc2327029b5354a41fde703f8e02,"After a brief introduction to the principles and promise of quantum information processing, the requirements for the physical implementation of quantum computation are discussed. These five requirements, plus two relating to the communication of quantum information, are extensively ex- plored and related to the many schemes in atomic physics, quantum optics, nuclear and electron magnetic resonance spectroscopy, superconducting electronics, and quantum-dot physics, for achiev- ing quantum computing. I. INTRODUCTION � The advent of quantum information processing, as an abstract concept, has given birth to a great deal of new thinking, of a very concrete form, about how to create physical computing devices that operate in the hitherto unexplored quantum mechanical regime. The efforts now underway to produce working laboratory devices that perform this profoundly new form of information pro- cessing are the subject of this book. In this chapter I provide an overview of the common objectives of the investigations reported in the remain- der of this special issue. The scope of the approaches, proposed and underway, to the implementation of quan- tum hardware is remarkable, emerging from specialties in atomic physics (1), in quantum optics (2), in nuclear (3) and electron (4) magnetic resonance spectroscopy, in su- perconducting device physics (5), in electron physics (6), and in mesoscopic and quantum dot research (7). This amazing variety of approaches has arisen because, as we will see, the principles of quantum computing are posed using the most fundamental ideas of quantum mechanics, ones whose embodiment can be contemplated in virtually every branch of quantum physics. The interdisciplinary spirit which has been fostered as a result is one of the most pleasant and remarkable fea- tures of this field. The excitement and freshness that has been produced bodes well for the prospect for discovery, invention, and innovation in this endeavor.",2000,60,1844,70,15,20,28,30,63,59,77,45,61,84
82b0507c2d6fddb1651b255209365cf1ce406ba2,"Preface to the English Edition. Preface to the German Edition. Real Interval Arithmetic. Further Concepts and Properties. Interval Evaluation and Range of Real Functions. Machine Interval Arithmetic. Complex Interval Arithmetic. Metric, Absolute, Value, and Width in. Inclusion of Zeros of a Function of One Real Variable. Methods for the Simultaneous Inclusion of Real Zeros of Polynomials. Methods for the Simultaneous Inclusion of Complex Zeros of Polynomials. Interval Matrix Operations. Fixed Point Iteration for Nonlinear Systems of Equations. Systems of Linear Equations Amenable to Interation. Optimality of the Symmetric Single Step Method with Taking Intersection after Every Component. On the Feasibility of the Gaussian Algorithm for Systems of Equations with Intervals as Coefficients. Hansen's Method. The Procedure of Kupermann and Hansen. Ireation Methods for the Inclusion of the Inverse Matrix and for Triangular Decompositions. Newton-like Methods for Nonlinear Systems of Equations. Newton-like Methods without Matrix Inversions. Newton-like Methods for Particular Systems of Nonlinear Equations. Newton-like Total step and Single Step Methods. Appendix A. The Order of Convergence of Iteration Methods in vn(Ic) and Mmn(iC) ). Appendix B. Realizations of Machine Interval Arithmetics in ALGOL 60. Appendix C. ALGOL Procedures. Bibliography. Index of Notation. Subject Index.",1983,1,1959,176,0,3,15,8,18,24,17,28,41,26
3500d8d44c28c316970d857b665fb42b100049c7,"To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ""Bayes' optimal"". This leads to the ""Bayesian coding hypothesis"": that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.",2004,67,1815,67,1,12,15,40,47,56,61,62,103,112
b65e7a34b0bab3f293eb26985a87d81bbced3311,"Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.",2003,48,1499,144,1,19,30,48,97,99,120,106,102,114
49cdf16489ee008649a00037cb22041a7bcd96ec,"CRC handbook of chemistry and physics , CRC handbook of chemistry and physics , کتابخانه مرکزی دانشگاه علوم پزشکی تهران",1990,0,17368,971,0,0,0,0,0,0,0,0,0,0
47552b2aa5f1137bc46b01daa91b9175837cc380,"Mathematical Introduction Acoustic Phonons Plasmons, Optical Phonons, and Polarization Waves Magnons Fermion Fields and the Hartree-Fock Approximation Many-body Techniques and the Electron Gas Polarons and the Electron-phonon Interaction Superconductivity Bloch Functions - General Properties Brillouin Zones and Crystal Symmetry Dynamics of Electrons in a Magnetic Field: de Haas-van Alphen Effect and Cyclotron Resonance Magnetoresistance Calculation of Energy Bands and Fermi Surfaces Semiconductor Crystals I: Energy Bands, Cyclotron Resonance, and Impurity States Semiconductor Crystals II: Optical Absorption and Excitons Electrodynamics of Metals Acoustic Attenuation in Metals Theory of Alloys Correlation Functions and Neutron Diffraction by Crystals Recoilless Emission Green's Functions - Application to Solid State Physics Appendix: Perturbation Theory and the Electron Gas Index.",1954,0,24309,953,0,0,0,0,0,0,0,0,0,0
958b8af460128a4c79e14f39329ca89e2e1294ad,1 The Atmosphere. 2 Atmospheric Trace Constituents. 3 Chemical Kinetics. 4 Atmospheric Radiation and Photochemistry. 5 Chemistry of the Stratosphere. 6 Chemistry of the Troposphere. 7 Chemistry of the Atmospheric Aqueous Phase. 8 Properties of the Atmospheric Aerosol. 9 Dynamics of Single Aerosol Particles. 10 Thermodynamics of Aerosols. 11 Nucleation. 12 Mass Transfer Aspects of Atmospheric Chemistry. 13 Dynamics of Aerosol Populations. 14 Organic Atmospheric Aerosols. 15 Interaction of Aerosols with Radiation. 16 Meteorology of the Local Scale. 17 Cloud Physics. 18 Atmospheric Diffusion. 19 Dry Deposition. 20 Wet Deposition. 21 General Circulation of the Atmosphere. 22 Global Cycles: Sulfur and Carbon. 23 Climate and Chemical Composition of the Atmosphere. 24 Aerosols and Climate. 25 Atmospheric Chemical Transport Models. 26 Statistical Models.,1997,0,12093,1517,0,0,0,0,0,0,0,0,1,1
a657fb77b5515d1968c331ebbda8919d5786d8be,"This biennial Review summarizes much of particle physics. Using data from previous editions., plus 2778 new measurements from 645 papers, we list, evaluate, and average measured properties of gauge bosons, leptons, quarks, mesons, and baryons. We also summarize searches for hypothetical particles such as Higgs bosons, heavy neutrinos, and supersymmetric particles. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as the Standard Model, particle detectors., probability, and statistics. Among the 108 reviews are many that are new or heavily revised including those on CKM quark-mixing matrix, V-ud & V-us, V-cb & V-ub, top quark, muon anomalous magnetic moment, extra dimensions, particle detectors, cosmic background radiation, dark matter, cosmological parameters, and big bang cosmology.",1996,761,13036,1066,2,0,1,0,0,1,1,3,3,20
e1d1e7dc2606b1d60cb85057ab7c5bbd52067661,,1979,0,14304,364,1,0,0,0,0,0,0,0,0,0
bf97653c065591cc23324b1ff5ddac020be67b0b,,1979,0,10408,438,0,0,0,0,0,0,42,123,123,177
34d19c80d05ef3ecb2c15d11663adb50f378710f,"The Monte Carlo method is a computer simulation method which uses random numbers to simulate statistical fluctuations. The method is used to model complex systems with many degrees of freedom. Probability distributions for these systems are generated numerically and the method then yields numerically exact information on the models. Such simulations may be used to see how well a model system approximates a real one or to see how valid the assumptions are in an analytical theory. A short and systematic theoretical introduction to the method forms the first part of this book. The second part is a practical guide with plenty of examples and exercises for the student. Problems treated by simple sampling (random and self-avoiding walks, percolation clusters, etc.) and by importance sampling (Ising models etc.) are included, along with such topics as finite-size effects and guidelines for the analysis of Monte Carlo simulations. The two parts together provide an excellent introduction to the theory and practice of Monte Carlo simulations.",1992,0,897,36,10,16,13,14,16,11,16,16,23,20
8d8e334076d16ba6cb157ae72f751276c1c1c78c,,1992,0,9911,258,76,100,123,380,418,368,299,257,320,280
af15b85cebb0a803f78f191a0fff1ff0ef3266c4,"Meeting the Universe Halfway is an ambitious book with far-reaching implications for numerous fields in the natural sciences, social sciences, and humanities. In this volume, Karen Barad, theoretical physicist and feminist theorist, elaborates her theory of agential realism. Offering an account of the world as a whole rather than as composed of separate natural and social realms, agential realism is at once a new epistemology, ontology, and ethics. The starting point for Barad’s analysis is the philosophical framework of quantum physicist Niels Bohr. Barad extends and partially revises Bohr’s philosophical views in light of current scholarship in physics, science studies, and the philosophy of science as well as feminist, poststructuralist, and other critical social theories. In the process, she significantly reworks understandings of space, time, matter, causality, agency, subjectivity, and objectivity.

In an agential realist account, the world is made of entanglements of “social” and “natural” agencies, where the distinction between the two emerges out of specific intra-actions. Intra-activity is an inexhaustible dynamism that configures and reconfigures relations of space-time-matter. In explaining intra-activity, Barad reveals questions about how nature and culture interact and change over time to be fundamentally misguided. And she reframes understanding of the nature of scientific and political practices and their “interrelationship.” Thus she pays particular attention to the responsible practice of science, and she emphasizes changes in the understanding of political practices, critically reworking Judith Butler’s influential theory of performativity. Finally, Barad uses agential realism to produce a new interpretation of quantum physics, demonstrating that agential realism is more than a means of reflecting on science; it can be used to actually do science.",2007,75,4103,345,10,12,38,62,87,102,170,217,250,367
60da513d2995b2710ad9fde8d263d96a128ffb99,"The PYTHIA program can be used to generate high-energy-physics `events', i.e. sets of outgoing particles produced in the interactions between two incoming particles. The objective is to provide as accurate as possible a representation of event properties in a wide range of reactions, with emphasis on those where strong interactions play a role, directly or indirectly, and therefore multihadronic final states are produced. The physics is then not understood well enough to give an exact description; instead the program has to be based on a combination of analytical results and various QCD-based models. This physics input is summarized here, for areas such as hard subprocesses, initial- and final-state parton showers, beam remnants and underlying events, fragmentation and decays, and much more. Furthermore, extensive information is provided on all program elements: subroutines and functions, switches and parameters, and particle and process data. This should allow the user to tailor the generation task to the topics of interest. 
The information in this edition of the manual refers to PYTHIA version 6.200, of 31 August 2001. 
The official reference to the latest published version is T. Sj\""ostrand, P. Ed\'en, C. Friberg, L. L\""onnblad, G. Miu, S. Mrenna and E. Norrbin, Computer Physics Commun. 135 (2001) 238.",2001,433,7441,288,6,14,20,38,53,67,137,199,256,324
539f4cdfd83a6e3487c4134509c9eb687c145dc0,,1972,0,7697,428,1,8,7,9,11,40,48,57,86,84
a844f4ce5d00e8f62ba3a756c7112ed207e3b605,"N G van Kampen 1981 Amsterdam: North-Holland xiv + 419 pp price Dfl 180 This is a book which, at a lower price, could be expected to become an essential part of the library of every physical scientist concerned with problems involving fluctuations and stochastic processes, as well as those who just enjoy a beautifully written book. It provides an extensive graduate-level introduction which is clear, cautious, interesting and readable.",1983,0,4260,674,2,0,4,3,4,6,4,5,4,6
0095c84f0a1a39cbc56930aab7839de0aab37505,The transformation of snow to ice mass balance heat budget and climatology structure and deformation of ice hydraulics and glaciers glacier sliding deformation of subglacial till structures and fabrics in glaciers and ice sheets distribution of temperature in glaciers and ice sheets steady flow of glaciers and ice sheets flow of ice shelves and ice streams non-steady flow of glaciers and ice sheets surging and tidewater glaciers ice core studies.,1981,0,4027,544,7,14,22,20,19,29,28,11,28,21
a442ba386113f089b64c55c423b39d99f4cee80e,"These are a set of notes I have made, based on lectures given by M.Moore at the University of Manchester Jan-June ’08. Please e-mail me with any comments/corrections: jap@watering.co.uk.",1962,0,6321,524,3,2,1,2,2,1,4,3,3,1
5366ae191ffd171dd8a5053b5356abee031ffaa4,Preface. Acknowledgments. Introduction. Hydrolysis and Condensation I: Nonsilicates. Hydrolysis and Condensation II: Silicates. Particulate Sols and Gels. Gelation. Aging of Gels. Theory of Deformation and Flow in Gels. Drying. Structural Evolution during Consolidation. Surface Chemistry and Chemical Modification. Sintering. Comparison of Gel-Derived and Conventional Ceramics. Film Formation. Applications. Index.,1990,0,7797,338,3,16,46,31,85,89,100,145,140,129
25371c278c41a7aa99a6446ab419a6925d278b76,"It has been recognized for some time that the spontaneous emission by atoms is not necessarily a fixed and immutable property of the coupling between matter and space, but that it can be controlled by modification of the properties of the radiation field. This is equally true in the solid state, where spontaneous emission plays a fundamental role in limiting the performance of semiconductor lasers, heterojunction bipolar transistors, and solar cells. If a three-dimensionally periodic dielectric structure has an electromagnetic band gap which overlaps the electronic band edge, then spontaneous emission can be rigorously forbidden.",1987,12,11487,177,0,0,0,0,0,0,0,0,0,0
5b9180506668890dff1491e5f0be295735427631,"3rd edition, complete modern revision C. Kittel London: John Wiley. 1966. Pp. 648. Price £4 14s. Kind's new edition is to be welcomed. There is a revised format and attractive illustrations, and with the inclusion of much new material this book has become one of the best sources for undergraduate teaching. It is above all an interesting book, likely to give the student a wish to dig deeper into the solid state.",1967,37,5539,323,0,5,4,4,2,8,17,8,9,5
f031e06500d6afe6e7c6593530bf84b7bad3c15c,"Part 1 Liquid crystals - main types and properties: introduction - what is a liquid crystal? the building blocks nematics and cholesterics smectics columnar phases more on long-, quasi-long and short-range order remarkable features of liquid crystals. Part 2 Long- and short-range order in nematics: definition of an order parameter statistical theories of the nematic order phenomonological description of the nematic-isotopic mixtures. Part 3 Static distortion in a nematic single crystal: principles of the continuum theory magnetic field effects electric field effects in an insulating nematic fluctuations in the alignment hydrostatics of nematics. Part 4 Defects and textures in nematics: observations disclination lines point disclinations walls under magnetic fields umbilics surface disclinations. Part 5 Dynamical properties of nematics: the equations of ""nematodynamics"" experiments measuring the Leslie co-efficients convective instabilities under electric fields molecular motions. Part 6 Cholesterics: optical properties of an ideal helix agents influencing the pitch dynamical properties textures and defects in cholesterics. Part 7 Smectics: symmetry of the main smectic phases continuum description of smectics A and C remarks on phase and precritical phenomena.",1974,0,7666,258,2,33,53,59,57,74,71,92,76,97
a0c3e1d0353f00fd9df6931af82010f2172168cf,Preface to the first edition. Preface to the second edition. Abbreviated references. I. Stochastic variables. II. Random events. III. Stochastic processes. IV. Markov processes. V. The master equation. VI. One-step processes. VII. Chemical reactions. VIII. The Fokker-Planck equation. IX. The Langevin approach. X. The expansion of the master equation. XI. The diffusion type. XII. First-passage problems. XIII. Unstable systems. XIV. Fluctuations in continuous systems. XV. The statistics of jump events. XVI. Stochastic differential equations. XVII. Stochastic behavior of quantum systems.,1981,0,6448,245,0,3,21,57,55,82,86,82,85,62
b029856e343340fc7c034f514dc15a2ad49f6c1a,"This paper reviews recent experimental and theoretical progress concerning many-body phenomena in dilute, ultracold gases. It focuses on effects beyond standard weak-coupling descriptions, such as the Mott-Hubbard transition in optical lattices, strongly interacting gases in one and two dimensions, or lowest-Landau-level physics in quasi-two-dimensional gases in fast rotation. Strong correlations in fermionic gases are discussed in optical lattices or near-Feshbach resonances in the BCS-BEC crossover.",2007,576,4518,146,33,117,220,244,322,319,323,285,304,356
529595f0bbf7d8d38354436f5ce7a3293e66bd05,"On the program it says this is a keynote speech--and I don't know what a keynote speech is. I do not intend in any way to suggest what should be in this meeting as a keynote of the subjects or anything like that. I have my own things to say and to talk about and there's no implication that anybody needs to talk about the same thing or anything like it. So what I want to talk about is what Mike Dertouzos suggested that nobody would talk about. I want to talk about the problem of simulating physics with computers and I mean that in a specific way which I am going to explain. The reason for doing this is something that I learned about from Ed Fredkin, and my entire interest in the subject has been inspired by him. It has to do with learning something about the possibilities of computers, and also something about possibilities in physics. If we suppose that we know all the physical laws perfectly, of course we don't have to pay any attention to computers. It's interesting anyway to entertain oneself with the idea that we've got something to learn about physical laws; and if I take a relaxed view here (after all I 'm here and not at home) I'll admit that we don't understand everything. The first question is, What kind of computer are we going to use to simulate physics? Computer theory has been developed to a point where it realizes that it doesn't make any difference; when you get to a universal computer, it doesn't matter how it's manufactured, how it's actually made. Therefore my question is, Can physics be simulated by a universal computer? I would like to have the elements of this computer locally interconnected, and therefore sort of think about cellular automata as an example (but I don't want to force it). But I do want something involved with the",1999,59,5495,206,65,65,90,99,87,124,118,149,151,167
c210410d292663287dd188832f17d4917ad6b03f,"Allis and Herlin Thermodynamics and Statistical Mechanics Becker Introduction to Theoretical Mechanics Clark Applied X-rays Collin Field Theory of Guided Waves Evans The Atomic Nucleus Finkelnburg Atomic Physics Ginzton Microwave Measurements Green Nuclear Physics Gurney Introduction to Statistical Mechanics Hall Introduction to Electron Microscopy Hardy and Perrin The Principles of Optics Harnwell Electricity and Electromagnetism Harnwell and Livingood Experimental Atomic Physics Harnwell and Stephens Atomic Physics Henley and Thirring Elementary Quantum Field Theory Houston Principles of Mathematical Physics Hund High-frequency Measurements Kennard Kinetic Theory of Gases Lane Superfluid Physics Leighton Principles of Modern Physics Lindsay Mechanical Radiation Livingston and Blewett Particle Accelerators Middleton An Introduction to Statistical Communication Theory Morse Vibration and Sound Morse and Feshbach Methods of Theoretical Physics Muskat Physical Principles of Oil Production Present Kinetic Theory of Gases Read Dislocations in Crystals Richtmyer, Kennard, and Lauritsen Introduction to Modern Physics Schiff Quantum Mechanics Seitz The Modern Theory of Solids Slater Introduction to Chemical Physics Slater Quantum Theory of Matter Slater Quantum Theory of Atomic Structure, Vol. I Slater Quantum Theory of Atomic Structure, Vol. II Slater Quantum Theory of Molecules and Solids, Vol. 1 Slater and Frank Electromagnetism Slater and Frank Introduction to Theoretical Physics Slater and Frank Mechanics Smythe Static and Dynamic Electricity Stratton Electromagnetic Theory Thorndike Mesons: A Summary of Experimental Facts Townes and Schawlow Microwave Spectroscopy White Introduction to Atomic Spectra",1955,11,9463,155,7,19,22,33,33,48,61,49,55,82
63e9a5df23631a9584ba9a7745f9b12e85cf4f95,"Statistical Physics. By F. Mandl. Pp. xiii + 379. (Wiley: London and New York, July 1971.) £2.75. Statistical Physics. By A. Isihara. Pp. xv + 439. (Academic: New York and London, June 1971.) $18.50; £8.65.",1971,4,3702,288,3,1,1,0,4,1,3,2,4,3
d4686dec40085e3f5415c21b162ec06efdece47c,,1973,0,32448,17,0,0,0,0,0,0,0,0,0,0
06b8f0e6e9918fad9c381ac98a13f134febfdb2e,"Preface. Introduction. PART I: SEMICONDUCTOR PHYSICS. Energy Bands and Carrier Concentration in Thermal Equilibrium. Carrier Transport Phenomena. PART II: SEMICONDUCTOR DEVICES. p-n Junction. Bipolar Transistor and Related Devices. MOSFET and Related Devices. MESFET and Related Devices. Microwave Diodes, Quantum-Effect, and Hot-Electron Devices. Photonic Devices. PART III: SEMICONDUCTOR TECHNOLOGY. Crystal Growth and Epitaxy. Film Formation. Lithography and Etching. Impurity Doping. Integrated Devices. Appendix A: List of Symbols. Appendix B: International Systems of Units (SI Units). Appendix C: Unit Prefixes. Appendix D: Greek Alphabet. Appendix E: Physical Constants. Appendix F: Properties of Important Element and Binary Compound Semiconductors at 300 K. Appendix G: Properties of Si and GaAs at 300 K. Appendix H: Derivation of the Density of States in Semiconductor. Appendix I: Derivation of Recombination Rate for Indirect Recombination. Appendix J: Calculation of the Transmission Coefficient for a Symmetric Resonant-Tunneling Diode. Appendix K: Basic Kinetic Theory of Gases. Appendix L: Answers to Selected Problems. Index.",1985,0,3384,222,0,0,9,5,17,25,27,35,38,37
594ee853ff36c1d26d1d2e067d53a97531dc5dea,,1973,0,6569,202,57,60,63,69,71,75,75,89,78,99
8de008afdc3535d09dbbb33e52976a4277ba51d7,"Contents: General results and concepts on invariant sets and attractors.- Elements of functional analysis.- Attractors of the dissipative evolution equation of the first order in time: reaction-diffusion equations.- Fluid mechanics and pattern formation equations.- Attractors of dissipative wave equations.- Lyapunov exponents and dimensions of attractors.- Explicit bounds on the number of degrees of freedom and the dimension of attractors of some physical systems.- Non-well-posed problems, unstable manifolds. lyapunov functions, and lower bounds on dimensions.- The cone and squeezing properties.- Inertial manifolds.- New chapters: Inertial manifolds and slow manifolds the nonselfadjoint case.",1993,4,3674,229,38,46,69,70,73,96,86,79,109,113
fc00cf7afd7b3fc4a6fad4c4ba8bc20b64ae5c07,"Nanocrystals are fundamental to modern science and technology. Mastery over the shape of a nanocrystal enables control of its properties and enhancement of its usefulness for a given application. Our aim is to present a comprehensive review of current research activities that center on the shape-controlled synthesis of metal nanocrystals. We begin with a brief introduction to nucleation and growth within the context of metal nanocrystal synthesis, followed by a discussion of the possible shapes that a metal nanocrystal might take under different conditions. We then focus on a variety of experimental parameters that have been explored to manipulate the nucleation and growth of metal nanocrystals in solution-phase syntheses in an effort to generate specific shapes. We then elaborate on these approaches by selecting examples in which there is already reasonable understanding for the observed shape control or at least the protocols have proven to be reproducible and controllable. Finally, we highlight a number of applications that have been enabled and/or enhanced by the shape-controlled synthesis of metal nanocrystals. We conclude this article with personal perspectives on the directions toward which future research in this field might take.",2009,564,4355,34,38,139,265,377,437,455,469,444,400,361
16ef4cc3a80ee7ba8f59e0a55b2ef134c31e18b3,"The representation of physics problems in relation to the organization of physics knowledge is investigated in experts and novices. Four experiments examine (a) the existence of problem categories as a basis for representation; (b) differences in the categories used by experts and novices; (c) differences in the knowledge associated with the categories; and (d) features in the problems that contribute to problem categorization and representation. Results from sorting tasks and protocols reveal that experts and novices begin their problem representations with specifiably different problem categories, and completion of the representations depends on the knowledge associated with the categories. For, the experts initially abstract physics principles to approach and solve a problem representation, whereas novices base their representation and approaches on the problem's literal features.",1981,26,5021,129,1,17,24,30,34,70,81,62,83,96
972528bb6930bbb2b9936e79d68383c6886c5aae,"1. Introduction.- 1.1 What Is the Subject of Gas Discharge Physics.- 1.2 Typical Discharges in a Constant Electric Field.- 1.3 Classification of Discharges.- 1.4 Brief History of Electric Discharge Research.- 1.5 Organization of the Book. Bibliography.- 2. Drift, Energy and Diffusion of Charged Particles in Constant Fields.- 2.1 Drift of Electrons in a Weakly Ionized Gas.- 2.2 Conduction of Ionized Gas.- 2.3 Electron Energy.- 2.4 Diffusion of Electrons.- 2.5 Ions.- 2.6 Ambipolar Diffusion.- 2.7 Electric Current in Plasma in the Presence of Longitudinal Gradients of Charge Density.- 2.8 Hydrodynamic Description of Electrons.- 3. Interaction of Electrons in an Ionized Gas with Oscillating Electric Field and Electromagnetic Waves.- 3.1 The Motion of Electrons in Oscillating Fields.- 3.2 Electron Energy.- 3.3 Basic Equations of Electrodynamics of Continuous Media.- 3.4 High-Frequency Conductivity and Dielectric Permittivity of Plasma.- 3.5 Propagation of Electromagnetic, Waves in Plasmas.- 3.6 Total Reflection of Electromagnetic Waves from Plasma and Plasma Oscillations.- 4. Production and Decay of Charged Particles.- 4.1 Electron Impact Ionization in a Constant Field.- 4.2 Other Ionization Mechanisms.- 4.3 Bulk Recombination.- 4.4 Formation and Decay of Negative Ions.- 4.5 Diffusional Loss of Charges.- 4.6 Electron Emission from Solids.- 4.7 Multiplication of Charges in a Gas via Secondary Emission.- 5. Kinetic Equation for Electrons in a Weakly Ionized Gas Placed in an Electric Field.- 5.1 Description of Electron Processes in Terms of the Velocity Distribution Function.- 5.2 Formulation of the Kinetic Equation.- 5.3 Approximation for the Angular Dependence of the Distribution Function.- 5.4 Equation of the Electron Energy Spectrum.- 5.5 Validity Criteria for the Spectrum Equation.- 5.6 Comparison of Some Conclusions Implied by the Kinetic Equation with the Result of Elementary Theory.- 5.7 Stationary Spectrum of Electrons in a Field in the Case of only Elastic Losses.- 5.8 Numerical Results for Nitrogen and Air.- 5.9 Spatially Nonuniform Fields of Arbitrary Strength.- 6. Electric Probes.- 6.1 Introduction. Electric Circuit.- 6.2 Current-Voltage Characteristic of a Single Probe.- 6.3 Theoretical Foundations of Electronic Current Diagnostics of Rarefied Plasmas.- 6.4 Procedure for Measuring the Distribution Function.- 6.5 Ionic Current to a Probe in Rarefied Plasma.- 6.6 Vacuum Diode Current and Space-Charge Layer Close to a Charged Body.- 6.7 Double Probe.- 6.8 Probe in a High-Pressure Plasma.- 7. Breakdown of Gases in Fields of Various Frequency Ranges.- 7.1 Essential Characteristics of the Phenomenon.- 7.2 Breakdown and Triggering of Self-Sustained Discharge in a Constant Homogeneous Field at Moderately Large Product of Pressure and Discharge Gap Width.- 7.3 Breakdown in Microwave Fields and Interpretation of Experimental Data Using the Elementary Theory.- 7.4 Calculation of Ionization Frequencies and Breakdown Thresholds Using the Kinetic Equation.- 7.5 Optical Breakdown.- 7.6 Methods of Exciting an RF Field in a Discharge Volume.- 7.7 Breakdown in RF and Low-Frequency Ranges.- 8. Stable Glow Discharge.- 8.1 General Structure and Observable Features.- 8.2 Current-Voltage Characteristic of Discharge Between Electrodes.- 8.3 Dark Discharge and the Role Played by Space Charge in the Formation of the Cathode Layer.- 8.4 Cathode Layer.- 8.5 Transition Region Between the Cathode Layer and the Homogeneous Positive Column.- 8.6 Positive Column.- 8.7 Heating of the Gas and Its Effect on the Current-Voltage Characteristic.- 8.8 Electronegative Gas Plasma.- 8.9 Discharge in Fast Gas Flow.- 8.10 Anode Layer.- 9. Glow Discharge Instabilities and Their Consequences.- 9.1 Causes and Consequences of Instabilities.- 9.2 Quasisteady Parameters.- 9.3 Field and Electron Temperature Perturbations in the Case of Quasisteady-State Te.- 9.4 Thermal Instability.- 9.5 Attachment Instability.- 9.6 Some Other Frequently Encountered Destabilizing Mechanisms.- 9.7 Striations.- 9.8 Contraction of the Positive Column.- 10. Arc Discharge.- 10.1 Definition and Characteristic Features of Arc Discharge.- 10.2 Arc Types.- 10.3 Arc Initiation.- 10.4 Carbon Arc in Free Air.- 10.5 Hot Cathode Arc: Processes near the Cathode.- 10.6 Cathode Spots and Vacuum Arc.- 10.7 Anode Region.- 10.8 Low-Pressure Arc with Externally Heated Cathode.- 10.9 Positive Column of High-Pressure Arc (Experimental Data).- 10.10 Plasma Temperature and V - i Characteristic of High-Pressure Arc Columns.- 10.11 The Gap Between Electron and Gas Temperatures in ""Equilibrium"" Plasma.- 11. Suslainment and Production of Equilibrium Plasma by Fields in Various Frequency Ranges.- 11.1 Introduction. Energy Balance in Plasma.- 11.2 Arc Column in a Constant Field.- 11.3 Inductively Coupled Radio-Frequency Discharge.- 11.4 Discharge in Microwave Fields.- 11.5 Continuous Optical Discharges.- 11.6 Plasmatrons: Generators of Dense Low-Temperature Plasma.- 12. Spark and Corona Discharges.- 12.1 General Concepts.- 12.2 Individual Electron Avalanche.- 12.3 Concept of Streamers.- 12.4 Breakdown and Streamers in Electronegative Gases (Air) in Moderately Wide Gaps with a Uniform Field.- 12.5 Spark Channel.- 12.6 Corona Discharge.- 12.7 Models of Streamer Propagation.- 12.8 Breakdown in Long Air Gaps with Strongly Nonuniform Fields (Experimental Data).- 12.9 Leader Mechanism of Breakdown of Long Gaps.- 12.10 Return Wave (Return Stroke).- 12.11 Lightning.- 12.12 Negative Stepped Leader.- 13. Capacitively Coupled Radio-Frequency Discharge.- 13.1 Drift Oscillations of Electron Gas.- 13.2 Idealized Model of the Passage of High-Frequency Current Through a Long Plane Gap at Elevated Pressures.- 13.3 V - i Characteristic of Homogeneous Positive Columns.- 13.4 Two Forms of CCRF Discharge Realization and Constant Positive Potential of Space: Experiment.- 13.5 Electrical Processes in a Nonconducting Electrode Layer and the Mechanism of Closing the Circuit Current.- 13.6 Constant Positive Potential of the Weak-Current Discharge Plasma.- 13.7 High-Current Mode.- 13.8 The Structure of a Medium-Pressure Discharge: Results of Numerical Modeling.- 13.9 Normal Current Density in Weak-Current Mode and Limits on the Existence of this Mode.- 14. Discharges in High-Power CW CO2 Lasers.- 14.1 Principles of Operation of Electric-Discharge CO2 Lasers.- 14.2 Two Methods of Heat Removal from Lasers.- 14.3 Methods of Suppressing Instabilities.- 14.4 Organization of Large-Volume Discharges Involving Gas Pumping.- References.",1991,0,3812,117,2,1,8,7,26,35,26,49,59,87
4e72246678077558b4623954f0301bfb82b20807,"1. Introductory Material.- 1.1. Harmonic Oscillators and Phonons.- 1.2. Second Quantization for Particles.- 1.3. Electron - Phonon Interactions.- A. Interaction Hamiltonian.- B. Localized Electron.- C. Deformation Potential.- D. Piezoelectric Interaction.- E. Polar Coupling.- 1.4. Spin Hamiltonians.- A. Homogeneous Spin Systems.- B. Impurity Spin Models.- 1.5. Photons.- A. Gauges.- B. Lagrangian.- C. Hamiltonian.- 1.6. Pair Distribution Function.- Problems.- 2. Green's Functions at Zero Temperature.- 2.1. Interaction Representation.- A. Schrodinger.- B. Heisenberg.- C. Interaction.- 2.2. S Matrix.- 2.3. Green's Functions.- 2.4. Wick's Theorem.- 2.5. Feynman Diagrams.- 2.6. Vacuum Polarization Graphs.- 2.7. Dyson's Equation.- 2.8. Rules for Constructing Diagrams.- 2.9. Time-Loop S Matrix.- A. Six Green's Functions.- B. Dyson's Equation.- 2.10. Photon Green's Functions.- Problems.- 3. Green's Functions at Finite Temperatures.- 3.1. Introduction.- 3.2. Matsubara Green's Functions.- 3.3. Retarded and Advanced Green's Functions.- 3.4. Dyson's Equation.- 3.5. Frequency Summations.- 3.6. Linked Cluster Expansions.- A. Thermodynamic Potential.- B. Green's Functions.- 3.7. Real Time Green's Functions.- Wigner Distribution Function.- 3.8. Kubo Formula for Electrical Conductivity.- A. Transverse Fields, Zero Temperature.- B. Finite Temperatures.- C. Zero Frequency.- D. Photon Self-Energy.- 3.9. Other Kubo Formulas.- A. Pauli Paramagnetic Susceptibility.- B. Thermal Currents and Onsager Relations.- C. Correlation Functions.- Problems.- 4. Exactly Solvable Models.- 4.1. Potential Scattering.- A. Reaction Matrix.- B. T Matrix.- C. Friedel's Theorem.- D. Phase Shifts.- E. Impurity Scattering.- F. Ground State Energy.- 4.2. Localized State in the Continuum.- 4.3. Independent Boson Models.- A. Solution by Canonical Transformation.- B. Feynman Disentangling of Operators.- C. Einstein Model.- D. Optical Absorption and Emission.- E. Sudden Switching.- F. Linked Cluster Expansion.- 4.4. Tomonaga Model.- A. Tomonaga Model.- B. Spin Waves.- C. Luttinger Model.- D. Single-Particle Properties.- E. Interacting System of Spinless Fermions.- F. Electron Exchange.- 4.5. Polaritons.- A. Semiclassical Discussion.- B. Phonon-Photon Coupling.- C. Exciton-Photon Coupling.- Problems.- 5. Electron Gas.- 5.1. Exchange and Correlation.- A. Kinetic Energy.- B. Direct Coulomb.- C. Exchange.- D. Seitz' Theorem.- E. ?(2a).- F. ?(2b).- G. ?(2c).- H. High-Density Limit.- I. Pair Distribution Function.- 5.2. Wigner Lattice and Metallic Hydrogen.- Metallic Hydrogen.- 5.3. Cohesive Energy of Metals.- 5.4. Linear Screening.- 5.5. Model Dielectric Functions.- A. Thomas-Fermi.- B. Lindhard, or RPA.- C. Hubbard.- D. Singwi-Sjolander.- 5.6. Properties of the Electron Gas.- A. Pair Distribution Function.- B. Screening Charge.- C. Correlation Energies.- D. Compressibility.- 5.7. Sum Rules.- 5.8. One-Electron Properties.- A. Renormalization Constant ZF.- B. Effective Mass.- C. Pauli Paramagnetic Susceptibility.- D. Mean Free Path.- Problems.- 6. Electron-Phonon Interaction.- 6.1 Frohlich Hamiltonian.- A. Brillouin-Wigner Perturbation Theory.- B. Rayleigh-Schrodinger Perturbation Theory.- C. Strong Coupling Theory.- D. Linked Cluster Theory.- 6.2 Small Polaron Theory.- A. Large Polarons.- B. Small Polarons.- C. Diagonal Transitions.- D. Nondiagonal Transitions.- E. Dispersive Phonons.- F. Einstein Model.- G. Kubo Formula.- 6.3 Heavily Doped Semiconductors.- A. Screened Interaction.- B. Experimental Verifications.- C. Electron Self-Energies.- 6.4 Metals.- A. Phonons in Metals.- B. Electron Self-Energies.- Problems.- 7. dc Conductivities.- 7.1. Electron Scattering by Impurities.- A. Boltzmann Equation.- B. Kubo Formula: Approximate Solution.- C. Kubo Formula: Rigorous Solution.- D. Ward Identities.- 7.2. Mobility of Frohlich Polarons.- A. Single-Particle Properties.- B. ??1 Term in the Mobility.- 7.3. Electron-Phonon Interactions in Metals.- A. Force-Force Correlation Function.- B. Kubo Formula.- C. Mass Enhancement.- D. Thermoelectric Power.- 7.4. Quantum Boltzmann Equation.- A. Derivation of the Quantum Boltzmann Equation.- B. Gradient Expansion.- C. Electron Scattering by Impurities.- D. T2 Contribution to the Electrical Resistivity.- Problems.- 8. Optical Properties of Solids.- 8.1. Nearly Free-Electron System.- A. General Properties.- B. Force-Force Correlation Functions.- C. Frohlich Polarons.- D. Interband Transitions.- E. Phonons.- 8.2. Wannier Excitons.- A. The Model.- B. Solution by Green's Functions.- C. Core-Level Spectra.- 8.3. X-Ray Spectra in Metals.- A. Physical Model.- B. Edge Singularities.- C. Orthogonality Catastrophe.- D. MND Theory.- E. XPS Spectra.- Problems.- 9. Superconductivity.- 9.1. Cooper Instability.- 9.2. BCS Theory.- 9.3. Electron Tunneling.- A. Tunneling Hamiltonian.- B. Normal Metals.- C. Normal-Superconductor.- D. Two Superconductors.- E. Josephson Tunneling.- 9.4. Infrared Absorption.- 9.5. Acoustic Attenuation.- 9.6. Excitons in Superconductors.- 9.7. Strong Coupling Theory.- Problems.- 10. Liquid Helium.- 10.1. Pairing Theory.- A. Hartree and Exchange.- B. Bogoliubov Theory of 4He.- 10.2. 4He: Ground State Properties.- A. Off-Diagonal Long-Range Order.- B. Correlated Basis Functions.- C. Experiments on nk.- 10.3. 4He: Excitation Spectrum.- A. Bijl-Feynman Theory.- B. Improved Excitation Spectra.- C. Superfluidity.- 10.4. 3He: Normal Liquid.- A. Fermi Liquid Theory.- B. Experiments and Microscopic Theories.- C. Interaction between Quasiparticles: Excitations.- D. Quasiparticle Transport.- 10.5. Superfluid 3He.- A. Triplet Pairing.- B. Equal Spin Pairing.- Problems.- 11. Spin Fluctuations.- 11.1. Kondo Model.- A. High-Temperature Scattering.- B. Low-Temperature State.- C. Kondo Temperature.- 11.2. Anderson Model.- A. Collective States.- B. Green's Functions.- C. Spectroscopies.- Problems.- References.- Author Index.",1981,0,5112,151,0,3,10,18,20,17,20,21,21,24
8a54948904ebbdb40aaf12b8e79aed5a3ef09ef3,,1995,0,3034,270,11,27,28,46,58,49,38,70,74,80
1e491759364db68bf0175c1de47edbd42aca8c61,,1956,0,4482,186,1,2,35,46,50,72,54,63,70,72
524954d758539d0db33cf89a2ebbb8ecf7b73235,,1969,0,15412,25,0,0,0,0,0,0,0,0,0,0
565f416e97eac888d88b367ebdaadbb6c52b405b,,1993,0,2772,254,1,5,19,18,14,16,12,14,23,41
1e44defdfe2e7b795ac2b8d2b5a28c9bfb547248,,1943,0,6431,127,0,0,1,4,6,5,17,6,12,20
5aeff09300fcda097719430e0dab6ab1ab3b0264,"An introduction to fractional calculus, P.L. Butzer & U. Westphal fractional time evolution, R. Hilfer fractional powers of infinitesimal generators of semigroups, U. Westphal fractional differences, derivatives and fractal time series, B.J. West and P. Grigolini fractional kinetics of Hamiltonian chaotic systems, G.M. Zaslavsky polymer science applications of path integration, integral equations, and fractional calculus, J.F. Douglas applications to problems in polymer physics and rheology, H. Schiessel et al applications of fractional calculus and regular variation in thermodynamics, R. Hilfer.",2000,8,4649,111,13,12,51,37,37,55,68,77,112,148
9e0c986e906869febb0f87258edab602bc51fcaf,,1964,0,4121,180,27,26,37,44,46,50,44,44,56,56
4c42f07d606f510b7409227db33bb41627bde1b2,1. Peculiarities of d=1 2. Bosonization 3. Luttinger liquids 4. Refinements 5. Microscopic methods 6. Spin 1/2 chains 7. Interacting fermions on a lattice 8. Coupled fermionic chains 9. Disordered systems 10. Boundaries and isolated impurities 11. Significant others A. Basics of many body B. Not so important fine technical points C. Correlation functions D. Bosonization directory E. Sine-Gordon F. Numerical solution,2004,0,2653,261,27,42,87,137,153,151,155,168,160,172
d1168dc15e456520d5a78d0c04fa2985654b39f4,,1971,0,3746,144,2,2,4,4,4,3,2,4,4,4
e3e2901a80d7f663fc4ff7cf89456cf79d040e6e,"This book is designed for the junior-senior thermodynamics course given in all departments as a standard part of the curriculum. The book is devoted to a discussion of some of the basic physical concepts and methods useful in the description of situations involving systems which consist of very many particulars. It attempts, in particular, to introduce the reader to the disciplines of thermodynamics, statistical mechanics, and kinetic theory from a unified and modern point of view. The presentation emphasizes the essential unity of the subject matter and develops physical insight by stressing the microscopic content of the theory.",1965,0,3323,180,0,1,4,1,5,5,2,5,6,3
fdc354d08984f0b36be818b8a287a767cad727ed,Air Pollutants Effects of Air Pollution Sources of Pollutants in Combustion Processes Gas-Phase Atmospheric Chemistry Aqueous-Phase Atmospheric Chemistry Mass Transfer Aspects of Atmospheric Chemistry Properties of Aerosols Dynamics of Single Aerosol Particles Thermodynamics of Aerosols and Nucleation Theory Dynamics of Aerosol Population Air Pollution Meteorology Micrometeorology Atmospheric Diffusion Theories The Gaussian Plume Equation The Atmospheric Diffusion Equation and Air Quality Models Atmospheric Removal Processes and Residence Times Air Pollution Statistics Acid Rain Index.,1986,0,2652,272,1,15,21,31,51,56,52,74,70,77
37491ed087839d86c9ba7783cb12ccc6c9918bf6,"Microfabricated integrated circuits revolutionized computation by vastly reducing the space, labor, and time required for calculations. Microfluidic systems hold similar promise for the large-scale automation of chemistry and biology, suggesting the possibility of numerous experiments performed rapidly and in parallel, while consuming little reagent. While it is too early to tell whether such a vision will be realized, significant progress has been achieved, and various applications of significant scientific and practical interest have been developed. Here a review of the physics of small volumes (nanoliters) of fluids is presented, as parametrized by a series of dimensionless numbers expressing the relative importance of various physical phenomena. Specifically, this review explores the Reynolds number Re, addressing inertial effects; the Peclet number Pe, which concerns convective and diffusive transport; the capillary number Ca expressing the importance of interfacial tension; the Deborah, Weissenberg, and elasticity numbers De, Wi, and El, describing elastic effects due to deformable microstructural elements like polymers; the Grashof and Rayleigh numbers Gr and Ra, describing density-driven flows; and the Knudsen number, describing the importance of noncontinuum molecular effects. Furthermore, the long-range nature of viscous flows and the small device dimensions inherent in microfluidics mean that the influence of boundaries is typically significant. A variety of strategies have been developed to manipulate fluids by exploiting boundary effects; among these are electrokinetic effects, acoustic streaming, and fluid-structure interactions. The goal is to describe the physics behind the rich variety of fluid phenomena occurring on the nanoliter scale using simple scaling arguments, with the hopes of developing an intuitive sense for this occasionally counterintuitive world.",2005,1080,3427,70,17,77,129,221,219,226,241,278,285,260
a8d6e6553a2724f2aa11d31fd9d3d617d9e86d57,,1972,0,3384,168,0,5,0,5,6,5,15,6,9,16
e419cfbbdd1de7f9a2ed6bb2d5392840dcb2a4fd,"Statistical physics has proven to be a fruitful framework to describe phenomena outside the realm of traditional physics. Recent years have witnessed an attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. A wide list of topics are reviewed ranging from opinion and cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, and social spreading. The connections between these problems and other, more traditional, topics of statistical physics are highlighted. Comparison of model results with empirical data from social systems are also emphasized.",2007,1011,2977,77,5,33,85,140,183,249,230,273,276,252
281f3c1d9cc26c75c09510be27e02f11456e36e5,,2006,0,3568,26,5,20,39,56,84,108,133,211,269,346
82b6305d085d0f77a64c6e2ff0c1d413be23275d,,2009,0,1986,194,69,78,99,127,146,152,158,153,162,179
eabb2f4d4b9f767475cd5dc37d70c32196545a4c,Partial table of contents: THE ALGEBRA OF LINEAR TRANSFORMATIONS AND QUADRATIC FORMS. Transformation to Principal Axes of Quadratic and Hermitian Forms. Minimum-Maximum Property of Eigenvalues. SERIES EXPANSION OF ARBITRARY FUNCTIONS. Orthogonal Systems of Functions. Measure of Independence and Dimension Number. Fourier Series. Legendre Polynomials. LINEAR INTEGRAL EQUATIONS. The Expansion Theorem and Its Applications. Neumann Series and the Reciprocal Kernel. The Fredholm Formulas. THE CALCULUS OF VARIATIONS. Direct Solutions. The Euler Equations. VIBRATION AND EIGENVALUE PROBLEMS. Systems of a Finite Number of Degrees of Freedom. The Vibrating String. The Vibrating Membrane. Green's Function (Influence Function) and Reduction of Differential Equations to Integral Equations. APPLICATION OF THE CALCULUS OF VARIATIONS TO EIGENVALUE PROBLEMS. Completeness and Expansion Theorems. Nodes of Eigenfunctions. SPECIAL FUNCTIONS DEFINED BY EIGENVALUE PROBLEMS. Bessel Functions. Asymptotic Expansions. Additional Bibliography. Index.,1962,0,4077,68,26,18,22,34,30,38,27,35,32,42
04434bdd67911dfce26d57466e9a02d70de0cb61,"Preface 1. Overview 2. Structure and scattering 3. Thermodynamics and statistical mechanics 4. Mean-field theory 5. Field theories, critical phenomena, and the renormalization group 6. Generalized elasticity 7. Dynamics: correlation and response 8. Hydrodynamics 9. Topological defects 10. Walls, kinks and solitons Glossary Index.",2000,0,2557,178,56,97,82,114,111,115,112,110,124,149
3f639b1dc7a1d60129248888bc7f388b148dccf7,Preface 1. Basic tools 2. Elasticity and Hooke's law 3. Seismic wave propagation 4. Effective media 5. Granular media 6. Fluid effects on wave propagation 7. Empirical relations 8. Flow and diffusion 9. Electrical properties Appendices.,1998,0,2195,211,2,1,17,24,26,37,43,64,48,60
52e1eba3b9132599c4788c0ed80751a0975aed80,"BiFeO3 is perhaps the only material that is both magnetic and a strong ferroelectric at room temperature. As a result, it has had an impact on the field of multiferroics that is comparable to that of yttrium barium copper oxide (YBCO) on superconductors, with hundreds of publications devoted to it in the past few years. In this Review, we try to summarize both the basic physics and unresolved aspects of BiFeO3 (which are still being discovered with several new phase transitions reported in the past few months) and device applications, which center on spintronics and memory devices that can be addressed both electrically and magnetically.",2009,202,2896,15,17,99,157,193,223,236,300,289,239,309
b26d845e7dcfee0b6f9f236440477be4166711c8,"FIRST EXPERIMENTS IN JET. Results obtained from JET since June 1983 are described which show that this large tokamak behaves in a similar manner to smaller tokamaks, but with correspondingly improved plasma parameters. Long-duration hydrogen and deuterium plasmas (>10 s) have been obtained with electron temperatures reaching > 4 keV for power dissipations < 3 MW and with * Euratom-IPP Association, Institut fur Plasmaphysik, Garching, Federal Republic of Germany. ** Euratom-ENEA Association, Centro di Frascati, Italy. *** Euratom-UKAEA Association, Culham Laboratory, Abingdon, Oxfordshire, United Kingdom. **** University of Dusseldorf, Dusseldorf, Federal Republic of Germany. + Euratom-Ris0 Association, Ris<f> National Laboratory, Roskilde, Denmark. ++ Euratom-CNR Association, Istituto di Física del Plasma, Milan, Italy. +++ Imperial College of Science and Technology, University of London, London, United Kingdom. ++++ Euratom-FOM Association, FOM Instituut voor Plasmafysica,. Nieuwegein, Netherlands. ® Euratom-Suisse Association, Centre de Recherches en Physique des Plasmas, Lausanne, Switzerland.",1987,65,3499,17,185,176,174,228,213,238,197,196,202,178
e8e4b786031e421912cf904d577994ccfcd0ca48,1. General Physical Properties of Rubber 2. Internal Energy and Entropy Changes on Deformation 3. The Elasticity of Long-Chain Molecules 4. The Elasticity of a Molecular Network 5Ex5 Experimental Examination of the Statistical Theory 6. Non-Gaussian Chain Statistics and Network Theory 7. Swelling Phenomena 8. Cross-linking and Modulus 9. Photoelastic Properties of Rubbers 10. The General Strain: Phenomenological Theory 11. Alternative Forms of Strain-Energy Function 12. Large-Deformation Theory: Shear and Torsion 13. Thermodynamic Analysis of Gaussian Network,1949,0,3703,120,0,0,2,4,6,5,7,5,2,4
9c4fb83f9c95c677ca5fa7e478de438b733a002d,"The identification of genetically homogeneous groups of individuals is a long standing issue in population genetics. A recent Bayesian algorithm implemented in the software structure allows the identification of such groups. However, the ability of this algorithm to detect the true number of clusters (K) in a sample of individuals when patterns of dispersal among populations are not homogeneous has not been tested. The goal of this study is to carry out such tests, using various dispersal scenarios from data generated with an individual‐based model. We found that in most cases the estimated ‘log probability of data’ does not provide a correct estimation of the number of clusters, K. However, using an ad hoc statistic ΔK based on the rate of change in the log probability of data between successive K values, we found that structure accurately detects the uppermost hierarchical level of structure for the scenarios we tested. As might be expected, the results are sensitive to the type of genetic marker used (AFLP vs. microsatellite), the number of loci scored, the number of populations sampled, and the number of individuals typed in each sample.",2005,52,16245,1728,0,0,0,0,0,0,0,0,0,7
4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3,"First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.",1956,24,21278,766,0,0,0,0,0,0,0,0,0,0
81f8fb4361b2bfa0159d5165641b87d30a9701f6,"The magnitudes of the systematic biases involved in sample heterozygosity and sample genetic distances are evaluated, and formulae for obtaining unbiased estimates of average heterozygosity and genetic distance are developed. It is also shown that the number of individuals to be used for estimating average heterozygosity can be very small if a large number of loci are studied and the average heterozygosity is low. The number of individuals to be used for estimating genetic distance can also be very small if the genetic distance is large and the average heterozygosity of the two species compared is low.",1978,3,10867,1708,0,0,3,1,2,3,3,5,4,4
7dd73816b9e1d5079ef23d7a45ffc7ae42507464,"Examining the pattern of nucleotide substitution for the control region of mitochondrial DNA (mtDNA) in humans and chimpanzees, we developed a new mathematical method for estimating the number of transitional and transversional substitutions per site, as well as the total number of nucleotide substitutions. In this method, excess transitions, unequal nucleotide frequencies, and variation of substitution rate among different sites are all taken into account. Application of this method to human and chimpanzee data suggested that the transition/transversion ratio for the entire control region was approximately 15 and nearly the same for the two species. The 95% confidence interval of the age of the common ancestral mtDNA was estimated to be 80,000-480,000 years in humans and 0.57-2.72 Myr in common chimpanzees.",1993,30,9144,1153,5,16,29,37,76,76,95,111,166,186
379df72de684003963f11427c97490a8c2d2a593,,1966,19,11622,679,0,0,0,0,0,0,1,0,0,0
439672067967d3501b0d201bdb50a60886459b00,"Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population. Despite mixture models' usefulness in practice, one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deciding on the number of classes in a study population. This article presents the results of a simulation study that examines the performance of likelihood-based tests and the traditionally used Information Criterion (ICs) used for determining the number of classes in mixture modeling. We look at the performance of these tests and indexes for 3 types of mixture models: latent class analysis (LCA), a factor mixture model (FMA), and a growth mixture models (GMM). We evaluate the ability of the tests and indexes to correctly identify the number of classes at three different sample sizes (n = 200, 500, 1,000). Whereas the Bayesian Information Criterion performed the best of the ICs, the bootstrap likelihood ratio test proved to be a very consistent indicator of classes across all of the models considered.",2007,47,6183,417,27,53,88,131,188,268,311,388,444,569
c8f359b3967ddef8e6d7f6ad58213a543d33ea22,"Miller (1956) summarized evidence that people can remember about seven chunks in short-term memory (STM) tasks. However, that number was meant more as a rough estimate and a rhetorical device than as a real capacity limit. Others have since suggested that there is a more precise capacity limit, but that it is only three to five chunks. The present target article brings together a wide variety of data on capacity limits suggesting that the smaller capacity limit is real. Capacity limits will be useful in analyses of information processing only if the boundary conditions for observing them can be carefully described. Four basic conditions in which chunks can be identified and capacity limits can accordingly be observed are: (1) when information overload limits chunks to individual stimulus items, (2) when other steps are taken specifically to block the recoding of stimulus items into larger chunks, (3) in performance discontinuities caused by the capacity limit, and (4) in various indirect effects of the capacity limit. Under these conditions, rehearsal and long-term memory cannot be used to combine stimulus items into chunks of an unknown size; nor can storage mechanisms that are not capacity-limited, such as sensory memory, allow the capacity-limited storage mechanism to be refilled during recall. A single, central capacity limit averaging about four chunks is implicated along with other, noncapacity-limited sources. The pure STM capacity limit expressed in chunks is distinguished from compound STM limits obtained when the number of separately held chunks is unclear. Reasons why pure capacity estimates fall within a narrow range are discussed and a capacity limit for the focus of attention is proposed.",2001,484,5275,534,24,48,70,96,129,143,173,216,260,253
db966ed68e8e435885b67012ec96c20662e6de9b,"A direct numerical simulation of a turbulent channel flow is performed. The unsteady Navier-Stokes equations are solved numerically at a Reynolds number of 3300, based on thc mean centreline velocity and channel half-width, with about 4 x los grid points (192 x 129 x 160 in 2, y, 2). All essential turbulence scales are resolved on the computational grid and no subgrid model is used. A large number of turbulence statistics are computed and compared with the existing experimental data at comparable Reynolds numbers. Agreements as well as discrepancies are discussed in detail. Particular attention is given to the behaviour of turbulence correlations near the wall. In addition, a number of statistical correlations which are complementary to the existing experimental data are reported for the first time.",1987,58,4457,383,13,15,38,39,66,48,83,74,81,94
df719e6e99b07912c33645e174a8aa7ff0ace68b,"AIM: To estimate the prevalence of glaucoma among people worldwide. METHODS: Available published data on glaucoma prevalence were reviewed to determine the relation of open angle and angle closure glaucoma with age in people of European, African, and Asian origin. A comparison was made with estimated world population data for the year 2000. RESULTS: The number of people with primary glaucoma in the world by the year 2000 is estimated at nearly 66.8 million, with 6.7 million suffering from bilateral blindness. In developed countries, fewer than 50% of those with glaucoma are aware of their disease. In the developing world, the rate of known disease is even lower. CONCLUSIONS: Glaucoma is the second leading cause of vision loss in the world. Improved methods of screening and therapy for glaucoma are urgently needed.",1996,59,5358,294,0,11,27,37,49,62,67,104,78,104
1efc86d78b94931fefdfd47614299f6c6c17980e,"It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares “capitalization” on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted.",1965,8,5806,346,1,3,4,4,1,4,3,2,5,2
d745f239021ce1ff976eb9bfe0e5b788c12304c6,"Abstract The distribution is obtained for the number of segregating sites observed in a sample from a population which is subject to recurring, new, mutations but not subject to recombination. After allowance is made for the different effective population sizes, the results apply approximately to three population models, due to Wright, Burrows and Cockerham, and Moran. Included as extreme special cases are the distributions of the number of segregating sites in the whole population and of the number of heterozygous sites in a diploid individual. Some results of Fisher, Haldane, Kimura, and Ewens concerning the means of the distributions for different models are confirmed, but the variances, and the distributions themselves, are new.",1975,36,3693,391,0,2,1,1,3,1,4,6,2,4
2454e846733f0f0d6ae98c489a632a7199d07ed6,"Preface 1. Monte Carlo methods and Quasi-Monte Carlo methods 2. Quasi-Monte Carlo methods for numerical integration 3. Low-discrepancy point sets and sequences 4. Nets and (t,s)-sequences 5. Lattice rules for numerical integration 6. Quasi- Monte Carlo methods for optimization 7. Random numbers and pseudorandom numbers 8. Nonlinear congruential pseudorandom numbers 9. Shift-Register pseudorandom numbers 10. Pseudorandom vector generation Appendix A. Finite fields and linear recurring sequences Appendix B. Continued fractions Bibliography Index.",1992,0,3654,361,4,17,30,59,71,79,110,75,82,101
6fac7b336a33916919050e17543ca8f4d1cf9ee4,"Low Reynolds number flow theory finds wide application in such diverse fields as sedimentation, fluidization, particle-size classification, dust and mist collection, filtration, centrifugation, polymer and suspension rheology, flow through porous media, colloid science, aerosol and hydrosal technology, lubrication theory, blood flow, Brownian motion, geophysics, meteorology, and a host of other disciplines. This text provides a comprehensive and detailed account of the physical and mathematical principles underlying such phenomena, heretofore available only in the original literature.",1965,0,4307,286,0,5,6,8,7,17,16,17,23,22
4249e1d658670c2f3253ecf384f31029acbaddeb,"Copy number variation (CNV) of DNA sequences is functionally significant but has yet to be fully ascertained. We have constructed a first-generation CNV map of the human genome through the study of 270 individuals from four populations with ancestry in Europe, Africa or Asia (the HapMap collection). DNA from these individuals was screened for CNV using two complementary technologies: single-nucleotide polymorphism (SNP) genotyping arrays, and clone-based comparative genomic hybridization. A total of 1,447 copy number variable regions (CNVRs), which can encompass overlapping or adjacent gains or losses, covering 360 megabases (12% of the genome) were identified in these populations. These CNVRs contained hundreds of genes, disease loci, functional elements and segmental duplications. Notably, the CNVRs encompassed more nucleotide content per genome than SNPs, underscoring the importance of CNV in genetic diversity and evolution. The data obtained delineate linkage disequilibrium patterns for many CNVs, and reveal marked variation in copy number among populations. We also demonstrate the utility of this resource for genetic disease studies.",2006,82,4121,185,13,328,453,509,460,351,340,310,238,208
cf7d72b1c0f495cf36a1d9a37565ca7f1ae07cca,"On applique la methode d'Efron (1981, 1982) a la construction d'intervalles de confiance bases sur des distributions du bootstrap",1984,23,3450,381,0,0,0,2,2,1,0,3,4,1
89c8179cce5887300a8b588c86cfd3e6db0b2801,"We propose a method (the ‘gap statistic’) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.",2000,20,4411,220,1,11,24,49,47,55,81,111,125,148
2f5ec0fcdee047e21c3e6987a2e17f642d128574,"This is the first volume of a two-volume textbook which evolved from a course (Mathematics 160) offered at the California Institute of Technology during the last 25 years. It provides an introduction to analytic number 
theory suitable for undergraduates with some background in advanced calculus, but with no previous knowledge of number theory. Actually, a great deal of the book requires no calculus at all and could profitably be studied by sophisticated high school students. 
 
Number theory is such a vast and rich field that a one-year course cannot do justice to all its parts. The choice of topics included here is intended to provide some variety and some depth. Problems which have fascinated generations of professional and amateur mathematicians are discussed 
together with some of the techniques for solving them. 
 
One of the goals of this course has been to nurture the intrinsic interest that many young mathematics students seem to have in number theory and to open some doors for them to the current periodical literature. It has been 
gratifying to note that many of the students who have taken this course during the past 25 years have become professional mathematicians, and some have made notable contributions of their own to number theory. To all of 
them this book is dedicated.",1976,0,3254,301,1,2,7,5,10,7,10,8,9,7
61f7a2593b323f3ecdf1a556d83c614a9995cb08,"Abstract : This paper discusses some aspects of selecting and testing random and pseudorandom number generators. The outputs of such generators may he used in many cryptographic applications, such as the generation of key material. Generators suitable for use in cryptographic applications may need to meet stronger requirements than for other applications. In particular, their outputs must he unpredictable in the absence of knowledge of the inputs. Some criteria for characterizing and selecting appropriate generators are discussed in this document. The subject of statistical testing and its relation to cryptanalysis is also discussed, and some recommended statistical tests are provided. These tests may he useful as a first step in determining whether or not a generator is suitable for a particular cryptographic application. The design and cryptanalysis of generators is outside the scope of this paper.",2000,52,3194,356,1,5,10,17,29,42,53,58,77,119
098d5792ffa43e9885f9fc644ffdd7b6a59b0922,"A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 2<supscrpt>19937</supscrpt> −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to <italic>v</italic> bits accuracy for 1 ≤ <italic>v</italic> ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity <italic>O(p<supscrpt>2</supscrpt>)</italic> where  <italic>p</italic> is the degree of the polynomial.
We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.",1998,62,5342,203,9,13,33,27,61,86,119,178,196,216
856e2c985825d055ec619a4ac9bf408182fd35a8,"Popular statistical software packages do not have the proper procedures for determining the number of components in factor and principal components analyses. Parallel analysis and Velicer’s minimum average partial (MAP) test are validated procedures, recommended widely by statisticians. However, many researchers continue to use alternative, simpler, but flawed procedures, such as the eigenvaluesgreater-than-one rule. Use of the proper procedures might be increased if these procedures could be conducted within familiar software environments. This paper describes brief and efficient programs for using SPSS and SAS to conduct parallel analyses and the MAP test.",2000,24,3220,219,3,0,2,10,20,44,62,92,107,109
afc8da96925a24c778533ec5c9d02ca8db5f06b2,"In this paper we develop some econometric theory for factor models of large dimensions. The focus is the determination of the number of factors, which is an unresolved issue in the rapidly growing literature on multifactor models. We propose some panel C(p) criteria and show that the number of factors can be consistently estimated using the criteria. The theory is developed under the framework of large cross-sections (N) and large time dimensions (T). No restriction is imposed on the relation between N and T. Simulations show that the proposed criteria yield almost precise estimates of the number of factors for configurations of the panel data encountered in practice.",2000,51,2871,606,3,8,21,36,50,73,70,89,98,120
b49caa46aa223d5443846adb65ccf2d0ceb9a729,"Aim: To estimate the number of people with open angle (OAG) and angle closure glaucoma (ACG) in 2010 and 2020. Methods: A review of published data with use of prevalence models. Data from population based studies of age specific prevalence of OAG and ACG that satisfied standard definitions were used to construct prevalence models for OAG and ACG by age, sex, and ethnicity, weighting data proportional to sample size of each study. Models were combined with UN world population projections for 2010 and 2020 to derive the estimated number with glaucoma. Results: There will be 60.5 million people with OAG and ACG in 2010, increasing to 79.6 million by 2020, and of these, 74% will have OAG. Women will comprise 55% of OAG, 70% of ACG, and 59% of all glaucoma in 2010. Asians will represent 47% of those with glaucoma and 87% of those with ACG. Bilateral blindness will be present in 4.5 million people with OAG and 3.9 million people with ACG in 2010, rising to 5.9 and 5.3 million people in 2020, respectively. Conclusions: Glaucoma is the second leading cause of blindness worldwide, disproportionately affecting women and Asians.",2006,63,5667,116,24,108,153,159,184,264,301,377,468,515
55f73ca18b8dd75eb0365e449321766e6860c23b,"The optimum routing of a fleet of trucks of varying capacities from a central depot to a number of delivery points may require a selection from a very large number of possible routes, if the number of delivery points is also large. This paper, after considering certain theoretical aspects of the problem, develops an iterative procedure that enables the rapid selection of an optimum or near-optimum route. It has been programmed for a digital computer but is also suitable for hand computation.",1964,1,3564,263,0,0,0,2,5,7,0,5,11,2
58bdeda2c4be279c060034c3eeef495e9d7c7799,"A high number of tree species, low density of adults of each species, and long distances between conspecific adults are characteristic of many low-land tropical forest habitats. I propose that these three traits, in large part, are the result of the action of predators on seeds and seedlings. A model is presented that allows detailed examination of the effect of different predators, dispersal agents, seed-crop sizes, etc. on these three traits. In short, any event that increases the efficiency of the predators at eating seeds and seedlings of a given tree species may lead to a reduction in population density of the adults of that species and/or to increased distance between new adults and their parents. Either event will lead to more space in the habitat for other species of trees, and therefore higher total number of tree species, provided seed sources are available over evolutionary time. As one moves from the wet lowland tropics to the dry tropics or temperate zones, the seed and seedling predators in a habitat are hypothesized to be progressively less efficient at keeping one or a few tree species from monopolizing the habitat through competitive superiority. This lowered efficiency of the predators is brought about by the increased severity and unpredictability of the physical environment, which in turn leads to regular or erratic escape of large seed or seedling cohorts from the predators.",1970,72,4029,187,0,8,11,10,28,23,23,29,19,21
8c362d0fd11e593bd27bd5b655c9cc04969a753f,"This article considers forecasting a single time series when there are many predictors (N) and time series observations (T). When the data follow an approximate factor model, the predictors can be summarized by a small number of indexes, which we estimate using principal components. Feasible forecasts are shown to be asymptotically efficient in the sense that the difference between the feasible forecasts and the infeasible forecasts constructed using the actual values of the factors converges in probability to 0 as both N and T grow large. The estimated factors are shown to be consistent, even in the presence of time variation in the factor model.",2002,30,2653,349,5,16,20,39,51,65,88,124,102,155
1ac6ffb033a1e2d704279f3a496a4b919ffcbb25,"In systematic searches for embryonic lethal mutants of Drosophila melanogaster we have identified 15 loci which when mutated alter the segmental pattern of the larva. These loci probably represent the majority of such genes in Drosophila. The phenotypes of the mutant embryos indicate that the process of segmentation involves at least three levels of spatial organization: the entire egg as developmental unit, a repeat unit with the length of two segments, and the individual segment.",1980,30,3953,124,3,12,13,10,25,46,53,53,80,67
c01d227077dae98b82c68d9871e8d28dc24a0887,"A description of 148 algorithms fundamental to number-theoretic computations, in particular for computations related to algebraic number theory, elliptic curves, primality testing and factoring. The first seven chapters guide readers to the heart of current research in computational algebraic number theory, including recent algorithms for computing class groups and units, as well as elliptic curve computations, while the last three chapters survey factoring and primality testing methods, including a detailed description of the number field sieve algorithm. The whole is rounded off with a description of available computer packages and some useful tables, backed by numerous exercises. Written by an authority in the field, and one with great practical and teaching experience, this is certain to become the standard and indispensable reference on the subject.",1993,229,2775,210,3,13,27,41,64,80,66,97,76,94
15f37ff89f68747a3f4cb34cb6097ba79b201f92,"We performed a Monte Carlo study to evaluate the effect of the number of events per variable (EPV) analyzed in logistic regression analysis. The simulations were based on data from a cardiac trial of 673 patients in which 252 deaths occurred and seven variables were cogent predictors of mortality; the number of events per predictive variable was (252/7 =) 36 for the full sample. For the simulations, at values of EPV = 2, 5, 10, 15, 20, and 25, we randomly generated 500 samples of the 673 patients, chosen with replacement, according to a logistic model derived from the full sample. Simulation results for the regression coefficients for each variable in each group of 500 samples were compared for bias, precision, and significance testing against the results of the model fitted to the original sample. For EPV values of 10 or greater, no major problems occurred. For EPV values less than 10, however, the regression coefficients were biased in both positive and negative directions; the large sample variance estimates from the logistic model both overestimated and underestimated the sample variance of the regression coefficients; the 90% confidence limits about the estimated values did not have proper coverage; the Wald statistic was conservative under the null hypothesis; and paradoxical associations (significance in the wrong direction) were increased. Although other factors (such as the total number of events, or sample size) may influence the validity of the logistic model, our findings indicate that low EPV can lead to major problems.",1996,19,5489,101,0,2,4,7,16,32,27,36,54,47
6befd7e6d780a3247d169ecaf858d720d7311c6c,"A stereological method for obtaining estimates of the total number of neurons in five major subdivisions of the rat hippocampus is described. The new method, the optical fractionator, combines two recent developments in stereology: a three‐dimensional probe for counting neuronal nuclei, the optical disector, and a systematic uniform sampling scheme, the fractionator. The optical disector results in unbiased estimates of neuron number, i.e., estimates that are free of assumptions about neuron size and shape, are unaffected by lost caps and over‐projection, and approach the true number of neurons in an unlimited manner as the number of samples is increased. The fractionator involves sampling a known fraction of a structural component. In the case of neuron number, a zero dimensional quantity, it provides estimates that are unaffected by shrinkage before, during, and after processing of the tissue. Because the fractionator involves systematic sampling, it also results in highly efficient estimates. Typically only 100–200 neurons must be counted in an animal to obtain a precision that is compatible with experimental studies. The methodology is compared with those used in earlier works involving estimates of neuron number in the rat hippocampus and a number of new stereological methods that have particular relevance to the quantitative study of the structure of the nervous system are briefly described in an appendix.",1991,33,2903,223,2,7,14,14,13,23,34,32,62,72
3972dbc40fd2733214028a5d6e9ab961d7c5c41b,"Abstract A new k-ϵ eddy viscosity model, which consists of a new model dissipation rate equation and a new realizable eddy viscosity formulation, is proposed in this paper. The new model dissipation rate equation is based on the dynamic equation of the mean-square vorticity fluctuation at large turbulent Reynolds number. The new eddy viscosity formulation is based on the realizability constraints; the positivity of normal Reynolds stresses and the Schwarz' inequality for turbulent shear stresses. We find that the present model with a set of unified model coefficients can perform well for a variety of flows. The flows that are examined include: (i) rotating homogeneous shear flows; (ii) boundary-free shear flows including a mixing layer, planar and round jets; (iii) a channel flow, and flat plate boundary layers with and without a pressure gradient; and (iv) backward facing step separated flows. The model predictions are compared with available experimental data. The results from the standard k-ϵ eddy viscosity model are also included for comparison. It is shown that the present model is a significant improvement over the standard k-ϵ eddy viscosity model.",1995,23,3852,102,5,7,4,8,8,7,11,33,39,51
e46ac802d7207e0e51b5333456a3f46519c2f92d,"All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. Regle generale, les methodes numeriques enregistrent des lignes avec beaucoup plus de donnees qu'il n'est necessaire a la reproduction graphique precise ou a la recherche par ordinateur. L'auteur presente deux algorithmes pour reduire le nombre de points necessaires pour representer la ligne et produire des caricatures si desire, et les compare aux methodes les plus prometteuses suggerees jusqu'ici. La reduction de la ligne constituera une partie importante de la generalisation automatique.",1973,5,3642,163,0,0,1,2,3,1,3,0,4,2
9d46c57d778b9a15634e9c42b3dbc8c23088ea43,,1989,0,4196,90,0,5,16,28,59,60,113,153,206,265
d7d385f45c096082812deb1623e5af2c2915b4a9,"Despite its popularity for general clustering, K-means suuers three major shortcomings; it scales poorly computationally, the number of clusters K has to be supplied by the user, and the search is prone to local minima. We propose solutions for the rst two problems, and a partial remedy for the third. Building on prior work for algorithmic acceleration that is not based on approximation, we introduce a new algorithm that eeciently, searches the space of cluster locations and number of clusters to optimize the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC) measure. The innovations include two new ways of exploiting cached suucient statistics and a new very eecient test that in one K-means sweep selects the most promising subset of classes for reenement. This gives rise to a fast, statistically founded algorithm that outputs both the number of classes and their parameters. Experiments show this technique reveals the true number of classes in the underlying distribution , and that it is much faster than repeatedly using accelerated K-means for different values of K.",2000,17,2456,214,4,11,17,14,37,49,68,93,88,108
f57819360c6e00d0ebf0a3d2d41ff79c2bf14d80,"Editor’s note: This is a reprint (slightly edited) of a paper of the same title that appeared in the book Physics and Our World: A Symposium in Honor of Victor F. Weisskopf, published by the American Institute of Physics (1976). The personal tone of the original talk has been preserved in the paper, which was itself a slightly edited transcript of a tape. The figures reproduce transparencies used in the talk. The demonstration involved a tall rectangular transparent vessel of corn syrup, projected by an overhead projector turned on its side. Some essential hand waving could not be reproduced.",1977,0,3268,159,1,2,0,4,5,4,8,9,10,10
9d0ace7494b1b0075cdec51e684f0de4974efce7,"We examined the number of tropical cyclones and cyclone days as well as tropical cyclone intensity over the past 35 years, in an environment of increasing sea surface temperature. A large increase was seen in the number and proportion of hurricanes reaching categories 4 and 5. The largest increase occurred in the North Pacific, Indian, and Southwest Pacific Oceans, and the smallest percentage increase occurred in the North Atlantic Ocean. These increases have taken place while the number of cyclones and cyclone days has decreased in all basins except the North Atlantic during the past decade.",2005,81,2850,114,15,141,207,216,176,226,179,200,193,176
29f456c100d70eea88805aa6d5091df3f4338200,Introduction Arithmetic functions Elementary theory of prime numbers Characters Summation formulas Classical analytic theory of $L$-functions Elementary sieve methods Bilinear forms and the large sieve Exponential sums The Dirichlet polynomials Zero-density estimates Sums over finite fields Character sums Sums over primes Holomorphic modular forms Spectral theory of automorphic forms Sums of Kloosterman sums Primes in arithmetic progressions The least prime in an arithmetic progression The Goldbach problem The circle method Equidistribution Imaginary quadratic fields Effective bounds for the class number The critical zeros of the Riemann zeta function The spacing of zeros of the Riemann zeta-function Central values of $L$-functions Bibliography Index.,2004,388,2643,174,23,37,61,80,109,117,169,161,179,157
a3b7e80260891dcd3844b1835df8dee3a1cd67c7,"L'A. propose quelques observations concernant l'ouvrage de Stanislas Dehaene The number sense. How the mind creates mathematics (1997) qui explore tous les aspects de la relation entre les hommes et les nombres : la numerosite chez les autres animaux, la numerosite et le calcul simple chez les bebes, l'histoire de l'expression du nombre dans le langage, l'histoire de la notation du nombre, le circuit neuronal necessaire pour faire de l'arithmetique et du calcul, la localisation dans le cerveau, l'ordre mathematique de l'univers, etc ... L'A. examine ici en particulier les questions portant sur la relation entre les nombres et le langage dans une perspective cognitive, puis explique ce que Dehaene entend par le sens du nombre en caracterisant les mathematiques comme une formalisation progressive de nos intuitions sur les ensembles, le nombre, l'espace, le temps et la logique",1998,0,2213,227,4,14,21,28,32,48,41,66,55,68
0c1c4b3b78e54381e1a1011843ffa1924af2da83,,1986,58,2865,151,2,32,11,26,21,25,24,34,21,36
fddf2adb6ab085bc6231e72ba39904d7d2af57af,,1980,0,2500,223,0,2,0,0,0,1,0,0,0,1
f442aca9c8a754a9ef75ef68d2bd436080609d17,"Did evolution endow the human brain with a predisposition to represent and acquire knowledge about numbers? Although the parietal lobe has been suggested as a potential substrate for a domain-specific representation of quantities, it is also engaged in verbal, spatial, and attentional functions that may contribute to calculation. To clarify the organisation of number-related processes in the parietal lobe, we examine the three-dimensional intersection of fMRI activations during various numerical tasks, and also review the corresponding neuropsychological evidence. On this basis, we propose a tentative tripartite organisation. The horizontal segment of the intraparietal sulcus (HIPS) appears as a plausible candidate for domain specificity: It is systematically activated whenever numbers are manipulated, independently of number notation, and with increasing activation as the task puts greater emphasis on quantity processing. Depending on task demands, we speculate that this core quantity system, analogous to an internal “number line,” can be supplemented by two other circuits. A left angular gyrus area, in connection with other left-hemispheric perisylvian areas, supports the manipulation of numbers in verbal form. Finally, a bilateral posterior superior parietal system supports attentional orientation on the mental number line, just like on any other spatial dimension.",2003,129,2160,210,9,35,54,76,80,107,137,119,146,131
22b4fc0bd039037cc903eb1c41fe851066289e0a,"BackgroundThe integrity of RNA molecules is of paramount importance for experiments that try to reflect the snapshot of gene expression at the moment of RNA extraction. Until recently, there has been no reliable standard for estimating the integrity of RNA samples and the ratio of 28S:18S ribosomal RNA, the common measure for this purpose, has been shown to be inconsistent. The advent of microcapillary electrophoretic RNA separation provides the basis for an automated high-throughput approach, in order to estimate the integrity of RNA samples in an unambiguous way.MethodsA method is introduced that automatically selects features from signal measurements and constructs regression models based on a Bayesian learning technique. Feature spaces of different dimensionality are compared in the Bayesian framework, which allows selecting a final feature combination corresponding to models with high posterior probability.ResultsThis approach is applied to a large collection of electrophoretic RNA measurements recorded with an Agilent 2100 bioanalyzer to extract an algorithm that describes RNA integrity. The resulting algorithm is a user-independent, automated and reliable procedure for standardization of RNA quality control that allows the calculation of an RNA integrity number (RIN).ConclusionOur results show the importance of taking characteristics of several regions of the recorded electropherogram into account in order to get a robust and reliable prediction of RNA integrity, especially if compared to traditional methods.",2006,23,2214,197,18,40,82,105,129,161,181,192,177,190
0308b4fdab1e08bc9492285edce9afd35f3dab0b,"We demonstrate that, under a theorem proposed by Vuong, the likelihood ratio statistic based on the Kullback-Leibler information criterion of the null hypothesis that a random sample is drawn from a k 0 -component normal mixture distribution against the alternative hypothesis that the sample is drawn from a k 1 -component normal mixture distribution is asymptotically distributed as a weighted sum of independent chi-squared random variables with one degree of freedom, under general regularity conditions. We report simulation studies of two cases where we are testing a single normal versus a two-component normal mixture and a two-component normal mixture versus a three-component normal mixture. An empirical adjustment to the likelihood ratio statistic is proposed that appears to improve the rate of convergence to the limiting distribution.",2001,0,2952,96,1,2,7,18,19,28,58,53,73,106
2c6fe95cdcb4aeca4bd6c0099fefa867fca27652,"Nine experiments of timed odd-even judgments examined how parity and number magnitude are accessed from Arabic and verbal numerals. With Arabic numerals, Ss used the rightmost digit to access a store of semantic number knowledge. Verbal numerals went through an additional stage of transcoding to base 10. Magnitude information was automatically accessed from Arabic numerals. Large numbers preferentially elicited a rightward response, and small numbers a leftward response. The Spatial-Numerical Association of Response Codes (SNARC) effect depended only on relative number magnitude and was weaker or absent with letters or verbal numerals. Direction did not vary with handedness or hemispheric dominance but was linked to the direction of writing, as it faded or even reversed in right-to-left writing Iranian Ss",1993,44,2131,240,0,1,4,3,5,4,13,6,12,10
39990d887387b14f6d8e6918b37c6f8ea93bf8ff,"We tested the hypothesis that de novo copy number variation (CNV) is associated with autism spectrum disorders (ASDs). We performed comparative genomic hybridization (CGH) on the genomic DNA of patients and unaffected subjects to detect copy number variants not present in their respective parents. Candidate genomic regions were validated by higher-resolution CGH, paternity testing, cytogenetics, fluorescence in situ hybridization, and microsatellite genotyping. Confirmed de novo CNVs were significantly associated with autism (P = 0.0005). Such CNVs were identified in 12 out of 118 (10%) of patients with sporadic autism, in 2 out of 77 (3%) of patients with an affected first-degree relative, and in 2 out of 196 (1%) of controls. Most de novo CNVs were smaller than microscopic resolution. Affected genomic regions were highly heterogeneous and included mutations of single genes. These findings establish de novo germline mutation as a more significant risk factor for ASD than previously recognized.",2007,31,2615,81,49,170,273,255,248,227,245,183,191,152
37223160fd148c3956f866f27e4e7d4198a64d4f,,1980,0,2578,117,0,0,0,0,0,0,0,1,1,0
4fd5f6b7c036c7522d82f48e7cfd384e38328d6f,I: Algebraic Integers.- II: The Theory of Valuations.- III: Riemann-Roch Theory.- IV: Abstract Class Field Theory.- V: Local Class Field Theory.- VI: Global Class Field Theory.- VII: Zeta Functions and L-series.,1999,7,2632,114,46,70,60,80,80,104,100,94,104,114
02b28f3b71138a06e40dbd614abf8568420ae183,"We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% for the combination of all features.",2008,20,1565,238,1,11,28,30,35,53,75,87,109,152
9ceedeb5e2ebc39a08c70e2d42d69bca0e064bc1,"A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hierarchical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the performance of some of the criteria may be data dependent.",1985,56,2759,108,4,2,14,10,10,16,10,18,27,37
874cc3d7f38afed3543f961b49511aead2c91457,"DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modification of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set.",2004,29,2095,183,2,25,41,100,86,117,156,149,149,148
5eca27f33bc6b051b3e7e9ac84b9bc41f4c0e13b,"Abstract— Recent studies provide increasing evidence that postnatal neovascularization involves bone marrow-derived circulating endothelial progenitor cells (EPCs). The regulation of EPCs in patients with coronary artery disease (CAD) is unclear at present. Therefore, we determined the number and functional activity of EPCs in 45 patients with CAD and 15 healthy volunteers. The numbers of isolated EPCs and circulating CD34/kinase insert domain receptor (KDR)-positive precursor cells were significantly reduced in patients with CAD by ≈40% and 48%, respectively. To determine the influence of atherosclerotic risk factors, a risk factor score including age, sex, hypertension, diabetes, smoking, positive family history of CAD, and LDL cholesterol levels was used. The number of risk factors was significantly correlated with a reduction of EPC levels (R =−0.394, P =0.002) and CD34-/KDR-positive cells (R =−0.537, P <0.001). Analysis of the individual risk factors demonstrated that smokers had significantly reduced levels of EPCs (P <0.001) and CD34-/KDR-positive cells (P =0.003). Moreover, a positive family history of CAD was associated with reduced CD34-/KDR-positive cells (P =0.011). Most importantly, EPCs isolated from patients with CAD also revealed an impaired migratory response, which was inversely correlated with the number of risk factors (R =−0.484, P =0.002). By multivariate analysis, hypertension was identified as a major independent predictor for impaired EPC migration (P =0.043). The present study demonstrates that patients with CAD revealed reduced levels and functional impairment of EPCs, which correlated with risk factors for CAD. Given the important role of EPCs for neovascularization of ischemic tissue, the decrease of EPC numbers and activity may contribute to impaired vascularization in patients with CAD. The full text of this article is available at http://www.circresaha.org.",2001,36,2384,121,0,22,53,102,122,140,190,177,168,239
281939819f7ca10ea68d66913bbe2de34852d63a,"The extent to which large duplications and deletions contribute to human genetic variation and diversity is unknown. Here, we show that large-scale copy number polymorphisms (CNPs) (about 100 kilobases and greater) contribute substantially to genomic variation between normal humans. Representational oligonucleotide microarray analysis of 20 individuals revealed a total of 221 copy number differences representing 76 unique CNPs. On average, individuals differed by 11 CNPs, and the average length of a CNP interval was 465 kilobases. We observed copy number variation of 70 different genes within CNP intervals, including genes involved in neurological function, regulation of cell growth, regulation of metabolism, and several genes known to be associated with disease.",2004,84,2520,81,14,156,220,229,224,253,229,171,169,162
6ce7876c0b2acb52b88610ce9cfe21d239c28922,"New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.",1997,121,1937,222,12,37,38,52,66,59,54,87,77,109
25f5eb7f3075ea69fbd6fbf6db60d3e342b23cf2,"The qEffectiveq Number of Parties: qA Measure with Application to West Europeq Laakso, Markku;Taagepera, Rein Comparative Political Studies; Apr 1, 1979; 12, 1; Proouest pg. 3 “EFFECTIVE” NUMBER OF PARTIES A Measure with Application to West Europe MARKKU LAAKSO University of Helsinki REIN TAAGEPERA University of California, Irvine I s a large number of parties bound to destabilize a political system (Duverger, 1954) or is it not (e.g., Lijphart, 1968; Nilson, 1974)? Before this question can be answered, the number of parties must be operationally deﬁned in a way that takes into account their relative size. Such a number is also needed if one wants to detect trends toward fewer or more numerous parties over time, or the effects of a proposed change in electoral rules. This article presents ways to calculate this important political variable, calculates it for I42 post-1944 elections in 15 West European countries, and analyzes its possible effect on stability. We often talk of two-party and multiparty systems. We further dis- tinguish three~ or four—party systems in some countries, and even talk (e. g., Blondel, 1969: 535) of a two-and-a-half-party system whenthere is a third party of marginal size. Mexico could be viewed as a one-and-a- half-party system because the PR1 is so much larger than all other parties. Rather than take the number of all existing parties, including even the very smallest, one visibly has a need for a number that takes into account their relative size. We will call this number the “effective number of parties,” using the word “effectiveq somewhat in the sense pressure group literature uses it when talking about “effective access” (Truman, 195]: 506), but even more in the operational sense physicists give it when they talk about effective current (Richards et al., 1960: 594), COMPARATIVE POLITICAL STUDIES. Vol. I2 No. I. April 1979 3-27 © I979 Sage Publications. Inc. Copyright (c) 2000 Bell & Howell Information and Learning Company Copyright (c) Sage Inc.",1979,26,2652,73,0,2,4,0,2,1,1,2,4,1
4457bde4d1698a8a678d04841d3b455558d7f19c,"The relationship between the energy expended per offspring, fitness of offspring, and parental fitness is presented in a two-dimensional graphical model. The validity of the model in determining an optimal parental strategy is demonstrated analytically. The model applies under various conditions of parental care and sibling care for the offspring but is most useful for species that produce numerous small offspring which are given no parental care.",1974,9,2664,105,0,1,8,7,15,9,11,13,19,7
53ddafe17cfe0bcbf0932e454cf6fcc627c68380,"Companies spend lots of time and money on complex tools to assess customer satisfaction. But they're measuring the wrong thing. The best predictor of top-line growth can usually be captured in a single survey question: Would you recommend this company to a friend? This finding is based on two years of research in which a variety of survey questions were tested by linking the responses with actual customer behavior--purchasing patterns and referrals--and ultimately with company growth. Surprisingly, the most effective question wasn't about customer satisfaction or even loyalty per se. In most of the industries studied, the percentage of customers enthusiastic enough about a company to refer it to a friend or colleague directly correlated with growth rates among competitors. Willingness to talk up a company or product to friends, family, and colleagues is one of the best indicators of loyalty because of the customer's sacrifice in making the recommendation. When customers act as references, they do more than indicate they've received good economic value from a company; they put their own reputations on the line. And they will risk their reputations only if they feel intense loyalty. The findings point to a new, simpler approach to customer research, one directly linked to a company's results. By substituting a single question--blunt tool though it may appear to be--for the complex black box of the customer satisfaction survey, companies can actually put consumer survey results to use and focus employees on the task of stimulating growth.",2003,0,2083,171,1,9,22,32,60,72,68,91,107,114
d07b6d71653fd96659939544214a30714ff7fbe0,"Bridging the gap between elementary number theory and the systematic study of advanced topics, A Classical Introduction to Modern Number Theory is a well-developed and accessible text that requires only a familiarity with basic abstract algebra. Historical development is stressed throughout, along with wide-ranging coverage of significant results with comparatively elementary proofs, some of them new. An extensive bibliography and many challenging exercises are also included. This second edition has been corrected and contains two new chapters which provide a complete proof of the Mordell-Weil theorem for elliptic curves over the rational numbers, and an overview of recent progress on the arithmetic of elliptic curves.",1982,0,2503,110,2,2,4,7,10,10,10,16,19,15
4463f383a43b63c9f4bd9bf3b332f166ec79f3ec,"Gene dosage variations occur in many diseases. In cancer, deletions and copy number increases contribute to alterations in the expression of tumour-suppressor genes and oncogenes, respectively. Developmental abnormalities, such as Down, Prader Willi, Angelman and Cri du Chat syndromes, result from gain or loss of one copy of a chromosome or chromosomal region. Thus, detection and mapping of copy number abnormalities provide an approach for associating aberrations with disease phenotype and for localizing critical genes. Comparative genomic hybridization(CGH) was developed for genome-wide analysis of DNA sequence copy number in a single experiment. In CGH, differentially labelled total genomic DNA from a 'test' and a 'reference' cell population are cohybridized to normal metaphase chromosomes, using blocking DNA to suppress signals from repetitive sequences. The resulting ratio of the fluorescence intensities at a location on the 'cytogenetic map', provided by the chromosomes, is approximately proportional to the ratio of the copy numbers of the corresponding DNA sequences in the test and reference genomes. CGH has been broadly applied to human and mouse malignancies. The use of metaphase chromosomes, however, limits detection of events involving small regions (of less than 20 Mb) of the genome, resolution of closely spaced aberrations and linking ratio changes to genomic/genetic markers. Therefore, more laborious locus-by-locus techniques have been required for higher resolution studies. Hybridization to an array of mapped sequences instead of metaphase chromosomes could overcome the limitations of conventional CGH (ref. 6) if adequate performance could be achieved. Copy number would be related to the test/reference fluorescence ratio on the array targets, and genomic resolution could be determined by the map distance between the targets, or by the length of the cloned DNA segments. We describe here our implementation of array CGH. We demonstrate its ability to measure copy number with high precision in the human genome, and to analyse clinical specimens by obtaining new information on chromosome 20 aberrations in breast cancer.",1998,38,2316,82,2,25,42,63,87,113,162,185,206,211
a1c9051a645df0009820cc588004798429409a95,"A three‐dimensional counting rule and its integral test system, the disector, for obtaining unbiased estimates of the number of arbitrary particles in a specimen is presented. Used in combination with ordinary and recently developed stereological methods unbiased estimates of various mean particle sizes and the variance of particle volume are obtainable on sets of two parallel sections with a known separation. The same principle allows the unbiased estimation of the distribution of individual particle volumes in sets of serial sections.",1984,26,2384,88,1,8,11,14,29,31,48,60,62,80
0cf16c3770d1ce1b9b1b4d875741ba51252882f7,"Abstract Ti–Ni-based alloys are quite attractive functional materials not only as practical shape memory alloys with high strength and ductility but also as those exhibiting unique physical properties such as pre-transformation behaviors, which are enriched by various martensitic transformations. The paper starts from phase diagram, structures of martensites, mechanisms of martensitic transformations, premartensitic behavior, mechanism of shape memory and superelastic effects etc., and covers most of the fundamental issues related with the alloys, which include not only martensitic transformations but also diffusional transformations, since the latter greatly affect the former, and are useful to improve shape memory characteristics. Thus the alloy system will serve as an excellent case study of physical metallurgy, as is the case for steels where all kinds of phase transformations are utilized to improve the physical properties. In short this review is intended to give a self-consistent and logical account of key issues on Ti–Ni based alloys from physical metallurgy viewpoint on an up-to-date basis.",2005,345,2950,99,6,37,60,102,100,110,141,134,195,206
c7ca77863c289ecb4bcfdd7e329d90f792d707be,"Physical Metallurgy Principles is intended for use in an introductory course in physical metallurgy and is designed for all engineering students at the junior or senior level. The approach is largely theoretical, but covers all aspects of physical metallurgy and behavior of metals and alloys. The treatment used in this textbook is in harmony with a more fundamental approach to engineering education.",1972,0,2195,50,5,3,3,1,7,9,7,7,8,13
eb4ec95f5ea8aa0b7e54b3b0975494999b113595,"Preface. 1. Introduction. 1.1 Ni-base Alloy Classification. 1.2 History of Nickel and Ni-base Alloys. 1.3 Corrosion Resistance. 1.4 Nickel Alloy Production. 2. Alloying Additions, Phase Diagrams, and Phase Stability. 2.1 Introduction. 2.2 General Influence of Alloying Additions. 2.3 Phase Diagrams for Solid-Solution Alloys. 2.4 Phase Diagrams for Precipitation Hardened Alloys--gamma' Formers. 2.5 Phase Diagrams for Precipitation-Hardened Alloys--gamma"" Formers. 2.6 Calculated Phase Stability Diagrams. 2.7 PHACOMP Phase Stability Calculations. 3. Solid-Solution Strengthened Ni-base Alloys. 3.1 Standard Alloys and Consumables. 3.2 Physical Metallurgy and Mechanical Properties. 3.3 Welding Metallurgy. 3.4 Mechanical Properties of Weldments. 3.5 Weldability. 3.6 Corrosion Resistance. 3.7 Case Studies. 4. Precipitation Strengthened Ni-base Alloys. 4.1 Standard Alloys and Consumables. 4.2 Physical Metallurgy and Mechanical Properties. 4.3 Welding Metallurgy. 4.4 Mechanical Properties of Weldments. 4.5 Weldability. 5. Oxide Dispersion Strengthened Alloys and Nickel Aluminides. 5.1 Oxide Dispersion Strengthened Alloys. 5.2 Nickel Aluminide Alloys. 6. Repair Welding of Ni-base Alloys. 6.1 Solid-Solution Strengthened Alloys. 6.2 Precipitation Strengthened Alloys. 6.3 Single Crystal Superalloys. 7. Dissimilar Welding. 7.1 Application of Dissimilar Welds. 7.2 Influence of Process Parameters on Fusion Zone Composition. 7.3 Carbon, Low Alloys and Stainless Steels. 7.4 Postweld Heat Treatment Cracking in Stainless Steels Welded with Ni-base Filler Metals. 7.5 Super Austenitic Stainless Steels. 7.6 Dissimilar Welds in Ni-base Alloys - Effect on Corrosion Resistance. 7.7 9%Ni Steels. 7.8 Super Duplex Stainless Steels. 7.9 Case Studies. 8. Weldability Testing. 8.1 Introduction. 8.2 The Varestraint Test. 8.3 Modified Cast Pin Tear Test. 8.4 The Sigmajig Test. 8.5 The Hot Ductility Test. 8.6 The Strain-to-Fracture Test. 8.7 Other Weldability Tests. Appendix A Composition of Wrought and Cast Nickel-Base Alloys. Appendix B Composition of Nickel and Nickel Alloy Consumables. Appendix C Corrosion Acceptance Testing Methods. Appendix D Etching Techniques for Ni-base Alloys and Welds. Author Index. Subject Index.",2009,66,699,60,0,3,18,18,47,54,53,71,68,85
2524a882d157961d70b6106459aed3bb442b74c1,"Comprehensive information for the American aluminium industry Collective effort of 53 recognized experts on aluminium and aluminium alloys Joint venture by world renowned authorities-the Aluminium Association Inc. and American Society for Metals. The completely updated source of information on aluminium industry as a whole rather than its individual contributors. this book is an opportunity to gain from The knowledge of the experts working for prestigious companies such as Alcoa, Reynolds Metals Co., Alcan International Ltd., Kaiser Aluminium & Chemical Corp., Martin Marietta Laboratories and Anaconda Aluminium Co. It took four years of diligent work to complete this comprehensive successor to the classic volume, Aluminium, published by ASM in 1967. Contents: Properties of Pure Aluminum Constitution of Alloys Microstructure of Alloys Work Hardening Recovery, Recrystalization and Growth Metallurgy of Heat Treatment and General Principles of Precipitation Hardening Effects of Alloying Elements and Impurities on Properties Corrosion Behaviour Properties of Commercial Casting Alloys Properties of Commercial Wrought Alloys Aluminum Powder and Powder Metallurgy Products.",1984,0,1561,73,0,0,2,7,6,5,3,10,13,14
34058cc7431331f5af8cd63ac3c9df14765c7eac,Preface. 1. Introduction. 2. Phase Diagrams. 3. Alloying Elements and Constitution Diagrams. 4. Martensitic Stainless Steels. 5. Ferritic Stainless Steels. 6. Austenitic Stainless Steels. 7. Duplex Stainless Steels. 8. Precipitation-Hardening Stainless Steels. 9. Dissimilar Welding of Stainless Steels. 10. Weldability Testing. Appendix 1: Nominal Compositions of Stainless Steels. Appendix 2: Etching Techniques for Stainless Steel Welds. Author Index. Subject Index.,2005,0,1025,77,1,3,13,28,27,34,60,56,74,77
d8133fb8e17ed2a5809e03cc43c7914ea49f2b0d,"This practical reference provides thorough and systematic coverage on both basic metallurgy and the practical engineering aspects of metallic material selection and application. Contents includes: Practical information on the engineering properties and applications of steels, cast irons, nonferrous alloys, and metal matrix composites. Concise overviews and practical implications of metallic structure, imperfections, deformation, and phase transformations Process metallurgy of solidification and casting, recovery, recrystallization and grain growth, precipitation hardening Mechanical deformation during processing and in-service properties of fatigue, fracture, and creep. Physical properties and corrosion.",2008,0,567,50,0,1,4,12,16,42,52,57,74,68
eafe94460769dc1dca747eb9d9c9490a6d8ec12f,"Abstract The generation of zinc and zinc alloy coatings on steel is one of the commercially most important processing techniques used to protect steel components exposed to corrosive environments. From a technological standpoint, the principles of galvanizing have remained unchanged since this coating came into use over 200 years ago. However, because of new applications in the automotive and construction industry, a considerable amount of research has recently occurred on all aspects of the galvanizing process and on new types of Zn coatings. This review will discuss the metallurgy of zinc-coated steel from a scientific standpoint to develop relationships to practical applications. Hot-dip zinc coating methods, i.e. batch and continuous processes, will first be reviewed along with Fe–Zn phase equilibria and kinetics. Commercially, the addition of aluminum to the zinc bath results in three important types of coatings, galvanized, galfan and galvalume, and produces complex reactions at the coating/substrate interface. Fe–Zn–Al equilibrium will be reviewed in the light of recent studies of solubility and inhibition layer formation and breakdown. The effect of steel substrate composition on these reactions will also be critically analyzed. The overlay coating formation, or the coating alloy, is specifically chosen for its desired properties. The morphology of the galvanize, galfan and galvalume coating overlays will be reviewed, as well as the effect of heat treatment to produce a galvanneal coating. Finally, the effect of the microstructures of these coatings on the important properties of corrosion, formability, weldability and paintability will be discussed.",2000,64,1046,39,0,2,2,6,13,25,32,48,46,40
5c1af053ef5d859221eab1d0e56418b04d8f9249,"Skripta Fizikalna metalurgija I je sažeti prikaz znanosti o materijalima u kojoj se na znanstvenim i inženjerskim principima tumaci kristalna građa metala, dizajniranje legura i mikrostrukture, te odnos između strukture metala i njegovih mehanickih i fizickih svojstava.",2009,8,395,29,21,25,30,32,25,28,40,25,24,35
abbe5dee41d472e0468ae30e7bc6dc7d53284b2e,"This volume provides a substantial background to microalloyed steels with a wide selection of applications, some of which are very recent. A well-illustrated practical guide, this book acts as a useful source of data and a concise account of the theoretical aspects of the subject. Both academic institutions and the world-wide steel industry will find it indispensable.",1997,0,958,75,0,6,4,10,18,15,16,22,39,32
3e29d4c182490ea94be52b5593f7d218d4782358,"1. Stress and strain 2. Plasticity 3. Strain hardening 4. Plastic instability 5. Temperature and strain-rate dependence 6. Work balance 7. Slab analysis and friction 8. Friction and lubrication 9. Upper-bound analysis 10. Slip-line field analysis 11. Deformation zone geometry 12. Formability 13. Bending 14. Plastic anisotropy 15. Cupping, redrawing and ironing 16. Forming limit diagrams 17. Stamping 18. Hydroforming 19. Other sheet forming operations 20. Formability tests 21. Sheet metal properties.",1993,13,1086,42,9,12,9,14,15,15,9,16,20,17
6b09d20de5ccfcf15453d0f953147fd1e9079587,,1984,0,1147,69,0,0,3,0,2,5,9,8,7,9
c40cea80b4fa2e5f36a96c11f85d4b90d0a6816a,"AbstractA comprehensive review is presented of the extractive metallurgy of rare earths. The topics covered are: world rare earth resources and production; ore processing and separation of individual rare earths; reduction, refining, and ultrapurification of rare earth elements; methods for rare earth materials analysis; and a selection of the numerous rare earth applications. World rare earth reserves are abundant and would last for well beyond the next century. However, all of the 16 naturally occurring rare earth elements are not equally distributed in the ore minerals. This, compounded with the problems specific to the isolation and recovery of each of the rare earths, sets the stage for an unequal rare earth availability. The close chemical similarity of rare earths looses its importance when divergent physical properties determine the processes for rare earth element reduction and refining. The rare earth metals, alloys, and compounds have been as pure as could be determined. Finally, the commercial...",2004,562,805,42,1,2,1,3,10,5,9,15,22,39
67eefe022de61f0371da08207a5169db34029239,"MARK provides parameter estimates from marked animals when they are re-encountered at a later time as dead recoveries, or live recaptures or re-sightings. The time intervals between re-encounters do not have to be equal. More than one attribute group of animals can be modelled. The basic input to MARK is the encounter history for each animal. MARK can also estimate the size of closed populations. Parameters can be constrained to be the same across re-encounter occasions, or by age, or group, using the parameter index matrix. A set of common models for initial screening of data are provided. Time effects, group effects, time x group effects and a null model of none of the above, are provided for each parameter. Besides the logit function to link the design matrix to the parameters of the model, other link functions include the log—log, complimentary log—log, sine, log, and identity. The estimates of model parameters are computed via numerical maximum likelihood techniques. The number of parameters that are...",1999,52,6813,1642,20,35,63,151,157,217,215,282,313,371
a4760abdebd2c2d10fbb8cc3b16bd6061fb5d69f,The Committee for Research and Ethical Issues of the International Association for the Study of Pain (IASP®) is concerned with the ethical aspects of studies producing experimental pain and any suffering it may cause in animals. Such studies are essential if new and clinically relevant knowledge about the mechanisms of pain is to be acquired. Investigations in conscious animals intended to stimulate chronic pain in man are being performed. Such experiments require careful planning to avoid or at least minimize pain in the animals.,1983,4,7299,625,0,3,3,5,4,6,17,15,28,39
0c7f2f51c76aa75dec1807332fd133f73f77d43b,"The influence of diet on the distribution of nitrogen isotopes in animals was investigated by analyzing animals grown in the laboratory on diets of constant nitrogen isotopic composition. 
The isotopic composition of the nitrogen in an animal reflects the nitrogen isotopic composition of its diet. The δ^(15)N values of the whole bodies of animals are usually more positive than those of their diets. Different individuals of a species raised on the same diet can have significantly different δ^(15)N values. The variability of the relationship between the δ^(15)N values of animals and their diets is greater for different species raised on the same diet than for the same species raised on different diets. Different tissues of mice are also enriched in ^(15)N relative to the diet, with the difference between the δ^(15)N values of a tissue and the diet depending on both the kind of tissue and the diet involved. The δ^(15)N values of collagen and chitin, biochemical components that are often preserved in fossil animal remains, are also related to the δ^(15)N value of the diet. 
The dependence of the δ^(15)N values of whole animals and their tissues and biochemical components on the δ^(15)N value of diet indicates that the isotopic composition of animal nitrogen can be used to obtain information about an animal's diet if its potential food sources had different δ^(15)N values. The nitrogen isotopic method of dietary analysis probably can be used to estimate the relative use of legumes vs non-legumes or of aquatic vs terrestrial organisms as food sources for extant and fossil animals. However, the method probably will not be applicable in those modern ecosystems in which the use of chemical fertilizers has influenced the distribution of nitrogen isotopes in food sources. 
The isotopic method of dietary analysis was used to reconstruct changes in the diet of the human population that occupied the Tehuacan Valley of Mexico over a 7000 yr span. Variations in the δ^(15)C and δ^(15)N values of bone collagen suggest that C_4 and/or CAM plants (presumably mostly corn) and legumes (presumably mostly beans) were introduced into the diet much earlier than suggested by conventional archaeological analysis.",1978,56,5896,510,4,6,7,13,12,22,17,21,28,16
8e055c46c78ca7df9af63a32816ea1eef067064a,"The understanding of the dynamics of animal populations and of related ecological and evolutionary issues frequently depends on a direct analysis of life history parameters. For instance, examination of trade-offs between reproduction and survival usually rely on individually marked animals, for which the exact time of death is most often unknown, because marked individuals cannot be followed closely through time. Thus, the quantitative analysis of survival studies and experiments must be based on capture- recapture (or resighting) models which consider, besides the parameters of primary interest, recapture or resighting rates that are nuisance parameters. Capture-recapture models oriented to estimation of survival rates are the result of a recent change in emphasis from earlier approaches in which population size was the most important parameter, survival rates having been first introduced as nuisance parameters. This emphasis on survival rates in capture-recapture models developed rapidly in the 1980s and used as a basic structure the Cormack-Jolly-Seber survival model applied to an homogeneous group of animals, with various kinds of constraints on the model parameters. These approaches are conditional on first captures; hence they do not attempt to model the initial capture of unmarked animals as functions of population abundance in addition to survival and capture probabilities. This paper synthesizes, using a common framework, these recent developments together with new ones, with an emphasis on flexibility in modeling, model selection, and the analysis of multiple data sets. The effects on survival and capture rates of time, age, and categorical variables characterizing the individuals (e.g., sex) can be considered, as well as interactions between such effects. This ""analysis of variance"" philosophy emphasizes the structure of the survival and capture process rather than the technical characteristics of any particular model. The flexible array of models encompassed in this synthesis uses a common notation. As a result of the great level of flexibility and relevance achieved, the focus is changed from fitting a particular model to model building and model selection. The following procedure is recommended: (1) start from a global model compatible with the biology of the species studied and with the design of the study, and assess its fit; (2) select a more parsimonious model using Akaike's Information Criterion to limit the number of formal tests; (3) test for the most important biological questions by comparing this model with neighboring ones using likelihood ratio tests; and (4) obtain maximum likelihood estimates of model parameters with estimates of precision. Computer software is critical, as few of the models now available have parameter estimators that are in closed form. A comprehensive table of existing computer software is provided. We used RELEASE for data summary and goodness-of-fit tests and SURGE for iterative model fitting and the computation of likelihood ratio tests. Five increasingly complex examples are given to illustrate the theory. The first, using two data sets on the European Dipper (Cinclus cinclus), tests for sex-specific parameters,",1992,112,4145,462,12,16,18,53,44,65,73,95,83,104
695c252520246d8a76a8b785ef57e95ad838d565,"""Animals and Why They Matter"" examines the barriers that our philosophical traditions have erected between human beings and animals and reveals that the too-often ridiculed subject of animal rights is an issue crucially related to such problems within the human community as racism, sexism, and age discrimination. Mary Midgley's profound and clearly written narrative is a thought-provoking study of the way in which the opposition between reason and emotion has shaped our moral and political ideas and the problems it has raised. Whether considering vegetarianism, women's rights, or the ""humanity"" of pets, this book goes to the heart of the question of why all animals matter.",1984,0,478,15,1,1,2,3,2,5,31,6,3,4
a32728cf2a8baf408e3270da5c6ced7aba9e4068,"Abstract The practical analysis of space use and habitat selection by animals is often a problem due to the lack of well-designed programs. I present here the “adehabitat” package for the R software, which offers basic GIS (Geographic Information System) functions, methods to analyze radio-tracking data and habitat selection by wildlife, and interfaces with other R packages. These tools can be downloaded freely on the internet. Because the functions of this package can be combined with other functions of R, “adehabitat” provides a powerful environment for the analysis of the space and habitat use.",2006,22,2743,465,1,10,29,32,49,79,112,130,188,240
c375287977ae3f84f456ca93b8dee25375bf1a0c,"With a standard set of primers directed toward conserved regions, we have used the polymerase chain reaction to amplify homologous segments of mtDNA from more than 100 animal species, including mammals, birds, amphibians, fishes, and some invertebrates. Amplification and direct sequencing were possible using unpurified mtDNA from nanogram samples of fresh specimens and microgram amounts of tissues preserved for months in alcohol or decades in the dry state. The bird and fish sequences evolve with the same strong bias toward transitions that holds for mammals. However, because the light strand of birds is deficient in thymine, thymine to cytosine transitions are less common than in other taxa. Amino acid replacement in a segment of the cytochrome b gene is faster in mammals and birds than in fishes and the pattern of replacements fits the structural hypothesis for cytochrome b. The unexpectedly wide taxonomic utility of these primers offers opportunities for phylogenetic and population research.",1989,22,4670,214,2,15,41,43,60,97,90,96,110,126
9739f7c963191891f04d56d1138b93a6c815d141,"Over the past 100 years, the global average temperature has increased by approximately 0.6 °C and is projected to continue to rise at a rapid rate. Although species have responded to climatic changes throughout their evolutionary history, a primary concern for wild species and their ecosystems is this rapid rate of change. We gathered information on species and global warming from 143 studies for our meta-analyses. These analyses reveal a consistent temperature-related shift, or ‘fingerprint’, in species ranging from molluscs to mammals and from grasses to trees. Indeed, more than 80% of the species that show changes are shifting in the direction expected on the basis of known physiological constraints of species. Consequently, the balance of evidence from these studies strongly suggests that a significant impact of global warming is already discernible in animal and plant populations. The synergism of rapid temperature rise and other stresses, in particular habitat destruction, could easily disrupt the connectedness among species and lead to a reformulation of species communities, reflecting differential changes in species, and to numerous extirpations and possibly extinctions.",2003,46,4262,204,56,124,172,185,236,231,234,273,269,303
fab4e2bc033cb8bc1a5587b235184c66e4faea2b,"By Matthew James Keelingand Pejman RohaniPrinceton, NJ: Princeton University Press,2008.408 pp., Illustrated. $65.00 (hardcover).Mathematical modeling of infectious dis-eases has progressed dramatically over thepast 3 decades and continues to ﬂourishat the nexus of mathematics, epidemiol-ogy, and infectious diseases research. Nowrecognized as a valuable tool, mathemat-ical models are being integrated into thepublic health decision-making processmore than ever before. However, despiterapid advancements in this area, a formaltraining program for mathematical mod-eling is lacking, and there are very fewbooks suitable for a broad readership. Tosupport this bridging science, a commonlanguage that is understood in all con-tributing disciplines is required.",2007,39,3010,261,2,18,57,93,159,163,195,201,215,267
18d1dada692a5414b044d9570b6178639de5141d,,1996,597,11366,22,4,6,4,6,8,3,7,311,370,404
f96efe5d624b6357ec956e33481cabf84c93d446,"Toxoplasmosis is one of the more common parasitic zoonoses world-wide. Its causative agent, Toxoplasma gondii, is a facultatively heteroxenous, polyxenous protozoon that has developed several potential routes of transmission within and between different host species. If first contracted during pregnancy, T. gondii may be transmitted vertically by tachyzoites that are passed to the foetus via the placenta. Horizontal transmission of T. gondii may involve three life-cycle stages, i.e. ingesting infectious oocysts from the environment or ingesting tissue cysts or tachyzoites which are contained in meat or primary offal (viscera) of many different animals. Transmission may also occur via tachyzoites contained in blood products, tissue transplants, or unpasteurised milk. However, it is not known which of these routes is more important epidemiologically. In the past, the consumption of raw or undercooked meat, in particular of pigs and sheep, has been regarded as a major route of transmission to humans. However, recent studies showed that the prevalence of T. gondii in meat-producing animals decreased considerably over the past 20 years in areas with intensive farm management. For example, in several countries of the European Union prevalences of T. gondii in fattening pigs are now <1%. Considering these data it is unlikely that pork is still a major source of infection for humans in these countries. However, it is likely that the major routes of transmission are different in human populations with differences in culture and eating habits. In the Americas, recent outbreaks of acute toxoplasmosis in humans have been associated with oocyst contamination of the environment. Therefore, future epidemiological studies on T. gondii infections should consider the role of oocysts as potential sources of infection for humans, and methods to monitor these are currently being developed. This review presents recent epidemiological data on T. gondii, hypotheses on the major routes of transmission to humans in different populations, and preventive measures that may reduce the risk of contracting a primary infection during pregnancy.",2000,815,2953,243,2,10,32,40,43,55,81,86,107,152
f168482f78426e3baf262352fafd973e4f765b90,Introduction to resource selection studies. Examples of the use of resource selectory studies. Examples of the use of resource selection functions. Statistical modelling procedures. Studies with resources defined by several categories. Estimating a resource selection probability function from a census of resource units using logistic regression. Estimating a resource selection probability function from a census of resource units at several points in time using the proportional hazards model. Estimating a resource selection function from samples of resource units using proportional hazards and log-linear models. Estimating a resource selection function from two samples of resource units using logistic regression and discriminant function methods. General log-linear modelling. Analysis of the amount of use. The comparison of selection for different types of resource unit. References. Index.,1994,0,2940,295,6,4,3,15,15,26,35,30,41,53
c03e8924fe6e1e9da5b4a7dadee8d5d01ad15f6b,"There is good evidence that the complex microbial flora present in the gastrointestinal tract of all warm-blooded animals is effective in providing resistance to disease. However, the composition of this protective flora can be altered by dietary and environmental influences, making the host animal susceptible to disease and/or reducing its efficiency of food utilization. What we are doing with the probiotic treatments is re-establishing the natural condition which exists in the wild animal but which has been disrupted by modern trends in conditions used for rearing young animals, including human babies, and in modern approaches to nutrition and disease therapy. These are all areas where the gut flora can be altered for the worse and where, by the administration of probiotics, the natural balance of the gut microflora can be restored and the animal returned to its normal nutrition, growth and health status.",1989,84,3584,121,0,5,12,18,12,12,12,13,37,32
bf7defd7918575b38a6dffa77f71d09ac4270cab,"Small RNAs of 20–30 nucleotides can target both chromatin and transcripts, and thereby keep both the genome and the transcriptome under extensive surveillance. Recent progress in high-throughput sequencing has uncovered an astounding landscape of small RNAs in eukaryotic cells. Various small RNAs of distinctive characteristics have been found and can be classified into three classes based on their biogenesis mechanism and the type of Argonaute protein that they are associated with: microRNAs (miRNAs), endogenous small interfering RNAs (endo-siRNAs or esiRNAs) and Piwi-interacting RNAs (piRNAs). This Review summarizes our current knowledge of how these intriguing molecules are generated in animal cells.",2009,224,2985,178,94,228,276,313,368,300,278,223,256,202
c760865d5250effb18e3b4a761482413cc96426e,,1993,0,2299,400,0,2,2,4,5,6,9,8,7,50
d91dcb167ec9c097d1ff87ba1c2beaf3973a3869,"T.B. Farver, Concepts of Normality in Clinical Biochemistry. J.G. Hauge, DNA Technology in Diagnosis, Breeding, and Therapy. J.J. Kaneko, Carbohydrate Metabolism and Its Diseases. M.L. Bruss, Lipids and Ketones. J.J. Kaneko, Serum Proteins and the Dysproteinemias. L.J. Gershwin, Clinical Immunology. J.W. Harvey, The Erythrocyte: Physiology, Metabolism, and Biochemical Disorders. J.J. Kaneko, Porphyrins and the Porhyrias. J.E. Smith, Iron Metabolism and Its Disorders. W.J. Dodds, Hemostasis. J.G. Zinkl and M.B. Kabbur, Neutrophil Function. J.W. Kramer and W.E. Hoffmann, Clinical Enzymology. B.C. Tennant, Hepatic Function. D.F. Brobst, Pancreatic Function. W.E. Hornbuckle and B.C. Tennant, Gastrointestinal Function. G.H. Cardinet III, Skeletal Muscle Function. D.R. Finco, Kidney Function. G.P. Carlson, Fluid, Electrolyte, and Acid-Base Balance. J.A. Mol and A. Rijnberk, Pituitary Function. A. Rijnberk and J.A. Mol, Adrenocortical Function. J.J. Kaneko, Thyroid Function. L-E. Edqvist and M. Forsberg, Clinical Reproductive Endocrinology. T.J. Rosol and C.C. Capen, Calcium-Regulating Hormones and Diseases of Abnormal Mineral (Calcium, Phosphorus, Magnesium) Metabolism. R.B. Rucker and J.G. Morris, The Vitamins. M. Haskins and U. Giger, Lysosomal Storage Diseases. B.R. Madewell, Tumor Markers. C.S. Bailey and W. Vernau, Cerebrospinal Fluid. J.R. Turk and S.W. Casteel, Clinical Biochemistry in Toxicology. W.F. Loeb, Clinical Biochemistry of Laboratory Rodents and Rabbits. J.T. Lumeij, Avian Clinical Biochemistry. Appendixes. Index.",1963,0,3627,199,1,2,0,2,2,3,2,2,6,7
129169f853b1e71e05d97fe6ff733d7cd57b22d8,"This paper summarizes the current views on coping styles as a useful concept in understanding individual adaptive capacity and vulnerability to stress-related disease. Studies in feral populations indicate the existence of a proactive and a reactive coping style. These coping styles seem to play a role in the population ecology of the species. Despite domestication, genetic selection and inbreeding, the same coping styles can, to some extent, also be observed in laboratory and farm animals. Coping styles are characterized by consistent behavioral and neuroendocrine characteristics, some of which seem to be causally linked to each other. Evidence is accumulating that the two coping styles might explain a differential vulnerability to stress mediated disease due to the differential adaptive value of the two coping styles and the accompanying neuroendocrine differentiation.",1999,102,2410,262,0,5,24,25,51,46,62,51,91,72
5ec88f2510deece23ceaaffaa0ef515abc3d67ec,"When you did me the honor of asking me to fill your presidential chair, I accepted perhaps without duly considering the duties of the president of a society, founded largely to further the study of evolution, at the close of the year that marks the centenary of Darwin and Wallace's initial presentation of the theory of natural selection. It seemed to me that most of the significant aspects of modern evolutionary theory have come either from geneticists, or from those heroic museum workers who suffering through years of neglect, were able to establish about 20 years ago what has come to be called the ""new systematics."" You had, however, chosen an ecologist as your president and one of that school at times supposed to study the environment without any relation to the organism. A few months later I happened to be in Sicily. An early interest in zoogeography and in aquatic insects led me to attempt to collect near Palermo, certain species of water-bugs, of the genus Cprixa, described a century ago by Fieber and supposed to occur in the region, but never fully reinvestigated. It is hard to find suitable localities in so highly cultivated a landscape as the Concha d'Oro. Fortunately, I was driven up Monte Pellegrino, the hill that rises to the west of the city, to admire the view. A little below the summit, a church with a simple baroque facade stands in front of a cave in the limestone of the hill. Here in the 16th century a stalactite encrusted skeleton associated with a cross and twelve beads was discovered. Of this skeleton nothing is certainly known save that it is that of Santa Rosalia, a saint of whom little is reliably reported save that she seems to have lived in the 12th century, that her skeleton was found in this cave, and that she has been the chief patroness of Palermo ever since. Other limestone caverns on Monte Pellegrino had yielded bones of extinct Pleistocene Equus, and on the walls of one of the rock shelters at the bottom of the hill there are beautiful Gravettian engravings. Moreover, a small relic of the saint that I saw in the treasury of the Cathedral of Monreale has a venerable and *Address of the President, American Society of Naturalists, delivered at the annual meeting, Washington, D. C., December 30, 1958.",1959,33,3684,152,1,2,2,7,1,7,4,10,8,11
f22304fc493c3057d99df4fe35d8cc549a99e6b1,"SUMMARY Various gram-negative animal and plant pathogens use a novel, sec-independent protein secretion system as a basic virulence mechanism. It is becoming increasingly clear that these so-called type III secretion systems inject (translocate) proteins into the cytosol of eukaryotic cells, where the translocated proteins facilitate bacterial pathogenesis by specifically interfering with host cell signal transduction and other cellular processes. Accordingly, some type III secretion systems are activated by bacterial contact with host cell surfaces. Individual type III secretion systems direct the secretion and translocation of a variety of unrelated proteins, which account for species-specific pathogenesis phenotypes. In contrast to the secreted virulence factors, most of the 15 to 20 membrane-associated proteins which constitute the type III secretion apparatus are conserved among different pathogens. Most of the inner membrane components of the type III secretion apparatus show additional homologies to flagellar biosynthetic proteins, while a conserved outer membrane factor is similar to secretins from type II and other secretion pathways. Structurally conserved chaperones which specifically bind to individual secreted proteins play an important role in type III protein secretion, apparently by preventing premature interactions of the secreted factors with other proteins. The genes encoding type III secretion systems are clustered, and various pieces of evidence suggest that these systems have been acquired by horizontal genetic transfer during evolution. Expression of type III secretion systems is coordinately regulated in response to host environmental stimuli by networks of transcription factors. This review comprises a comparison of the structure, function, regulation, and impact on host cells of the type III secretion systems in the animal pathogens Yersinia spp., Pseudomonas aeruginosa, Shigella flexneri, Salmonella typhimurium, enteropathogenic Escherichia coli, and Chlamydia spp. and the plant pathogens Pseudomonas syringae, Erwinia spp., Ralstonia solanacearum, Xanthomonas campestris, and Rhizobium spp.",1998,545,2427,189,12,119,176,162,140,184,191,167,123,97
303e936f367602ee3556fefafb7a41ee65ce1738,,1993,325,2873,72,4,11,21,19,29,36,44,40,51,63
561d6920bfe33e087fd9d411d873e83a5e71872b,,1959,0,3656,9,1,3,5,3,3,4,4,8,5,5
62c1699c60cd032cbbec64d9125beff2788e8465,"As I have pointed out earlier, when I met Oliver Zangwill in 1961 at a meeting on dyslexia in Baltimore, he listened patiently to the exposition of my ideas on the significance of the cortico-cortical connections for the higher functions. A short time later, while on a trip to Boston, he suggested to me that I should prepare an extended account of these ideas. This paper would never have been written without Professor Zangwill’s urging, and I am grateful to him for having brought me to a more careful review of the older literature and a more precise statement of my own ideas. Although Russell Brain, who was then the editor of Brain, had some misgivings about the section on philosophical implications he agreed to take the manuscript unchanged.",1965,127,3000,60,1,5,11,11,21,20,19,19,17,27
b19d5c577dfd6383e5a4a2be98e1ef631c6ec622,"The Guide for the Care and Use of Laboratory Animals by the Institute for Laboratory Animal Research (ILAR) of the National Research Council in the USA, is well known among most individuals involved in laboratory animal care and use. Most of the time insiders refer to it as the ‘Guide’. In the year 2011, ILAR published its eighth edition. This new edition gathered the latest facts, incorporated up-to-date knowledge and reorganized the contents to provide better guidance to laboratory animal care and use programmes. The history of the Guide began in 1946, when Dr Nathan R Brewer and his colleagues in the Chicago area started to improve the care and wellbeing of laboratory animals by exchanging ideas at monthly meetings. The activities of this group led, in 1950, to the foundation of the Animal Care Panel (ACP), which became a growing non-profit organization and was later renamed as American Association for Laboratory Animal Science (AALAS). In 1963, the Animal Facilities Standards Committee of the ACP prepared the first edition of the Guide for the Care and Use of Laboratory Animals. Gradually, the Guide has become the primary reference in many research organizations in the USA, and compliance with it is obligatory for the Public Health Service (PHS)-assured institutions. When the Association for the Assessment and Accreditation for Laboratory Animal Care (AAALAC International) expanded its activities beyond the USA and went international, the Guide became a resource for animal care and use programmes around the world. Today the Guide is one of three primary standards AAALAC uses to evaluate an institution’s animal care and use programme. The other two documents are the Guide for the Care and Use of Agricultural Animals in Research and Teaching (Ag Guide), FASS 2010; and the European Convention for the Protection of Vertebrate Animals Used for Experimental and Other Scientific Purposes, Council of Europe (ETS 123). Over time, the Guide was updated several times culminating in the seventh edition being published in 1996. In the last decade, laboratory animal science advanced so significantly that another update was considered necessary to promote the best animal care and use practices. In 2006, a committee was appointed by the National Research Council in the USA, and started the process of updating the Guide. This process was accompanied by extensive public hearings and solicited comments from a wide range of scientific communities and the public. Fifteen years after its seventh edition, the new eighth edition of the Guide was finally completed and published. The Guide is not a handbook; it is an extensive collection of detailed descriptions of standards for all components of a good laboratory animal care and use programme. The frame of the book not only focuses on the wellbeing of lab animals, but also on the health and safety of people working with animals. Compared with the previous edition, the eighth edition is organized differently. After a brief overview, the Guide is divided into five chapters covering details of recommended standards for the care and use of laboratory animals. In addition, extra information related to the Guide can be found in the appendices at the end of the book. The first chapter introduces and defines the key concepts and terms used in the Guide. It describes the goals of the Guide as well as the intended audience and how to use the Guide. The overall intention is to support the readers to build a programme which creates a system of selfregulation and regulatory oversight, a concept that has been proven of value in many research situations. The concept of the 3Rs (Replacement, Reduction, Refinement) was always part of the philosophy of the Guide. In this eighth version now, this concept is mentioned expressis verbis and the individual definitions of each ‘R’ are outlined. Chapter 2 highlights the components of a state of the art animal care and use programme. After a short summary of the programme management, the chapter defines in details the respective roles and responsibilities of programme oversight. In the past, the primary responsibility of programme oversight fell primarily on the Animal Care and Use Committee (IACUC); now it is shared with the institutional official (IO) and the attending veterinarian (AV). The chapter further defines the roles and responsibilities of the key management for all elements of the programme and supplies definitions for regulations and policies. Many recommendations on occupational health and safety are provided here. The need for a disaster plan is now changed from the ‘should’ to the ‘must’ requirement. Environment, housing and management of laboratory animals are the topics of the third chapter. It should be noted that, in this chapter, a new section was added for addressing the care and use of aquatic animals. With this, the authors acknowledge the increased importance of these species, especially zebrafishes, in the laboratory environment. As in the previous editions, this chapter provides well-organized tables for quick references for housing space for species typically used in research. It is important to mention that the Guide stresses in particular for this topic the uses of a performance-based approach to decide on the space requirements for each particular case. Therefore, the recommended space is now defined as ‘recommended minimum space’. The next chapter discusses issues related to veterinary care. It covers regular aspects of veterinary care in laboratory animals, such as acquisition and clinical care of animals, surgery, pain management and anaesthesia, and preventive medicine. The Guide gives a lot of importance",1979,0,2725,40,0,0,0,3,1,0,2,5,2,1
1e6a25cb1d49fefd81cf2af3e7e2771558d73cba,"A novel coronavirus (SCoV) is the etiological agent of severe acute respiratory syndrome (SARS). SCoV-like viruses were isolated from Himalayan palm civets found in a live-animal market in Guangdong, China. Evidence of virus infection was also detected in other animals (including a raccoon dog, Nyctereutes procyonoides) and in humans working at the same market. All the animal isolates retain a 29-nucleotide sequence that is not found in most human isolates. The detection of SCoV-like viruses in small, live wild mammals in a retail market indicates a route of interspecies transmission, although the natural reservoir is not known.",2003,22,1970,75,28,177,143,130,86,82,60,50,47,45
ef216b547c246c1c9b6c842c51274fe0dd54cbd2,"Many studies have described the effects of urbanization on species richness. These studies indicate that urbanization can increase or decrease species richness, depending on several variables. Some of these variables include: taxonomic group, spatial scale of analysis, and intensity of urbanization. Recent reviews of birds (the most-studied group) indicate that species richness decreases with increasing urbanization in most cases but produces no change or even increases richness in some studies. Here I expand beyond the bird studies by reviewing 105 studies on the effects of urbanization on the species richness of non-avian species: mammals, reptiles, amphibians, invertebrates and plants. For all groups, species richness tends to be reduced in areas with extreme urbanization (i.e., central urban core areas). However, the effects of moderate levels of urbanization (i.e., suburban areas) vary significantly among groups. Most of the plant studies (about 65%) indicate increasing species richness with moderate urbanization whereas only a minority of invertebrate studies (about 30%) and a very small minority of non-avian vertebrate studies (about 12%) show increasing species richness. Possible explanations for these results are discussed, including the importance of nonnative species importation, spatial heterogeneity, intermediate disturbance and scale as major factors influencing species richness.",2008,153,1581,100,1,17,30,53,64,71,110,140,161,185
798657e6e98fc7958a4b9dee079c96a8d5c23e1d,"Helminths, arthropods, & protozoa of domesticated animals , Helminths, arthropods, & protozoa of domesticated animals , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1969,0,2522,102,1,0,1,2,2,4,2,2,4,3
f07f33137788f9351fe8ee6d47e70d9674b6896c,"The most commonly recognized behavioral patterns of animals and people at the onset of febrile infectious diseases are lethargy, depression, anorexia, and reduction in grooming. Findings from recent lines of research are reviewed to formulate the perspective that the behavior of sick animals and people is not a maladaptive response or the effect of debilitation, but rather an organized, evolved behavioral strategy to facilitate the role of fever in combating viral and bacterial infections. The sick individual is viewed as being at a life or death juncture and its behavior is an all-out effort to overcome the disease.",1988,206,1786,118,0,1,3,6,7,7,14,20,17,38
e4b58ccd4385a6f4d9b97237d68a409b10b1c22a,"The arthropods constitute the most diverse animal group, but, despite their rich fossil record and a century of study, their phylogenetic relationships remain unclear1. Taxa previously proposed to be sister groups to the arthropods include Annelida, Onychophora, Tardigrada and others, but hypotheses of phylogenetic relationships have been conflicting2,3. For example, onychophorans, like arthropods, moult periodically, have an arthropod arrangement of haemocoel1,4, and have been related to arthropods in morphological and mitochondrial DNA sequence analyses4,5. Like annelids, they possess segmental nephridia and muscles that are a combination of smooth and obliquely striated fibres6. Our phylogenetic analysis of 18S ribosomal DNA sequences indicates a close relationship between arthropods, nematodes and all other moulting phyla. The results suggest that ecdysis (moulting) arose once and support the idea of a new clade, Ecdysozoa, containing moulting animals: arthropods, tardigrades, onychophorans, nematodes, nematomorphs, kinor-hynchs and priapulids. No support is found for a clade of segmented animals, the Articulata, uniting annelids with arthropods. The hypothesis that nematodes are related to arthropods has important implications for developmental genetic studies using as model systems the nematode Caenorhabditis elegans and the arthropod Drosophila melanogaster, which are generally held to be phylogenetically distant from each other.",1997,25,1463,113,4,59,74,100,94,95,72,96,91,80
f18b42ff20acf234ca57b36ad9acc97b655ba3ea,"Development of methods that allow an efficient expression of exogenous genes in animals would provide tools for gene function studies, treatment of diseases and for obtaining gene products. Therefore, we have developed a hydrodynamics-based procedure for expressing transgenes in mice by systemic administration of plasmid DNA. Using cDNA of luciferase and β-galactosidase as a reporter gene, we demonstrated that an efficient gene transfer and expression can be achieved by a rapid injection of a large volume of DNA solution into animals via the tail vein. Among the organs expressing the transgene, the liver showed the highest level of gene expression. As high as 45 μg of luciferase protein per gram of liver can be achi- eved by a single tail vein injection of 5 μg of plasmid DNA into a mouse. Histochemical analysis using β-galactosidase gene as a reporter reveals that approximately 40% of hepatocytes express the transgene. The time–response curve shows that the level of transgene expression in the liver reaches the peak level in approximately 8 h after injection and decreases thereafter. The peak level of gene expression can be regained by repeated injection of plasmid DNA. These results suggest that a simple, convenient and efficient method has been developed and which can be used as an effective means for studying gene function, gene regulation and molecular pathophysiology through gene transfer, as well as for expressing proteins in animals.",1999,28,1608,72,0,14,39,48,80,70,90,94,98,77
57baceddb3aa4c1744d11c468c47160796c373a8,VOLUME 1 Chapter 1 Bones and joints Chapter 2 Muscle and tendon Chapter 3 The nervous system Chapter 4 The eye and ear Chapter 5 The skin and appendages VOLUME 2 Chapter 1 Alimentary and peritoneum Chapter 2 The liver and biliary system Chapter 3 The pancreas Chapter 4 The urinary system Chapter 5 The respiratory system VOLUME 3 Chapter 1 The cardiovascular system Chapter 2 The hematopoietic system Chapter 3 The endocrine glands Chapter 4 The female genital system Chapter 5 The male genital system,1970,0,2313,42,19,16,25,16,21,22,24,15,23,23
5d9dfa842ee30f3a308737d1a422551c94e8c709,"This report summarizes the results of a multinational pharmaceutical company survey and the outcome of an International Life Sciences Institute (ILSI) Workshop (April 1999), which served to better understand concordance of the toxicity of pharmaceuticals observed in humans with that observed in experimental animals. The Workshop included representatives from academia, the multinational pharmaceutical industry, and international regulatory scientists. The main aim of this project was to examine the strengths and weaknesses of animal studies to predict human toxicity (HT). The database was developed from a survey which covered only those compounds where HTs were identified during clinical development of new pharmaceuticals, determining whether animal toxicity studies identified concordant target organ toxicities in humans. Data collected included codified compounds, therapeutic category, the HT organ system affected, and the species and duration of studies in which the corresponding HT was either first identified or not observed. This survey includes input from 12 pharmaceutical companies with data compiled from 150 compounds with 221 HT events reported. Multiple HTs were reported in 47 cases. The results showed the true positive HT concordance rate of 71% for rodent and nonrodent species, with nonrodents alone being predictive for 63% of HTs and rodents alone for 43%. The highest incidence of overall concordance was seen in hematological, gastrointestinal, and cardiovascular HTs, and the least was seen in cutaneous HT. Where animal models, in one or more species, identified concordant HT, 94% were first observed in studies of 1 month or less in duration. These survey results support the value of in vivo toxicology studies to predict for many significant HTs associated with pharmaceuticals and have helped to identify HT categories that may benefit from improved methods.",2000,37,1596,43,7,5,18,9,20,28,25,36,44,62
20dddc153678a64917edca0b0091b3f31839763e,"It is difficult to review reprints of the classical titles. Even more so, to review a book written by somebody about whom the foreword says in the first sentence: ""Charles Elton was a founder of ecology ..."". In addition, this new edition of a fundamental ecological work, first published in 1958, already contains a very competent review of itself this is represented by the foreword written by Daniel Simberloff. Simberloff's text is therefore, ironically, more interesting to review than the book itself everything seems to have been said about the book, and you could hardly find a monograph in the field that does not cite it. Everybody claims it as inspirational and still timely. Let me therefore comment rather on the foreword instead (or better say, on the book through the viewpoint of the foreword). By showing what Elton got right (most of the issue) and what he did not (much less), it provides us with a bright analysis of the four decades of development in the field of biological invasions. Elton's interest in the issue started with an article in the Times in 1933 and developed through a series of broadcasts. The examples he used in the book remain timely this is actually an indication of how serious a problem it is if almost all noxious invaders of the 1950s are still with us. To mention that introduced species are believed to be the second greatest cause of species extinctions and endangerment, following habitat destruction, is like carrying coals to Newcastle. Elton himself lamented on the low level of predictiveness generally associated with biological invasions. However, Simbedoff points out that ""he was prophetic"". Is this not an argument against definite scepticism and, at the same time, an indication that, to some extent, predictions is possible? Provided that it is made by a bright enough mind and brilliant observer, then maybe. Elton clearly foresaw some pattems and generalities which have been confirmed and statistically proven since then. The only widely criticized opinion is his belief in the importance of native species richness and its effect on invasibility. There are other issues which he ignored or did not recognize their importance such as propagule pressure, hybridization of native species with aliens and associated genetical effects, and evolutionary consequences. This does not make his work less valuable or influential it just means that he left us with a lot of problems to study. The book, a foundation stone of invasion biology, put a strong emphasis on animals; plants are obviously the second here. It is a credit to plant ecologists worldwide that they were able to cope with this handicap. At present, the generalizing theories attempting to explain the mechanisms underlying invasions are probably more developed for plants than for animals. ""Anyone wishing to understand the problem, from ordinary citizens through specialized researchers can profit from reading (or rereading) The Ecology of invasions,"" concludes Daniel Simberloff. I would like to add that to produce this new edition was an excellent idea. I strongly suspect that this was one of those books that everybody cites but quite a few of those who do that have never read it. Its the classics, you know everything is supposed to be in there so why read it? Now the new generation of invasion biologists can have it on their library shelves. Just go and get it.",1958,213,2095,103,0,0,1,0,2,1,3,1,5,1
881d8db3d0c1671f6dfc0fa7180747af7350fd1b,"AbstractIt is well documented that animals take risk of predation into account when making decisions about how to behave in particular situations, often trading-off risk against opportunities for mating or acquiring energy. Such an ability implies that animals have reliable information about the risk of predation at a given place and time. Chemosensory cues are an important source of such information. They reliably reveal the presence of predators (or their presence in the immediate past) and may also provide information on predator activity level and diet. In certain circumstances (e.g., in the dark, for animals in hiding) they may be the only cues available. Although a vast literature exists on the responses of prey to predator chemosensory cues (or odours), these studies are widely scattered, from marine biology to biological control, and not well known or appreciated by behavioural ecologists. In this paper, we provide an exhaustive review of this literature, primarily in tabular form. We highlight so...",1998,273,1321,81,6,9,30,51,43,50,65,59,63,68
029ea9d26e90ddf18a3e99caf24c021189363e34,"The worldwide contamination of foods and feeds with mycotoxins is a significant problem. Mycotoxins are secondary metabolites of molds that have adverse effects on humans, animals, and crops that result in illnesses and economic losses. Aflatoxins, ochratoxins, trichothecenes, zearelenone, fumonisins, tremorgenic toxins, and ergot alkaloids are the mycotoxins of greatest agro-economic importance. Some molds are capable of producing more than one mycotoxin and some mycotoxins are produced by more than one fungal species. Often more than one mycotoxin is found on a contaminated substrate. Factors influencing the presence of mycotoxins in foods or feeds include environmental conditions related to storage that can be controlled. Other extrinsic factors such as climate or intrinsic factors such as fungal strain specificity, strain variation, and instability of toxigenic properties are more difficult to control. Mycotoxins have various acute and chronic effects on humans and animals (especially monogastrics) depending on species and susceptibility of an animal within a species. Ruminants have, however, generally been more resistant to the adverse effects of mycotoxins. This is because the rumen microbiota is capable of degrading mycotoxins. The economic impact of mycotoxins include loss of human and animal life, increased health care and veterinary care costs, reduced livestock production, disposal of contaminated foods and feeds, and investment in research and applications to reduce severity of the mycotoxin problem. Although efforts have continued internationally to set guidelines to control mycotoxins, practical measures have not been adequately implemented.",2001,220,1446,57,0,9,22,29,28,45,48,65,72,54
fe58fc4efe9ab80a86037e0878663f011718e89b,"The stable nitrogen and carbon isotope ratios of bone collagen prepared from more than 100 animals representing 66 species of birds, fish, and mammals are presented. The δ15N values of bone collagen from animals that fed exclusively in the marine environment are, on average, 9%. more positive than those from animals that fed exclusively in the terrestrial environment; ranges for the two groups overlap by less than 1%. Bone collagen δ15N values also serve to separate marine fish from the small number of freshwater fish we analyzed. The bone collagen δ15N values of birds and fish that spent part of their life cycles feeding in the marine environment and part in the freshwater environment are intermediate between those of animals that fed exclusively in one or the other system. Further, animals that fed at successive trophic levels in the marine and terrestrial environment are separated, on average, by a 3%. difference in the δ15N values of their bone collagen. Specifically, carnivorous and herbivorous terrestrial animals have mean δ15N values for bone collagen of + 8.0 and + 5.3%., respectively. Among marine animals, those that fed on fish have a mean δ15N value for bone collagen of + 16.5%., whereas those that fed on invertebrates have a mean δ15N value of + 13.3%. These results support previous suggestions of a 3%. enrichment in δ15N values at each successively higher trophic level. In contrast to the results for δ15N values, the ranges of bone collagen δ13C values from marine and terrestrial feeders overlap to a great extent. Additionally, bone collagen δ13C values do not reflect the trophic levels at which the animals fed. These results indicate that bone collagen δ15N values will be useful in determining relative dependence on marine and terrestrial food sources and in investigating trophic level relationships among different animal species within an ecosystem. This approach should be applicable to animals represented by prehistoric or fossilized bone in which collagen is preserved.",1984,76,1691,95,0,7,11,9,12,12,10,11,9,10
6eef363c38186ba6a52cc7134446a5d34eff6ae6,,1954,0,2424,26,0,3,7,8,11,11,8,16,15,11
e47f5da58f0276bddd54b9575938e6cbad65a31d,"The medicinal properties of curcumin obtained from Curcuma longa L. cannot be utilised because of poor bioavailability due to its rapid metabolism in the liver and intestinal wall. In this study, the effect of combining piperine, a known inhibitor of hepatic and intestinal glucuronidation, was evaluated on the bioavailability of curcumin in rats and healthy human volunteers. When curcumin was given alone, in the dose 2 g/kg to rats, moderate serum concentrations were achieved over a period of 4 h. Concomitant administration of piperine 20 mg/kg increased the serum concentration of curcumin for a short period of 1-2 h post drug. Time to maximum was significantly increased (P < 0.02) while elimination half life and clearance significantly decreased (P < 0.02), and the bioavailability was increased by 154%. On the other hand in humans after a dose of 2 g curcumin alone, serum levels were either undetectable or very low. Concomitant administration of piperine 20 mg produced much higher concentrations from 0.25 to 1 h post drug (P < 0.01 at 0.25 and 0.5 h; P < 0.001 at 1 h), the increase in bioavailability was 2000%. The study shows that in the dosages used, piperine enhances the serum concentration, extent of absorption and bioavailability of curcumin in both rats and humans with no adverse effects.",1998,19,1471,42,1,3,3,7,3,13,13,19,26,28
3d96555276b099a61d0040004a40188fee05d419,"Increasing concern about the impacts of global warming on biodiversity has stimulated extensive discussion, but methods to translate broad-scale shifts in climate into direct impacts on living animals remain simplistic. A key missing element from models of climatic change impacts on animals is the buffering influence of behavioral thermoregulation. Here, we show how behavioral and mass/energy balance models can be combined with spatial data on climate, topography, and vegetation to predict impacts of increased air temperature on thermoregulating ectotherms such as reptiles and insects (a large portion of global biodiversity). We show that for most “cold-blooded” terrestrial animals, the primary thermal challenge is not to attain high body temperatures (although this is important in temperate environments) but to stay cool (particularly in tropical and desert areas, where ectotherm biodiversity is greatest). The impact of climate warming on thermoregulating ectotherms will depend critically on how changes in vegetation cover alter the availability of shade as well as the animals' capacities to alter their seasonal timing of activity and reproduction. Warmer environments also may increase maintenance energy costs while simultaneously constraining activity time, putting pressure on mass and energy budgets. Energy- and mass-balance models provide a general method to integrate the complexity of these direct interactions between organisms and climate into spatial predictions of the impact of climate change on biodiversity. This methodology allows quantitative organism- and habitat-specific assessments of climate change impacts.",2009,41,795,55,10,33,33,45,73,87,62,68,88,88
9e9af5b393fad67bb98a3e11674f75d3238e2aef,,1969,0,1605,247,2,2,0,1,1,0,0,1,1,1
419e738bba85a8061553b7ae7a691062fba69065,"▪ Abstract The functional causes of life history trade-offs have been a topic of interest to evolutionary biologists for over six decades. Our review of life history trade-offs discusses conceptual issues associated with physiological aspects of trade-offs, and it describes recent advances on this topic. We focus on studies of four model systems: wing polymorphic insects, Drosophila, lizards, and birds. The most significant recent advances have been: (a) incorporation of genetics in physiological studies of trade-offs, (b) integration of investigations of nutrient input with nutrient allocation, (c) development of more sophisticated models of resource acquisition and allocation, (d) a shift to more integrated, multidisciplinary studies of intraspecific trade-offs, and (e) the first detailed investigations of the endocrine regulation of life history trade-offs.",2001,209,1283,57,7,10,22,37,34,38,57,53,49,64
9cc17e349f73c6fe769ec8eb14eeb100857d70c0,"Summary:  Innate immunity constitutes the first line of defense against attempted microbial invasion, and it is a well‐described phenomenon in vertebrates and insects. Recent pioneering work has revealed striking similarities between the molecular organization of animal and plant systems for nonself recognition and anti‐microbial defense. Like animals, plants have acquired the ability to recognize invariant pathogen‐associated molecular patterns (PAMPs) that are characteristic of microbial organisms but which are not found in potential host plants. Such structures, also termed general elicitors of plant defense, are often indispensable for the microbial lifestyle and, upon receptor‐mediated perception, inevitably betray the invader to the plant's surveillance system. Remarkable similarities have been uncovered in the molecular mode of PAMP perception in animals and plants, including the discovery of plant receptors resembling mammalian Toll‐like receptors or cytoplasmic nucleotide‐binding oligomerization domain leucine‐rich repeat proteins. Moreover, molecular building blocks of PAMP‐induced signaling cascades leading to the transcriptional activation of immune response genes are shared among the two kingdoms. In particular, nitric oxide as well as mitogen‐activated protein kinase cascades have been implicated in triggering innate immune responses, part of which is the production of anti‐microbial compounds. In addition to PAMP‐mediated pathogen defense, disease resistance programs are often initiated upon plant‐cultivar‐specific recognition of microbial race‐specific virulence factors, a recognition specificity that is not known from animals.",2004,167,1174,84,8,41,87,110,103,88,92,75,74,70
42bbd01ef11a27598bfca4d491bb39026d2e6b20,"A clear picture of animal relationships is a prerequisite to understand how the morphological and ecological diversity of animals evolved over time. Among others, the placement of the acoelomorph flatworms, Acoela and Nemertodermatida, has fundamental implications for the origin and evolution of various animal organ systems. Their position, however, has been inconsistent in phylogenetic studies using one or several genes. Furthermore, Acoela has been among the least stable taxa in recent animal phylogenomic analyses, which simultaneously examine many genes from many species, while Nemertodermatida has not been sampled in any phylogenomic study. New sequence data are presented here from organisms targeted for their instability or lack of representation in prior analyses, and are analysed in combination with other publicly available data. We also designed new automated explicit methods for identifying and selecting common genes across different species, and developed highly optimized supercomputing tools to reconstruct relationships from gene sequences. The results of the work corroborate several recently established findings about animal relationships and provide new support for the placement of other groups. These new data and methods strongly uphold previous suggestions that Acoelomorpha is sister clade to all other bilaterian animals, find diminishing evidence for the placement of the enigmatic Xenoturbella within Deuterostomia, and place Cycliophora with Entoprocta and Ectoprocta. The work highlights the implications that these arrangements have for metazoan evolution and permits a clearer picture of ancestral morphologies and life histories in the deep past.",2009,74,682,65,4,62,68,67,70,70,86,71,56,40
eaf811e6e91c687c4026d7f0f096c3409af089d5,"Recent advances in integrative studies of locomotion have revealed several general principles. Energy storage and exchange mechanisms discovered in walking and running bipeds apply to multilegged locomotion and even to flying and swimming. Nonpropulsive lateral forces can be sizable, but they may benefit stability, maneuverability, or other criteria that become apparent in natural environments. Locomotor control systems combine rapid mechanical preflexes with multimodal sensory feedback and feedforward commands. Muscles have a surprising variety of functions in locomotion, serving as motors, brakes, springs, and struts. Integrative approaches reveal not only how each component within a locomotor system operates but how they function as a collective whole.",2000,197,1294,37,6,27,23,29,31,39,52,56,45,66
d31dfbb2f9a210e41ade9112fd3ba75aed3227e2,"I . Introduction, 365 2. Definition, 365 3. The gut microflora and its contribution to resistance, 366 4. Causes of induced changes in gut flora, 366 5. Composition of probiotics, 367 6. Mode of action of probiotics, 368 7. Practical results with probiotics. 370 7.1. Growth promotion of farm animals. 370 7.2. Effects on intestinal infections, 371 7.3. Alleviation of lactose intolerance, 372 7.4. Relief of constipation. 372 7.5. Antitumour activities, 372 7.6. Anticholesterolaemic effects, 372 8. Characteristics of a good probiotic, 373 9. Future developments, 374 10. Summary, 374 11. References, 374",2008,77,1000,62,36,44,46,59,64,84,71,87,76,88
4f76d400566e256852da82a2935e7483a053aff0,"IntroductionPleasure and reward are generated by brain circuits that are largely shared between humans and other animals.DiscussionHere, we survey some fundamental topics regarding pleasure mechanisms and explicitly compare humans and animals.ConclusionTopics surveyed include liking, wanting, and learning components of reward; brain coding versus brain causing of reward; subjective pleasure versus objective hedonic reactions; roles of orbitofrontal cortex and related cortex regions; subcortical hedonic hotspots for pleasure generation; reappraisals of dopamine and pleasure-electrode controversies; and the relation of pleasure to happiness.",2008,268,995,46,9,32,63,69,68,74,79,88,87,70
fc7020ed1de0034afb37298a6cafb99490dd26fc,,1989,0,1592,110,1,19,15,22,18,16,17,26,30,37
2bacf1c33fdaa3a28c3b3eb6cbad94dfe7ed59a1,"The author considers the possibility that there is not, in fact, much hidden mass in galaxies and galaxy systems. If a certain modified version of the Newtonian dynamics is used to describe the motion of bodies in a gravitational field (of a galaxy, say), the observational results are reproduced with no need to assume hidden mass in appreciable quantities. Various characteristics of galaxies result with no further assumptions. The basis of the modification is the assumption that in the limit of small acceleration a very low a0, the acceleration of a particle at distance r from a mass M satisfies approximately a2/a0 a MGr-2, where a0 is a constant of the dimensions of an acceleration.",1983,2,2280,193,0,3,4,4,1,4,1,3,2,5
cffc507312c01839ef2dc32158f2ad3a57efa5ce,"Dispersions of solid spherical grains of diameter D = 0.13cm were sheared in Newtonian fluids of varying viscosity (water and a glycerine-water-alcohol mixture) in the annular space between two concentric drums. The density σ of the grains was balanced against the density ρ of the fluid, giving a condition of no differential forces due to radial acceleration. The volume concentration C of the grains was varied between 62 and 13 %. A substantial radial dispersive pressure was found to be exerted between the grains. This was measured as an increase of static pressure in the inner stationary drum which had a deformable periphery. The torque on the inner drum was also measured. The dispersive pressure P was found to be proportional to a shear stress λ attributable to the presence of the grains. The linear grain concentration λ is defined as the ratio grain diameter/mean free dispersion distance and is related to C by λ=1(C0/C)12−1 where C0 is the maximum possible static volume concentration. Both the stressesT and P, as dimensionless groups TσD2/λη2, and PσD2/λη 2, were found to bear single-valued empirical relations to a dimensionless shear strain group λ½σD2(dU/dy)lη for all the values of λ< 12(C= 57% approx.) where dU/dy is the rate of shearing of the grains over one another, and η the fluid viscosity. This relation gives Tασ(λD)2(dU/dy)2 and T∝λ12ηdU/dy according as dU/dy is large or small, i.e. according to whether grain inertia or fluid viscosity dominate. An alternative semi-empirical relation F = (1+λ)(1+½λ)ηdU/dy was found for the viscous case, when T is the whole shear stress. The ratio T/P was constant at 0·3 approx, in the inertia region, and at 0.75 approx, in the viscous region. The results are applied to a few hitherto unexplained natural phenomena.",1954,1,2296,201,0,0,0,0,0,0,1,0,0,1
d0a90be9e7d2ebb969e7c540b4d1cd4fcb93218e,,1959,0,2432,122,0,0,1,2,3,1,0,0,0,0
a88cc4d8e59e5105683ea299e4c9739e52ce2393,"This paper presents a systematic treatment of the linear theory of scalar gravitational perturbations in the synchronous gauge and the conformal Newtonian (or longitudinal) gauge. It differs from others in the literature in that we give, in both gauges, a complete discussion of all particle species that are relevant to any flat cold dark matter (CDM), hot dark matter (HDM), or CDM+HDM models (including a possible cosmological constant). The particles considered include CDM, baryons, photons, massless neutrinos, and massive neutrinos (an HDM candidate), where the CDM and baryons are treated as fluids while a detailed phase-space description is given to the photons and neutrinos. Particular care is applied to the massive neutrino component, which has been either ignored or approximated crudely in previous works. Isentropic initial conditions on superhorizon scales are derived. The coupled, linearized Boltzmann, Einstein, and fluid equations that govern the evolution of the metric and density perturbations are then solved numerically in both gauges for the standard CDM model and two CDM+HDM models with neutrino mass densities {Omega}{sub {nu}}=0.2 and 0.3, assuming a scale-invariant, adiabatic spectrum of primordial fluctuations. We also give the full details of the cosmic microwave background anisotropy, and present the first accurate calculationsmore » of the angular power spectra in the two CDM+HDM models including photon polarization, higher neutrino multipole moments, and helium recombination. The numerical programs for both gauges are available at http://arcturus.mit.edu/cosmics. {copyright} {ital 1995 The American Astronomical Society.}« less",1994,18,1277,86,0,2,4,11,15,16,15,12,16,24
d3482d91af463d97e64e4a6137f019066526a09a,"On the assumption that pseudoplastic flow is associated with the formation and rupture of structural linkages a new flow equation is derived. The equation takes the form 
ƞ = ƞ∞ + ƞ0 − ƞ∞1 + αD23, 
where D = rate of shear, η0 = limiting viscosity at zero rate of shear, η∞ = limiting viscosity at infinite rate of shear, and α is a constant associated with the rupture of linkages. 
 
Graphical methods for evaluating the three constants η0 , η∞ , and α are presented. 
 
Experimental data are presented on a wide range of pseudoplastic systems, ranging from suspensions to optically clear solutions, in both aqueous and nonaqueous media. In all cases the results conform to the equation with a high degree of accuracy over a wide range of shear rates.",1965,3,1451,68,0,8,2,6,3,4,3,1,2,0
d1e492b3369299373277b27f5c90089af8ba1561,"This book bridges the gap between the theoretical work of the rheologist, and the practical needs of those who have to design and operate the systems in which these materials are handled or processed. It is an established and important reference for senior level mechanical engineers, chemical and process engineers, as well as any engineer or scientist who needs to study or work with these fluids, including pharmaceutical engineers, mineral processing engineers, medical researchers, water and civil engineers. This new edition covers a considerably broader range of topics than its predecessor, including computational fluid dynamics modeling techniques, liquid/solid flows and applications to areas such as food processing, among others.Written by two of the world's leading experts, this is the only dedicated non-Newtonian flow reference in print. Since first publication significant advances have been made in almost all areas covered in this book, which are incorporated in the new edition, including developments in CFD and computational techniques, velocity profiles in pipes, liquid/solid flows and applications to food processing, and new heat/mass transfer methods and models. This book covers both basic rheology and the fluid mechanics of NN fluids. It is a truly self-contained reference for anyone studying or working with the processing and handling of fluids.",2008,0,513,39,0,5,18,22,34,40,47,60,56,55
4b41c58e95168c81e46e5c705da472f901dfc6d5,"We examine the hypothesis that every particle of mass $m$ is subject to a Brownian motion with diffusion coefficient $\frac{\ensuremath{\hbar}}{2m}$ and no friction. The influence of an external field is expressed by means of Newton's law $\mathbf{F}=m\mathbf{a}$, as in the Ornstein-Uhlenbeck theory of macroscopic Brownian motion with friction. The hypothesis leads in a natural way to the Schr\""odinger equation, but the physical interpretation is entirely classical. Particles have continuous trajectories and the wave function is not a complete description of the state. Despite this opposition to quantum mechanics, an examination of the measurement process suggests that, within a limited framework, the two theories are equivalent.",1966,0,1281,56,0,2,3,5,3,3,5,3,6,6
9037b98bb8fb9a423daafe074b57354433d2820b,"When people make donations to privately provided public goods, such as charity, there may be many factors influencing their decision other than altruism. Social pressure, guilt, sympathy, or simply a desire for a ""warm glow"" may all be important. This paper considers such impure altruism formally and develops a wide set of implications. In particular, this paper discusses the invariance proposition of public goods, solves for the sufficient conditions for neutrality to hold, examines the optimal tax treatment of charitable giving, and calibrates the model based on econometric studies in order to consider policy experiments. Impure altruism is shown to be more consistent with observed patterns of giving than the conventional pure altruism approach, and to have policy implications that may differ widely from those of the conventional models. Copyright 1990 by Royal Economic Society.",1990,42,4906,407,2,6,8,12,11,15,15,17,27,30
7fafbbaadc929e0673a1756305d95fe2d1cb016a,Foreword Preface 1. Valuing Public Goods Using the Contingent Valuation Method 2. Theoretical Basis of the Contingent Valuation Method 3. Benefits and Their Measurement 4. Variations in Contingent Valuation Scenario Designs 5. The Methodological Challenge 6. Will Respondents Answer Honestly? 7. Strategic Behavior and Contingent Valuation Studies 8. Can Respondents Answer Meaningfully? 9. Hypothetical Values and Contingent Valuation Studies 10. Enhancing Reliability 11. Measurement Bias,1989,0,4812,325,11,18,34,39,53,89,57,73,122,123
1360ad1a04ae3b1cf7fc550551a338910b013d79,,1973,0,7466,427,2,6,3,7,4,2,12,8,9,12
475819d6223c13c466d20a80ddced737da5bf347,"This paper provides evidence that free riders are heavily punished even if punishment is costly and does not provide any material benefits for the punisher. The more free riders negatively deviate from the group standard the more they are punished. As a consequence, the existence of an opportunity for costly punishment causes a large increase in cooperation levels because potential free riders face a credible threat. We show, in particular, that in the presence of a costly punishment opportunity almost complete cooperation can be achieved and maintained although, under the standard assumptions of rationality and selfishness, there should be no cooperation at all. We also show that free riding causes strong negative emotions among cooperators. The intensity of these emotions is the stronger the more the free riders deviate from the group standard. Our results provide, therefore, support for the hypothesis that emotions are guarantors of credible threats.",1999,117,3941,279,2,45,45,54,68,85,163,127,137,166
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
ba600625575385ecf9f2d3f7f38815aef2501d83,"The authors present a model that links heterogeneity of preferences across ethnic groups in a city to the amount and type of public good the city supplies. Results show that the shares of spending on productive public goods - education, roads, sewers, and trash pickup _ in U.S. cities (metro areas/urban counties) are inversely related to the city's (metro area's/county's) ethnic fragmentation, even after controlling for other socioeconomic and demographic determinants. They conclude that the ethnic conflict is an important determinant of local public finances. In cities where ethnic groups are polarized, and where politicians have ethnic constituencies, the share of spending that goes to public goods is low. Their results are driven mainly by how white-majority cities react to varying minority-groups sizes. Voters choose lower public goods when a significant fraction of tax revenues collected from one ethnic group is used to provide public goods shared with other ethnic groups.",1997,89,3041,295,6,8,23,31,43,58,60,58,90,84
5393f0cedc4ead17cb00111477771c198c1c8bc0,"Environments with public goods are a wonderful playground for those interested in delicate experimental problems, serious theoretical challenges, and difficult mechanism design issues. A review is made of various public goods experiments. It is found that the public goods environment is a very sensitive one with much that can affect outcomes but are difficult to control. The many factors interact with each other in unknown ways. Nothing is known for sure. Environments with public goods present a serious challenge even to skilled experimentalists and many opportunities for imaginative work.",1994,0,3153,212,3,7,17,23,37,53,70,55,100,101
19729e3c8eea13567cc62f509bb89de03e5791c4,,1966,0,8148,42,0,0,1,4,7,3,5,4,16,4
22bf5a3e727226548cdb9400b76a84cc1f3b6c6e,"The Chamberlinian monopolistically competitive equilibrium has been explored and extended in a number of recent papers. These analyses have paid only cursory attention to the existence of an industry outside the Chamberlinian group. In this article I analyze a model of spatial competition in which a second commodity is explicitly treated. In this two-industry economy, a zero-profit equilibrium with symmetrically located firms may exhibit rather strange properties. First, demand curves are kinked, although firms make ""Nash"" conjectures. If equilibrium lies at the kink, the effects of parameter changes are perverse. In the short run, prices are rigid in the face of small cost changes. In the long run, increases in costs lower equilibrium prices. Increases in market size raise prices. The welfare properties are also perverse at a kinked equilibrium.",1979,13,3078,265,0,5,9,8,9,13,24,15,17,12
82a7e6300c57aeedc5c0763e30c8400669a12a44,,1971,5,3196,228,0,0,1,0,2,1,10,7,16,8
81607ddcb153b35ab3484a5cff9061a702741291,"THIS PAPER DEVELOPS some models for limited dependent variables.2 The distinguishing feature of these variables is that the range of values which they may assume has a lower bound and that this lowest value occurs in a fair number of observations. This feature should be taken into account in the statistical analysis of observations on such variables. In particular, it renders invalid use of the usual regression model. The second section of this paper develops several models for such variables. Like Tobin's [10] model, they are extensions of the multiple probit analysis model.3 They differ from that model by allowing the determination of the size of the variable when it is not zero to depend on different parameters or variables from those determining the probability of its being zero. Estimation and discrimination in the models are considered in Section 3. The models, like their prototypes, seem particularly intractable to exact analysis and large sample approximations have to be used. The adequacy of inferences based on these procedures is explored in Section 4 through a small sampling experiment. Limited dependent variables arise naturally in the study of consumer purchases, particularly purchases of durable goods. When a durable good is to be purchased, the amount spent may vary in fine gradations, but for many durables it is probably the case that most consumers in a particular period make no purchase at all. In Section 5 we apply the models to the demand for durable goods to provide an application of the techniques.",1971,6,2652,213,0,0,0,2,2,3,4,5,0,2
0c2030b54ffb098b206773f420e84438e0d04f64,"Cultural meaning in a consumer society moves ceaselessly from one location to another. In the usual trajectory, cultural meaning moves first from the culturally constituted world to consumer goods and then from these goods to the individual consumer. Several instruments are responsible for this movement: advertising, the fashion system, and four consumption rituals. This article analyzes the movement of cultural meaning theoretically, showing both where cultural meaning is resident in the contemporary North American consumer system and the means by which this meaning is transferred from one location in this system to another.",1986,64,2612,171,0,6,8,9,10,7,8,16,10,12
3cefa7e5753b02234030c6bcfcf779801f1d4b31,"Abstract We consider a general model of the non-cooperative provision of a public good. Under very weak assumptions there will always exist a unique Nash equilibrium in our model. A small redistribution of wealth among the contributing consumers will not change the equilibrium amount of the public good. However, larger redistributions of wealth will change the set of contributors and thereby change the equilibrium provision of the public good. We are able to characterize the properties and the comparative statics of the equilibrium in a quite complete way and to analyze the extent to which government provision of a public good ‘crowds out’ private contributions.",1986,35,2183,175,4,8,3,9,18,14,19,19,19,16
5d99adf84df0915204bc7f1767af89d4f95d8ac1,"We study the importance of conditional cooperation in a one-shot public goods game by using a variant of the strategy-method. We find that a third of the subjects can be classified as free riders, whereas 50 percent are conditional cooperators.",2000,75,1895,183,13,23,17,28,33,34,48,67,67,108
e6733b9c10cc63ffe319174e32146a201dc5b72e,"Presents an alternative theoretical approach to industrial marketing and purchasing based on a research project carried out in France, Germany, Italy, Sweden, and Great Britain. Focuses on descriptions and analyses of actual marketing and purchasing problems.",1982,120,2145,180,2,0,1,6,5,6,4,4,5,5
d6faa4fd609d88f9b5e09b90d0c1450695c00326,"In this article, the authors examine how consumer choice between hedonic and utilitarian goods is influenced by the nature of the decision task. Building on research on elaboration, the authors propose that the relative salience of hedonic dimensions is greater when consumers decide which of several items to give up (forfeiture choices) than when they decide which item to acquire (acquisition choices). The resulting hypothesis that a hedonic item is relatively preferred over the same utilitarian item in forfeiture choices than in acquisition choices was supported in two choice experiments. In a subsequent experiment, these findings were extended to hypothetical choices in which the acquisition and forfeiture conditions were created by manipulating initial attribute-level reference states instead of ownership. Finally, consistent with the experimental findings, a field survey showed that, relative to market prices, owners of relatively hedonic cars value their vehicles more than do owners of relatively utilitarian cars. The authors discuss theoretical implications of these reference-dependent preference asymmetries and explore consequences for marketing managers and other decision makers.",2000,56,1742,80,3,11,14,22,23,39,36,36,73,78
ee8b05e1756126396c86bc144632a2ffca934c0f,"Contingent valuation surveys in which respondents state their willingness to pay (WTP) for public goods are coming into use in cost-benefit analyses and in litigation over environmental losses. The validity of the method is brought into question by several experimental observations. An embedding effect is dem(~ns~rated, in which WTP for a good \arie\ depending on whether it is evaluated on its own or as part of a more inclusive category. The ordering of various public issues by WTP is predicted with significant accuracy by independent ratings of the moral satisfaction associated with contributions to these causc~. Contingent valuation responses reflect the willingness to pay for the moral satisfaction ol contributing to public goods, not the economic value of thehe goods. ’ I‘J’I? ,Acxlcrnl~ Pi-L?\ lnc There is substantial demand for a practical technique for measuring the value of non-market goods. Measures of value are required for cost-benefit assessments ot public goods, for the analysis of policies that affect the environment. and for realistic estimates of environmental damages resulting from human action, such as oil spills. In recent years the contingent valuation method (CVM) has gained prominence as the major technique for the assessment of the value of environmental amenities. This paper is concerned with a critique of CVM. The idea of CVM is quite simple: respondents are asked to indicate their value for a public good, usually by specifying the maximum amount they would be willing to pay to obtain or to retain it. The total value of the good is estimated by nlultiplying the average willingness to pay (WTP) observed in the sample by the number of households in the relevant population. This value is sometimes divided into use r~lue and non-use Ll&e by comparing the WTP of respondents who expect to enjoy the public good personally (e.g., benefit from improved visibility or from the increased number of fish in a cleaned up stream) to the WTP ot respondents who have no such expectations. Specific questions are sometimes added to partition non-use value further into the value of retaining an option fol future use, a bequest value, and a pure existence value 1151. The accuracy of the CVM is a matter of substantial practical import, not only in cost-benefit assessments but also in litigation over liability and damages. The validity of the technique is take as a rebuttable presumption in envir~~nmental cases brought in the United States under the Comprehensive Environmental Response, Compensation and Liability Act of 1980 (CERCLA). The research on the method has been reviewed in two authoritative volumes. which offer detailed *This research was supported by Fisheries and Oceans Canada, the Ontario Ministry 01’ thi: Environment. and the Sloan Foundation. Interviews and preliminary statistical analyses were prrformed by Campbell-Goodell Consultants, Vancouver, British Columbia. We benefited from conversations with George Akerlof, James Bieke, Brian Binger, Ralph d‘Arge, Elizabeth Hoffman. Richard Thaler. and Frances van Loo. from a commentary by Glenn Harrison. and from the statistical expertise of Carol Nickerson.",1992,28,1879,75,19,39,53,43,32,44,63,40,45,56
63fdeac799f8c871b8ba2efe041dc4f7c3f1987d,"This article identifies ecological goods and services of coral reef ecosystems, with special emphasis on how they are generated. Goods are divided into renewable resources and reef mining. Ecological services are classified into physical structure services, biotic services, biogeochemical services, information services, and social:cultural services. A review of economic valuation studies reveals that only a few of the goods and services of reefs have been captured. We synthesize current understanding of the relationships between ecological services and functional groups of species and biological communities of coral reefs in different regions of the world. The consequences of human impacts on coral reefs are also discussed, including loss of resilience, or buffer capacity. Such loss may impair the capacity for recovery of coral reefs and as a consequence the quality and quantity of their delivery of ecological goods and services. Conserving the capacity of reefs to generate essential services requires that they are managed as components of a larger seascape-landscape of which human activities are seen as integrated parts. © 1999 Elsevier Science B.V. All rights reserved.",1999,160,1583,78,1,6,6,8,18,15,17,27,19,40
187ac58928df06b0b4dae0ac55332479d35c623a,"Import prices typically change by a smaller proportion than the exchange rate between the exporting and importing country. Recent research indicates that common-currency relative prices for similar goods exported to different markets are highly correlated with exchange rates between those markets. This evidence suggests that incomplete pass-through is a consequence of third-degree price discrimination. While distance matters for market segmentation, borders have independent effects. The source of the border effect has not been clearly identified. Furthermore, there is little evidence yet to suggest substantial market power is implied by the observed price discrimination.",1996,41,1546,82,2,4,13,24,42,56,72,61,84,96
d8cf1e11aa8b5ddd961922a122a3323cfa81f12a,"Product and labor market deregulation are fundamentally about reducing and redistributing rents, leading economic players to adjust in turn to this new distribution. Thus, even if deregulation eventually proves beneficial, it comes with strong distribution and dynamic effects. The transition may imply the decline of incumbent firms. Unemployment may increase for a while. Real wages may decrease before recovering, and so on. To study these issues, we build a model based on two central assumptions: Monopolistic competition in the goods market, which determines the size of rents; and bargaining in the labor market, which determines the distribution of rents between workers and firms. We then think of product market regulation as determining both the entry costs faced by firms, and the degree of competition between firms. We think of labor market regulation as determining the bargaining power of workers. Having characterized the effects of labor and product market deregulation, we then use our results to study two specific issues. First, to shed light on macroeconomic evolutions in Europe over the last twenty years, in particular on the behavior of the labor share. Second, to look at political economy interactions between product and labor market deregulation.",2000,36,1238,153,0,5,19,38,49,51,74,72,87,81
bce6ca4c1471e1035f8319b6d57dd042f61bc728,"This book develops an original theory of group and organizational behavior that cuts across disciplinary lines and illustrates the theory with empirical and historical studies of particular organizations. Applying economic analysis to the subjects of the political scientist, sociologist, and economist, Mr. Olson examines the extent to which the individuals that share a common interest find it in their individual interest to bear the costs of the organizational effort. The theory shows that most organizations produce what the economist calls ""public goods""--goods or services that are available to every member, whether or not he has borne any of the costs of providing them. Economists have long understood that defense, law and order were public goods that could not be marketed to individuals, and that taxation was necessary. They have not, however, taken account of the fact that private as well as governmental organizations produce public goods. The services the labor union provides for the worker it represents, or the benefits a lobby obtains for the group it represents, are public goods: they automatically go to every individual in the group, whether or not he helped bear the costs. It follows that, just as governments require compulsory taxation, many large private organizations require special (and sometimes coercive) devices to obtain the resources they need. This is not true of smaller organizations for, as this book shows, small and large organizations support themselves in entirely different ways. The theory indicates that, though small groups can act to further their interest much more easily than large ones, they will tend to devote too few resources to thesatisfaction of their common interests, and that there is a surprising tendency for the ""lesser"" members of the small group to exploit the ""greater"" members by making them bear a disproportionate share of the burden of any group action. All of the theory in the book is in Chapter 1; the remaining chapters contain empirical and historical evidence of the theory's relevance to labor unions, pressure groups, corporations, and Marxian class action.",1967,0,2024,73,0,0,1,4,0,2,2,2,2,1
6a2af14ed3174a5238a415bcfaf4c85c1ec803d1,"There is widespread belief that firms should pursue superiority in both customer satisfaction and productivity. However, there is reason to believe these two goals are not always compatible. If a firm improves productivity by “downsizing,” it may achieve an increase in productivity in the short-term, but future profitability may be threatened if customer satisfaction is highly dependent on the efforts of personnel. If so, there are potential tradeoffs between customer satisfaction and productivity for industries as diverse as airlines, banking, education, hotels, and restaurants. Managers in these types of service industries, as well as goods industries in which the service component is increasing, need to understand whether or not this is the case. For example, if efforts to improve productivity can actually harm customer satisfaction---and vice-versa---the downsizing of U.S. and European companies should be viewed with concern. It follows that developing a better understanding of how customer satisfaction and productivity relate to one another is of substantial and growing importance, especially in light of expected continued growth in services throughout the world economy. 
 
The objective of this paper is to investigate whether there are conditions under which there are tradeoffs between customer satisfaction and productivity. A review of the literature reveals two conflicting viewpoints. One school of thought argues that customer satisfaction and productivity are compatible, as improvements in customer satisfaction can decrease the time andeffort devoted to handling returns, rework, warranties, and complaint management, while at the same time lowering the cost of making future transactions. The second argues that increasing customer satisfaction should increase costs, as doing so often requires efforts to improve product attributes or overall product design. 
 
A conceptual framework useful in resolving these contradictory viewpoints is developed. The framework serves, in turn, as a basis for developing a theoretical model relating customer satisfaction and productivity. The model predicts that customer satisfaction and productivity are less likely to be compatible when: 1 customer satisfaction is relatively more dependent on customization---the degree to which the firm's offering is customized to meet heterogeneous customers' needs---as opposed to standardization---the degree to which the firm's offering is reliable, standardized, and free from deficiencies; and 2 when it is difficult costly to provide high levels of both customization and standardization simultaneously. 
 
To move forward from the model's propositions to the development of testable hypotheses, we argue that services are more likely than goods to have the preceding characteristics. Hence, tradeoffs between customer satisfaction and productivity should be more prevalent for services than for goods. Although this classification is not precise---many services are standardizable and many goods have a service component---it has the advantage of allowing an initial test of the propositions. 
 
The empirical work employs a database matching customer-based measures of firm performance with traditional measures of business performance, such as productivity and Return on Investment ROI. The central feature of this database is the set of customer satisfaction indices provided by the Swedish Customer Satisfaction Barometer SCSB. The SCSB provides a uniform set of comparable customer-based firm performance measures and offers a unique opportunity to test the study's hypotheses. 
 
The findings indicate that the association between changes in customer satisfaction and changes in productivity is positive for goods, but negative for services. In addition, while both customer satisfaction and productivity are positively associated with ROI for goods and services, the interaction between the two is positive for goods but significantly less so for services. 
 
Taken together, the findings suggest support for the contention that tradeoffs are more likely for services. Hence, simultaneous attempts to increase both customer satisfaction and productivity are likely to be more challenging in such industries. Of course, this does not imply that such firms should not seek improvements in both productivity and customer satisfaction. For example, appropriate applications of information technology may improve both customer satisfaction and productivity simultaneously. 
 
The findings should provide motivation for future research concerning the nature of customer satisfaction and productivity, as well as appropriate strategy and tactics for each one. It is worth emphasizing that this is an issue that is not only important today, but certainly will become even more important in the future. As the growth of services continues and world markets become increasingly competitive, the importance of customer satisfaction will also increase. To compete in such a world, firms must strike the right balance between their efforts to compete efficiently and their efforts to compete effectively.",1997,56,1355,66,1,8,9,18,14,16,25,42,38,56
46fb7520f6ce62ceed1fdf85154f719607209db1,"Abstract There are two logics or mindsets from which to consider and motivate a transition from goods to service(s). The first, “goods-dominant (G-D) logic”, views services in terms of a type of (e.g., intangible) good and implies that goods production and distribution practices should be modified to deal with the differences between tangible goods and services. The second logic, “service-dominant (S-D) logic”, considers service – a process of using ones resources for the benefit of and in conjunction with another party – as the fundamental purpose of economic exchange and implies the need for a revised, service-driven framework for all of marketing. This transition to a service-centered logic is consistent with and partially derived from a similar transition found in the business-marketing literature — for example, its shift to understanding exchange in terms value rather than products and networks rather than dyads. It also parallels transitions in other sub-disciplines, such as service marketing. These parallels and the implications for marketing theory and practice of a full transition to a service-logic are explored.",2008,57,904,105,12,28,52,78,94,81,102,96,69,68
19434c969ad13a173bddd81fa5b2568a36a03c35,Abstract Pigou's proposition that the use of distorting taxes rather than neutral head taxes reduces public service levels is examined in this paper. A simple model with a national system of competing local governments is utilized to demonstrate that the use of a distorting property tax on mobile capital decreases the level of residential public services. The case where public services are an intermediate producer good is also considered.,1986,40,1624,65,2,5,2,4,4,12,6,7,4,7
a4903cce8609d781c03ba450bc3d1b9b54ec1ba4,"This revised edition with new Introduction from a leading anthropologist and an economist is unique in being about consumption but not a sermon for consumers, nor a moan against consumerism. The World of Goods bridges the gap between what anthropologists know about why objects are desired and what economists say about the specialised topic called consumption behaviour. The economist treats the desire for objects as an individual urge grounded in psychology; according to the anthropologist it is for fulfilling social obligations and represents the distribution of goods as a symptom of the form of society. It is a totally different perspectice and raises issues that lie beyond economics. The World of Goods asks new questions about why people save, why they spend, what they buy, and why they sometimes but not always make fine distinctions about quality. It is well-understood now that consumption goods communicate, create identity and establish relationships. But not so well-known that goods exclude as well as include, and that the pattern of their flow shows up the form of society. This book will be essential reading to students and lecturers in anthropology and economics.",1982,0,1746,7,5,6,12,6,13,20,20,14,14,10
75c3672918c261762d6686be69883f57cad89cde,"Abstract This paper examines ethnic diversity and local public goods in rural western Kenya. The identification strategy relies on the stable historically determined patterns of ethnic land settlement. Ethnic diversity is associated with lower primary school funding and worse school facilities, and there is suggestive evidence that it leads to poor water well maintenance. The theoretical model illustrates how inability to impose social sanctions in diverse communities leads to collective action failures, and we find that school committees in diverse areas do impose fewer sanctions on defaulting parents. We relate these results to the literature on social capital and economic development and discuss implications for decentralization in less developed countries.",2005,57,1183,46,14,24,45,29,67,69,86,71,88,83
050873d0385ab5a8aa36d5a791795967298f99b6,"Humans often cooperate in public goods games and situations ranging from family issues to global warming. However, evolutionary game theory predicts that the temptation to forgo the public good mostly wins over collective cooperative action, and this is often also seen in economic experiments. Here we show how social diversity provides an escape from this apparent paradox. Up to now, individuals have been treated as equivalent in all respects, in sharp contrast with real-life situations, where diversity is ubiquitous. We introduce social diversity by means of heterogeneous graphs and show that cooperation is promoted by the diversity associated with the number and size of the public goods game in which each individual participates and with the individual contribution to each such game. When social ties follow a scale-free distribution, cooperation is enhanced whenever all individuals are expected to contribute a fixed amount irrespective of the plethora of public goods games in which they engage. Our results may help to explain the emergence of cooperation in the absence of mechanisms based on individual reputation and punishment. Combining social diversity with reputation and punishment will provide instrumental clues on the self-organization of social communities and their economical implications.",2008,37,983,27,5,42,58,57,83,67,84,88,81,86
89ee2f72f16c4c3990259e5eb494399c77829157,,1981,0,1615,55,0,0,1,0,5,3,3,8,6,14
ca3eb6e60f56d6c9279cdb035c928867850817ba,"A large and growing literature links high levels of ethnic diversity to low levels of public goods provision. Yet although the empirical connection between ethnic heterogeneity and the underprovision of public goods is widely accepted, there is little consensus on the specific mechanisms through which this relationship operates. We identify three families of mechanisms that link diversity to public goods provision—what we term “preferences,” “technology,” and “strategy selection” mechanisms—and run a series of experimental games that permit us to compare the explanatory power of distinct mechanisms within each of these three families. Results from games conducted with a random sample of 300 subjects from a slum neighborhood of Kampala, Uganda, suggest that successful public goods provision in homogenous ethnic communities can be attributed to a strategy selection mechanism: in similar settings, co-ethnics play cooperative equilibria, whereas non-co-ethnics do not. In addition, we find evidence for a technology mechanism: co-ethnics are more closely linked on social networks and thus plausibly better able to support cooperation through the threat of social sanction. We find no evidence for prominent preference mechanisms that emphasize the commonality of tastes within ethnic groups or a greater degree of altruism toward co-ethnics, and only weak evidence for technology mechanisms that focus on the impact of shared ethnicity on the productivity of teams.",2006,103,1044,66,2,11,20,45,58,70,64,82,88,102
e5d15af10232f9e32adaadf4917c8514da9a4853,"We study the strategy of bundling a large number of information goods, such as those increasingly available on the Internet, and selling them for a fixed price. We analyze the optimal bundling strategies for a multiproduct monopolist, and we find that bundling very large numbers of unrelated information goods can be surprisingly profitable. The reason is that the law of large numbers makes it much easier to predict consumers' valuations for a bundle of goods than their valuations for the individual goods when sold separately. As a result, this ""predictive value of bundling"" makes it possible to achieve greater sales, greater economic efficiency, and greater profits per good from a bundle of information goods than can be attained when the same goods are sold separately. Our main results do not extend to most physical goods, as the marginal costs of production for goods not used by the buyer typically negate any benefits from the predictive value of large-scale bundling. While determining optimal bundling strategies for more than two goods is a notoriously difficult problem, we use statistical techniques to provide strong asymptotic results and bounds on profits for bundles of any arbitrary size. We show how our model can be used to analyze the bundling of complements and substitutes, bundling in the presence of budget constraints, and bundling of goods with various types of correlations and how each of these conditions can lead to limits on optimal bundle size. In particular we find that when different market segments of consumers differ systematically in their valuations for goods, simple bundling will no longer be optimal. However, by offering a menu of different bundles aimed at each market segment, bundling makes traditional price discrimination strategies more powerful by reducing the role of unpredictable idiosyncratic components of valuations. The predictions of our analysis appear to be consistent with empirical observations of the markets for Internet and online content, cable television programming, and copyrighted music.",1999,49,1173,86,17,32,48,49,48,50,60,57,60,41
85c3b07d19ceb28b68c82c779c5fda285fdb3189,"Abstract Laboratory experiments on free riding have produced mixed results. Free riding is seldom observed with single-shot games; however, it is often approximated in finitely repeated games. There are two prevailing hypothesis for why this is so: strategies and learning. This paper discusses these hypotheses and presents an experiment that examines both.",1988,23,1163,146,1,3,1,7,3,4,6,3,12,12
121145a1715903fbfed3dfab52a3910493a0c059,"By lowering the costs of gathering and sharing information and offering new ways to learn about products before purchase, the Internet reduces traditional distinctions between search and experience goods. At the same time, differences in the type of information sought for search and experience goods can precipitate differences in the process through which consumers gather information and make decisions online. A preliminary experiment shows that though there are significant differences in consumers’ perceived ability to evaluate product quality before purchase between search and experience goods in traditional retail environments, these differences are blurred in online environments. An analysis of the online behavior of a representative sample of U.S. consumers shows that consumers spend similar amounts of time online gathering information for both search and experience goods, but there are important differences in the browsing and purchase behavior of consumers for these two types of goods. In particular, experience goods involve greater depth (time per page) and lower breadth (total number of pages) of search than search goods. In addition, free riding (purchasing from a retailer other than the primary source of product information) is less frequent for experience than for search goods. Finally, the presence of product reviews from other consumers and multimedia that enable consumers to interact with products before purchase has a greater effect on consumer search and purchase behavior for experience than for search goods.",2009,77,565,42,2,27,42,43,50,51,45,52,62,51
8d18235b9606a7e8f64036ab0b50aa7b6d0a3ed8,"This paper discusses Ricardian trade and payments theory in the case of a continuum of goods. The analysis thus extends the development of many-commodity, two-country comparative advantage analysis as presented, for example, in Gottfried Haberler (1937), Frank Graham (1923), Paul Samuelson (1964), and Frank W. Taussig (1927). The literature is historically reviewed by John Chipman (1965). Perhaps surprisingly, the continuum assumption simplifies the analysis neatly in comparison with the discrete many-commodity case. The distinguishing feature of the Ricardian approach emphasized in this paper is the determination of the competitive margin in production between imported and exported goods. The analysis advances the existing literature by formally showing precisely how tariffs and transport costs establish a range of commodities that are not traded, and how the price-specie flow mechanism does or does not give rise to movements in relative cost and price levels. The formal real model is introduced in Section 1. Its equilibrium determines the relative wage and price structure and the efficient international specialization pattern. Section II considers standard comparative static questions of growth, demand shifts,",1976,30,1375,103,0,0,1,3,6,5,3,5,2,6
cd0806361ec140668db16819882d5780889f8a75,"People want to have fun, and they are more likely to have fun if the situation allows them to justify it. This research studies how people's need for justifying hedonic consumption drives two choice patterns that are observed in typical purchase contexts. First, relative preferences between hedonic and utilitarian alternatives can reverse, depending on how the immediate purchase situation presents itself. A hedonic alternative tends to be rated more highly than a comparable utilitarian alternative when each is presented singly, but the utilitarian alternative tends to be chosen over the hedonic alternative when the two are presented jointly. Second, people have preferences for expending different combinations of time (effort) and money for acquiring hedonic versus utilitarian items. They are willing to pay more in time for hedonic goods and more in money for utilitarian goods. The author explores the topic through a combination of four experiments and field studies.",2005,48,856,86,5,13,11,27,23,39,44,54,39,69
3572e8b5d1c576a72f6a4cc45b15ea083259f43b,"To make economic choices between goods, the brain needs to compute representations of their values. A great deal of research has been performed to determine the neural correlates of value representations in the human brain. However, it is still unknown whether there exists a region of the brain that commonly encodes decision values for different types of goods, or if, in contrast, the values of different types of goods are represented in distinct brain regions. We addressed this question by scanning subjects with functional magnetic resonance imaging while they made real purchasing decisions among different categories of goods (food, nonfood consumables, and monetary gambles). We found activity in a key brain region previously implicated in encoding goal-values: the ventromedial prefrontal cortex (vmPFC) was correlated with the subjects' value for each category of good. Moreover, we found a single area in vmPFC to be correlated with the subjects' valuations for all categories of goods. Our results provide evidence that the brain encodes a “common currency” that allows for a shared valuation for different categories of goods.",2009,31,519,29,3,24,40,48,49,53,56,41,41,42
63df6bc8762444c21051ab996cd33c7c0777d9a4,"The persistence of cooperation in public-goods experiments has become an important puzzle for economists. This paper presents the first systematic attempt to separate the hypothesis that cooperation is due to kindness, altruism, or warm-glow from the hypothesis that cooperation is simply the result of errors or confusion. The experiment reveals that, on average, about half of all cooperation comes from subjects who understand free-riding but choose to cooperate out of some form of kindness. This suggests that the focus on errors and 'learning' in experimental research should shift to include studies of preferences for cooperation as well. Copyright 1995 by American Economic Association.",1995,21,1034,105,1,8,11,21,12,38,20,39,31,43
f001bc3add3c433d3f6801ded41c2782af12592a,"This book presents a theoretical treatment of externalities (i.e. uncompensated interdependencies), public goods, and club goods. The new edition updates and expands the discussion of externalities and their implications, coverage of asymmetric information, underlying game-theoretic formulations, and intuitive and graphical presentations. Topics investigated include Nash equilibrium, Lindahl equilibria, club theory, preference-revelation mechanism, Pigouvian taxes, the commons, Coase Theorem, and static and repeated games. The authors use mathematical techniques only as much as necessary to pursue the economic argument. They develop key principles of public economics that are useful for subfields such as public choice, labor economics, economic growth, international economics, environmental and natural resource economics, and industrial organization.",1996,0,1104,41,11,13,19,23,30,48,37,50,47,57
3a866c627f12fb54dc72cefb8990735570fcee16,"While there are many Web services which help users find things to buy, we know of none which actually try to automate the process of buying and selling. Kasbah is a system where users create autonomous agents to buy and sell goods on their behalf. In this paper, we describe how Kasbah works. We also discuss the implementation of a simple proof-of-concept prototype.",1997,54,1001,45,18,57,69,100,98,88,65,93,61,57
f77cf9ccf68656bb341576172983306484387ad6,"The Consumer Price Index (CPI) attempts to answer the question of how much more (or less) income does a consumer require to be as well off in period 1 as in period 0 given changes in prices, changes in the quality of goods, and the introduction of new goods (or the disappearance of existing goods). In this paper I explain the theory of cost-of-living indices and demonstrate how new goods should be included using the classical theory of Hicks and Rothbarth. The correct price to use for the good in the pre-intro- duction period is a `virtual&apos; price which sets demand to zero. Estimation of this virtual price requires estimation of a demand function which in turn provides the expenditure function which allows exact calucation of the cost of living index. The data requirements and need to specify and estimate a demand function for a new brand among many existing brands requires extensive data and some new econometric methods which may have proven obstacles to the inclusion of new goods in the CPI up to this point. As an example I use the introduction of a new cereal brand by General Mills in 1989-Apple Cinnamon Cheerios. I find the virtual price is about 2 times the actual price of Apple Cinnamon Cheerios and that increase in consumer surplus is substantial. Based on some simplifying approximations, I find that CPI may be overstated for cereal by about 25% because of its neglect of the effect of new brands. When I take imperfect competition into account I find that the increase in consumer welfare is only 85% as high with perfect competition so CPI for cereal would still be 20% too high",1994,33,965,56,0,0,8,16,11,6,19,24,25,37
ad9048785b7211efe4d0f7c2e65fb63a1cb6cbdb,"This study analyses trade flows in intermediate goods and services among OECD countries and with their main trading partners. Combining trade data and input-output tables, bilateral trade in intermediate goods and services is estimated according to the industry of origin and the using industry for the period 1995-2005. Trade in intermediate inputs takes place mostly among developed countries and represents respectively 56% and 73% of overall trade flows in goods and services. Gravity regressions indicate that in comparison to trade in final goods and services, imports of intermediates are more sensitive to trade costs and are less attracted by bilateral market size. Further findings are that the activities of multinational enterprises can be associated with higher trade flows of intermediate inputs and with a higher ratio of foreign to domestic inputs in using industries. Results from production function regressions and from a stochastic frontier analysis suggest that a higher share of imported inputs leads to productivity gains in domestic industries and reduces inefficiencies in the use of technology.",2009,29,328,17,2,12,23,36,35,35,35,34,34,32
52e1d4ad8472e48e6119e4e8e3b21129baf629f6,,2007,774,713,38,4,38,78,77,71,69,87,62,51,35
10d6d461d1b36ccb72f7b2be16075befde63cb63,"The well known travel cost method (TC)has been widely applied to outdoor recreation. A second approach has been referred to in the past as the Davis method, the questionnaire approach, and contingent valuation. It will here be termed hypothetical valuation (HV), since it involves creating a hypothetical situation designed to elicit willingness to pay for or willingness to accept compensation for a recreational or other extramarket good (or bad). TC and HV are termed ""indirect methods"", since they do not depend on the direct information about prices and quantities that economists would prefer to use where available to value goods and services.",1979,7,1189,39,0,2,5,6,5,10,9,23,11,16
d58a964c8391ea22054cfb593e6de4dc8b5eb037,"We study the importance of conditional cooperation in a one-shot public goods game by using a variant of the strategy-method. We find that a third of the subjects can be classified as free riders, whereas 50% are conditional cooperators.",2001,31,886,22,14,18,14,10,48,18,18,24,31,21
6f7047106bc6c4790239f8364f8caf9899243334,"This paper considers incentives to provide goods that are non-excludable along social or geographic links. We find, first, that networks can lead to specialization in public good provision. In every social network there is an equilibrium where some individuals contribute and others free ride. In many networks, this extreme is the only outcome. Second, specialization can benefit society as a whole. This outcome arises when contributors are linked, collectively, to many agents. Finally, a new link increases access to public goods, but reduces individual incentives to contribute. Hence, overall welfare can be higher when there are holes in a network.",2007,49,551,55,6,19,24,32,30,39,43,39,40,42
b4b5ee8e671627a17cc22fb9ec83731b74721dc5,"Aims. We present the final public data release of the VLT /ISAAC near-infrared imaging survey in the GOODS-South field . The survey covers an area of 172.5, 159.6 and 173.1 arcmin 2 in the J, H, and Ks bands, respectively. For point sources total limiting magnitudes of J = 25: 0, H = 24: 5, and Ks = 24: 4 (5� , AB) are reached within 75% of the survey area. Thus these observations are significantly deeper than the previous EIS Deep Public Surve y which covers the same region. The image quality is characterized by a point spread function ranging between 0.34 00 and 0.65 00 FWHM. The images are registered to a common astrometric grid defined by the GSC 2 with an accuracy of�0: 06 00 RMS over the whole field. The overall photometric accuracy, i ncluding all systematic effects, adds up to 0.05 mag. The data are publicly available from the ESO science archive facility. Methods. We describe the data reduction, the calibration, and the quality control process. The final data set is characterized in t erms of astrometric and photometric properties, including the PSF and the curve of growth. We establish an empirical model for the sky background noise in order to quantify the variation of limiting depth and statistical photometric errors over the surve y area. We define a catalog of Ks-selected sources which contains JHKs photometry for 7079 objects. Differential aperture corrections were applied to the color measurements in order to avoid possible biases as a result of the variation of the PSF. We briefly discuss the resu lting color distributions in the context of available redshift data. Fu rthermore, we estimate the completeness fraction and relative contamination due to spurious detections for source catalogs extracted fr om the survey data. For this purpose, an empirical study based on a deep Ks image of the Hubble Ultra Deep Field is combined with extensive image simulations. Results. With respect to previous deep near-infrared surveys, the surface density of faint galaxies has been established with unprecedented accuracy by virtue of the unique combination of depth and area of this survey. We derived galaxy number counts over eight magnitudes in flux up to J = 25: 25, H = 25: 0, Ks = 25: 25 (in the AB system). Very similar faint-end logarithmic slopes between 0.24 and 0.27 mag −1 were measured in the three bands. We found no evidence for a significant change in the slope of the logarithmic galaxy number counts at the faint end.",2009,44,103,25,2,14,24,15,4,8,12,7,1,7
928da9164cf5240094de1ceab2ee321276aa7edb,This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles.,1975,14,10031,461,15,17,16,13,31,37,32,31,53,60
42607bb3d65c74eb44364a379d5496e69567e323,"В статье производится анализ агрегированной производственной функции, вводится аппарат, позволяющий различать движение вдоль такой функции от ее сдвигов. На основании сделанных в статье предположений делаются выводы о характере технического прогресса и технологических изменений. Существенное внимание уделяется вариантам применения концепции агрегированной производственной функции.",1957,0,10413,459,0,0,0,0,0,0,0,0,0,0
4c17fd88f8f4eb15005416edac67875e8be6dfb1,"Evidence gleaned from the instrumental record of climate data identifies a robust, recurring pattern of ocean–atmosphere climate variability centered over the midlatitude North Pacific basin. Over the past century, the amplitude of this climate pattern has varied irregularly at interannual-to-interdecadal timescales. There is evidence of reversals in the prevailing polarity of the oscillation occurring around 1925, 1947, and 1977; the last two reversals correspond to dramatic shifts in salmon production regimes in the North Pacific Ocean. This climate pattern also affects coastal sea and continental surface air temperatures, as well as streamflow in major west coast river systems, from Alaska to California.",1997,66,6350,845,8,23,66,92,121,179,232,231,275,237
3d6ca6a699e579463e0b01526d9842d41f26b314,"Previous studies of the so-called frontier production function have not utilized an adequate characterization of the disturbance term for such a model. In this paper we provide an appropriate specification, by defining the disturbance term as the sum of symmetric normal and (negative) half-normal random variables. Various aspects of maximum-likelihood estimation for the coefficients of a production function with an additive disturbance term of this sort are then considered.",2001,17,4805,748,87,93,118,143,142,166,174,168,222,246
6df5ec442559d953bd95e5f20ee1685b33976764,"‘The Production of Space’, in: Frans Jacobi, Imagine, Space Poetry, Copenhagen, 1996, unpaginated.",1996,10,7340,731,11,11,18,22,55,69,111,104,136,151
071f211ee4799ea191743cc893496ce4e098cdcc,"A stochastic frontier production function is defined for panel data on firms, in which the non-negative technical inefficiency effects are assumed to be a function of firm-specific variables and time. The inefficiency effects are assumed to be independently distributed as truncations of normal distributions with constant variance, but with means which are a linear function of observable variables. This panel data model is an extension of recently proposed models for inefficiency effects in stochastic frontiers for cross-sectional data. An empirical application of the model is obtained using up to ten years of data on paddy farmers from an Indian village. The null hypotheses, that the inefficiency effects are not stochastic or do not depend on the farmer-specific variables and time of observation, are rejected for these data.",1995,19,5471,757,0,7,9,22,38,53,61,95,118,138
5544b476e318fc12320dfcc979456864983e36c0,"In this provocative and broad-ranging work, a distinguished team of authors argues that the ways in which knowledge  scientific, social and cultural  is produced are undergoing fundamental changes at the end of the twentieth century. They claim that these changes mark a distinct shift into a new mode of knowledge production which is replacing or reforming established institutions, disciplines, practices and policies. Identifying a range of features of the new moder of knowledge production  reflexivity, transdisciplinarity, heterogeneity  the authors show the connections between these features and the changing role of knowledge in social relations. While the knowledge produced by research and development in science and technology (both public and industrial) is accorded central concern, the authors also outline the changing dimensions of social scientific and humanities knowledge and the relations between the production of knowledge and its dissemination through education. Placing science policy and scientific knowledge in its broader context within contemporary societies, this book will be essential reading for all those concerned with the changing nature of knowledge, with the social study of science, with educational systems, and with the relations between R&D and social, economic and technological development.",1994,0,6451,315,2,8,29,19,39,65,96,111,156,183
3ea3eac3443404048aaec9bde79e2384c5f45f7b,"Inducible expression systems in which T7 RNA polymerase transcribes coding sequences cloned under control of a T7lac promoter efficiently produce a wide variety of proteins in Escherichia coli. Investigation of factors that affect stability, growth, and induction of T7 expression strains in shaking vessels led to the recognition that sporadic, unintended induction of expression in complex media, previously reported by others, is almost certainly caused by small amounts of lactose. Glucose prevents induction by lactose by well-studied mechanisms. Amino acids also inhibit induction by lactose during log-phase growth, and high rates of aeration inhibit induction at low lactose concentrations. These observations, and metabolic balancing of pH, allowed development of reliable non-inducing and auto-inducing media in which batch cultures grow to high densities. Expression strains grown to saturation in non-inducing media retain plasmid and remain fully viable for weeks in the refrigerator, making it easy to prepare many freezer stocks in parallel and use working stocks for an extended period. Auto-induction allows efficient screening of many clones in parallel for expression and solubility, as cultures have only to be inoculated and grown to saturation, and yields of target protein are typically several-fold higher than obtained by conventional IPTG induction. Auto-inducing media have been developed for labeling proteins with selenomethionine, 15N or 13C, and for production of target proteins by arabinose induction of T7 RNA polymerase from the pBAD promoter in BL21-AI. Selenomethionine labeling was equally efficient in the commonly used methionine auxotroph B834(DE3) (found to be metE) or the prototroph BL21(DE3).",2005,59,4740,337,15,71,88,155,198,258,292,292,364,393
f35c1c3bc573c9e096de33e3d83eb5be34f6ab57,"A doubling in global food demand projected for the next 50 years poses huge challenges for the sustainability both of food production and of terrestrial and aquatic ecosystems and the services they provide to society. Agriculturalists are the principal managers of global useable lands and will shape, perhaps irreversibly, the surface of the Earth in the coming decades. New incentives and policies for ensuring the sustainability of agriculture and ecosystem services will be crucial if we are to meet the demands of improving yields without compromising environmental integrity or public health.",2002,92,5989,237,4,27,40,61,78,103,134,150,230,273
3dc5c82b090dcd8e6dbf110200ce334f797b3f37,"Lignocellulosic biomass can be utilized to produce ethanol, a promising alternative energy source for the limited crude oil. There are mainly two processes involved in the conversion: hydrolysis of cellulose in the lignocellulosic biomass to produce reducing sugars, and fermentation of the sugars to ethanol. The cost of ethanol production from lignocellulosic materials is relatively high based on current technologies, and the main challenges are the low yield and high cost of the hydrolysis process. Considerable research efforts have been made to improve the hydrolysis of lignocellulosic materials. Pretreatment of lignocellulosic materials to remove lignin and hemicellulose can significantly enhance the hydrolysis of cellulose. Optimization of the cellulase enzymes and the enzyme loading can also improve the hydrolysis. Simultaneous saccharification and fermentation effectively removes glucose, which is an inhibitor to cellulase activity, thus increasing the yield and rate of cellulose hydrolysis.",2002,92,5457,274,4,13,28,38,50,83,130,165,271,392
81bc203a4e66f6440c02ce443c28f5a67f0a37db,"Using a unique international data set from a 1989–90 survey of 62 automotive assembly plants, the author tests two hypotheses: that innovative HR practices affect performance not individually but as interrelated elements in an internally consistent HR “bundle” or system; and that these HR bundles contribute most to assembly plant productivity and quality when they are integrated with manufacturing policies under the “organizational logic” of a flexible production system. Analysis of the survey data, which tests three indices representing distinct bundles of human resource and manufacturing practices, supports both hypotheses. Flexible production plants with team-based work systems, “high-commitment” HR practices (such as contingent compensation and extensive training), and low inventory and repair buffers consistently outperformed mass production plants. Variables capturing two-way and three-way interactions among the bundles of practices are even better predictors of performance, supporting the integration hypothesis.",1995,90,3904,322,14,26,40,57,57,87,92,106,118,140
62f5939746e3c0a3132b93c0221f3dc247ca3020,"This article reviews some of the criticisms directed towards the eclectic paradigm of international production over the past decade, and restates its main tenets. The second part of the article considers a number of possible extensions of the paradigm and concludes by asserting that it remains “a robust general framework for explaining and analysing not only the economic rationale of economic production but many organisational and impact issues in relation to MNE activity as well.”",1988,66,3941,251,4,4,11,16,19,25,21,22,33,44
814c4bdf69bccb4253c28f8337c6a072b0257ceb,,1990,11,4445,319,4,4,7,9,15,28,19,32,35,23
2fbcdd66c7ddcc5ca9ee04aeead8efe98eb3ad0e,"Integrating conceptually similar models of the growth of marine and terrestrial primary producers yielded an estimated global net primary production (NPP) of 104.9 petagrams of carbon per year, with roughly equal contributions from land and oceans. Approaches based on satellite indices of absorbed solar radiation indicate marked heterogeneity in NPP for both land and oceans, reflecting the influence of physical and ecological processes. The spatial and temporal distributions of ocean NPP are consistent with primary limitation by light, nutrients, and temperature. On land, water limitation imposes additional constraints. On land and ocean, progressive changes in NPP can result in altered carbon storage, although contrasts in mechanisms of carbon storage and rates of organic matter turnover result in a range of relations between carbon storage and changes in NPP.",1998,40,4159,227,5,18,20,26,37,41,63,67,72,93
5144695b43464a1eacccd73768ac6d31aa4c35c0,"The invention disclosed herein is a liquid fuel composition having reduced soot and smoking characterized comprising a major proportion of a liquid hydrocarbon fuel and a minor proportion of Group IIA and Group IIB metal salts of carboxylic acids. A preferred fuel composition is from 0.1 to 0.6 percent by weight of barium- and zinc 2-ethylhexanoates admixed in diesel fuel, wherein the weight ratio of barium to zinc is about 10 to 1. Further improvement in smoke and soot reduction is obtained in hydrocarbon fuels when an ether is additionally incorporated into the salt and fuel mixture. A mixture of from about 0.1 to 0.6 percent by weight of barium 2-ethylhexanoate and zinc 2-ethylhexanoate, from between 0.2 to 0.5 percent by weight of the monomethyl ether of ethylene glycol and the balance, a diesel fuel, has substantially reduced smoke and soot forming characteristics.",1996,18,4895,223,9,16,37,58,71,92,136,138,171,221
b3816479d1e3b33c6960062d9b34ee08b80bfe98,,1986,0,3533,262,0,1,0,2,4,9,7,9,20,12
091a00beaa12470fa819d6a3c7b6fe06da8f7b41,"Biodiesel has become more attractive recently because of its environmental benefits and the fact that it is made from renewable resources. The cost of biodiesel, however, is the main hurdle to commercialization of the product. The used cooking oils are used as raw material, adaption of continuous transesterification process and recovery of high quality glycerol from biodiesel by-product (glycerol) are primary options to be considered to lower the cost of biodiesel. There are four primary ways to make biodiesel, direct use and blending, microemulsions, thermal cracking (pyrolysis) and transesterification. The most commonly used method is transesterification of vegetable oils and animal fats. The transesterification reaction is aAected by molar ratio of glycerides to alcohol, catalysts, reaction temperature, reaction time and free fatty acids and water content of oils or fats. The mechanism and kinetics of the transesterification show how the reaction occurs and progresses. The processes of transesterification and its downstream operations are also addressed. ” 1999 Published by Elsevier Science B.V. All rights reserved.",1999,57,5207,184,1,3,3,10,13,35,60,81,134,232
dd45d5d76bf12745ee3ea805437ff7a60a4ae890,"Preparing words in speech production is normally a fast and accurate process. We generate them two or three per second in fluent conversation; and overtly naming a clear picture of an object can easily be initiated within 600 msec after picture onset. The underlying process, however, is exceedingly complex. The theory reviewed in this target article analyzes this process as staged and feed-forward. After a first stage of conceptual preparation, word generation proceeds through lexical selection, morphological and phonological encoding, phonetic encoding, and articulation itself. In addition, the speaker exerts some degree of output control, by monitoring of self-produced internal and overt speech. The core of the theory, ranging from lexical selection to the initiation of phonetic encoding, is captured in a computational model, called WEAVER++. Both the theory and the computational model have been developed in interaction with reaction time experiments, particularly in picture naming or related word production paradigms, with the aim of accounting for the real-time processing in normal word production. A comprehensive review of theory, model, and experiments is presented. The model can handle some of the main observations in the domain of speech errors (the major empirical domain for most other theories of lexical access), and the theory opens new ways of approaching the cerebral organization of speech production by way of high-temporal-resolution imaging.",1996,352,2846,440,0,0,13,21,40,59,100,100,94,91
59eede676a521227f5c24850567b9b8dc654b4dd,"Interest in graphene centres on its excellent mechanical, electrical, thermal and optical properties, its very high specific surface area, and our ability to influence these properties through chemical functionalization. There are a number of methods for generating graphene and chemically modified graphene from graphite and derivatives of graphite, each with different advantages and disadvantages. Here we review the use of colloidal suspensions to produce new materials composed of graphene and chemically modified graphene. This approach is both versatile and scalable, and is adaptable to a wide variety of applications.",2009,67,5668,44,26,214,452,558,670,653,607,540,507,456
08ba6c50d0b0d0509c9e51c55e532ce3270db930,"The use of renewable energy sources is becoming increasingly necessary, if we are to achieve the changes required to address the impacts of global warming. Biomass is the most common form of renewable energy, widely used in the third world but until recently, less so in the Western world. Latterly much attention has been focused on identifying suitable biomass species, which can provide high-energy outputs, to replace conventional fossil fuel energy sources. The type of biomass required is largely determined by the energy conversion process and the form in which the energy is required. In the first of three papers, the background to biomass production (in a European climate) and plant properties is examined. In the second paper, energy conversion technologies are reviewed, with emphasis on the production of a gaseous fuel to supplement the gas derived from the landfilling of organic wastes (landfill gas) and used in gas engines to generate electricity. The potential of a restored landfill site to act as a biomass source, providing fuel to supplement landfill gas-fuelled power stations, is examined, together with a comparison of the economics of power production from purpose-grown biomass versus waste-biomass. The third paper considers particular gasification technologies and their potential for biomass gasification.",2002,11,3726,208,3,6,15,20,26,43,55,75,119,170
44466565fdfd765ba5d3eb711795ad0d87d4c254,"With the radical changes in information production that the Internet has introduced, we stand at an important moment of transition, says Yochai Benkler in this thought-provoking book. The phenomenon he describes as social production is reshaping markets, while at the same time offering new opportunities to enhance individual freedom, cultural diversity, political discourse, and justice. But these results are by no means inevitable: a systematic campaign to protect the entrenched industrial information economy of the last century threatens the promise of today's emerging networked information environment. In this comprehensive social theory of the Internet and the networked information economy, Benkler describes how patterns of information, knowledge, and cultural production are changing--and shows that the way information and knowledge are made available can either limit or enlarge the ways people can create and express themselves. He describes the range of legal and policy choices that confront us and maintains that there is much to be gained--or lost--by the decisions we make today.",2006,170,3064,272,8,74,162,228,237,274,276,273,296,265
f8e208c67545cb9f66185ecfbbbe484721e58a04,"Fully exploiting the properties of graphene will require a method for the mass production of this remarkable material. Two main routes are possible: large-scale growth or large-scale exfoliation. Here, we demonstrate graphene dispersions with concentrations up to approximately 0.01 mg ml(-1), produced by dispersion and exfoliation of graphite in organic solvents such as N-methyl-pyrrolidone. This is possible because the energy required to exfoliate graphene is balanced by the solvent-graphene interaction for solvents whose surface energies match that of graphene. We confirm the presence of individual graphene sheets by Raman spectroscopy, transmission electron microscopy and electron diffraction. Our method results in a monolayer yield of approximately 1 wt%, which could potentially be improved to 7-12 wt% with further processing. The absence of defects or oxides is confirmed by X-ray photoelectron, infrared and Raman spectroscopies. We are able to produce semi-transparent conducting films and conducting composites. Solution processing of graphene opens up a range of potential large-area applications, from device and sensor fabrication to liquid-phase chemistry.",2008,91,4810,61,6,60,174,259,341,400,428,483,463,454
01f1c4f6ad67a44eca04adf672bb2608a93d4f04,"Many papers have regressed non-parametric estimates of productive efficiency on environmental variables in two-stage procedures to account for exogenous factors that might affect firms’ performance. None of these have described a coherent data-generating process (DGP). Moreover, conventional approaches to inference employed in these papers are invalid due to complicated, unknown serial correlation among the estimated efficiencies. We first describe a sensible DGP for such models. We propose single and double bootstrap procedures; both permit valid inference, and the double bootstrap procedure improves statistical efficiency in the second-stage regression. We examine the statistical performance of our estimators using Monte Carlo experiments.",2007,116,2632,386,45,80,123,145,148,156,171,182,193,184
8a895d50e51b5469c4acbc22aaea2e0511a07ead,,1986,99,2879,380,4,26,15,21,31,74,42,41,35,47
8e05a1ce6103477137f939b939a6a2b52c12d005,"Diabetic hyperglycaemia causes a variety of pathological changes in small vessels, arteries and peripheral nerves. Vascular endothelial cells are an important target of hyperglycaemic damage, but the mechanisms underlying this damage are not fully understood. Three seemingly independent biochemical pathways are involved in the pathogenesis: glucose-induced activation of protein kinase C isoforms; increased formation of glucose-derived advanced glycation end-products; and increased glucose flux through the aldose reductase pathway. The relevance of each of these pathways is supported by animal studies in which pathway-specific inhibitors prevent various hyperglycaemia-induced abnormalities. Hyperglycaemia increases the production of reactive oxygen species inside cultured bovine aortic endothelial cells. Here we show that this increase in reactive oxygen species is prevented by an inhibitor of electron transport chain complex II, by an uncoupler of oxidative phosphorylation, by uncoupling protein-1 and by manganese superoxide dismutase. Normalizing levels of mitochondrial reactive oxygen species with each of these agents prevents glucose-induced activation of protein kinase C, formation of advanced glycation end-products, sorbitol accumulation and NFκB activation.",2000,23,4006,158,21,75,144,188,161,218,210,226,187,194
862b60c9d8d5c522db074d0d7bf2a60d78968645,"Frontier production functions are important for the prediction of technical efficiencies of individual firms in an industry. A stochastic frontier production function model for panel data is presented, for which the firm effects are an exponential function of time. The best predictor for the technical efficiency of an individual firm at a particular time period is presented for this time-varying model. An empirical example is presented using agricultural data for paddy farmers in a village in India.",1992,43,2735,319,4,7,5,10,14,10,18,34,41,33
74fbaf9f0f2d9f132a9f5382501ffc9e340162ef,"Microalgae represent an exceptionally diverse but highly specialized group of micro-organisms adapted to various ecological habitats. Many microalgae have the ability to produce substantial amounts (e.g. 20-50% dry cell weight) of triacylglycerols (TAG) as a storage lipid under photo-oxidative stress or other adverse environmental conditions. Fatty acids, the building blocks for TAGs and all other cellular lipids, are synthesized in the chloroplast using a single set of enzymes, of which acetyl CoA carboxylase (ACCase) is key in regulating fatty acid synthesis rates. However, the expression of genes involved in fatty acid synthesis is poorly understood in microalgae. Synthesis and sequestration of TAG into cytosolic lipid bodies appear to be a protective mechanism by which algal cells cope with stress conditions, but little is known about regulation of TAG formation at the molecular and cellular level. While the concept of using microalgae as an alternative and renewable source of lipid-rich biomass feedstock for biofuels has been explored over the past few decades, a scalable, commercially viable system has yet to emerge. Today, the production of algal oil is primarily confined to high-value specialty oils with nutritional value, rather than commodity oils for biofuel. This review provides a brief summary of the current knowledge on oleaginous algae and their fatty acid and TAG biosynthesis, algal model systems and genomic approaches to a better understanding of TAG production, and a historical perspective and path forward for microalgae-based biofuel research and commercialization.",2008,179,3167,215,9,51,122,187,245,356,341,347,297,300
96f35d6c1ce2417bdb6c1dfc60fe735bf937821d,Are you looking to uncover the wealth of networks how social production transforms markets and freedom Digitalbook. Correct here it is possible to locate as well as download the wealth of networks how social production transforms markets and freedom Book. We've got ebooks for every single topic the wealth of networks how social production transforms markets and freedom accessible for download cost-free. Search the site also as find Jean Campbell eBook in layout. We also have a fantastic collection of information connected to this Digitalbook for you. As well because the best part is you could assessment as well as download for the wealth of networks how social production transforms markets and freedom eBook,2007,1,2305,294,73,101,131,157,168,184,226,206,167,163
e21b66810bc19b7145affd127591671dc924817d,"Previous studies of the so-called frontier production function have not utilized an adequate characterization of the disturbance term for such a model. In this paper we provide an appropriate specification, by defining the disturbance term as the sum of symmetric normal and (negative) half-normal random variables. Various aspects of maximum-likelihood estimation for the coefficients of a production function with an additive disturbance term of this sort are then considered.",1977,17,4900,94,2,5,7,12,6,7,10,8,9,14
5f06f0f5202c32631c81fdda2a419b0fc113bceb,"Lignocellulosic biomass has long been recognized as a potential sustainable source of mixed sugars for fermentation to biofuels and other biomaterials. Several technologies have been developed during the past 80 years that allow this conversion process to occur, and the clear objective now is to make this process cost-competitive in today's markets. Here, we consider the natural resistance of plant cell walls to microbial and enzymatic deconstruction, collectively known as “biomass recalcitrance.” It is this property of plants that is largely responsible for the high cost of lignocellulose conversion. To achieve sustainable energy production, it will be necessary to overcome the chemical and structural properties that have evolved in biomass to prevent its disassembly.",2007,34,3694,104,18,83,102,180,249,258,339,373,324,367
dbaf5a3e49a493d0bdfe295a530917461ef2fec2,"The efficiency of crop production is defined in thermodynamic terms as the ratio of energy output (carbohydrate) to energy input (solar radiation). Temperature and water supply are the main climatic constraints on efficiency. Over most of Britain, the radiation and thermal climates are uniform and rainfall is the main discriminant of yield between regions. Total production of dry matter by barley, potatoes, sugar beet, and apples is strongly correlated with intercepted radiation and these crops form carbohydrate at about 1.4 g per MJ solar energy, equivalent to 2.4% efficiency. Crop growth in Britain may therefore be analysed in terms of ( a ) the amount of light intercepted during the growing season and ( b ) the efficiency with which intercepted light is used. The amount intercepted depends on the seasonal distribution of leaf area which, in turn, depends on temperature and soil water supply. These variables are discussed in terms of the rate and duration of development phases. A factorial analysis of efficiency shows that the major arable crops in Britain intercept only about 40 % of annual solar radiation and their efficiency for supplying energy through economic yield is only about 0.3%. Some of the factors responsible for this figure are well understood and some are immutable. More work is needed to identify the factors responsible for the large differences between average commercial and record yields.",1977,18,3201,179,0,1,6,6,6,8,11,13,29,20
ddbbf87a2a0ec74f9c62239ddf3abdb45afd9c7d,"N RECENT YEARS, public and professional interest in schools has been heightened by a spate of reports, many of them critical of current school policy.' These policy documents have added to persistent and long-standing concerns about the cost, effectiveness, and fairness of the current school structure, and have made schooling once again a serious public issue. As in the past, however, any renewed interest in education is likely to be short-lived, doomed to dissipate as frustration over the inability of policy to improve school practice sets in. This frustration about school policy relates directly to knowledge about the educational production process and in turn to underlying research on schools. Although the educational process has been extensively researched, clear policy prescriptions flowing from this research have been difficult to derive.2 There exists, however, a consistency to the research findings that does have an immediate application to school policy: Schools differ dramatically in ""quality,""",1986,109,3076,204,2,7,15,20,23,40,44,36,49,49
5d79d86d4501b4a5381a7a8cc3562bf66cf975e6,,1960,1,3194,290,3,1,3,2,3,3,6,4,4,8
3a902940adf403f18ce778e394953142906f9cf3,,1993,0,3097,188,0,3,15,18,22,23,28,38,41,57
11339a042ddff3670b07dec625c9d7ac91d92dd0,"The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These operations are under the control of a schema, which is a theoretical formulation of the comprehender's goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is concerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psychological research report, and methods for the empirical testing of the model are developed.",1978,73,4653,58,2,16,68,57,81,108,93,105,131,71
3a932920c44c06b43fc24393c8710dfd2238eb37,"Biofuels produced from various lignocellulosic materials, such as wood, agricultural, or forest residues, have the potential to be a valuable substitute for, or complement to, gasoline. Many physicochemical structural and compositional factors hinder the hydrolysis of cellulose present in biomass to sugars and other organic compounds that can later be converted to fuels. The goal of pretreatment is to make the cellulose accessible to hydrolysis for conversion to fuels. Various pretreatment techniques change the physical and chemical structure of the lignocellulosic biomass and improve hydrolysis rates. During the past few years a large number of pretreatment methods have been developed, including alkali treatment, ammonia explosion, and others. Many methods have been shown to result in high sugar yields, above 90% of the theoretical yield for lignocellulosic biomasses such as woods, grasses, corn, and so on. In this review, we discuss the various pretreatment process methods and the recent literature that...",2009,136,3106,171,6,62,155,224,267,278,295,365,365,297
8ca7cd110597890534158e13e2e46519408afab2,"A number of in situ cosmogenic radionuclides and stable nuclides have been measured in natural exposed rock surfaces with a view to study their in situ production and rock erosion rates [1]. The in situ radionuclides can be used for a high-resolution tomography of the erosional history of an exposed surface; two stable nuclides (3He, 21Ne) and five radionuclides (10Be, 26Al, 36Cl, 14C, 39Ar) having half-lives in the range of ∼ 300-1.5 × 106 yr half-life are measurable in many rock types. 
 
A prerequisite for the application of the in situ nuclides for the study of erosional histories of surfaces is a knowledge of their production rates under different irradiation conditions; altitude, latitude, irradiation geometry and shielding. Relative nuclide production rates can be determined fairly accurately using the extensive available data on cosmic ray neutrons [2]. Absolute nuclide production rates cannot generally be predicted with any accuracy because of lack of data on excitation functions of nuclides unless some normalization is possible, as was done in the case of several cosmic ray produced isotopes in the atmosphere [3]. Based on a recent natural calibration experiment in which erosion free surfaces exposed to cosmic radiation for ∼ 11,000 yrs were sampled, the absolute production rates of 10Be and 26Al in quartz have been accurately estimated for mountain altitudes in Sierra Nevada [4]. The absolute production rates of 10Be and 26Al in quartz can therefore be estimated fairly accurately for any given latitude and altitude. Some measurements of 14C in rocks of low erosion rate [5] similarly allow an estimate of its production rate. Attempts made to measure the in situ production rates of 3He in rocks have not yet led to a convergent production rate. In view of the importance of knowing the production rates of isotopes of He, Ne and Ar, I present here theoretical estimates of their production rates based on available cross-section data. 
 
I discuss the information that can be extracted from the study of the in situ nuclides in rocks. Useful parameters characterizing the exposure history of a rock surface are: (1) the effective surface exposure age; and (2) the time-averaged erosion rate. The implications of these parameters for single and multiple nuclide studies are discussed in terms of the erosion models considered.",1991,27,2216,400,4,5,9,18,15,15,12,19,20,37
ad06686fc56233f4e8dcaf38403a8c3bf9770a26,,1970,9,3129,212,1,0,6,13,16,5,9,16,26,13
3eef25fd2af656581206fb7fb7aa1b632fea9e3e,,1977,3,5859,62,1,4,6,6,3,7,5,6,2,7
6593e6dad83e8ab574da70a29d638ff654ce5151,"We constructed an infectious molecular clone of acquired immunodeficiency syndrome-associated retrovirus. Upon transfection, this clone directed the production of infectious virus particles in a wide variety of cells in addition to human T4 cells. The progeny, infectious virions, were synthesized in mouse, mink, monkey, and several human non-T cell lines, indicating the absence of any intracellular obstacle to viral RNA or protein production or assembly. During the course of these studies, a human colon carcinoma cell line, exquisitely sensitive to DNA transfection, was identified.",1986,55,2795,200,0,12,17,24,50,45,84,59,78,81
943ea1b0f125611fadd677c2b70763403ecd0cb9,"Anaerobic digestion of energy crops, residues, and wastes is of increasing interest in order to reduce the greenhouse gas emissions and to facilitate a sustainable development of energy supply. Production of biogas provides a versatile carrier of renewable energy, as methane can be used for replacement of fossil fuels in both heat and power generation and as a vehicle fuel. For biogas production, various process types are applied which can be classified in wet and dry fermentation systems. Most often applied are wet digester systems using vertical stirred tank digester with different stirrer types dependent on the origin of the feedstock. Biogas is mainly utilized in engine-based combined heat and power plants, whereas microgas turbines and fuel cells are expensive alternatives which need further development work for reducing the costs and increasing their reliability. Gas upgrading and utilization as renewable vehicle fuel or injection into the natural gas grid is of increasing interest because the gas can be used in a more efficient way. The digestate from anaerobic fermentation is a valuable fertilizer due to the increased availability of nitrogen and the better short-term fertilization effect. Anaerobic treatment minimizes the survival of pathogens which is important for using the digested residue as fertilizer. This paper reviews the current state and perspectives of biogas production, including the biochemical parameters and feedstocks which influence the efficiency and reliability of the microbial conversion and gas yield.",2009,103,2207,175,2,28,67,103,146,194,238,239,263,242
0816dab8cfac8b622afc557e3d995916a488dccb,"Nano-sized TiO2 photocatalytic water-splitting technology has great potential for low-cost, environmentally friendly solar-hydrogen production to support the future hydrogen economy. Presently, the solar-to-hydrogen energy conversion efficiency is too low for the technology to be economically sound. The main barriers are the rapid recombination of photo-generated electron/hole pairs as well as backward reaction and the poor activation of TiO2 by visible light. In response to these deficiencies, many investigators have been conducting research with an emphasis on effective remediation methods. Some investigators studied the effects of addition of sacrificial reagents and carbonate salts to prohibit rapid recombination of electron/hole pairs and backward reactions. Other research focused on the enhancement of photocatalysis by modification of TiO2 by means of metal loading, metal ion doping, dye sensitization, composite semiconductor, anion doping and metal ion-implantation. This paper aims to review the up-to-date development of the above-mentioned technologies applied to TiO2 photocatalytic hydrogen production. Based on the studies reported in the literature, metal ion-implantation and dye sensitization are very effective methods to extend the activating spectrum to the visible range. Therefore, they play an important role in the development of efficient photocatalytic hydrogen production.",2007,124,3343,23,19,46,67,87,152,210,234,299,321,330
3e88d8a52759097d481b92ec0d06606afbadafab,"Apoptosis in vivo is followed almost inevitably by rapid uptake into adjacent phagocytic cells, a critical process in tissue remodeling, regulation of the immune response, or resolution of inflammation. Phagocytosis of apoptotic cells by macrophages has been suggested to be a quiet process that does not lead to production of inflammatory mediators. Here we show that phagocytosis of apoptotic neutrophils (in contrast to immunoglobulin G-opsonized apoptotic cells) actively inhibited the production of interleukin (IL)-1beta, IL-8, IL-10, granulocyte macrophage colony-stimulating factor, and tumor necrosis factor-alpha, as well as leukotriene C4 and thromboxane B2, by human monocyte-derived macrophages. In contrast, production of transforming growth factor (TGF)-beta1, prostaglandin E2, and platelet-activating factor (PAF) was increased. The latter appeared to be involved in the inhibition of proinflammatory cytokine production because addition of exogenous TGF-beta1, prostaglandin E2, or PAF resulted in inhibition of lipopolysaccharide-stimulated cytokine production. Furthermore, anti-TGF-beta antibody, indomethacin, or PAF receptor antagonists restored cytokine production in lipopolysaccharide-stimulated macrophages that had phagocytosed apoptotic cells. These results suggest that binding and/or phagocytosis of apoptotic cells induces active antiinflammatory or suppressive properties in human macrophages. Therefore, it is likely that resolution of inflammation depends not only on the removal of apoptotic cells but on active suppression of inflammatory mediator production. Disorders in either could result in chronic inflammatory diseases.",1998,52,2972,148,14,56,76,101,106,145,123,126,152,145
ba274095c25b1917367acb0d22d6516e96ea4581,"Abstract The error term in the stochastic frontier model is of the form ( v – u ), where v is a normal error term representing pure randomness, and u is a non-negative error term representing technical inefficiency. The entire ( v – u ) is easily estimated for each observation, but a previously unsolved problem is how to separate it into its two components, v and u . This paper suggests a solution to this problem, by considering the expected value of u , conditional on ( v – u ). An explicit formula is given for the half-normal and exponential cases.",1982,6,3196,181,2,1,8,6,9,7,10,23,15,19
12a1245fef2f0fa02260ee1514c924acfc04bb4b,THE CONTEXT AND IMPORTANCE OF INVENTORY MANAGEMENT AND PRODUCTION PLANNING AND SCHEDULING. The Importance of Inventory Management and Production Planning and Scheduling. Strategic Issues. Frameworks for Inventory Management and Production Planning and Scheduling. Forecasting. TRADITIONAL REPLENISHMENT SYSTEMS FOR MANAGING INDIVIDUAL--ITEM INVENTORIES. Order Quantities When Demand is Approximately Level. Lot Sizing for Individual Items with Time--Varying Demand. Individual Items with Probabilistic Demand. SPECIAL CLASSES OF ITEMS. Managing the Most Important (Class A) Inventories. Managing Routine (Class C) Inventories. Style Goods and Perishable Items. THE COMPLEXITIES OF MULTIPLE ITEMS AND MULTIPLE LOCATIONS. Coorinated Replenishments at a Single Stocking Point. Supply Chain Management and Multiechelon Inventories. PRODUCTION PLANNING AND SCHEDULING. An Overall Framework for Production Planning and Scheduling. Medium--Range Aggregate Production Planning. Material Requirements Planning and its Extensions. Just--in--Time and Optimized Production Technology. Short--Range Production Scheduling. Summary. Appendices. Indexes.,1998,1,2628,186,3,19,43,65,80,94,100,128,136,156
653019d2f88c9102b5a01a530b18e4a9ff61d82f,"Notes on contributors Acknowledgements 1. The Idiom of Co-production Sheila Jasanoff 2. Ordering Knowledge, Ordering Society Sheila Jasanoff 3. Climate Science and the Making of a Global Political Order Clark A. Miller 4. Co-producing CITES and the African Elephant Charis Thompson 5. Knowledge and Political Order in the European Environment Agency Claire Waterton and Brian Wynne 6. Plants, Power and Development: Founding the Imperial Department of Agriculture for the West Indies, 1880-1914 William K. Storey 7. Mapping Systems and Moral Order: Constituting property in genome laboratories Stephen Hilgartner 8. Patients and Scientists in French Muscular Dystrophy Research Vololona Rabeharisoa and Michel Callon 9. Circumscribing Expertise: Membership categories in courtroom testimony Michael Lynch 10. The Science of Merit and the Merit of Science: Mental order and social order in early twentieth-century France and America John Carson 11. Mysteries of State, Mysteries of Nature: Authority, knowledge and expertise in the seventeenth century Peter Dear 12. Reconstructing Sociotechnical Order: Vannevar Bush and US science policy Michael Aaron Dennis 13. Science and the Political Imagination in Contemporary Democracies Yaron Ezrah 14. Afterword Sheila Jasanoff References Index",2004,0,2822,151,5,12,31,52,73,97,116,136,154,170
784b1ecff1021f33344fae1bb2e43cb26fa187c8,"Identifying and building a sustainable energy system are perhaps two of the most critical issues that today's society must address. Replacing our current energy carrier mix with a sustainable fuel is one of the key pieces in that system. Hydrogen as an energy carrier, primarily derived from water, can address issues of sustainability, environmental emissions, and energy security. Issues relating to hydrogen production pathways are addressed here. Future energy systems require money and energy to build. Given that the United States has a finite supply of both, hard decisions must be made about the path forward, and this path must be followed with a sustained and focused effort.",2004,8,3755,12,2,11,27,27,43,23,44,64,79,117
cc44a5f5387b5bfe375f566cc396ad20ff0c36a4,,1997,0,2390,251,6,6,12,29,35,50,76,68,79,109
28ef2fb8d1c03235801489d68244fa8184381a19,"Biodiesel is gaining more and more importance as an attractive fuel due to the depleting fossil fuel resources. Chemically biodiesel is monoalkyl esters of long chain fatty acids derived from renewable feed stock like vegetable oils and animal fats. It is produced by transesterification in which, oil or fat is reacted with a monohydric alcohol in presence of a catalyst. The process of transesterification is affected by the mode of reaction condition, molar ratio of alcohol to oil, type of alcohol, type and amount of catalysts, reaction time and temperature and purity of reactants. In the present paper various methods of preparation of biodiesel with different combination of oil and catalysts have been described. The technical tools and processes for monitoring the transesterification reactions like TLC, GC, HPLC, GPC, 1H NMR and NIR have also been summarized. In addition, fuel properties and specifications provided by different countries are discussed.",2006,57,3117,101,15,56,98,170,209,231,244,312,287,259
3d1b77f644961a31a5508d6423575c702049a7ef,Translatora s Acknowledgements. 1. Plan of the Present Work. 2. Social Space. 3. Spatial Architectonics. 4. From Absolute Space to Abstract Space. 5. Contradictory Space. 6. From the Contradictions of Space to Differential Space. 7. Openings and Conclusions. Afterword by David Harvey. Index.,1992,0,3858,12,2,16,18,13,18,29,38,54,24,48
066a12b69280f53cc074d13ac47ab1d369fbcde8,"Solid lipid nanoparticles (SLN) have attracted increasing attention during recent years. This paper presents an overview about the selection of the ingredients, different ways of SLN production and SLN applications. Aspects of SLN stability and possibilities of SLN stabilization by lyophilization and spray drying are discussed. Special attention is paid to the relation between drug incorporation and the complexity of SLN dispersions, which includes the presence of alternative colloidal structures (liposomes, micelles, drug nanosuspensions, mixed micelles, liquid crystals) and the physical state of the lipid (supercooled melts, different lipid modifications). Appropriate analytical methods are needed for the characterization of SLN. The use of several analytical techniques is a necessity. Alternative structures and dynamic phenomena on the molecular level have to be considered. Aspects of SLN administration and the in vivo fate of the carrier are discussed.",2001,124,2565,130,2,14,24,30,38,43,62,65,101,106
70b5a5f938996e66f1f4851b4bcb726dce4acec4,* Starting from Need* Evolution of the Toyota Production System* Further Development* Genealogy of the Toyota Production System* The True Intention of the Ford System* Surviving the Low-Growth Period,1988,0,1964,290,0,0,0,1,0,2,0,3,4,4
3c9dc71914a48574811458ce0056831a0fa31f52,"Part 1 The field of cultural production: the field of cultural production, or - the economic world reversed the production of belief - contribution to an economy of symbolic goods the market of symbolic goods. Part 2 Flaubert and the French literary field: is the structure of ""sentimental education"" an instance of social self-analysis? field of power, literary field and habitus principles for a sociology of cultural works Flaubert's point of view. Part 3 The pure gaze - essays on art: outline of a sociological theory of art perception the institutionalization of Anomie the historical genesis of a pure aesthetic.",1993,0,2597,162,0,2,5,4,12,10,10,26,34,47
fd671723ea64a0fdfc216d09197e887972104744,"IL-10 inhibits the ability of macrophage but not B cell APC to stimulate cytokine synthesis by Th1 T cell clones. In this study we have examined the direct effects of IL-10 on both macrophage cell lines and normal peritoneal macrophages. LPS (or LPS and IFN-gamma)-induced production of IL-1, IL-6, and TNF-alpha proteins was significantly inhibited by IL-10 in two macrophage cell lines. Furthermore, IL-10 appears to be a more potent inhibitor of monokine synthesis than IL-4 when added at similar concentrations. LPS or LPS- and IFN-gamma-induced expression of IL-1 alpha, IL-6, or TNF-alpha mRNA was also inhibited by IL-10 as shown by semiquantitative polymerase chain reaction or Northern blot analysis. Inhibition of LPS-induced IL-6 secretion by IL-10 was less marked in FACS-purified peritoneal macrophages than in the macrophage cell lines. However, IL-6 production by peritoneal macrophages was enhanced by addition of anti-IL-10 antibodies, implying the presence in these cultures of endogenous IL-10, which results in an intrinsic reduction of monokine synthesis after LPS activation. Consistent with this proposal, LPS-stimulated peritoneal macrophages were shown to directly produce IL-10 detectable by ELISA. Furthermore, IFN-gamma was found to enhance IL-6 production by LPS-stimulated peritoneal macrophages, and this could be explained by its suppression of IL-10 production by this same population of cells. In addition to its effects on monokine synthesis, IL-10 also induces a significant change in morphology in IFN-gamma-stimulated peritoneal macrophages. The potent action of IL-10 on the macrophage, particularly at the level of monokine production, supports an important role for this cytokine not only in the regulation of T cell responses but also in acute inflammatory responses.",1991,0,2934,92,0,26,54,73,90,114,111,158,169,151
3354c8ce02811f46e500e7787b3d4acd567ee453,"Abstract Our research addresses the confusion and inconsistency associated with “lean production.” We attempt to clarify the semantic confusion surrounding lean production by conducting an extensive literature review using a historical evolutionary perspective in tracing its main components. We identify a key set of measurement items by charting the linkages between measurement instruments that have been used to measure its various components from the past literature, and using a rigorous, two-stage empirical method and data from a large set of manufacturing firms, we narrow the list of items selected to represent lean production to 48 items, empirically identifying 10 underlying components. In doing so, we map the operational space corresponding to conceptual space surrounding lean production. Configuration theory provides the theoretical underpinnings and helps to explain the synergistic relationships among its underlying components.",2007,84,1950,185,1,15,35,69,91,128,152,175,159,153
bc3a9a223aa89a85232fab3d5b3eda3525af0be7,"The reaction centers of PSI and PSII in chloroplast thylakoids are the major generation site of reactive oxygen species (ROS). Photoreduction of oxygen to hydrogen peroxide (H2O2) in PSI was discovered over 50 years ago by [Mehler (1951)][1]. Subsequently, the primary reduced product was identified",2006,52,2114,189,2,38,62,78,98,92,125,135,178,187
eda66985466dac06ea756d7a5e0336f95c8c81de,"The cosmic ray flux increases at higher altitude as air pressure and the shielding effect of the atmosphere decrease. Altitude-dependent scaling factors are required to compensate for this effect in calculating cosmic ray exposure ages. Scaling factors in current use assume a uniform relationship between altitude and atmospheric pressure over the Earth's surface. This masks regional differences in mean annual pressure and spatial variation in cosmogenic isotope production rates. Outside Antarctica, air pressures over land depart from the standard atmosphere by ±4.4 hPa (1σ) near sea level, corresponding to offsets of ±3–4% in isotope production rates. Greater offsets occur in regions of persistent high and low pressure such as Siberia and Iceland, where conventional scaling factors predict production rates in error by ±10%. The largest deviations occur over Antarctica where ground level pressures are 20–40 hPa lower than the standard atmosphere at all altitudes. Isotope production rates in Antarctica are therefore 25–30% higher than values calculated by scaling Northern Hemisphere production rates with conventional scaling factors. Exposure ages of old Antarctic surfaces, especially those based on cosmogenic radionuclides at levels close to saturation, may be millions of years younger than published estimates.",2000,27,1908,397,1,10,16,23,37,45,78,83,72,97
7af58e0d3f3f6b7d3c498849914d30b2cca8b995,"Although the concept of justification has played a significant role in many social psychological theories, its presence in recent examinations of stereotyping has been minimal. We describe and evaluate previous notions of stereotyping as ego-justification and group-justification and propose an additional account, that of system-justification, which refers to psychological processes contributing to the preservation of existing social arrangements even at the expense of personal and group interest. It is argued that the notion of system-justification is necessary to account for previously unexplained phenomena, most notably the participation by disadvantaged individuals and groups in negative stereotypes of themselves, and the consensual nature of stereotypic beliefs despite differences in social relations within and between social groups. We offer a selective review of existing research that demonstrates the role of stereotypes in the production of false consciousness and develop the implications of a system-justification approach. 
 
[T]he rationalizing and justifying function of a stereotype exceeds its function as a reflector of group attributes—G. W. Allport (1958, p. 192).",1994,211,2486,133,1,5,9,8,17,28,28,37,53,45
70cf53a7e288df25c39c9d212eae3cf4452a3a26,This paper first sets out the main features of the eclectic theory of international production and then seeks to evaluate its significance of ownership- and location- specific variables in explaining the industrial pattern and geographical distribution of the sales of U.S. affiliates in fourteen manufacturing industries in seven countries in 1970.,1980,29,2711,170,0,4,2,2,3,2,4,1,5,5
9af8e429d9c65fb0b2ca18f5bc721426b1f432e2,Each of a collection of items are to be produced on two machines (or stages). Each machine can handle only one item at a time and each item must be processed through machine one and then through machine two. The setup time plus work time for each item for each machine is known. A simple decision rule is obtained in this paper for the optimal scheduling of the production so that the total elapsed time is a minimum. A three‐machine problem is also discussed and solved for a restricted case.,1954,0,3052,262,3,0,4,2,2,4,4,3,1,0
5fbe31a3c6c44df33eb245f591f43a3c33b1fb66,"The concept sectoral system of innovation and production provides a multidimensional, integrated and dynamic view of sectors. It is proposed that a sectoral system is a set of products and the set of agents carrying out market and non-market interactions for the creation, production and sale of those products. A sectoral systems has a specific knowledge base, technologies, inputs and demand. Agents are individuals and organizations at various levels of aggregation. They interact through processes of communication, exchange, co-operation, competition and command, and these interactions are shaped by institutions. A sectoral system undergoes change and transformation through the co-evolution of its various elements. © 2002 Elsevier Science B.V. All rights reserved.",2002,143,2377,161,20,26,46,43,63,78,110,116,131,156
ec2ca0daeeae2a4a1023369ac6b5df8a4e6fd4c9,"The capacity of 12 cytokines to induce NO2- or H2O2 release from murine peritoneal macrophages was tested by using resident macrophages, or macrophages elicited with periodate, casein, or thioglycollate broth. Elevated H2O2 release in response to PMA was observed in resident macrophages after a 48-h incubation with IFN-gamma, TNF-alpha, TNF-beta, or CSF-GM. Of these, only IFN-gamma induced substantial NO2- secretion during the culture period. The cytokines inactive in both assays under the conditions tested were IL-1 beta, IL-2, IL-3, IL-4, IFN-alpha, IFN-beta, CSF-M, and transforming growth factor-beta 1. Incubation of macrophages with IFN-gamma for 48 h in the presence of LPS inhibited H2O2 production but augmented NO2- release, whereas incubation in the presence of the arginine analog NG-monomethylarginine inhibited NO2- release but not H2O2 production. Although neither TNF-alpha nor TNF-beta induced NO2- synthesis on its own, addition of either cytokine together with IFN-gamma increased macrophage NO2- production up to six-fold over that in macrophages treated with IFN-gamma alone. Moreover, IFN-alpha or IFN-beta in combination with LPS could also induce NO2- production in macrophages, as was previously reported for IFN-gamma plus LPS. These data suggest that: 1) tested as a sole agent, IFN-gamma was the only one of the 12 cytokines capable of inducing both NO2- and H2O2 release; 2) the pathways leading to secretion of H2O2 and NO2- are independent; 3) either IFN-gamma and TNF-alpha/beta or IFN-alpha/beta/gamma and LPS can interact synergistically to induce NO2- release.",1988,0,2800,57,1,9,19,40,55,83,109,143,162,137
4fed748c8cd0adfd353c5ba79ab666e33242125b,"Global industrialization is the result of an integrated system of production and trade. Open international trade has encouraged nations to specialize in different branches of manufacturing and even in different stages of production within a specific industry. This process, fueled by the explosion of new products and new technologies since World War II, has led to the emergence of a global manufacturing system in which production capacity is dispersed to an unprecedented number of developing as well as industrialized countries (Harris, 1987; Gereffi, 1989b). The revolution in transportation and communications technology has permitted manufacturers and retailers alike to establish international production and trade networks that cover vast geographical distances. While considerable attention has been given to the involvement of industrial capital in international contracting, the key role played by commercial capital (i.e., large retailers and brand-named companies that buy but don't make the goods they sell) in the expansion of manufactured exports from developing countries has been relatively ignored. This chapter will show how these ‘big buyers’ have shaped the production networks established in the world's most dynamic exporting countries, especially the newly industrialized countries (NICs) of East Asia. The argument proceeds in several stages. First, a distinction is made between producer-driven and buyer-driven commodity chains, which represent alternative modes of organizing international industries. These commodity chains, though primarily controlled by private economic agents, are also influenced by state policies in both the producing (exporting) and consuming (importing) countries. Second, the main organizational features of buyer-driven commodity chains are identified, using the apparel industry as a case study. The apparel commodity chain contains two very different segments. The companies that make and sell standardized clothing have production patterns and sourcing strategies that contrast with firms in the fashion segment of the industry, which has been the most actively committed to global sourcing. Recent changes within the retail sector of the United States are analyzed in this chapter to identify the emergence of new types of big buyers and to show why they have distinct strategies of global sourcing. Third, the locational patterns of global sourcing in apparel are charted, with an emphasis on the production frontiers favored by different kinds of US buyers. Several of the primary mechanisms used by big buyers to source products from overseas are outlined in order to demonstrate how transnational production systems are sustained and altered by American retailers and branded apparel companies.",1994,0,2111,226,1,2,9,8,10,16,33,39,55,59
b76c7f7247cf206ad373e30a94bc1679e3b0fab4,"Abstract Currently, hydrogen is primarily used in the chemical industry, but in the near future it will become a significant fuel. There are many processes for hydrogen production. This paper reviews the technologies related to hydrogen production from both fossil and renewable biomass resources including reforming (steam, partial oxidation, autothermal, plasma, and aqueous phase) and pyrolysis. In addition, electrolysis and other methods for generating hydrogen from water, hydrogen storage related approaches, and hydrogen purification methods such as desulfurization and water-gas-shift are discussed.",2009,238,2221,58,24,91,122,124,152,179,190,212,210,208
711dc5324ab6d95cdd1e4a7ed3f4acd64ce2abec,"Lignocelluloses are often a major or sometimes the sole components of different waste streams from various industries, forestry, agriculture and municipalities. Hydrolysis of these materials is the first step for either digestion to biogas (methane) or fermentation to ethanol. However, enzymatic hydrolysis of lignocelluloses with no pretreatment is usually not so effective because of high stability of the materials to enzymatic or bacterial attacks. The present work is dedicated to reviewing the methods that have been studied for pretreatment of lignocellulosic wastes for conversion to ethanol or biogas. Effective parameters in pretreatment of lignocelluloses, such as crystallinity, accessible surface area, and protection by lignin and hemicellulose are described first. Then, several pretreatment methods are discussed and their effects on improvement in ethanol and/or biogas production are described. They include milling, irradiation, microwave, steam explosion, ammonia fiber explosion (AFEX), supercritical CO2 and its explosion, alkaline hydrolysis, liquid hot-water pretreatment, organosolv processes, wet oxidation, ozonolysis, dilute-and concentrated-acid hydrolyses, and biological pretreatments.",2008,194,2244,102,2,21,51,113,179,247,221,212,244,211
379ffd011c11510b0ea4bf6b1940f7f5603af6c6,"The services of ecological systems and the natural capital stocks that produce them are critical to the functioning of the Earth's life-support system. They contribute to human welfare, both directly and indirectly, and therefore represent part of the total economic value of the planet. We have estimated the current economic value of 17 ecosystem services for 16 biomes, based on published studies and a few original calculations. For the entire biosphere, the value (most of which is outside the market) is estimated to be in the range of US$16-54 trillion (1012) per year, with an average of US$33 trillion per year. Because of the nature of the uncertainties, this must be considered a minimum estimate. Global gross national product total is around US$18 trillion per year.",1997,57,15926,796,0,0,0,0,0,0,0,0,1,0
986c41e8279fc669ecb48147690a92e0dc20df4f,"The use of stable isotopes to solve biogeochemical problems in ecosystem analysis is increasing rapidly because stable isotope data can contribute both source-sink (tracer) and process information: The elements C, N, S, H, and all have more than one isotope, and isotopic compositions of natural materials can be measured with great precision with a mass spectrometer. Isotopic compositions change in predictable ways as elements cycle through the biosphere. These changes have been exploited by geochemists to understand the global elemental cycles. Ecologists have not until quite recently employed these techniques. The reasons for this are, first, that most ecologists do not have the background in chemistry and geochemistry to be fully aware of the possibilities for exploiting the natural variations in stable isotopic compositions, and second, that stable isotope ratio measurements require equipment not normally available to ecologists. This is unfortunate because some of the more intractable problems in ecology can be profitably addressed using stable isotope measurements. Stable isotopes are ideally suited to increase our understanding of element cycles in ecosystems. This review is written for ecologists who would like to learn more about how stable isotope analyses have been and can be used in ecosystem studies. We begin with an explanation of isotope terminology and fractionation, then summarize isotopic distributions in the C, N, and S biogeochemical cycles, and conclude with five case studies that show how stable isotope measurements can provide crucial information for ecosystem analysis. We restrict this review to studies of natural variations in C, N, and S isotopic abundances, cxcluding from consideration ~5N enrichment studies and hydrogen and oxygen isotope studies. Our focus on C, N, and S derives in part from our",1987,95,4969,556,0,2,8,13,16,20,24,22,19,32
22b7cd42eab103c1fb75da08458733b5fbb992e9,"Humans are altering the composition of biological communities through a variety of activities that increase rates of species invasions and species extinctions, at all scales, from local to global. These changes in components of the Earth's biodiversity cause concern for ethical and aesthetic reasons, but they also have a strong potential to alter ecosystem properties and the goods and services they provide to humanity. Ecological experiments, observations, and theoretical developments show that ecosystem properties depend greatly on biodiversity in terms of the functional characteristics of organisms present in the ecosystem and the distribution and abundance of those organisms over space and time. Species effects act in concert with the effects of climate, resource availability, and disturbance regimes in influencing ecosystem properties. Human activities can modify all of the above factors; here we focus on modification of these biotic controls. The scientific community has come to a broad consensus on many aspects of the re- lationship between biodiversity and ecosystem functioning, including many points relevant to management of ecosystems. Further progress will require integration of knowledge about biotic and abiotic controls on ecosystem properties, how ecological communities are struc- tured, and the forces driving species extinctions and invasions. To strengthen links to policy and management, we also need to integrate our ecological knowledge with understanding of the social and economic constraints of potential management practices. Understanding this complexity, while taking strong steps to minimize current losses of species, is necessary for responsible management of Earth's ecosystems and the diverse biota they contain.",2005,566,6399,329,28,136,225,281,283,347,422,433,465,483
137128a6623ab5265983db14636a0df94b954735,"Understanding the negative and positive effects of agricultural land use for the conservation of biodiversity, and its relation to ecosystem services, needs a landscape perspective. Agriculture can contribute to the conservation of high-diversity systems, which may provide important ecosystem services such as pollination and biological control via complementarity and sampling effects. Land-use management is often focused on few species and local processes, but in dynamic, agricultural landscapes, only a diversity of insurance species may guarantee resilience (the capacity to reorganize after disturbance). Interacting species experience their surrounding landscape at different spatial scales, which influences trophic interactions. Structurally complex landscapes enhance local diversity in agroecosystems, which may compensate for local high-intensity management. Organisms with high-dispersal abilities appear to drive these biodiversity patterns and ecosystem services, because of their recolonization ability and larger resources experienced. Agri-environment schemes (incentives for farmers to benefit the environment) need to broaden their perspective and to take the different responses to schemes in simple (high impact) and complex (low impact) agricultural landscapes into account. In simple landscapes, local allocation of habitat is more important than in complex landscapes, which are in total at risk. However, little knowledge of the relative importance of local and landscape management for biodiversity and its relation to ecosystem services make reliable recommendations difficult.",2005,175,3366,243,1,25,77,82,104,176,172,202,249,248
967fd616d68c0663e3ab02cc86b6b0f6f771890b,"Climate change due to greenhouse gas emissions is predicted to raise the mean global temperature by 1.0–3.5°C in the next 50–100 years. The direct and indirect effects of this potential increase in temperature on terrestrial ecosystems and ecosystem processes are likely to be complex and highly varied in time and space. The Global Change and Terrestrial Ecosystems core project of the International Geosphere-Biosphere Programme has recently launched a Network of Ecosystem Warming Studies, the goals of which are to integrate and foster research on ecosystem-level effects of rising temperature. In this paper, we use meta-analysis to synthesize data on the response of soil respiration, net N mineralization, and aboveground plant productivity to experimental ecosystem warming at 32 research sites representing four broadly defined biomes, including high (latitude or altitude) tundra, low tundra, grassland, and forest. Warming methods included electrical heat-resistance ground cables, greenhouses, vented and unvented field chambers, overhead infrared lamps, and passive night-time warming. Although results from individual sites showed considerable variation in response to warming, results from the meta-analysis showed that, across all sites and years, 2–9 years of experimental warming in the range 0.3–6.0°C significantly increased soil respiration rates by 20% (with a 95% confidence interval of 18–22%), net N mineralization rates by 46% (with a 95% confidence interval of 30–64%), and plant productivity by 19% (with a 95% confidence interval of 15–23%). The response of soil respiration to warming was generally larger in forested ecosystems compared to low tundra and grassland ecosystems, and the response of plant productivity was generally larger in low tundra ecosystems than in forest and grassland ecosystems. With the exception of aboveground plant productivity, which showed a greater positive response to warming in colder ecosystems, the magnitude of the response of these three processes to experimental warming was not generally significantly related to the geographic, climatic, or environmental variables evaluated in this analysis. This underscores the need to understand the relative importance of specific factors (such as temperature, moisture, site quality, vegetation type, successional status, land-use history, etc.) at different spatial and temporal scales, and suggests that we should be cautious in ""scaling up"" responses from the plot and site level to the landscape and biome level. Overall, ecosystem-warming experiments are shown to provide valuable insights on the response of terrestrial ecosystems to elevated temperature.",2001,123,892,51,0,1,0,1,1,4,6,10,3,5
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
29e0e64cab4edba28ae2f8fe2d54913cea248756,"The principles of ecological succession bear importantly on the relationships between man and nature. The framework of successional theory needs to be examined as a basis for resolving man’s present environmental crisis. Most ideas pertaining to the development of ecological systems are based on descriptive data obtained by observing changes in biotic communities over long periods, or on highly theoretical assumptions; very few of the generally accepted hypotheses have been tested experimentally. Some of the confusion, vagueness, and lack of experimental work in this area stems from the tendency of ecologists to regard “succession” as a single straightforward idea; in actual fact, it entails an interacting complex of processes, some of which counteract one another.",1969,51,4727,232,6,12,7,28,21,27,29,27,33,46
dbe748c03a04d81ba6ce7e8825fb9a896e75b78a,"Abstract. Abstract Research on fragmented ecosystems has focused mostly on the biogeograpbic consequences of the creation of habitat “islands” of different sizes and has provided little of practical value to managers. However, ecosystem fragmentation causes large changes in the physical environment as well as biogeograpbic changes. Fragmentation generally results in a landscape that consists of remnant areas of native vegetation surrounded by a matrix of agricultural or other developed land. As a result fluxes of radiation, momentum (La, wind), water, and nutrients across the landscape are altered significantly. These in turn can have important influences on biota within remnant areas, especially at or near the edge between the remnant and the surrounding matrix. The isolation of remnant areas by clearing also has important consequences for the biota. These consequences vary with the time since isolation distance from other remnants, and degree of connectivity with other remnants. The influences of physical and biogeographic changes are modified by the size, shape, and position in the landscape of individual remnant, with larger remnants being less adversely affected by the fragmentation process. The Dynamics of remnant areas are predominantly driven by factors arising in the surrounding landscape. Management of, and research on, fragmented ecosystems should be directed at understanding and controlling these external influences as much as at the biota of the remnants themselves. There is a strong need to develop an integrated approach to landscape management that places conservation reserves in the context of the overall landscape",1991,176,4076,155,1,10,36,27,42,53,73,71,94,106
fa6e86a392d79c5e363a2dec9b2f75c5894d7eb9,"The ecological consequences of biodiversity loss have aroused considerable interest and controversy during the past decade. Major advances have been made in describing the relationship between species diversity and ecosystem processes, in identifying functionally important species, and in revealing underlying mechanisms. There is, however, uncertainty as to how results obtained in recent experiments scale up to landscape and regional levels and generalize across ecosystem types and processes. Larger numbers of species are probably needed to reduce temporal variability in ecosystem processes in changing environments. A major future challenge is to determine how biodiversity dynamics, ecosystem processes, and abiotic factors interact.",2001,72,3725,156,1,45,110,121,148,186,180,163,179,183
816d9b199d07340238fd263a6369271c556b6ca8,"Interactions between organisms are a major determinant of the distribution and abundance of species. Ecology textbooks (e.g., Ricklefs 1984, Krebs 1985, Begon et al. 1990) summarise these important interactions as intra- and interspecific competition for abiotic and biotic resources, predation, parasitism and mutualism. Conspicuously lacking from the list of key processes in most text books is the role that many organisms play in the creation, modification and maintenance of habitats. These activities do not involve direct trophic interactions between species, but they are nevertheless important and common. The ecological literature is rich in examples of habitat modification by organisms, some of which have been extensively studied (e.g. Thayer 1979, Naiman et al. 1988).",1994,89,5139,165,2,2,14,27,39,61,56,63,64,92
6ba3fb26c35e0e003103ecfd0e01c8385614d4ca,"▪ Abstract We review the evidence of regime shifts in terrestrial and aquatic environments in relation to resilience of complex adaptive ecosystems and the functional roles of biological diversity in this context. The evidence reveals that the likelihood of regime shifts may increase when humans reduce resilience by such actions as removing response diversity, removing whole functional groups of species, or removing whole trophic levels; impacting on ecosystems via emissions of waste and pollutants and climate change; and altering the magnitude, frequency, and duration of disturbance regimes. The combined and often synergistic effects of those pressures can make ecosystems more vulnerable to changes that previously could be absorbed. As a consequence, ecosystems may suddenly shift from desired to less desired states in their capacity to generate ecosystem services. Active adaptive management and governance of resilience will be required to sustain desired ecosystem states and transform degraded ecosystems...",2004,169,3119,160,5,23,57,75,98,114,169,195,208,264
4db2daec1fb9c1e8c49d0c549ea2c3af940485ba,"Abstract The concept of ecosystems services has become an important model for linking the functioning of ecosystems to human welfare. Understanding this link is critical for a wide-range of decision-making contexts. While there have been several attempts to come up with a classification scheme for ecosystem services, there has not been an agreed upon, meaningful and consistent definition for ecosystem services. In this paper we offer a definition of ecosystem services that is likely to be operational for ecosystem service research and several classification schemes. We argue that any attempt at classifying ecosystem services should be based on both the characteristics of the ecosystems of interest and a decision context for which the concept of ecosystem services is being mobilized. Because of this there is not one classification scheme that will be adequate for the many contexts in which ecosystem service research may be utilized. We discuss several examples of how classification schemes will be a function of both ecosystem and ecosystem service characteristics and the decision-making context.",2009,62,2451,155,19,80,102,157,195,237,263,275,244,231
3d73cfb71b0bb32fc94b186a16e9c24228a30ed4,I. CONTEXT * The Ecosystem Concept * Earth's Climate System * Geology and Soils * II. MECHANISMS * Terrestrial Water and Energy Balance * Carbon Input to Terrestrial Ecosystems * Terrestrial Production Processes * Terrestrial Decomposition * Terrestrial Plant Nutrient Use * Terrestrial Nutrient Cycling * Aquatic Carbon and Nutrient Cycling * Trophic Dynamics * Community Effects on Ecosystem Processes * III. PATTERNS * Temporal Dynamics * Landscape Heterogeneity and Ecosystem Dynamics * IV. INTEGRATION * Global Biogeochemical Cycles * Managing and Sustaining Ecosystem * Abbreviations * Glossary * References,2002,757,2862,202,1,9,31,49,81,74,83,143,128,175
7f66f74d21e5c61f8ce91c36a054fa6b0da230c2,"The functioning and stability of terrestrial ecosystems are determined by plant biodiversity and species composition. However, the ecological mechanisms by which plant biodiversity and species composition are regulated and maintained are not well understood. These mechanisms need to be identified to ensure successful management for conservation and restoration of diverse natural ecosystems. Here we show, by using two independent, but complementary, ecological experiments, that below-ground diversity of arbuscular mycorrhizal fungi (AMF) is a major factor contributing to the maintenance of plant biodiversity and to ecosystem functioning. At low AMF diversity, the plant species composition and overall structure of microcosms that simulate European calcareous grassland fluctuate greatly when the AMF taxa that are present are changed. Plant biodiversity, nutrient capture and productivity in macrocosms that simulate North American old-fields increase significantly with increasing AMF-species richness. These results emphasize the need to protect AMF and to consider these fungi in future management practices in order to maintain diverse ecosystems. Our results also show that microbial interactions can drive ecosystem functions such as plant biodiversity, productivity and variability.",1998,36,3153,79,0,24,66,92,108,111,138,142,143,115
fe4ae2bd0cf62ece4b265e4d09fea56c3c6b3aa5,"The Lund-Potsdam-Jena Dynamic Global Vegetation Model (LPJ) combines process-based, large-scale representations of terrestrial vegetation dynamics and land-atmosphere carbon and water exchanges in a modular framework. Features include feedback through canopy conductance between photosynthesis and transpiration and interactive coupling between these 'fast' processes and other ecosystem processes including resource competition, tissue turnover, population dynamics, soil organic matter and litter dynamics and fire disturbance. Ten plants functional types (PFTs) are differentiated by physiological, morphological, phenological, bioclimatic and fire-response attributes. Resource competition and differential responses to fire between PFTs influence their relative fractional cover from year to year. Photosynthesis, evapotranspiration and soil water dynamics are modelled on a daily time step, while vegetation structure and PFT population densities are updated annually. Simulations have been made over the industrial period both for specific sites where field measurements were available for model evaluation, and globally on a 0.5degrees x 0.5degrees grid. Modelled vegetation patterns are consistent with observations, including remotely sensed vegetation structure and phenology. Seasonal cycles of net ecosystem exchange and soil moisture compare well with local measurements. Global carbon exchange fields used as input to an atmospheric tracer transport model (TM2) provided a good fit to observed seasonal cycles of CO2 concentration at all latitudes. Simulated inter-annual variability of the global terrestrial carbon balance is in phase with and comparable in amplitude to observed variability in the growth rate of atmospheric CO2 . Global terrestrial carbon and water cycle parameters (pool sizes and fluxes) lie within their accepted ranges. The model is being used to study past, present and future terrestrial ecosystem dynamics, biochemical and biophysical interactions between ecosystems and the atmosphere, and as a component of coupled Earth system models.",2003,84,2288,185,14,38,52,69,94,106,121,135,146,141
f4f74923c8bec54b4f2e984830b320151bd7b7fa,"This paper discusses the advantages and disadvantages of the different methods that separate net ecosystem exchange (NEE) into its major components, gross ecosystem carbon uptake (GEP) and ecosystem respiration (Reco). In particular, we analyse the effect of the extrapolation of night-time values of ecosystem respiration into the daytime; this is usually done with a temperature response function that is derived from long-term data sets. For this analysis, we used 16 one-year-long data sets of carbon dioxide exchange measurements from European and US-American eddy covariance networks. These sites span from the boreal to Mediterranean climates, and include deciduous and evergreen forest, scrubland and crop ecosystems. We show that the temperature sensitivity of Reco, derived from long-term (annual) data sets, does not reflect the short-term temperature sensitivity that is effective when extrapolating from night- to daytime. Specifically, in summer active ecosystems the long",2005,60,2512,114,3,18,47,80,88,103,118,120,148,162
bd3863b2cab689edb7791df8d131403880e43e22,"The links between plant diversity and ecosystem functioning remain highly controversial. There is a growing consensus, however, that functional diversity, or the value and range of species traits, rather than species numbers per se, strongly determines ecosystem functioning. Despite its importance, and the fact that species diversity is often an inadequate surrogate, functional diversity has been studied in relatively few cases. Approaches based on species richness on the one hand, and on functional traits and types on the other, have been extremely productive in recent years, but attempts to connect their findings have been rare. Crossfertilization between these two approaches is a promising way of gaining mechanistic insight into the links between plant diversity and ecosystem processes and contributing to practical management for the conservation of diversity and ecosystem services.",2001,80,2448,121,0,10,25,33,57,54,58,57,72,73
43f8c19b183a13981d41a57e01240cfef728b127,"Humans are modifying both the identities and numbers of species in ecosystems, but the impacts of such changes on ecosystem processes are controversial. Plant species diversity, functional diversity, and functional composition were experimentally varied in grassland plots. Each factor by itself had significant effects on many ecosystem processes, but functional composition and functional diversity were the principal factors explaining plant productivity, plant percent nitrogen, plant total nitrogen, and light penetration. Thus, habitat modifications and management practices that change functional diversity and functional composition are likely to have large impacts on ecosystem processes.",1997,20,2621,107,4,25,51,73,71,68,87,78,90,82
c9ddc8456b25a1abdd515c30ea7b0dd08270fd59,"Expansion and intensification of cultivation are among the predominant global changes of this century. Intensification of agriculture by use of high-yielding crop varieties, fertilization,irrigation, and pesticides has contributed substantially to the tremendous increases in food production over the past 50 years. Land conversion and intensification,however, also alter the biotic interactions and patterns of resource availability in ecosystems and can have serious local, regional, and global environmental consequences.The use of ecologically based management strategies can increase the sustainability of agricultural production while reducing off-site consequences.",1997,44,2721,112,2,12,20,31,40,58,43,55,66,65
ffb02c149df2907aa4e395aa56b41a2461c60be4,"Summary 1. The concept of plant functional type proposes that species can be grouped according to common responses to the environment and/or common effects on ecosystem processes. However, the knowledge of relationships between traits associated with the response of plants to environmental factors such as resources and disturbances (response traits), and traits that determine effects of plants on ecosystem functions (effect traits), such as biogeochemical cycling or propensity to disturbance, remains rudimentary. 2. We present a framework using concepts and results from community ecology, ecosystem ecology and evolutionary biology to provide this linkage. Ecosystem functioning is the end result of the operation of multiple environmental filters in a hierarchy of scales which, by selecting individuals with appropriate responses, result in assemblages with varying trait composition. Functional linkages and trade-offs among traits, each of which relates to one or several processes, determine whether or not filtering by different factors gives a match, and whether ecosystem effects can be easily deduced",2002,106,2529,109,1,15,16,48,57,64,72,79,86,116
0bd82f5b93c8e2e52d584c3c9bd126fee243e61d,"Humanity is increasingly urban, but continues to depend on Nature for its survival. Cities are dependent on the ecosystems beyond the city limits, but also benefit from internal urban ecosystems. The aim of this paper is to analyze the ecosystem services generated by ecosystems within the urban area. ‘Ecosystem services’ refers to the benefits human populations derive from ecosystems. Seven different urban ecosystems have been identified: street trees; lawns:parks; urban forests; cultivated land; wetlands; lakes:sea; and streams. These systems generate a range of ecosystem services. In this paper, six local and direct services relevant for Stockholm are addressed: air filtration, micro climate regulation, noise reduction, rainwater drainage, sewage treatment, and recreational and cultural values. It is concluded that the locally generated ecosystem services have a substantial impact on the quality-of-life in urban areas and should be addressed in land-use planning. © 1999 Elsevier Science B.V. All rights reserved.",1999,39,2194,94,0,2,4,3,7,4,27,23,43,45
d6dc0c48c647b71399d03b500803566f2b1cf207,"This paper presents a modeling approach aimed at seasonal resolution of global climatic and edaphic controls on patterns of terrestrial ecosystem production and soil microbial respiration. We use satellite imagery (Advanced Very High Resolution Radiometer and International Satellite Cloud Climatology Project solar radiation), along with historical climate (monthly temperature and precipitation) and soil attributes (texture, C and N contents) from global (1°) data sets as model inputs. The Carnegie-Ames-Stanford approach (CASA) Biosphere model runs on a monthly time interval to simulate seasonal patterns in net plant carbon fixation, biomass and nutrient allocation, litterfall, soil nitrogen mineralization, and microbial CO2 production. The model estimate of global terrestrial net primary production is 48 Pg C yr−1 with a maximum light use efficiency of 0.39 g C MJ−1PAR. Over 70% of terrestrial net production takes place between 30°N and 30°S latitude. Steady state pools of standing litter represent global storage of around 174 Pg C (94 and 80 Pg C in nonwoody and woody pools, respectively), whereas the pool of soil C in the top 0.3 m that is turning over on decadal time scales comprises 300 Pg C. Seasonal variations in atmospheric CO2 concentrations from three stations in the Geophysical Monitoring for Climate Change Flask Sampling Network correlate significantly with estimated net ecosystem production values averaged over 50°–80° N, 10°–30° N, and 0°–10° N.",1993,115,2145,187,0,7,42,39,35,36,39,49,43,45
85a4155ee7d04554df0821d28a7a1fe0469beda3,"Concern is growing about the consequences of biodiversity loss for ecosystem functioning, for the provision of ecosystem services, and for human well being. Experimental evidence for a relationship between biodiversity and ecosystem process rates is compelling, but the issue remains contentious. Here, we present the first rigorous quantitative assessment of this relationship through meta-analysis of experimental work spanning 50 years to June 2004. We analysed 446 measures of biodiversity effects (252 in grasslands), 319 of which involved primary producer manipulations or measurements. Our analyses show that: biodiversity effects are weaker if biodiversity manipulations are less well controlled; effects of biodiversity change on processes are weaker at the ecosystem compared with the community level and are negative at the population level; productivity-related effects decline with increasing number of trophic links between those elements manipulated and those measured; biodiversity effects on stability measures ('insurance' effects) are not stronger than biodiversity effects on performance measures. For those ecosystem services which could be assessed here, there is clear evidence that biodiversity has positive effects on most. Whilst such patterns should be further confirmed, a precautionary approach to biodiversity management would seem prudent in the meantime.",2006,148,2169,93,4,32,83,100,108,134,171,178,205,201
86ecad6482ae56f75b5e6bd44bcb45c634a24f75,"Fabry, V. J., Seibel, B. A., Feely, R. A., and Orr, J. C. 2008. Impacts of ocean acidification on marine fauna and ecosystem processes. - ICES Journal of Marine Science, 65: 414-432.Oceanic uptake of anthropogenic carbon dioxide (CO 2 ) is altering the seawater chemistry of the world’s oceans with consequences for marine biota. Elevated partial pressure of CO 2 (pCO 2 ) is causing the calcium carbonate saturation horizon to shoal in many regions, particularly in high latitudes and regions that intersect with pronounced hypoxic zones. The ability of marine animals, most importantly pteropod molluscs, foraminifera, and some benthic invertebrates, to produce calcareous skeletal structures is directly affected by seawater CO 2 chemistry. CO 2 influences the physiology of marine organisms as well through acid-base imbalance and reduced oxygen transport capacity. The few studies at relevant pCO 2 levels impede our ability to predict future impacts on foodweb dynamics and other ecosystem processes. Here we present new observations, review available data, and identify priorities for future research, based on regions, ecosystems, taxa, and physiological processes believed to be most vulnerable to ocean acidification. We conclude that ocean acidification and the synergistic impacts of other anthropogenic stressors provide great potential for widespread changes to marine ecosystems.",2008,272,1892,154,21,83,110,145,149,219,178,164,181,167
1a15e7ee67b3e34fc7277341a0574daaac60af27,"Human-dominated marine ecosystems are experiencing accelerating loss of populations and species, with largely unknown consequences. We analyzed local experiments, long-term regional time series, and global fisheries data to test how biodiversity loss affects marine ecosystem services across temporal and spatial scales. Overall, rates of resource collapse increased and recovery potential, stability, and water quality decreased exponentially with declining diversity. Restoration of biodiversity, in contrast, increased productivity fourfold and decreased variability by 21%, on average. We conclude that marine biodiversity loss is increasingly impairing the ocean's capacity to provide food, maintain water quality, and recover from perturbations. Yet available data suggest that at this point, these trends are still reversible.",2006,102,2407,39,4,71,128,140,153,177,163,186,150,156
baebed6da6c8a7ddda676fb8b0d98d5d2168f219,"The Millennium Ecosystem Assessment (MA) introduced a new framework for analyzing social–ecological systems that has had wide influence in the policy and scientific communities. Studies after the MA are taking up new challenges in the basic science needed to assess, project, and manage flows of ecosystem services and effects on human well-being. Yet, our ability to draw general conclusions remains limited by focus on discipline-bound sectors of the full social–ecological system. At the same time, some polices and practices intended to improve ecosystem services and human well-being are based on untested assumptions and sparse information. The people who are affected and those who provide resources are increasingly asking for evidence that interventions improve ecosystem services and human well-being. New research is needed that considers the full ensemble of processes and feedbacks, for a range of biophysical and social systems, to better understand and manage the dynamics of the relationship between humans and the ecosystems on which they rely. Such research will expand the capacity to address fundamental questions about complex social–ecological systems while evaluating assumptions of policies and practices intended to advance human well-being through improved ecosystem services.",2009,79,1745,75,29,75,101,119,178,189,207,167,160,154
3283f3ca81ff395643ca49722d522e3606eca006,"Nature provides a wide range of benefits to people. There is increasing consensus about the importance of incorporating these “ecosystem services” into resource management decisions, but quantifying the levels and values of these services has proven difficult. We use a spatially explicit modeling tool, Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST), to predict changes in ecosystem services, biodiversity conservation, and commodity production levels. We apply InVEST to stakeholder-defined scenarios of land-use/land-cover change in the Willamette Basin, Oregon. We found that scenarios that received high scores for a variety of ecosystem services also had high scores for biodiversity, suggesting there is little tradeoff between biodiversity conservation and ecosystem services. Scenarios involving more development had higher commodity production values, but lower levels of biodiversity conservation and ecosystem services. However, including payments for carbon sequestration alleviates this tradeoff. Quantifying ecosystem services in a spatially explicit manner, and analyzing tradeoffs between them, can help to make natural resource decisions more effective, efficient, and defensible.",2009,85,1881,75,28,71,97,136,191,196,182,185,178,189
a20cd573efefe1eac59f51faf28e0fbc18d69e35,"Over the past decade, efforts to value and protect ecosystem services have been promoted by many as the last, best hope for making conservation mainstream – attractive and commonplace worldwide. In theory, if we can help individuals and institutions to recognize the value of nature, then this should greatly increase investments in conservation, while at the same time fostering human well-being. In practice, however, we have not yet developed the scientific basis, nor the policy and finance mechanisms, for incorporating natural capital into resource- and land-use decisions on a large scale. Here, we propose a conceptual framework and sketch out a strategic plan for delivering on the promise of ecosystem services, drawing on emerging examples from Hawai‘i. We describe key advances in the science and practice of accounting for natural capital in the decisions of individuals, communities, corporations, and governments.",2009,112,1667,78,15,48,81,106,151,187,193,142,168,159
f81183ede9e5455e82bad824718540fce6f2d1a7,"Ecosystem management that attempts to maximize the production of one ecosystem service often results in substantial declines in the provision of other ecosystem services. For this reason, recent studies have called for increased attention to development of a theoretical understanding behind the relationships among ecosystem services. Here, we review the literature on ecosystem services and propose a typology of relationships between ecosystem services based on the role of drivers and the interactions between services. We use this typology to develop three propositions to help drive ecological science towards a better understanding of the relationships among multiple ecosystem services. Research which aims to understand the relationships among multiple ecosystem services and the mechanisms behind these relationships will improve our ability to sustainably manage landscapes to provide multiple ecosystem services.",2009,70,1550,100,2,20,42,70,106,113,147,177,177,158
63825973f020269e522f87d4785f405ec9f93292,"Viruses are by far the most abundant 'lifeforms' in the oceans and are the reservoir of most of the genetic diversity in the sea. The estimated 1030 viruses in the ocean, if stretched end to end, would span farther than the nearest 60 galaxies. Every second, approximately 1023 viral infections occur in the ocean. These infections are a major source of mortality, and cause disease in a range of organisms, from shrimp to whales. As a result, viruses influence the composition of marine communities and are a major force behind biogeochemical cycles. Each infection has the potential to introduce new genetic information into an organism or progeny virus, thereby driving the evolution of both host and viral assemblages. Probing this vast reservoir of genetic and biological diversity continues to yield exciting discoveries.",2007,177,2139,111,0,50,69,84,101,131,145,168,195,192
0a66086a2f23ef968f65395f88cdb2f4d458923a,"Statistical guidelines and expert statements are now available to assist in the analysis and reporting of studies in some biomedical disciplines. We present here a more progressive resource for sample-based studies, meta-analyses, and case studies in sports medicine and exercise science. We offer forthright advice on the following controversial or novel issues: using precision of estimation for inferences about population effects in preference to null-hypothesis testing, which is inadequate for assessing clinical or practical importance; justifying sample size via acceptable precision or confidence for clinical decisions rather than via adequate power for statistical significance; showing SD rather than SEM, to better communicate the magnitude of differences in means and nonuniformity of error; avoiding purely nonparametric analyses, which cannot provide inferences about magnitude and are unnecessary; using regression statistics in validity studies, in preference to the impractical and biased limits of agreement; making greater use of qualitative methods to enrich sample-based quantitative projects; and seeking ethics approval for public access to the depersonalized raw data of a study, to address the need for more scrutiny of research and better meta-analyses. Advice on less contentious issues includes the following: using covariates in linear models to adjust for confounders, to account for individual differences, and to identify potential mechanisms of an effect; using log transformation to deal with nonuniformity of effects and error; identifying and deleting outliers; presenting descriptive, effect, and inferential statistics in appropriate formats; and contending with bias arising from problems with sampling, assignment, blinding, measurement error, and researchers' prejudices. This article should advance the field by stimulating debate, promoting innovative approaches, and serving as a useful checklist for authors, reviewers, and editors.",2009,32,5143,651,13,61,100,112,185,279,343,381,670,728
82e320e06b1c717b0d924d257aa7b6710f53a38e,"1. Oxygen is a toxic gas - an introductionto oxygen toxicity and reactive species 2. The chemistry of free radicals and related 'reactive species' 3. Antioxidant defences Endogenous and Diet Derived 4. Cellular responses to oxidative stress: adaptation, damage, repair, senescence and death 5. Measurement of reactive species 6. Reactive species can pose special problems needing special solutions. Some examples. 7. Reactive species can be useful some more examples 8. Reactive species can be poisonous: their role in toxicology 9. Reactive species and disease: fact, fiction or filibuster? 10. Ageing, nutrition, disease, and therapy: A role for antioxidants?",1985,0,20977,1140,0,0,0,0,0,0,1,0,0,0
d98c9181996d06fe9e91389c1a0385b7e2575762,"Boken presenterer en helhetlig strategi for hvordan myndigheter, helsepersonell, industri og forbrukere kan redusere medisinske feil.",2000,175,13861,443,1,1,0,3,1,0,0,11,730,837
951865c6d89e9e7822700dad42caf8bd29896d48,"It's about integrating individual clinical expertise and the best external evidence

Evidence based medicine, whose philosophical origins extend back to mid-19th century Paris and earlier, remains a hot topic for clinicians, public health practitioners, purchasers, planners, and the public. There are now frequent workshops in how to practice and teach it (one sponsored by the BMJ will be held in London on 24 April); undergraduate1 and postgraduate2 training programmes are incorporating it3 (or pondering how to do so); British centres for evidence based practice have been established or planned in adult medicine, child health, surgery, pathology, pharmacotherapy, nursing, general practice, and dentistry; the Cochrane Collaboration and Britain's Centre for Review and Dissemination in York are providing systematic reviews of the effects of health care; new evidence based practice journals are being launched; and it has become a common topic in the lay media. But enthusiasm has been mixed with some negative reaction.4 5 6 Criticism has ranged from evidence based medicine being old hat to it being a dangerous innovation, perpetrated by the arrogant to serve cost cutters and suppress clinical freedom. As evidence based medicine continues to evolve and adapt, now is a useful time to refine the discussion of what it is and what it is not.

Evidence based medicine is the conscientious, explicit, and judicious use of current best evidence in making decisions about the care of individual patients. The …",1996,43,11760,333,1,1,1,7,0,0,2,2,68,393
be75109902f5689f7114e9e0fa783a12ceaa9b3a,"SUMMARY
In 1995 the American College of Sports Medicine and the Centers for Disease Control and Prevention published national guidelines on Physical Activity and Public Health. The Committee on Exercise and Cardiac Rehabilitation of the American Heart Association endorsed and supported these recommendations. The purpose of the present report is to update and clarify the 1995 recommendations on the types and amounts of physical activity needed by healthy adults to improve and maintain health. Development of this document was by an expert panel of scientists, including physicians, epidemiologists, exercise scientists, and public health specialists. This panel reviewed advances in pertinent physiologic, epidemiologic, and clinical scientific data, including primary research articles and reviews published since the original recommendation was issued in 1995. Issues considered by the panel included new scientific evidence relating physical activity to health, physical activity recommendations by various organizations in the interim, and communications issues. Key points related to updating the physical activity recommendation were outlined and writing groups were formed. A draft manuscript was prepared and circulated for review to the expert panel as well as to outside experts. Comments were integrated into the final recommendation.


PRIMARY RECOMMENDATION
To promote and maintain health, all healthy adults aged 18 to 65 yr need moderate-intensity aerobic (endurance) physical activity for a minimum of 30 min on five days each week or vigorous-intensity aerobic physical activity for a minimum of 20 min on three days each week. [I (A)] Combinations of moderate- and vigorous-intensity activity can be performed to meet this recommendation. [IIa (B)] For example, a person can meet the recommendation by walking briskly for 30 min twice during the week and then jogging for 20 min on two other days. Moderate-intensity aerobic activity, which is generally equivalent to a brisk walk and noticeably accelerates the heart rate, can be accumulated toward the 30-min minimum by performing bouts each lasting 10 or more minutes. [I (B)] Vigorous-intensity activity is exemplified by jogging, and causes rapid breathing and a substantial increase in heart rate. In addition, every adult should perform activities that maintain or increase muscular strength and endurance a minimum of two days each week. [IIa (A)] Because of the dose-response relation between physical activity and health, persons who wish to further improve their personal fitness, reduce their risk for chronic diseases and disabilities or prevent unhealthy weight gain may benefit by exceeding the minimum recommended amounts of physical activity. [I (A)].",2007,128,7122,516,30,242,461,565,666,695,711,735,743,590
6365c7e578b3ba30b85560515f8b7956a2a915cd,"CONTEXT
A prior national survey documented the high prevalence and costs of alternative medicine use in the United States in 1990.


OBJECTIVE
To document trends in alternative medicine use in the United States between 1990 and 1997.


DESIGN
Nationally representative random household telephone surveys using comparable key questions were conducted in 1991 and 1997 measuring utilization in 1990 and 1997, respectively.


PARTICIPANTS
A total of 1539 adults in 1991 and 2055 in 1997.


MAIN OUTCOMES MEASURES
Prevalence, estimated costs, and disclosure of alternative therapies to physicians.


RESULTS
Use of at least 1 of 16 alternative therapies during the previous year increased from 33.8% in 1990 to 42.1% in 1997 (P < or = .001). The therapies increasing the most included herbal medicine, massage, megavitamins, self-help groups, folk remedies, energy healing, and homeopathy. The probability of users visiting an alternative medicine practitioner increased from 36.3% to 46.3% (P = .002). In both surveys alternative therapies were used most frequently for chronic conditions, including back problems, anxiety, depression, and headaches. There was no significant change in disclosure rates between the 2 survey years; 39.8% of alternative therapies were disclosed to physicians in 1990 vs 38.5% in 1997. The percentage of users paying entirely out-of-pocket for services provided by alternative medicine practitioners did not change significantly between 1990 (64.0%) and 1997 (58.3%) (P=.36). Extrapolations to the US population suggest a 47.3% increase in total visits to alternative medicine practitioners, from 427 million in 1990 to 629 million in 1997, thereby exceeding total visits to all US primary care physicians. An estimated 15 million adults in 1997 took prescription medications concurrently with herbal remedies and/or high-dose vitamins (18.4% of all prescription users). Estimated expenditures for alternative medicine professional services increased 45.2% between 1990 and 1997 and were conservatively estimated at $21.2 billion in 1997, with at least $12.2 billion paid out-of-pocket. This exceeds the 1997 out-of-pocket expenditures for all US hospitalizations. Total 1997 out-of-pocket expenditures relating to alternative therapies were conservatively estimated at $27.0 billion, which is comparable with the projected 1997 out-of-pocket expenditures for all US physician services.


CONCLUSIONS
Alternative medicine use and expenditures increased substantially between 1990 and 1997, attributable primarily to an increase in the proportion of the population seeking alternative therapies, rather than increased visits per patient.",1998,31,6714,384,9,118,246,353,411,432,426,458,449,391
0588b721a655327ec6a70bee072b62db323658c8,"OBJECTIVE
To encourage increased participation in physical activity among Americans of all ages by issuing a public health recommendation on the types and amounts of physical activity needed for health promotion and disease prevention.


PARTICIPANTS
A planning committee of five scientists was established by the Centers for Disease Control and Prevention and the American College of Sports Medicine to organize a workshop. This committee selected 15 other workshop discussants on the basis of their research expertise in issues related to the health implications of physical activity. Several relevant professional or scientific organizations and federal agencies also were represented.


EVIDENCE
The panel of experts reviewed the pertinent physiological, epidemiologic, and clinical evidence, including primary research articles and recent review articles.


CONSENSUS PROCESS
Major issues related to physical activity and health were outlined, and selected members of the expert panel drafted sections of the paper from this outline. A draft manuscript was prepared by the planning committee and circulated to the full panel in advance of the 2-day workshop. During the workshop, each section of the manuscript was reviewed by the expert panel. Primary attention was given to achieving group consensus concerning the recommended types and amounts of physical activity. A concise ""public health message"" was developed to express the recommendations of the panel. During the ensuing months, the consensus statement was further reviewed and revised and was formally endorsed by both the Centers for Disease Control and Prevention and the American College of Sports Medicine.


CONCLUSION
Every US adult should accumulate 30 minutes or more of moderate-intensity physical activity on most, preferably all, days of the week.",1995,64,7300,324,34,71,112,142,164,243,246,230,303,343
d1a70f3162140e69cec8444a7fa10e4ab22b0867,"The 11th edition of Harrison's Principles of Internal Medicine welcomes Anthony Fauci to its editorial staff, in addition to more than 85 new contributors. While the organization of the book is similar to previous editions, major emphasis has been placed on disorders that affect multiple organ systems. Important advances in genetics, immunology, and oncology are emphasized. Many chapters of the book have been rewritten and describe major advances in internal medicine. Subjects that received only a paragraph or two of attention in previous editions are now covered in entire chapters. Among the chapters that have been extensively revised are the chapters on infections in the compromised host, on skin rashes in infections, on many of the viral infections, including cytomegalovirus and Epstein-Barr virus, on sexually transmitted diseases, on diabetes mellitus, on disorders of bone and mineral metabolism, and on lymphadenopathy and splenomegaly. The major revisions in these chapters and many",1988,0,7488,252,4,5,2,3,4,7,6,14,12,44
d66529ac36547455eabe1214e5510aee19d575fa,"AbstractReliability refers to the reproducibility of values of a test, assay or other measurement in repeated trials on the same individuals. Better reliability implies better precision of single measurements and better tracking of changes in measurements in research or practical settings. The main measures of reliability are within-subject random variation, systematic change in the mean, and retest correlation. A simple, adaptable form of within-subject variation is the typical (standard) error of measurement: the standard deviation of an individual’s repeated measurements. For many measurements in sports medicine and science, the typical error is best expressed as a coefficient of variation (percentage of the mean). A biased, more limited form of within-subject variation is the limits of agreement: the 95% likely range of change of an individual’s measurements between 2 trials. Systematic changes in the mean of a measure between consecutive trials represent such effects as learning, motivation or fatigue; these changes need to be eliminated from estimates of within-subject variation. Retest correlation is difficult to interpret, mainly because its value is sensitive to the heterogeneity of the sample of participants. Uses of reliability include decision-making when monitoring individuals, comparison of tests or equipment, estimation of sample size in experiments and estimation of the magnitude of individual differences in the response to a treatment. Reasonable precision for estimates of reliability requires approximately 50 study participants and at least 3 trials. Studies aimed at assessing variation in reliability between tests or equipment require complex designs and analyses that researchers seldom perform correctly. A wider understanding of reliability and adoption of the typical error as the standard measure of reliability would improve the assessment of tests and equipment in our disciplines.
",2000,25,3704,579,3,14,18,28,41,62,62,89,104,117
9bd569e2cd6cce34e11b00cc82c7c11c875cc9db,"The clinical performance of a laboratory test can be described in terms of diagnostic accuracy, or the ability to correctly classify subjects into clinically relevant subgroups. Diagnostic accuracy refers to the quality of the information provided by the classification device and should be distinguished from the usefulness, or actual practical value, of the information. Receiver-operating characteristic (ROC) plots provide a pure index of accuracy by demonstrating the limits of a test's ability to discriminate between alternative states of health over the complete spectrum of operating conditions. Furthermore, ROC plots occupy a central or unifying position in the process of assessing and using diagnostic tools. Once the plot is generated, a user can readily go on to many other activities such as performing quantitative ROC analysis and comparisons of tests, using likelihood ratio to revise the probability of disease in individual subjects, selecting decision thresholds, using logistic-regression analysis, using discriminant-function analysis, or incorporating the tool into a clinical strategy by using decision analysis.",1993,81,6103,281,6,33,43,57,84,76,99,104,122,137
876f3e75121b2de085d850ca8d4816f17e9c9ab7,"Medicinal plants and traditional medicine in Africa , Medicinal plants and traditional medicine in Africa , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,5116,202,0,0,0,2,4,3,1,7,7,8
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
bb2832354af5f5690710fa951a1db18b4176f5ca,"Minimal measurement error (reliability) during the collection of interval- and ratio-type data is critically important to sports medicine research. The main components of measurement error are systematic bias (e.g. general learning or fatigue effects on the tests) and random error due to biological or mechanical variation. Both error components should be meaningfully quantified for the sports physician to relate the described error to judgements regarding ‘analytical goals’ (the requirements of the measurement tool for effective practical use) rather than the statistical significance of any reliability indicators.Methods based on correlation coefficients and regression provide an indication of ‘relative reliability’. Since these methods are highly influenced by the range of measured values, researchers should be cautious in: (i) concluding acceptable relative reliability even if a correlation is above 0.9; (ii) extrapolating the results of a test-retest correlation to a new sample of individuals involved in an experiment; and (iii) comparing test-retest correlations between different reliability studies.Methods used to describe ‘absolute reliability’ include the standard error of measurements (SEM), coefficient of variation (CV) and limits of agreement (LOA). These statistics are more appropriate for comparing reliability between different measurement tools in different studies. They can be used in multiple retest studies from ANOVA procedures, help predict the magnitude of a ‘real’ change in individual athletes and be employed to estimate statistical power for a repeated-measures experiment.These methods vary considerably in the way they are calculated and their use also assumes the presence (CV) or absence (SEM) of heteroscedasticity. Most methods of calculating SEM and CV represent approximately 68% of the error that is actually present in the repeated measurements for the ‘average’ individual in the sample. LOA represent the test-retest differences for 95% of a population. The associated Bland-Altman plot shows the measurement error schematically and helps to identify the presence of heteroscedasticity. If there is evidence of heteroscedasticity or non-normality, one should logarithmically transform the data and quote the bias and random error as ratios. This allows simple comparisons of reliability across different measurement tools.It is recommended that sports clinicians and researchers should cite and interpret a number of statistical methods for assessing reliability. We encourage the inclusion of the LOA method, especially the exploration of heteroscedasticity that is inherent in this analysis. We also stress the importance of relating the results of any reliability statistic to ‘analytical goals’ in sports medicine.",1998,106,2953,336,0,4,19,30,28,28,39,59,60,75
baef5c6685cc25d52592b7900857efd1d34a731b,"Evidence-based Healthcare: How to Make Health Policy and Management Decisions, by J. A. Muir Gray, 270 pp, with illus, paper, $29.95, ISBN 0-443-05721-4, New York, NY, Churchill Livingstone, 1997. We develop clinical expertise with bedside training and experience. How well do we integrate this experience with the best available external evidence for the purpose of direct patient care? I suspect that we do not carry out this function very well. Evidence-based Medicine (EBM) is the practice of applying valid evidence and data to a specific clinical question engendered during patient care. Lately, EBM has been on the lips and pen tips of clinicians, perhaps as a close runner-up to the other shibboleths managed care, gag clause, and networking. Is EBM then another mere mantra, a novel paradigm, or a practicable concept to help with ordinary, day-to-day clinical care? I believe it is the last, but you should be",1997,0,3452,195,8,30,55,54,65,105,120,136,155,152
8552f26dc0d4d08717e5d5ae44339f1a06d343b5,"Part 1. General Medicine: Clinical Examination and Making a Diagnosis. General Systemic States. Diseases of the Newborn. Practical Antimicrobial Therapeutics. Diseases of the Alimentary Tract I. Diseases of the Alimentary Tract II. Diseases of the Liver and Pancreas. Diseases of the Cardiovascular System. Diseases of the Blood and Blood-Forming Organs. Diseases of the Respiratory System. Diseases of the Urinary System. Diseases of the Nervous System. Diseases of the Musculoskeletal System. Diseases of the Skin, Conjunctiva and External Ear. Part 2. Special Medicine: Mastitis. Diseases Caused By Bacteria V. Diseases Caused By Viruses and Chlamydia I. Diseases Caused By Viruses and Chlamydia II. Diseases Caused By Rickettsia. Diseases Caused By Algae and Fungi. Diseases Caused By Protozoa. Diseases Caused By Helminth Parasites. Diseases Caused By Arthropod Parasites. Metabolic Diseases. Diseases Caused By Nutritional Deficiencies. Diseases Caused By Physical Agents. Diseases Caused By Inorganic and Farm Chemicals. Diseases Caused By Toxins in Plants, Fungi, Cyanobacteria, Clavibacteria, Insects and Animals. Diseases Caused By Allergy. Diseases Caused By the Inheritance of Undesirable Characters. Specific Diseases of Uncertain Etiology. Conversion Tables. Reference Laboratory Values. Index.",1994,0,3084,241,6,12,12,23,18,17,23,48,43,55
c6b5380f8e79382257fc4315d0b7d891c436b935,"BACKGROUND
Many people use unconventional therapies for health problems, but the extent of this use and the costs are not known. We conducted a national survey to determine the prevalence, costs, and patterns of use of unconventional therapies, such as acupuncture and chiropractic.


METHODS
We limited the therapies studied to 16 commonly used interventions neither taught widely in U.S. medical schools nor generally available in U.S. hospitals. We completed telephone interviews with 1539 adults (response rate, 67 percent) in a national sample of adults 18 years of age or older in 1990. We asked respondents to report any serious or bothersome medical conditions and details of their use of conventional medical services; we then inquired about their use of unconventional therapy.


RESULTS
One in three respondents (34 percent) reported using at least one unconventional therapy in the past year, and a third of these saw providers for unconventional therapy. The latter group had made an average of 19 visits to such providers during the preceding year, with an average charge per visit of $27.60. The frequency of use of unconventional therapy varied somewhat among socio-demographic groups, with the highest use reported by nonblack persons from 25 to 49 years of age who had relatively more education and higher incomes. The majority used unconventional therapy for chronic, as opposed to life-threatening, medical conditions. Among those who used unconventional therapy for serious medical conditions, the vast majority (83 percent) also sought treatment for the same condition from a medical doctor; however, 72 percent of the respondents who used unconventional therapy did not inform their medical doctor that they had done so. Extrapolation to the U.S. population suggests that in 1990 Americans made an estimated 425 million visits to providers of unconventional therapy. This number exceeds the number of visits to all U.S. primary care physicians (388 million). Expenditures associated with use of unconventional therapy in 1990 amounted to approximately $13.7 billion, three quarters of which ($10.3 billion) was paid out of pocket. This figure is comparable to the $12.8 billion spent out of pocket annually for all hospitalizations in the United States.


CONCLUSIONS
The frequency of use of unconventional therapy in the United States is far higher than previously reported. Medical doctors should ask about their patients' use of unconventional therapy whenever they obtain a medical history.",1993,27,4277,155,19,44,77,97,163,209,178,240,228,269
029929987c1321e6880234bf0492689a7a379e38,"In medicine we often want to compare two different methods of measuring some quantity, such as blood pressure, gestational age, or cardiac stroke volume. Sometimes we compare an approximate or simple method with a very precise one. This is a calibration problem, and we shall not discuss it further here. Frequently, however, we cannot regard either method as giving the true value of the quantity being measured. In this case we want to know whether the methods give answers which are, in some sense, comparable. For example, we may wish to see whether a new, cheap and quick method produces answers that agree with those from an established method sufficiently well for clinical purposes. Many such studies, using a variety of statistical techniques, have been reported. Yet few really answer the question “Do the two methods of measurement agree sufficiently closely?” In this paper we shall describe what is usually done, show why this is inappropriate, suggest a better approach, and ask why such studies are done so badly. We will restrict our consideration to the comparison of two methods of measuring a continuous variable, although similar problems can arise with categorical variables.",1983,23,3533,171,1,3,10,17,18,12,18,21,36,33
16399f848bb04faabd7f0107459132cc1c4777b0,"Life is the interplay between structure and energy, yet the role of energy deficiency in human disease has been poorly explored by modern medicine. Since the mitochondria use oxidative phosphorylation (OXPHOS) to convert dietary calories into usable energy, generating reactive oxygen species (ROS) as a toxic by-product, I hypothesize that mitochondrial dysfunction plays a central role in a wide range of age-related disorders and various forms of cancer. Because mitochondrial DNA (mtDNA) is present in thousands of copies per cell and encodes essential genes for energy production, I propose that the delayed-onset and progressive course of the age-related diseases results from the accumulation of somatic mutations in the mtDNAs of post-mitotic tissues. The tissue-specific manifestations of these diseases may result from the varying energetic roles and needs of the different tissues. The variation in the individual and regional predisposition to degenerative diseases and cancer may result from the interaction of modern dietary caloric intake and ancient mitochondrial genetic polymorphisms. Therefore the mitochondria provide a direct link between our environment and our genes and the mtDNA variants that permitted our forbears to energetically adapt to their ancestral homes are influencing our health today.",2005,299,2871,173,11,95,151,161,213,207,203,234,230,220
ee4f3da2b55d75a8e978275fd36eaa10124ea88f,"The purpose of this Position Stand is to provide an overview of issues critical to understanding the importance of exercise and physical activity in older adult populations. The Position Stand is divided into three sections: Section 1 briefly reviews the structural and functional changes that characterize normal human aging, Section 2 considers the extent to which exercise and physical activity can influence the aging process, and Section 3 summarizes the benefits of both long-term exercise and physical activity and shorter-duration exercise programs on health and functional capacity. Although no amount of physical activity can stop the biological aging process, there is evidence that regular exercise can minimize the physiological effects of an otherwise sedentary lifestyle and increase active life expectancy by limiting the development and progression of chronic disease and disabling conditions. There is also emerging evidence for significant psychological and cognitive benefits accruing from regular exercise participation by older adults. Ideally, exercise prescription for older adults should include aerobic exercise, muscle strengthening exercises, and flexibility exercises. The evidence reviewed in this Position Stand is generally consistent with prior American College of Sports Medicine statements on the types and amounts of physical activity recommended for older adults as well as the recently published 2008 Physical Activity Guidelines for Americans. All older adults should engage in regular physical activity and avoid an inactive lifestyle.",2009,238,2916,144,48,91,169,232,240,205,285,231,257,268
34f3877c84af60d0b1029145a6e6ff7bbad6ad76,"In order to stimulate further adaptation toward specific training goals, progressive resistance training (RT) protocols are necessary. The optimal characteristics of strength-specific programs include the use of concentric (CON), eccentric (ECC), and isometric muscle actions and the performance of bilateral and unilateral single- and multiple-joint exercises. In addition, it is recommended that strength programs sequence exercises to optimize the preservation of exercise intensity (large before small muscle group exercises, multiple-joint exercises before single-joint exercises, and higher-intensity before lower-intensity exercises). For novice (untrained individuals with no RT experience or who have not trained for several years) training, it is recommended that loads correspond to a repetition range of an 8-12 repetition maximum (RM). For intermediate (individuals with approximately 6 months of consistent RT experience) to advanced (individuals with years of RT experience) training, it is recommended that individuals use a wider loading range from 1 to 12 RM in a periodized fashion with eventual emphasis on heavy loading (1-6 RM) using 3- to 5-min rest periods between sets performed at a moderate contraction velocity (1-2 s CON; 1-2 s ECC). When training at a specific RM load, it is recommended that 2-10% increase in load be applied when the individual can perform the current workload for one to two repetitions over the desired number. The recommendation for training frequency is 2-3 d x wk(-1) for novice training, 3-4 d x wk(-1) for intermediate training, and 4-5 d x wk(-1) for advanced training. Similar program designs are recommended for hypertrophy training with respect to exercise selection and frequency. For loading, it is recommended that loads corresponding to 1-12 RM be used in periodized fashion with emphasis on the 6-12 RM zone using 1- to 2-min rest periods between sets at a moderate velocity. Higher volume, multiple-set programs are recommended for maximizing hypertrophy. Progression in power training entails two general loading strategies: 1) strength training and 2) use of light loads (0-60% of 1 RM for lower body exercises; 30-60% of 1 RM for upper body exercises) performed at a fast contraction velocity with 3-5 min of rest between sets for multiple sets per exercise (three to five sets). It is also recommended that emphasis be placed on multiple-joint exercises especially those involving the total body. For local muscular endurance training, it is recommended that light to moderate loads (40-60% of 1 RM) be performed for high repetitions (>15) using short rest periods (<90 s). In the interpretation of this position stand as with prior ones, recommendations should be applied in context and should be contingent upon an individual's target goals, physical capacity, and training status.",2009,415,2914,116,89,118,149,181,198,193,196,220,287,278
391adf80e9a43ded3f591b911136876513ee1c39,Introduction biology and pathophysiology of skin disorders presenting in the skin and mucous membranes dermatology and internal medicine diseases due to microbial agents therapeutics paediatric and geriatric dermatology.,1971,0,4224,37,1,7,16,23,25,26,19,21,14,26
aeff86720a66ae6a0ce26ae29c68fbd8dbebfbee,"A NEW paradigm for medical practice is emerging. Evidence-based medicine de-emphasizes intuition, unsystematic clinical experience, and pathophysiologic rationale as sufficient grounds for clinical decision making and stresses the examination of evidence from clinical research. Evidence-based medicine requires new skills of the physician, including efficient literature searching and the application of formal rules of evidence evaluating the clinical literature. An important goal of our medical residency program is to educate physicians in the practice of evidence-based medicine. Strategies include a weekly, formal academic half-day for residents, devoted to learning the necessary skills; recruitment into teaching roles of physicians who practice evidence-based medicine; sharing among faculty of approaches to teaching evidence-based medicine; and providing faculty with feedback on their performance as role models and teachers of evidence-based medicine. The influence of evidencebased medicine on clinical practice and medical education is increasing. CLINICAL SCENARIO A junior medical resident working in a teaching hospital",1992,42,3742,29,0,19,32,48,61,89,93,80,94,99
527bf3363fcbb24c814e47934a5b0f5204131461,"The growth of blood vessels (a process known as angiogenesis) is essential for organ growth and repair. An imbalance in this process contributes to numerous malignant, inflammatory, ischaemic, infectious and immune disorders. Recently, the first anti-angiogenic agents have been approved for the treatment of cancer and blindness. Angiogenesis research will probably change the face of medicine in the next decades, with more than 500 million people worldwide predicted to benefit from pro- or anti-angiogenesis treatments.",2005,32,3160,118,4,75,190,228,238,232,292,225,254,211
f155714ac666017a4d8883c2afbd7589801e5fda,"Part 1: Normal Sleep. Normal Sleep and Its Variations. Phylogeny. Sleep Mechanisms. Physiology in Sleep. Chronobiology. Pharmacology. Psychophysiology and Dreaming. Part 2: Abnormal Sleep. Impact, Presentation, And Diagnosis. Disorders of Chrononbiology. Insomnia. Primary Disorders of Daytime Somnolence. Parasomnias. Sleep Breathing Disorders. Medical Disorders. Psychiatric Disorders. Methodology.",1989,0,4263,38,3,19,23,26,26,30,62,82,72,93
251b1c8bce87bae9ec1e0f542ea9152aa7781b8f,"I might have guessed that a book dedicated to ""H.L. Mencken, Kurt Vonnegut, Jr., Douglas Adams, and the Emperor's New Clothes"" would be fun to read. It was! Readers will sense the authors' enthusiasm for their subject on each page, from the preface to the final chapter. The authors prepared this book for ""users"" rather than ""doers"" of clinical research. Physicians and others who wish to recognize key clinical epidemiologic features of the diagnosis and management of patients will benefit from reading Clinical Epidemiology. Those who wish to conduct actual research studies will need to look elsewhere for a detailed discussion of clinical epidemiologic methodology. In this review, I will mention",1985,0,3553,64,5,15,31,59,58,64,71,95,93,110
418516bbea55b3b66594158d6808be66f05baba2,,1981,0,2676,213,1,1,1,4,4,3,3,3,3,1
2b98f39bfb8abb50813f25596192cf9c9041d523,"OBJECTIVE
This report presents selected estimates of complementary and alternative medicine (CAM) use among U.S. adults and children, using data from the 2007 National Health Interview Survey (NHIS), conducted by the Centers for Disease Control and Prevention's (CDC) National Center for Health Statistics (NCHS). Trends in adult use were assessed by comparing data from the 2007 and 2002 NHIS.


METHODS
Estimates were derived from the Complementary and Alternative Medicine supplements and Core components of the 2007 and 2002 NHIS. Estimates were generated and comparisons conducted using the SUDAAN statistical package to account for the complex sample design.


RESULTS
In 2007, almost 4 out of 10 adults had used CAM therapy in the past 12 months, with the most commonly used therapies being nonvitamin, nonmineral, natural products (17.7%) and deep breathing exercises (12.7%). American Indian or Alaska Native adults (50.3%) and white adults (43.1%) were more likely to use CAM than Asian adults (39.9%) or black adults (25.5%). Results from the 2007 NHIS found that approximately one in nine children (11.8%) used CAM therapy in the past 12 months, with the most commonly used therapies being nonvitamin, nonmineral, natural products (3.9%) and chiropractic or osteopathic manipulation (2.8%). Children whose parent used CAM were almost five times as likely (23.9%) to use CAM as children whose parent did not use CAM (5.1%). For both adults and children in 2007, when worry about cost delayed receipt of conventional care, individuals were more likely to use CAM than when the cost of conventional care was not a worry. Between 2002 and 2007 increased use was seen among adults for acupuncture, deep breathing exercises, massage therapy, meditation, naturopathy, and yoga. CAM use for head or chest colds showed a marked decrease from 2002 to 2007 (9.5% to 2.0%).",2008,46,2675,87,4,59,176,203,278,296,311,307,249,218
66a586cc77c0f01d5f481c644d58e8f1931b120a,"OBJECTIVE
To issue a recommendation on the types and amounts of physical activity needed to improve and maintain health in older adults.


PARTICIPANTS
A panel of scientists with expertise in public health, behavioral science, epidemiology, exercise science, medicine, and gerontology.


EVIDENCE
The expert panel reviewed existing consensus statements and relevant evidence from primary research articles and reviews of the literature.


PROCESS
After drafting a recommendation for the older adult population and reviewing drafts of the Updated Recommendation from the American College of Sports Medicine (ACSM) and the American Heart Association (AHA) for Adults, the panel issued a final recommendation on physical activity for older adults.


SUMMARY
The recommendation for older adults is similar to the updated ACSM/AHA recommendation for adults, but has several important differences including: the recommended intensity of aerobic activity takes into account the older adult's aerobic fitness; activities that maintain or increase flexibility are recommended; and balance exercises are recommended for older adults at risk of falls. In addition, older adults should have an activity plan for achieving recommended physical activity that integrates preventive and therapeutic recommendations. The promotion of physical activity in older adults should emphasize moderate-intensity aerobic activity, muscle-strengthening activity, reducing sedentary behavior, and risk management.",2007,76,2236,118,12,72,128,153,184,191,216,189,221,186
140f7c7d5fafba3fc9911476c6cbc911b35e93a0,"Hydrophilic polymers are the center of research emphasis in nanotechnology because of their perceived “intelligence”. They can be used as thin films, scaffolds, or nanoparticles in a wide range of biomedical and biological applications. Here we highlight recent developments in engineering uncrosslinked and crosslinked hydrophilic polymers for these applications. Natural, biohybrid, and synthetic hydrophilic polymers and hydrogels are analyzed and their thermodynamic responses are discussed. In addition, examples of the use of hydrogels for various therapeutic applications are given. We show how such systems’ intelligent behavior can be used in sensors, microarrays, and imaging. Finally, we outline challenges for the future in integrating hydrogels into biomedical applications.",2006,274,3088,52,5,29,67,120,156,202,231,245,276,263
efa296faa52fd7799002315956f082f631b99ac4,,1998,0,1927,248,33,50,55,58,84,65,85,74,99,81
9b5474dd271db17cf56a64800f2993e1feff7ca0,"PART I General Considerations of Cardiovascular Disease PART II Examination of the Patient PART III Normal and Abnormal Cardiac Function: Heart Failure and Arrhythmias PART IV Hypertensive and Atherosclerotic Cardiovascular Disease PART V Diseases of the Heart, Pericardium and Pulmonary Vascular Bed PART VI Molecular Biology and Genetics PART VII Cardiovascular Disease in Special Populations PART VIII Cardiovascular Disease and Disorders of other Organ Systems",1979,0,3650,12,0,1,6,10,16,16,22,17,21,24
3df1d83247b2865db49966bfcd950672c0d98b26,,1987,0,2846,82,2,2,14,13,21,26,23,41,70,73
4f1bc78d826b558472ddef7d5b19dc78a6e7db55,"The Textbook was written by O.M. Radostits, C.C. Gay, K.V. Hinchcliff and P.D. Constable and produced in 2007 by Mosby Elsevier publisher. Tenth edition textbook of 2,156 pages including 148 instructive tables and 30 illustrations provides a source of detailed information about all important transmissible and nontransmissible diseases of cattle, horse, sheep, pigs and goats. The textbook is divided in two main parts. The first one is dedicated to general medicine and the second one to specific medicine. The general medicine part is subdivided in following chapters: clinical examination and making a diagnosis, general systemic states, diseases of the newborn, practical antimicrobial therapeutics, diseases of the alimentary tract, of the liver and pancreas, of the cardiovascular system, of the hemolymphatic and immune systems, of the respiratory system, of the urinary system, of the nervous system, of the musculoskeletal system, of the skin, conjunctiva and external ear and of the mammary gland. The specific medicine part is subdivided in diseases associated with bacteria, with viruses and Chlamydia, with prions, with Rickettsiales, algae and fungi, with protozoa, with helminth parasites and with arthropod parasites. This part continues with metabolic diseases, diseases associated with nutritional deficiencies, with physical agents, with inorganic and farm chemicals, with toxins in plants, fungi, cyanobacteria, clavibacteria, insects and animals, with allergy, with the inheritance of undesirable characteristics and finally specific diseases of uncertain etiology. At the end there are appendices dealing with conversion factors, reference laboratory values, drug doses for horses and ruminants and drug doses for pigs. Detailed Index required 89 pages. The publication meets the demand not only of undergraduate veterinary students and graduate veterinarians working in the field of large-animal medicine but also of other specialists dealing with livestock husbandry. The book has international scope including most of the diseases occurring in large animals worldwide. It produces up-to-date review of the large-animal medicine. Review of communicable diseases includes etiology, epidemiology, pathogenesis, treatment and control. This edition is giving special emphasis to zoonotic implications, individual diagnostic tests, provision of emergency service to the owners of herds and flocks. The textbook is giving priority attention to food-producing animals and industrial animal agriculture. Necessary information are provided on equine practice and companion animal practice. Very useful are the procedures aimed at the efficiency of livestock production: providing the most economical method of diagnosis and treatment, monitoring animal health and production, recommending specific disease control and prevention programmes, organizing planned herds and flock health programmes, advising on nutrition, breeding and general management practices. The authors give the attention also to animal welfare and food safety. This publication serves as an encyclopedia of modern veterinary medicine supporting through the best possible health the most effective performance of food-producing large-animals under the conditions of developed as well as developing countries.",2007,0,1598,275,6,13,30,53,61,66,100,131,117,133
f14b8ef5a3edc5fdc9613a8a1c5545aa6cb626ad,"This Position Stand provides guidance on fluid replacement to sustain appropriate hydration of individuals performing physical activity. The goal of prehydrating is to start the activity euhydrated and with normal plasma electrolyte levels. Prehydrating with beverages, in addition to normal meals and fluid intake, should be initiated when needed at least several hours before the activity to enable fluid absorption and allow urine output to return to normal levels. The goal of drinking during exercise is to prevent excessive (>2% body weight loss from water deficit) dehydration and excessive changes in electrolyte balance to avert compromised performance. Because there is considerable variability in sweating rates and sweat electrolyte content between individuals, customized fluid replacement programs are recommended. Individual sweat rates can be estimated by measuring body weight before and after exercise. During exercise, consuming beverages containing electrolytes and carbohydrates can provide benefits over water alone under certain circumstances. After exercise, the goal is to replace any fluid electrolyte deficit. The speed with which rehydration is needed and the magnitude of fluid electrolyte deficits will determine if an aggressive replacement program is merited.",2007,10,1797,207,36,62,53,93,111,114,140,150,158,152
dccd557eb3d10b858458c17e0f2e51ac0e622115,"The emerging field of regenerative medicine will require a reliable source of stem cells in addition to biomaterial scaffolds and cytokine growth factors. Adipose tissue represents an abundant and accessible source of adult stem cells with the ability to differentiate along multiple lineage pathways. The isolation, characterization, and preclinical and clinical application of adipose-derived stem cells (ASCs) are reviewed in this article.",2007,160,2058,84,6,43,74,112,165,158,229,193,218,190
ae83ba2c166ca4a6bb19661b998830f8ad05bb57,"Biomaterials have played an enormous role in the success of medical devices and drug delivery systems. We discuss here new challenges and directions in biomaterials research. These include synthetic replacements for biological tissues, designing materials for specific medical applications, and materials for new applications such as diagnostics and array technologies.",2004,78,2697,24,11,50,82,111,136,141,179,204,212,205
2f48a1937b4502fc11b3b0cc0889bcba02d34750,"The factors that cause large individual differences in professional achievement are only partially understood. Nobody becomes an outstanding professional without experience, but extensive experience does not invariably lead people to become experts. When individuals are first introduced to a professional domain after completing their education, they are often overwhelmed and rely on help from others to accomplish their responsibilities. After months or years of experience, they attain an acceptable level of proficiency and are able to work independently. Although everyone in a given domain tends to improve with experience initially, some develop faster than others and continue to improve during ensuing years. These individuals are eventually recognized as experts and masters. In contrast, most professionals reach a stable, average level of performance within a relatively short time frame and maintain this mediocre status for the rest of their careers. The nature of the individual differences that cause the large variability in attained performance is still debated. The most common explanation is that achievement in a given domain is limited by innate factors that cannot be changed through experience and training; hence, limits of attainable performance are determined by one’s basic endowments, such as abilities, mental capacities, and innate talents. Educators with this widely held view of professional development have focused on identifying and selecting students who possess the necessary innate talents that would allow them to reach expert levels with adequate experience. Therefore, the best schools and professional organizations nearly always rely on extensive testing and interviews to find the most talented applicants. This general view also explains age-related declines in professional achievement in terms of the inevitable reductions in general abilities and capacities believed to result from aging. In this article, I propose an alternative framework to account for individual differences in attained professional development, as well as many aspects of age-related decline. This framework is based on the assumption that acquisition of expert performance requires engagement in deliberate practice and that continued deliberate practice is necessary for maintenance of many types of professional performance. In order to contrast this alternative framework with the traditional view, I first describe the account based on innate talent. I then provide a brief review of the evidence on deliberate practice in the acquisition of expert performance in several performance domains, including music, chess, and sports. Finally, I review evidence from the acquisition and maintenance of expert performance in medicine and examine the role of deliberate practice in this domain.",2004,133,2267,85,0,23,44,81,78,95,123,141,166,165
8955aea3446060d2aee29fb8a2fdcaed872aa489,This paper reports on the current status of structure validation in chemical crystallography.,2009,47,11513,1679,1,442,1257,1140,859,900,1014,959,881,762
abcacba645152530b080db0fda434a5664936df6,"The solar chemical composition is an important ingredient in our understanding of the formation, structure, and evolution of both the Sun and our Solar System. Furthermore, it is an essential refer ...",2009,475,5297,1913,45,190,280,323,412,447,490,508,508,557
ff8ba4dd5e38358d23a42551ca8e7a7aba7411bf,"Summary Trace-element data for mid-ocean ridge basalts (MORBs) and ocean island basalts (OIB) are used to formulate chemical systematics for oceanic basalts. The data suggest that the order of trace-element incompatibility in oceanic basalts is Cs ≈ Rb ≈ (≈ Tl) ≈ Ba(≈ W) > Th > U ≈ Nb = Ta ≈ K > La > Ce ≈ Pb > Pr (≈ Mo) ≈ Sr > P ≈ Nd (> F) > Zr = Hf ≈ Sm > Eu ≈ Sn (≈ Sb) ≈ Ti > Dy ≈ (Li) > Ho = Y > Yb. This rule works in general and suggests that the overall fractionation processes operating during magma generation and evolution are relatively simple, involving no significant change in the environment of formation for MORBs and OIBs. In detail, minor differences in element ratios correlate with the isotopic characteristics of different types of OIB components (HIMU, EM, MORB). These systematics are interpreted in terms of partial-melting conditions, variations in residual mineralogy, involvement of subducted sediment, recycling of oceanic lithosphere and processes within the low velocity zone. Niobium data indicate that the mantle sources of MORB and OIB are not exact complementary reservoirs to the continental crust. Subduction of oceanic crust or separation of refractory eclogite material from the former oceanic crust into the lower mantle appears to be required. The negative europium anomalies observed in some EM-type OIBs and the systematics of their key element ratios suggest the addition of a small amount (⩽1% or less) of subducted sediment to their mantle sources. However, a general lack of a crustal signature in OIBs indicates that sediment recycling has not been an important process in the convecting mantle, at least not in more recent times (⩽2 Ga). Upward migration of silica-undersaturated melts from the low velocity zone can generate an enriched reservoir in the continental and oceanic lithospheric mantle. We propose that the HIMU type (eg St Helena) OIB component can be generated in this way. This enriched mantle can be re-introduced into the convective mantle by thermal erosion of the continental lithosphere and by the recycling of the enriched oceanic lithosphere back into the mantle.",1989,144,18256,1007,0,0,0,0,0,0,0,0,0,0
eb2beacef72033cfc3364de191e703e363b7721a,,1982,0,14015,886,0,0,0,0,0,0,0,0,1,0
64caa66b1c2ad226b18b6fee6ebbf83a77400703,"Publisher Summary This chapter discusses the sequencing end-labeled DNA with base-specific chemical cleavages. In the chemical DNA sequencing method, one end-labels the DNA, partially cleaves it at each of the four bases in four reactions, orders the products by size on a slab gel, and then reads the sequence from an autoradiogram by noting which base-specific agent cleaved at each successive nucleotide along the strand. This technique sequences the DNA made in and purified from cells. No enzymatic copying in vitro is required, and either single- or double-stranded DNA can be sequenced. Most chemical schemes that cleave at one or two of the four bases involve three consecutive steps: modification of a base, removal of the modified base from its sugar, and DNA strand scission at that sugar. Base-specific chemical cleavage is only one step in sequencing DNA. The chapter presents techniques for producing discrete DNA fragments, end-labeling DNA, segregating end-labeled fragments, extracting DNA from gels, and the protocols for partially cleaving it at specific bases using the chemical reactions. The chapter also discusses the electrophoresis of the chemical cleavage products on long-distance sequencing gels and a guide for troubleshooting problems in sequencing patterns.",1980,45,10412,289,1,6,232,555,766,847,916,855,875,760
74974454d1e1805de5e890bc38b36baed2b4ec19,"There are two formalisms for mathematically describing the time behavior of a spatially homogeneous chemical system: The deterministic approach regards the time evolution as a continuous, wholly predictable process which is governed by a set of coupled, ordinary differential equations (the “reaction-rate equations”); the stochastic approach regards the time evolution as a kind of random-walk process which is governed by a single differential-difference equation (the “master equation”). Fairly simple kinetic theory arguments show that the stochastic formulation of chemical kinetics has a firmer physical basis than the deterministic formulation, but unfortunately the stochastic master equation is often mathematically intractable. There is, however, a way to make exact numerical calculations within the framework of the stochastic formulation without having to deal with the master equation directly. It is a relatively simple digital computer algorithm which uses a rigorously derived Monte Carlo procedure to numerically simulate the time evolution of the given chemical system. Like the master equation, this “stochastic simulation algorithm” correctly accounts for the inherent fluctuations and correlations that are necessarily ignored in the deterministic formulation. In addition, unlike most procedures for numerically solving the deterministic reaction-rate equations, this algorithm never approximates infinitesimal time increments df by finite time steps At. The feasibility and utility of the simulation algorithm are demonstrated by applying it to several well-known model chemical systems, including the Lotka model, the Brusselator, and the Oregonator.",1977,30,9106,836,0,5,5,1,7,7,5,4,4,1
e0a7cdd0e0c9ade81d03ab6515767a2d2d9a90ff,,1939,0,13284,374,0,0,1,0,0,0,1,1,0,0
48152e8b704df77db10ca85c064e7a565bb52908,,1958,0,7564,1574,1,0,3,3,0,2,3,2,7,2
ec2f624e0a37acbf84a3c6b4d8b731de118b41eb,"Conversion Factors.Physical and Chemical Data.Mathematics.Thermodynamics.Heat and Mass Transfer.Fluid and Particle Mechanics.Reaction Kinetics.Process Control and Instrumentation.Process Economics.Transport and Storage of Fluids.Heat Transfer Operations and Equipment.Drying, Humidification and Evaporative Cooling.Distillation.Gas Absorption and Other Gas-Liquid Operations and Equipment.Solid-State Operations and Equipment.Size Reduction and Size Enlargement.Handling of Bulk Solids and Packaging of Solids and Liquids.Alternative Separation Processes.Chemical Reactors.Biochemcial Engineering.Waste Management.Safety and Handling of Hazardous Materials.Energy Sources, Conversion and Utilization.Materials of Construction.Process Machinery Drives.Analysis of Plant Performance.",2007,0,7927,285,421,434,427,494,554,558,653,611,601,554
54832f5f4a629a115ad0069c62222973b3568eac,"Abstract An exact method is presented for numerically calculating, within the framework of the stochastic formulation of chemical kinetics, the time evolution of any spatially homogeneous mixture of molecular species which interreact through a specified set of coupled chemical reaction channels. The method is a compact, computer-oriented, Monte Carlo simulation procedure. It should be particularly useful for modeling the transient behavior of well-mixed gas-phase systems in which many molecular species participate in many highly coupled chemical reactions. For “ordinary” chemical systems in which fluctuations and correlations play no significant role, the method stands as an alternative to the traditional procedure of numerically solving the deterministic reaction rate equations. For nonlinear systems near chemical instabilities, where fluctuations and correlations may invalidate the deterministic equations, the method constitutes an efficient way of numerically examining the predictions of the stochastic master equation. Although fully equivalent to the spatially homogeneous master equation, the numerical simulation algorithm presented here is more directly based on a newly defined entity called “the reaction probability density function.” The purpose of this article is to describe the mechanics of the simulation algorithm, and to establish in a rigorous, a priori manner its physical and mathematical validity; numerical applications to specific chemical systems will be presented in subsequent publications.",1976,26,5063,428,0,1,2,3,3,5,4,4,6,2
05b9e01b36be8f688cdbd7131b89920fa46ac048,"Chemical Thermodynamics and Kinetics Acid-Base Dissolved Carbon Dioxide Atmosphere-Water Interactions Metal Ions in Aqueous Solution Aspects of Coordination Chemistry Precipitation and Dissolution Oxidation and Reduction Equilibria the Solid-Solution Interface Trace Metals: Cycling, Regulation and Biological Role Kinetics and Redox Processes Photochemical Processes Kinetics at the Solid-Water Interface Adsorption Dissolution of Minerals Nucleation and Crystal Growth Particle-Particle Interaction Colloids Coagulation and Filtration Regulation of the Chemical Composition of Natural Waters (Examples) Thermodynamic Data.",1970,0,5780,534,0,0,0,0,0,0,0,0,0,0
0540f1ce83134444515d3a67f450e8dc3976452b,"The chemical composition of natural water is derived from many different sources of solutes, including gases and aerosols from the atmosphere, weathering and erosion of rocks and soil, solution or precipitation reactions occurring below the land surface, and cultural effects resulting from human activities. Broad interrelationships among these processes and their effects can be discerned by application of principles of chemical thermodynamics. Some of the processes of solution or precipitation of minerals can be closely evaluated by means of principles of chemical equilibrium, including the law of mass action and the Nernst equation. Other processes are irreversible and require consideration of reaction mechanisms and rates. The chemical composition of the crustal rocks of the Earth and the composition of the ocean and the atmosphere are significant in evaluating sources of solutes in natural freshwater. The ways in which solutes are taken up or precipitated and the amounts present in solution are influenced by many environmental factors, especially climate, structure and position of rock strata, and biochemical effects associated with life cycles of plants and animals, both microscopic and macroscopic. Taken together and in application with the further influence of the general circulation of all water in the hydrologic cycle, the chemical principles and environmental factors form a basis for the developing science of natural-water chemistry. Fundamental data used in the determination of water quality are obtained by the chemical analysis of water samples in the laboratory or onsite sensing of chemical properties in the field. Sampling is complicated by changes in the composition of moving water and by the effects of particulate suspended material. Some constituents are unstable and require onsite determination or sample preservation. Most of the constituents determined are reported in gravimetric units, usually milligrams per liter or milliequivalents",1989,722,5535,618,76,79,47,79,95,81,106,116,100,84
d635e2843c6fb034e9126aa73ef9c2e4e2c4714d,"It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading. 1. A Model of the Embryo. Morphogens. In this section a mathematical model of the growing embryo will be described. This model will be a simplification and an idealization, and consequently a falsification. It is to be hoped that the features retained for discussion are those of greatest importance in the present state of knowledge.",1990,120,5100,467,10,7,11,20,23,14,27,29,35,44
628389e5454a0b12398eafa375dbe111d51288b2,,1996,0,5683,402,1,1,13,21,39,46,87,104,129,141
e06bd6aaf990dd41c71447a154e8d8cb62965705,"Chemical equilibria in soils , Chemical equilibria in soils , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1979,0,3794,445,0,3,8,22,29,27,34,35,35,66
c54866766d02ede47649a5edf76c9dc7a2e1727b,Partial table of contents: Overview of Chemical Reaction Engineering. HOMOGENEOUS REACTIONS IN IDEAL REACTORS. Introduction to Reactor Design. Design for Single Reactions. Design for Parallel Reactions. Potpourri of Multiple Reactions. NON IDEAL FLOW. Compartment Models. The Dispersion Model. The Tank--in--Series Model. REACTIONS CATALYZED BY SOLIDS. Solid Catalyzed Reactions. The Packed Bed Catalytic Reactor. Deactivating Catalysts. HETEROGENEOUS REACTIONS. Fluid--Fluid Reactions: Kinetics. Fluid--Particle Reactions: Design. BIOCHEMICAL REACTIONS. Enzyme Fermentation. Substrate Limiting Microbial Fermentation. Product Limiting Microbial Fermentation. Appendix. Index.,1972,1,7453,319,24,18,18,23,32,30,25,35,46,51
f6f215bf741217b7acf66ffe301dd11b2a4f4d2b,,1984,0,5617,434,3,2,9,9,19,23,33,50,37,48
7fa94fd8a528f3d4d622b0ffa11b3d0bcd0b5c30,"Preface. General notes on analytical techniques. Nutrients. Soluble organic material. Particulate material. Plant pigments. Photosynthesis. Bacteria. Gases in seawater. Counting, media and preservatives. Terms and equivalents.",1984,0,6541,496,2,2,15,12,23,49,46,47,61,75
59eede676a521227f5c24850567b9b8dc654b4dd,"Interest in graphene centres on its excellent mechanical, electrical, thermal and optical properties, its very high specific surface area, and our ability to influence these properties through chemical functionalization. There are a number of methods for generating graphene and chemically modified graphene from graphite and derivatives of graphite, each with different advantages and disadvantages. Here we review the use of colloidal suspensions to produce new materials composed of graphene and chemically modified graphene. This approach is both versatile and scalable, and is adaptable to a wide variety of applications.",2009,67,5668,44,26,214,452,558,670,653,607,540,507,456
573563f780dd06764d24825c64d9ec426102cb34,"In the past few years several methods have been developed for the analysis of serum lipoproteins. Lindgren, Elliott, and Gofman (1) have utilized the relatively low density of the lipoproteins to separate them from the other serum proteins by ultracentrifugal flotation. Quantitation was subsequently performed by refractometric methods in the analytical ultracentrifuge. Separations of lipoproteins have also been made by Cohn fractionation in cold ethanol, and the quantities of lipoprotein have been estimated from the lipid. content of the fractions (2, 3). Widely used at the present time is the method of zone electrophoresis with quantitation either by staining (4) or by chemical analysis of eluates from the support",1955,26,8145,218,1,4,8,8,12,17,29,17,18,23
8be9c35feb862041e39082a35bee317e878cb4a7,"A universal method to detect and determine siderophores was developed by using their high affinity for iron(III). The ternary complex chrome azurol S/iron(III)/hexadecyltrimethylammonium bromide, with an extinction coefficient of approximately 100,000 M-1 cm-1 at 630 nm, serves as an indicator. When a strong chelator removes the iron from the dye, its color turns from blue to orange. Because of the high sensitivity, determination of siderophores in solution and their characterization by paper electrophoresis chromatography can be performed directly on supernatants of culture fluids. The method is also applicable to agar plates. Orange halos around the colonies on blue agar are indicative of siderophore excretion. It was demonstrated with Escherichia coli strains that biosynthetic, transport, and regulatory mutations in the enterobactin system are clearly distinguishable. The method was successfully used to screen mutants in the iron uptake system of two Rhizobium meliloti strains, DM5 and 1021.",1987,16,4922,258,3,14,23,25,55,43,36,45,43,54
85d3432db02840f07f7ce8603507cb6125ac1334,"Reduction of a colloidal suspension of exfoliated graphene oxide sheets in water with hydrazine hydrate results in their aggregation and subsequent formation of a high-surface-area carbon material which consists of thin graphene-based sheets. The reduced material was characterized by elemental analysis, thermo-gravimetric analysis, scanning electron microscopy, X-ray photoelectron spectroscopy, NMR spectroscopy, Raman spectroscopy, and by electrical conductivity measurements.",2007,60,11509,138,0,0,0,0,1,404,1033,1190,1265,1224
7d8ced0b17a12deb6bd64e4f51df4e7396270c45,,1976,0,3303,282,5,3,7,18,28,37,41,53,56,47
f106b9efd9f7e336e7968277a630858427ea20b7,,1960,16,3528,294,1,1,2,4,1,1,1,1,0,1
e5ba538944747a6b2ba2a59213268ca2f8df5456,"The increasing amount of genomic and molecular information is the basis for understanding higher-order biological systems, such as the cell and the organism, and their interactions with the environment, as well as for medical, industrial and other practical applications. The KEGG resource () provides a reference knowledge base for linking genomes to biological systems, categorized as building blocks in the genomic space (KEGG GENES) and the chemical space (KEGG LIGAND), and wiring diagrams of interaction networks and reaction networks (KEGG PATHWAY). A fourth component, KEGG BRITE, has been formally added to the KEGG suite of databases. This reflects our attempt to computerize functional interpretations as part of the pathway reconstruction process based on the hierarchically structured knowledge about the genomic, chemical and network spaces. In accordance with the new chemical genomics initiatives, the scope of KEGG LIGAND has been significantly expanded to cover both endogenous and exogenous molecules. Specifically, RPAIR contains curated chemical structure transformation patterns extracted from known enzymatic reactions, which would enable analysis of genome-environment interactions, such as the prediction of new reactions and new enzyme genes that would degrade new environmental compounds. Additionally, drug information is now stored separately and linked to new KEGG DRUG structure maps.",2005,15,2793,249,2,106,266,315,227,266,239,216,184,170
b69e927c5e0a673c8738fad96419a0e12e289f28,"Abstract A particle which is caught in a potential hole and which, through the shuttling action of Brownian motion, can escape over a potential barrier yields a suitable model for elucidating the applicability of the transition state method for calculating the rate of chemical reactions.",1940,8,6202,172,1,0,0,0,1,0,1,1,1,3
e274c1b6e17825feab52de205fd0bc4917d5be6c,"Chemical shifts of backbone atoms in proteins are exquisitely sensitive to local conformation, and homologous proteins show quite similar patterns of secondary chemical shifts. The inverse of this relation is used to search a database for triplets of adjacent residues with secondary chemical shifts and sequence similarity which provide the best match to the query triplet of interest. The database contains 13Cα, 13Cβ, 13C′, 1Hα and 15N chemical shifts for 20 proteins for which a high resolution X-ray structure is available. The computer program TALOS was developed to search this database for strings of residues with chemical shift and residue type homology. The relative importance of the weighting factors attached to the secondary chemical shifts of the five types of resonances relative to that of sequence similarity was optimized empirically. TALOS yields the 10 triplets which have the closest similarity in secondary chemical shift and amino acid sequence to those of the query sequence. If the central residues in these 10 triplets exhibit similar φ and Ψ backbone angles, their averages can reliably be used as angular restraints for the protein whose structure is being studied. Tests carried out for proteins of known structure indicate that the root-mean-square difference (rmsd) between the output of TALOS and the X-ray derived backbone angles is about 15°. Approximately 3% of the predictions made by TALOS are found to be in error.",1999,71,2852,232,9,36,73,86,120,186,199,213,224,264
d786d73c09a8580583ccd4a5e5c53bb418f06e91,"A system is presented whereby volcanic rocks may be classified chemically as follows:I. Subalkaline Rocks:A. Tholeiitic basalt series:Tholeiitic picrite-basalt; tholeiite; tholeiitic andesite.B. Calc-alkali series:High-alumina basalt; andesite; dacite; rhyolite.II. Alkaline Rocks:A. Alkali olivine basalt series:(1) Alkalic picrite–basalt; ankaramite; alkali basalt; hawaiite; mugearite; benmorite; trachyte.(2) Alkalic picrite–basalt; ankaramite; alkali basalt; trachybasalt; tristanite; trachyte.B. Nephelinic, leucitic, and analcitic rocks.III. Peralkaline Rocks:pantellerite, commendite, etc.",1971,23,5758,145,1,4,9,17,23,25,20,32,36,39
37fa4423d7ce39718d7b52cc56b2451340b59815,,1986,9,5075,144,0,9,13,24,34,54,57,42,46,56
3fd096fb208bd8d3c5d41fa6b800f0faccbd07a3,"The Origin of Porosity and Permeability. Ground-Water Movement. Main Equations of Flow, Boundary Conditions, and Flow Nets. Ground Water in the Basin Hydrologic Cycle. Hydraulic Testing: Models, Methods, and Applications. Ground Water as a Resource. Stress, Strain, and Pore Fluids. Heat Transport in Ground-Water Flow. Solute Transport. Principles of Aqueous Geochemistry. Chemical Reactions. Colloids and Microorganisms. The Equations of Mass Transport. Mass Transport in Natural Ground-Water Systems. Mass Transport in Ground-Water Flow: Geologic Systems. Introduction to Contaminant Hydrogeology. Modeling the Transport of Dissolved Contaminants. Multiphase Fluid Systems. Remediation: Overview and Removal Options. In Situ Destruction and Risk Assessment. Answers to Problems. Appendices. References. Index.",1990,0,2779,256,1,1,12,12,21,35,44,49,51,48
9b3508e82af76f3da1b233f57756ae9e6328e6f5,"1. Mole Balances. The Rate of Reaction The General Mole Balance Equation Batch Reactors Continuous-Flow Reactors Industrial Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 2. Conversion and Reactor Sizing. Definition of Conversion Batch Reactor Design Equations Design Equations for Flow Reactors Applications of the Design Equations for Continuous-Flow Reactors Reactors in Series Some Further Definitions Summary CD-ROM Materials Questions and Problems Supplementary Reading 3. Rate Laws and Stoichiometry. Part 1. Rate Laws Basic Definitions The Reaction Order and the Rate Law The Reaction Rate Constant Present Status of Our Approach to Reactor Sizing and Design Part 2. Stoichiometry Batch Systems Flow Systems Summary CD-ROM Material Questions and Problems Supplementary Reading 4. Isothermal Reactor Design. Part 1. Mole Balances in Terms of Conversion Design Structure for Isothermal Reactors Scale-Up of Liquid-Phase Batch Reactor Data to the Design of a CSTR Design of Continuous Stirred Tank Reactors (CSTRs) Tubular Reactors Pressure Drop in Reactors Synthesizing the Design of a Chemical Plant Part 2. Mole Balances Written in Terms of Concentration and Molar Flow Rate Mole Balances on CSTRs, PFRs, PBRs, and Batch Reactors Microreactors Membrane Reactors Unsteady-State Operation of Stirred Reactors The Practical Side Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Some Thoughts on Critiquing What You read Journal Critique Problems Supplementary Reading 5. Collection and Analysis of Rate Data. The Algorithm for Data Analysis Batch Reactor Data Method of Initial Rates Method of Half-Lives Differential Reactors Experimental Planning Evaluation of Laboratory Reactors Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 6. Multiple Reactions. Definitions Parallel Reactions Maximizing the Desired Product in Series Reactions Algorithm for Solution of Complex Reactions Multiple Reactions in a PFR/PBR Multiple Reactions in a CSTR Membrane Reactors to Improve Selectivity in Multiple Reactions Complex Reactions of Ammonia Oxidation Sorting It All Out The Fun Part Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 7. Reaction Mechanisms, Pathways, Bioreactions, and Bioreactors. Active Intermediates and Nonelementary Rate Laws Enzymatic Reaction Fundamentals Inhibition of Enzyme Reactions Bioreactors Physiologically Based Pharmacokinetic (PBPK) Models Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 8. Steady-State Nonisothermal Reactor Design. Rationale The Energy Balance Adiabatic Operation Steady-State Tubular Reactor with Heat Exchange Equilibrium Conversion CSTR with Heat Effects Multiple Steady States Nonisothermal Multiple Chemical Reactions Radial and Axial Variations in a Tubular Reactor The Practical Side Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 9. Unsteady-State Nonisothermal Reactor Design. The Unsteady-State Energy Balance Energy Balance on Batch Reactors Semibatch Reactors with a Heat Exchanger Unsteady Operation of a CSTR Nonisothermal Multiple Reactions Unsteady Operation of Plug-Flow Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 10. Catalysis and Catalytic Reactors. Catalysts Steps in a Catalytic Reaction Synthesizing a Rate Law, Mechanism, and Rate-Limiting Step Heterogeneous Data Analysis for Reactor Design Reaction Engineering in Microelectronic Fabrication Model Discrimination Catalyst Deactivation Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 11. External Diffusion Effects on Heterogeneous Reactions. Diffusion Fundamentals Binary Diffusion External Resistance to Mass Transfer What If ... ? (Parameter Sensitivity) The Shrinking Core Model Summary CD-ROM Material Questions and Problems Supplementary Reading 12. Diffusion and Reaction. Diffusion and Reaction in Spherical Catalyst Pellets Internal Effectiveness Factor Falsified Kinetics Overall Effectiveness Factor Estimation of Diffusion- and Reaction-Limited Regimes Mass Transfer and Reaction in a Packed Bed Determination of Limiting Situations from Reaction Data Multiphase Reactors Fluidized Bed Reactors Chemical Vapor Deposition (CVD) Summary CD-ROM Material Questions and Problems Journal Article Problems Journal Critique Problems Supplementary Reading 13. Distributions of Residence Times for Chemical Reactors. General Characteristics Part 1. Characteristics and Diagnostics Measurement of the RTD Characteristics of the RTD RTD in Ideal Reactors Diagnostics and Troubleshooting Part 2. Predicting Conversion and Exit Concentration Reactor Modeling Using the RTD Zero-Parameter Models Using Software Packages RTD and Multiple Reactions Summary CD-ROM Material Questions and Problems Supplementary Reading 14. Models for Nonideal Reactors. Some Guidelines Tanks-in-Series (T-I-S) Model Dispersion Model Flow, Reaction, and Dispersion Tanks-in-Series Model Versus Dispersion Model Numerical Solutions to Flows with Dispersion and Reaction Two-Parameter Models-Modeling Real Reactors with Combinations of Ideal Reactors Use of Software Packages to Determine the Model Parameters Other Models of Nonideal Reactors Using CSTRs and PFRs Applications to Pharmacokinetic Modeling Summary CD-ROM Material Questions and Problems Supplementary Reading Appendix A: Numerical Techniques. Appendix B: Ideal Gas Constant and Conversion Factors. Appendix C: Thermodynamic Relationships Involving the Equilibrium Constant. Appendix D: Measurement of Slopes on Semilog Paper. Appendix E: Software Packages. Appendix F: Nomenclature. Appendix G: Rate Law Data. Appendix H: Open-Ended Problems. Appendix I: How to Use the CD-ROM. Appendix J: Use of Computational Chemistry Software Packages. Index. About the CD-ROM.",1986,0,3428,177,0,1,1,1,3,4,2,7,12,24
5923de1dcbe5b4927500c57c62824194338e118c,"5.1. Detection Formats 475 5.2. Food Quality and Safety Analysis 477 5.2.1. Pathogens 477 5.2.2. Toxins 479 5.2.3. Veterinary Drugs 479 5.2.4. Vitamins 480 5.2.5. Hormones 480 5.2.6. Diagnostic Antibodies 480 5.2.7. Allergens 481 5.2.8. Proteins 481 5.2.9. Chemical Contaminants 481 5.3. Medical Diagnostics 481 5.3.1. Cancer Markers 481 5.3.2. Antibodies against Viral Pathogens 482 5.3.3. Drugs and Drug-Induced Antibodies 483 5.3.4. Hormones 483 5.3.5. Allergy Markers 483 5.3.6. Heart Attack Markers 484 5.3.7. Other Molecular Biomarkers 484 5.4. Environmental Monitoring 484 5.4.1. Pesticides 484 5.4.2. 2,4,6-Trinitrotoluene (TNT) 485 5.4.3. Aromatic Hydrocarbons 485 5.4.4. Heavy Metals 485 5.4.5. Phenols 485 5.4.6. Polychlorinated Biphenyls 487 5.4.7. Dioxins 487 5.5. Summary 488 6. Conclusions 489 7. Abbreviations 489 8. Acknowledgment 489 9. References 489",2008,0,3198,95,30,106,140,199,224,220,267,344,300,321
4b0b2db5ee057de28aee48403652cc5dac26d402,"Twelve zeolitic imidazolate frameworks (ZIFs; termed ZIF-1 to -12) have been synthesized as crystals by copolymerization of either Zn(II) (ZIF-1 to -4, -6 to -8, and -10 to -11) or Co(II) (ZIF-9 and -12) with imidazolate-type links. The ZIF crystal structures are based on the nets of seven distinct aluminosilicate zeolites: tetrahedral Si(Al) and the bridging O are replaced with transition metal ion and imidazolate link, respectively. In addition, one example of mixed-coordination imidazolate of Zn(II) and In(III) (ZIF-5) based on the garnet net is reported. Study of the gas adsorption and thermal and chemical stability of two prototypical members, ZIF-8 and -11, demonstrated their permanent porosity (Langmuir surface area = 1,810 m2/g), high thermal stability (up to 550°C), and remarkable chemical resistance to boiling alkaline water and organic solvents.",2006,30,4298,56,1,9,22,52,77,148,197,224,308,351
6eb64578b01ce2ae51b6ecbb025c6ee8d408ad5c,"Boron-doped silicon nanowires (SiNWs) were used to create highly sensitive, real-time electrically based sensors for biological and chemical species. Amine- and oxide-functionalized SiNWs exhibit pH-dependent conductance that was linear over a large dynamic range and could be understood in terms of the change in surface charge during protonation and deprotonation. Biotin-modified SiNWs were used to detect streptavidin down to at least a picomolar concentration range. In addition, antigen-functionalized SiNWs show reversible antibody binding and concentration-dependent detection in real time. Lastly, detection of the reversible binding of the metabolic indicator Ca2+ was demonstrated. The small size and capability of these semiconductor nanowires for sensitive, label-free, real-time detection of a wide range of chemical and biological species could be exploited in array-based screening and in vivo diagnostics.",2001,56,5080,66,2,32,64,119,191,228,252,304,351,340
bcacbbeec2ecb88e716e5dd66be4bd5b2699c168,"This handbook of chemical tests for diagnostic, agricultural, and environmental purposes promotes the use of consistent methods, procedures and terminologies in soil and land surveys undertaken throughout Australia. Soil and water chemical methods include sampling and sample preparation, and measuring electrical conductivity and pH. Soil analysis includes: chloride, carbon, nitrogen, phosphorus, sulfur, gypsum, Other CABI sites ",1992,0,2504,332,0,1,6,8,24,21,24,41,47,55
9dad457c31f7688b432689e48bf573a09b2d456f,"It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.",1952,0,5825,90,0,5,1,3,4,4,2,2,2,1
5392c8d762d3f6fe4a6ff28ad8265ecb5cb7db83,"Chemical sensors based on individual single-walled carbon nanotubes (SWNTs) are demonstrated. Upon exposure to gaseous molecules such as NO(2) or NH(3), the electrical resistance of a semiconducting SWNT is found to dramatically increase or decrease. This serves as the basis for nanotube molecular sensors. The nanotube sensors exhibit a fast response and a substantially higher sensitivity than that of existing solid-state sensors at room temperature. Sensor reversibility is achieved by slow recovery under ambient conditions or by heating to high temperatures. The interactions between molecular species and SWNTs and the mechanisms of molecular sensing with nanotube molecular wires are investigated.",2000,49,5158,61,10,43,79,135,181,225,307,337,355,350
9e4c2c2c7ecdc0a4fcc19c88b6c0e4bfb09a4864,,1991,0,3211,131,3,18,18,44,46,69,68,86,68,93
c7f679ffc767b5df24cee55860f46cdd1576214f,"Global energy consumption is projected to increase, even in the face of substantial declines in energy intensity, at least 2-fold by midcentury relative to the present because of population and economic growth. This demand could be met, in principle, from fossil energy resources, particularly coal. However, the cumulative nature of CO2 emissions in the atmosphere demands that holding atmospheric CO2 levels to even twice their preanthropogenic values by midcentury will require invention, development, and deployment of schemes for carbon-neutral energy production on a scale commensurate with, or larger than, the entire present-day energy supply from all sources combined. Among renewable energy resources, solar energy is by far the largest exploitable resource, providing more energy in 1 hour to the earth than all of the energy consumed by humans in an entire year. In view of the intermittency of insolation, if solar energy is to be a major primary energy source, it must be stored and dispatched on demand to the end user. An especially attractive approach is to store solar-converted energy in the form of chemical bonds, i.e., in a photosynthetic process at a year-round average efficiency significantly higher than current plants or algae, to reduce land-area requirements. Scientific challenges involved with this process include schemes to capture and convert solar energy and then store the energy in the form of chemical bonds, producing oxygen from water and a reduced fuel such as hydrogen, methane, methanol, or other hydrocarbon species.",2006,114,6113,53,1,16,51,90,128,222,313,408,547,614
1777b325b569ded8515323adf54d4f6e5b160070,"Indirect evidence is presented that free‐standing Si quantum wires can be fabricated without the use of epitaxial deposition or lithography. The novel approach uses electrochemical and chemical dissolution steps to define networks of isolated wires out of bulk wafers. Mesoporous Si layers of high porosity exhibit visible (red) photoluminescence at room temperature, observable with the naked eye under <1 mW unfocused (<0.1 W cm−2) green or blue laser line excitation. This is attributed to dramatic two‐dimensional quantum size effects which can produce emission far above the band gap of bulk crystalline Si.",1990,24,6950,37,1,63,170,279,257,244,308,266,268,246
8fc260f0a6070925e693e7048e95b55a0eed92b3,"An empirical many-body potential-energy expression is developed for hydrocarbons that can model intramolecular chemical bonding in a variety of small hydrocarbon molecules as well as graphite and diamond lattices. The potential function is based on Tersoff's covalent-bonding formalism with additional terms that correct for an inherent overbinding of radicals and that include nonlocal effects. Atomization energies for a wide range of hydrocarbon molecules predicted by the potential compare well to experimental values. The potential correctly predicts that the \ensuremath{\pi}-bonded chain reconstruction is the most stable reconstruction on the diamond {111} surface, and that hydrogen adsorption on a bulk-terminated surface is more stable than the reconstruction. Predicted energetics for the dimer reconstructed diamond {100} surface as well as hydrogen abstraction and chemisorption of small molecules on the diamond {111} surface are also given. The potential function is short ranged and quickly evaluated so it should be very useful for large-scale molecular-dynamics simulations of reacting hydrocarbon molecules.",1990,0,3269,107,0,8,17,14,32,31,38,38,45,34
7d7e9fa2b79dbaace65755a4e86d1ed3f1695cf4,"Examination of nature's favorite molecules reveals a striking preference for making carbon-heteroatom bonds over carbon-carbon bonds-surely no surprise given that carbon dioxide is nature's starting material and that most reactions are performed in water. Nucleic acids, proteins, and polysaccharides are condensation polymers of small subunits stitched together by carbon-heteroatom bonds. Even the 35 or so building blocks from which these crucial molecules are made each contain, at most, six contiguous C-C bonds, except for the three aromatic amino acids. Taking our cue from nature's approach, we address here the development of a set of powerful, highly reliable, and selective reactions for the rapid synthesis of useful new compounds and combinatorial libraries through heteroatom links (C-X-C), an approach we call ""click chemistry"". Click chemistry is at once defined, enabled, and constrained by a handful of nearly perfect ""spring-loaded"" reactions. The stringent criteria for a process to earn click chemistry status are described along with examples of the molecular frameworks that are easily made using this spartan, but powerful, synthetic strategy.",2001,17,5784,43,1,2,2,8,13,72,129,236,314,411
a252446c4054d5b000dcb80cf5ec0b50afa99580,"As part of a series of evaluated sets, rate constants and photochemical cross sections compiled by the NASA Panel for Data Evaluation are provided. The primary application of the data is in the modeling of stratospheric processes, with particular emphasis on the ozone layer and its possible perturbation by anthropogenic and natural phenomena. Copies of this evaluation are available from the Jet Propulsion Laboratory.",1985,12,3406,108,4,7,12,12,9,21,72,83,94,124
f66608476a3cf3c6147823f55fc0ba235234175f,Computer program is described for numerical solution of chemical equilibria in complex systems by using nonlinear algebraic equations. Free-energy minimization technique is used.,1972,37,2939,194,4,5,8,6,10,10,12,19,23,26
2c0f30d8e044c7443eb1d9a44f287dbce709a44c,3.2.3. Hydroformylation 2467 3.2.4. Dimerization 2468 3.2.5. Oxidative Cleavage and Ozonolysis 2469 3.2.6. Metathesis 2470 4. Terpenes 2472 4.1. Pinene 2472 4.1.1. Isomerization: R-Pinene 2472 4.1.2. Epoxidation of R-Pinene 2475 4.1.3. Isomerization of R-Pinene Oxide 2477 4.1.4. Hydration of R-Pinene: R-Terpineol 2478 4.1.5. Dehydroisomerization 2479 4.2. Limonene 2480 4.2.1. Isomerization 2480 4.2.2. Epoxidation: Limonene Oxide 2480 4.2.3. Isomerization of Limonene Oxide 2481 4.2.4. Dehydroisomerization of Limonene and Terpenes To Produce Cymene 2481,2007,17,4251,12,5,56,96,130,211,242,353,425,410,427
515609af801dc07869249a89c9496136ab556813,"The ability to sustain a diatropic ring current is the defining characteristic of aromatic species.1-7 Cyclic electron delocalization results in enhanced stability, bond length equalization, and special magnetic as well as chemical and physical properties.1 In contrast, antiaromatic compounds sustain paratropic ring currents3 despite their localized, destabilized structures.1-7 We have demonstrated the direct, quantitative relationships among energetic, geometrical, and magnetic criteria of aromaticity in a wide-ranging set of aromatic/antiaromatic fivemembered rings.5a While the diamagnetic susceptibility exaltation (Λ) is uniquely associated with aromaticity, it is highly dependent on the ring size (area2) and requires suitable calibration standards.6 Aromatic stabilization energies (ASEs) of strained and more complicated systems are difficult to evaluate. CC bond length variations in polybenzenoid hydrocarbons can be just as large as those in linear conjugated polyenes.2 The abnormal proton chemical shifts of aromatic molecules are the most commonly employed indicators of ring current effects.1 However, the ca. 2-4 ppm displacements of external protons to lower magnetic fields are relatively modest (e.g., δH ) 7.3 for benzene vs 5.6 for dC-H in cyclohexene). In contrast, the upfield chemical shifts of protons located inside aromatic rings are more unusual. The six inner hydrogens of [18]annulene, for example, resonate at -3.0 ppm vs δ ) 9.28 for the outer protons. This relationship is inverted dramatically in the antiaromatic [18]annulene dianion, C18H18, where δ ) 20.8 and 29.5 (in) vs. -1.1 (out).8 Similar demonstrations of paratropic ring currents in antiaromatic compounds are well documented.3,8,9 Chemical shifts of encapsulated 3He atoms are now employed as experimental and computed measures of aromaticity in fullerenes and fullerene derivatives.10 While the rings of most aromatic systems are too small to accommodate atoms internally, the chemical shifts of hydrogens in bridging positions have long been used as aromaticity and antiaromaticity probes.9 δLi+ can be employed similarly, with the advantage that Li+ complexes with individual rings in polycyclic systems can be computed.4,11 We now propose the use of absolute magnetic shieldings, computed at ring centers (nonweighted mean of the heavy atom coordinates) with available quantum mechanics programs,12 as a new aromaticity/antiaromaticity criterion. To correspond to the familiar NMR chemical shift convention, the signs of the computed values are reversed: Negative “nucleus-independent chemical shifts” (NICSs) denote aromaticity; positive NICSs, antiaromaticity (see Table 1 for selected results). Figure 1, a plot of NICSs vs the ASEs for our set of five-membered ring heterocycles,5a provides calibration. The equally good correlations with magnetic susceptibility exaltations and with structural variations establish NICS as an effective aromaticity criterion. Unlike Λ,6 NICS values for [n]annulenes (Table 1) show only a modest dependence on ring size. The 10 π electron systems give significantly higher values than those with 6 π electrons. The antiaromatic 4n π electron compounds, cyclobutadiene (27.6), pentalene (18.1), heptalene (22.7), and planar D4h cyclooctatetraene (30.1), all show highly positive NICSs. Like the Li+-complex probe,4 the NICS evaluates the aromaticity and antiaromaticity contributions of individual rings in polycyclic systems. Scheme 1 (HF/6-31+G*, data from Table 1) shows NICSs for selected examples. The benzenoid aromatic NICSs provide evidence both for localized and “perimeter” models. The naphthalene (1) NICS (-9.9) resembles that of benzene (-9.7), as do the NICSs for the outer rings of phenanthrene (2) (-10.2) and triphenylene (3); the aromaticity of the central rings of the latter two are reduced. The NICS of the central ring of anthracene (4) (-13.3) exceeds the benzene value in contrast to the outer ring NICS (-8.2). Remarkably, the NICS (-7.0) for the seven-membered ring of azulene (5) is very close to that of the tropylium ion (-7.6 ppm), whereas the azulene five-membered ring NICS (-19.7) is even larger in magnitude than that of the cyclopentadienyl anion (-14.3). The four-membered rings in benzocyclobutadiene (6) (NICS ) 22.5) and in biphenylene (7) (19.0) are antiaromatic, but less so than cyclobutadiene itself (27.6). The six-membered rings in these polycycles are still aromatic, but their NICSs (-2.5 (1) (a) Minkin, V. I.; Glukhovtsev, M. N.; Simkin, B. Y. Aromaticity and Antiaromaticity; Wiley: New York, 1994. (b) Garratt, P. J. Aromaticity; Wiley: New York, 1986. (c) Eluidge, J. A.; Jackman, L. M. J. Chem. Soc. 1961, 859. (2) Schleyer, P. v. R.; Jiao, H. Pure Appl. Chem. 1996, 28, 209. (3) Pople, J. A.; Untch, K. G. J. Am. Chem. Soc. 1966, 88, 4811. (4) Jiao, H; Schleyer, P. v. R. AIP Conference Proceedings 330, E.C.C.C.1, Computational Chemistry; Bernardi, F., Rivail, J.-L., Eds.; American Institute of Physics: Woodbury, New York, 1995; p 107. (5) (a) Schleyer, P. v. R.; Freeman, P.; Jiao, H.; Goldfuss, B. Angew. Chem., Int. Ed. Engl. 1995, 34, 337. (b) Jiao, H.; Schleyer, P. v. R. Unpublished IGLO results. (c) Kutzelnigg, W.; Fleischer, U.; Schindler, M. In NMR: Basic Princ. Prog.; Springer: Berlin, 1990; Vol. 23, p 165. (6) Dauben, H. J., Jr.; Wilson, J. D.; Laity, J. L. In Non-Benzenoid Aromatics; Synder, J., Ed.; Academic Press, 1971; Vol. 2, and references cited. The partitioning of ring current or ring current susceptabilitites among various rings in polycyclic syestems were considered earlier, e.g., by Aihara (Aihara, J. J. Am. Chem. Soc. 1985, 207, 298 and refs cited) and by Mallion (Haigh, C. W.; Mallion, J. Chem. Phys. 1982, 76, 1982). (7) Fleischer, U.; Kutzelnigg, W.; Lazzeretti, P.; Mühlenkamp, V. J. Am. Chem. Soc. 1994, 116, 5298. (8) Sondheimer, F. Acc. Chem. Res. 1972, 5, 81. (9) (a) Hunandi, R. J. J. Am. Chem. Soc. 1983, 105, 6889. (b) Pascal, R. A., Jr.; Winans, C. G.; Van Engen, D. J. Am. Chem. Soc. 1989, 111, 3007. (10) (a) Bühl, M.; Thiel, W.; Jiao, H.; Schleyer, P. v. R.; Saunders, M.; Anet, F. A. L. J. Am. Chem. Soc. 1994, 116, 7429 and references cited. (b) Bühl, M.; van Wüllen, C. Chem. Phys. Lett. 1995, 247, 63. The authors have shown that the negative absolute shielding in the center of C60 is nearly the same as δ3He, computed at the same level. (11) Paquette, L. A.; Bauer, W.; Sivik, M. R.; Bühl, M.; Feigel, M.; Schleyer, P. v. R. J. Am. Chem. Soc. 1990, 112, 8776. (12) Frisch, M. J.; Trucks, G. W.; Schlegel, H. B.; Gill, P. M. W.; Johnson, B. G.; Robb, M. A.; Cheeseman, J. R.; Keith, T. A.; Petersson, G. A.; Montgomery, J. A.; Raghavachari, K.; Al-Laham, M. A.; Zakrewski, V. G.; Ortiz, J. V.; Foresman, J. B.; Cioslowski, J.; Stefanov, B. B.; Nanayakkara, A.; Challacombe, M.; Peng, C. Y.; Ayala, P. Y.; Chen, W.; Wong, M. W.; Andres, J. L.; Replogle, E. S.; Gomperts, R.; Stewart, J. P.; Head-Gordon, M.; Gonzalez, C.; Pople, J. A. Gaussian 94, ReVision B.2; Gaussian Inc., Pittsburgh, PA, 1995. Figure 1. Plot of NICSs (ppm) vs the aromatic stabilization energies (ASEs, kcal/mol)5a for a set of five-membered ring heterocycles, C4H4X (X ) as shown) (cc ) 0.966). 6317 J. Am. Chem. Soc. 1996, 118, 6317",1996,16,3888,22,3,9,27,30,25,54,58,61,71,107
3f7983818b76a5f1b5daf9b605877ed401c8e73c,"(1) Klamer, A. D. “Some Results Concerning Polyominoes”. Fibonacci Q. 1965, 3(1), 9-20. (2) Golomb, S. W. Polyominoes·, Scribner, New York, 1965. (3) Harary, F.; Read, R. C. “The Enumeration of Tree-like Polyhexes”. Proc. Edinburgh Math. Soc. 1970, 17, 1-14. (4) Lunnon, W. F. “Counting Polyominoes” in Computers in Number Theory·, Academic: London, 1971; pp 347-372. (5) Lunnon, W. F. “Counting Hexagonal and Triangular Polyominoes”. Graph Theory Comput. 1972, 87-100. (6) Brunvoll, J.; Cyvin, S. J.; Cyvin, B. N. “Enumeration and Classification of Benzenoid Hydrocarbons”. J. Comput. Chem. 1987, 8, 189-197. (7) Balaban, A. T., et al. “Enumeration of Benzenoid and Coronoid Hydrocarbons”. Z. Naturforsch., A: Phys., Phys. Chem., Kosmophys. 1987, 42A, 863-870. (8) Gutman, I. “Topological Properties of Benzenoid Systems”. Bull. Soc. Chim., Beograd 1982, 47, 453-471. (9) Gutman, I.; Polansky, O. E. Mathematical Concepts in Organic Chemistry·, Springer: Berlin, 1986. (10) To3i6, R.; Doroslovacki, R.; Gutman, I. “Topological Properties of Benzenoid Systems—The Boundary Code”. MATCH 1986, No. 19, 219-228. (11) Doroslovacki, R.; ToSic, R. “A Characterization of Hexagonal Systems”. Rev. Res. Fac. Sci.-Univ. Novi Sad, Math. Ser. 1984,14(2) 201-209. (12) Knop, J. V.; Szymanski, K.; Trinajstic, N. “Computer Enumeration of Substituted Polyhexes”. Comput. Chem. 1984, 8(2), 107-115. (13) Stojmenovic, L; Tosió, R.; Doroslovaóki, R. “Generating and Counting Hexagonal Systems”. Proc. Yugosl. Semin. Graph Theory, 6th, Dubrovnik 1985; pp 189-198. (14) Doroslovaóki, R.; Stojmenovió, I.; Tosió, R. “Generating and Counting Triangular Systems”. BIT 1987, 27, 18-24. (15) Knop, J. V.; Miller, W. R.; Szymanski, K.; Trinajstic, N. Computer Generation of Certain Classes of Molecules·, Association of Chemists and Technologists of Croatia: Zagreb, 1985.",1988,15,3133,151,2,2,8,9,8,7,5,11,23,15
3537d34e0910792df21db1d6750a485396a3976e,"In contrast to a recently expressed, and widely cited, view that ""Ionic liquids are starting to leave academic labs and find their way into a wide variety of industrial applications"", we demonstrate in this critical review that there have been parallel and collaborative exchanges between academic research and industrial developments since the materials were first reported in 1914 (148 references).",2008,177,3782,10,30,85,151,228,287,308,338,388,403,349
7f7daa06e89a9ac23c3dfb928048a6a47efcc71f,"Encyclopedia of Chemical Technology The Third Edition of the Encyclopedia of Chemical Technology is built on the solid foundation of the previous editions. All of the articles have been rewritten and updated and many new subjects have been added to reflect changes in chemical technology through the 1970s. The new edition, however, will be familiar to users of the earlier editions comprehensive, authoritative, accessible, lucid. The Encyclopedia remains an indispensable information source for all producers and users of chemical products and materials. In the Third Edition, emphasis is given to major present-day topics of concern to all chemists, scientists, and engineers--energy, health, safety, toxicology, and new materials. New subjects have been added, especially those related to polymer and plastics technology, fuels and energy, inorganic and solid-state chemistry, composite materials, coating, fermentation and enzymes, pharmaceuticals, surfactant technology, fibers and textiles. New features include the use of SI units as well as English units, Chemical Abstracts Service's Registry Numbers, and complete indexing based on automated retrieval from a machine-readable composition system. Once again this classic serves as an unrivaled library of information for the chemical and allied industries. Some comments about Kirk-Othmer-- The First Edition ""No reference library worthy of the name will be without this series. It is simply a must for the chemist and chemical engineer..."" --Chemical and Engineering News The Second Edition ""A necessity for any technical library."" --Choice",1998,0,3287,25,90,102,96,113,137,122,123,172,171,186
b6316cdad5c28ea26dc4a9d90ae464cd0ff21ab5,,1981,35,3820,23,0,3,4,3,4,5,1,8,7,7
b03c9cf546e6b18c9a4467ab555a53f995299795,"Glucosinolates (beta-thioglucoside-N-hydroxysulfates), the precursors of isothiocyanates, are present in sixteen families of dicotyledonous angiosperms including a large number of edible species. At least 120 different glucosinolates have been identified in these plants, although closely related taxonomic groups typically contain only a small number of such compounds. Glucosinolates and/or their breakdown products have long been known for their fungicidal, bacteriocidal, nematocidal and allelopathic properties and have recently attracted intense research interest because of their cancer chemoprotective attributes. Numerous reviews have addressed the occurrence of glucosinolates in vegetables, primarily the family Brassicaceae (syn. Cruciferae; including Brassica spp and Raphanus spp). The major focus of much previous research has been on the negative aspects of these compounds because of the prevalence of certain ""antinutritional"" or goitrogenic glucosinolates in the protein-rich defatted meal from widely grown oilseed crops and in some domesticated vegetable crops. There is, however, an opposite and positive side of this picture represented by the therapeutic and prophylactic properties of other ""nutritional"" or ""functional"" glucosinolates. This review addresses the complex array of these biologically active and chemically diverse compounds many of which have been identified during the past three decades in other families. In addition to the Brassica vegetables, these glucosinolates have been found in hundreds of species, many of which are edible or could provide substantial quantities of glucosinolates for isolation, for biological evaluation, and potential application as chemoprotective or other dietary or pharmacological agents.",2001,304,2471,167,12,45,43,51,73,88,89,117,103,124
24e70394a03ee6f25e582e7ebb8039f1207fae0f,"There is a general consensus that supports the need for standardized reporting of metadata or information describing large-scale metabolomics and other functional genomics data sets. Reporting of standard metadata provides a biological and empirical context for the data, facilitates experimental replication, and enables the re-interrogation and comparison of data by others. Accordingly, the Metabolomics Standards Initiative is building a general consensus concerning the minimum reporting standards for metabolomics experiments of which the Chemical Analysis Working Group (CAWG) is a member of this community effort. This article proposes the minimum reporting standards related to the chemical analysis aspects of metabolomics experiments including: sample preparation, experimental analysis, quality control, metabolite identification, and data pre-processing. These minimum standards currently focus mostly upon mass spectrometry and nuclear magnetic resonance spectroscopy due to the popularity of these techniques in metabolomics. However, additional input concerning other techniques is welcomed and can be provided via the CAWG on-line discussion forum at http://msi-workgroups.sourceforge.net/ or http://Msi-workgroups-feedback@lists.sourceforge.net. Further, community input related to this document can also be provided via this electronic forum.",2007,24,2495,69,5,19,29,36,38,51,103,122,186,164
66cabbe9e53bc56a695f65e9d3efbe0b91ce756c,"From the Publisher: 
This thoroughly revised edition presents important methods in the quantitative analysis of geologic data. Retains the basic arrangement of the previous edition but expands sections on probability, nonparametric statistics, and Fourier analysis. Contains revised coverage of eigenvalues and eigenvectors, and new coverage of data analysis methods, such as the semivariogram and the process of kriging.",1988,0,5458,198,79,79,81,91,113,95,86,102,115,112
1eb9f88bcf886525b4d46a6eb3f33ea34749e40e,"The Roots of Isotope Geology. The Internal Structure of Atoms. Decay Mechanisms of Radioactive Atoms. Radioactive Decay and Growth. Mass Spectrometry. The K-Ar Method of Dating. The 40 Ar/39 Ar Method of Dating. The Rb-Sr Method of Dating. Two-Component Mixtures. Isotope Geology of Strontium in Meteorites and Igneous Rocks. Isotope Geology of Strontium in Sedimentary Rocks. The Sm-Nd Method of Dating. Isotope Geology of Neodymium and Strontium in Igneous Rocks. Isotope Geology of Neodymium in Sedimentary Rocks. The Lu-Hf Method of Dating. The RE-Os Method of Dating. The Re-Os Method of Dating. The K-Ca Method of Dating. The U, Th-Pb Methods of Dating. The Isotope Geology of Lead. The Fission-Track and Other Radiation Damage Methods of Dating. The U-Series Disequilibrium Methods of Dating. Cosmogenic Radionuclides. Cosmogenic Carbon-14 and Tritium. Carbon. Sulfur.",1977,0,3004,198,0,1,13,14,22,11,15,18,18,22
09cd71b29d32e0f891fdd6ea022fddb2deb98b4d,"For the past three centuries, the effects of humans on the global environment have escalated. Because of these anthro-pogenic emissions of carbon dioxide, global climate may depart significantly from natural behaviour for many millennia to come. It seems appropriate to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene—the warm period of the past 10–12 millennia.",2002,1,2870,149,3,4,12,10,21,24,26,37,53,90
c0f22604d7137aabe1d6d0f03499dbbcd0635260,"The development of petroleum geochemistry and geology carbon and origin of life petroleum and its products how oil forms - natural hydrocarbons how oil forms - generated hydrocarbons modeling petroleum generation the origin of natural gas migration and accumulation abnormal pressures the source rock coals, shales, and other terrestrial source rocks petroleum in the reservoir seeps and surface prospecting a geochemical program for petroleum exploration crude oil correlation prospect evaluation.",1995,0,2405,208,41,18,35,45,48,39,41,53,56,52
1e9dbb399fcc002868fe2c7564ce58e905ce50b1,"This book introduces the fundamental concepts of fractal geometry and chaotic dynamics. These concepts are then related to a variety of geological and geophysical problems, illustrating just what chaos theory and fractals really tell us and how they can be applied to the earth sciences. Petroleum and mineral reserves, earthquakes, mantle convection and magnetic field generation are among the earth's properties that come under scrutiny. This is the first book that covers these topics at an accessible level; the concepts are introduced at the lowest possible level of mathematics and are consistently understandable, so that the reader requires only a background in basic physics and mathematics. Problems are also included for the reader to solve.",1992,0,2455,160,2,9,30,41,49,42,59,75,76,74
d01edba298a2c058ae150c416ed569e4a097388a,"Keywords: Geologie ; Analyse des contraintes ; Fissuration Reference Record created on 2004-09-07, modified on 2016-08-08",1987,0,2137,95,29,30,27,28,25,28,37,37,61,46
9c7af96eb2dd71d8525161c026847f386b9e01b9,1 Introduction.- 2 Historical Background.- 3 Concepts of Scale.- 4 Methods of Architectural-Element Analysis.- 5 Lithofacies.- 6 Architectural Elements Formed Within Channels.- 7 Architectural Elements of the Overbank Environment.- 8 Fluvial Styles and Facies Models.- 9 The Stratigraphic Architecture of Fluvial Depositional Systems.- 10 Fluvial Depositional Systems and Autogenic Sedimentary Controls.- 11 Tectonic Control of Fluvial Sedimentation.- 12 What Does Fluvial Lithofacies Reveal About Climate?.- 13 Sequence Stratigraphy.- 14 Stratigraphic and Tectonic Controls on the Distribution and Architecture of Fluvial Oil and Gas Reservoirs.- 15 Case Studies of Oil and Gas Fields in Fluvial Reservoirs.- 16 Future Research Trends.- References.- Author Index.,1996,3,1546,178,0,4,10,8,14,20,36,30,31,41
c561af7a53d3511e7230c37977bbfba4277b44fc,,1976,0,1824,266,1,3,2,4,4,4,6,11,8,5
9fce4527c064d044d982c9a37406ab8d658fe490,"The first € price and the £ and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the €(D) includes 7% for Germany, the €(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. A. Miall The Geology of Fluvial Deposits",2006,0,1129,101,52,58,42,49,61,46,50,83,72,83
8bca2f026bb8bf225405b01dee99a614d3c578ad,GEOLOGY - IGNEOUS AND METAMORPHIC ROCKS.- The Basement Complex.- The Younger Granites.- GEOLOGY - SEDIMENTARY BASINS.- Cretaceous - Cenozoic Magmatism and Volcanism.- The Benue Trough.- The Bornu Basin (Nigerian Sector of the Chad Basin).- The Sokoto Basin (Nigerian Sector of the Iullemmeden Basin).- The Mid-Niger (Bida) Basin.- The Dahomey Basin.- MINERAL RESOURCES.- The Niger Delta Basin.- Solid Mineral Resources.- Petroleum Resources.- Policy Issues and Development Options.,2009,0,531,70,0,1,5,22,43,40,35,35,51,52
fadf14179f827dc98da5c9752cd1c944b6218d2d,"Coral reefs are generally associated with shallow tropical seas; however, recent deep-ocean exploration using advanced acoustics and submersibles has revealed unexpectedly widespread and diverse coral ecosystems in deep waters on continental shelves, slopes, seamounts, and ridge systems around the world. Advances reviewed here include the use of corals as paleoclimatic archives and their biogeological functioning, biodiversity, and biogeography. Threats to these fragile, long-lived, and rich ecosystems are mounting: The impacts of deep-water trawling are already widespread, and effects of ocean acidification are potentially devastating.",2006,31,841,63,10,20,48,58,50,59,61,63,70,40
28f060d89e0e81a8e20e66f96e7dd017658eb54c,"This third edition of the Glossary of Geology contains approximately 37,000 terms, or 1,000 more than the second edition. New entries are especially numerous in the fields of carbonate sedimentology, hydrogeology, marine geology, mineralogy, ore deposits, plate tectonics, snow and ice, and stratigraphic nomenclature. Many of the definitions provide background information.",1987,0,1252,19,5,19,16,22,19,22,24,24,30,26
6d70584bdd93f94dcd294d168834cfb99ec56976,"Seismic anisotropy beneath continents is analyzed from shear-wave splitting recorded at more than 300 continental seismic stations. Anisotropy is found to be a ubiquitous property that is due to mantle deformation from past and present orogenic activity. The observed coherence with crustal deformation implies that the mantle plays a major, if not dominant, role in orogenies. No evidence is found for a continental asthenospheric decoupling zone, suggesting that continents are coupled to general mantle circulation.",1996,122,1018,150,7,12,29,28,31,26,56,34,80,25
42cdd4d3e2dbb242a61ec38dbec9a79441d75ad4,"Scholars from Egypt, Germany and the US review and analyze the results of work carried out on the geology of Egypt: geomorphology and evolution of landscape, tectonics, geophysical regime, volcanicity, Precambrian geology, geologic history and paleogeography, paleontology of selected taxa, ore depos",1962,0,1526,16,0,0,3,5,4,3,6,4,7,3
fce492c7366a7cef2b0462352081ea94bb632cfb,"Acknowledgements 1. Seabed fluid flow introduction 2. Pockmarks, shallow gas and seeps: an initial appraisal 3. Seabed fluid flow around the world 4. The contexts of seabed fluid flow 5. The nature and origins of flowing fluids 6. Shallow gas and gas hydrates 7. Migration and seabed features 8. Seabed fluid flow and biology 9. Seabed fluid flow and mineral precipitation 10. Impacts on the hydrosphere and atmosphere 11. Implications for man References Index.",2007,0,685,96,6,15,30,51,39,59,31,52,55,58
22e7d4e5da07e26f127d0f30b3c1ff4f8ac2d8db,,1964,0,1360,90,0,0,0,2,1,5,4,9,9,10
e1cdf720c38fd8697a6a65b311d6cbed24a9805a,"STUDENTS of Geology will welcome this third and much enlarged edition of Prof. Green's excellent text-book, though they may at first sight regret the exchange of the old convenient manual form of the book for that of the present handsome and well-printed octavo. One of the first features that strikes the reader in this new issue of the work is the large augmentations made to the lithological sections. In fact this part of the treatise may be said to have been re-cast and almost wholly re-written. The author devotes 150 closely printed pages to crystallography and the description of minerals. It may be open to question whether the full details which he gives to the crystallographic characters of minerals are not rather out of place in a geological treatise. They are not ample enough for the mineralogical student, and the geologist who takes up the subject must necessarily study text-books of mineralogy, where they are given at much greater length. Prof. Green, however, has put them so clearly and succinctly that this portion of his book cannot fail to be of use.Geology.By A. H. Green. Part I. Physical Geology. Third and Enlarged Edition. (London: Rivingtons, 1882.)",1882,9,1098,67,0,0,0,0,0,0,0,0,0,0
d2c099523c52acf06c6ec30d917a8af1f59829db,"Sediment flux to the coastal zone is conditioned by geomorphic and tectonic influences (basin area and relief), geography (temperature, runoff), geology (lithology, ice cover), and human activities (reservoir trapping, soil erosion). A new model, termed “BQART” in recognition of those factors, accounts for these varied influences. When applied to a database of 488 rivers, the BQART model showed no ensemble over‐ or underprediction, had a bias of just 3% across six orders of magnitude in observational values, and accounted for 96% of the between‐river variation in the long‐term (±30 years) sediment load or yield of these rivers. The geographical range of the 488 rivers covers 63% of the global land surface and is highly representative of global geology, climate, and socioeconomic conditions. Based strictly on geological parameters (basin area, relief, lithology, ice erosion), 65% of the between‐river sediment load is explained. Climatic factors (precipitation and temperature) account for an additional 14% of the variability in global patterns in load. Anthropogenic factors account for an additional 16% of the between‐river loads, although with ever more dams being constructed or decommissioned and socioeconomic conditions and infrastructure in flux, this contribution is temporally variable. The glacial factor currently contributes only 1% of the signal represented by our globally distributed database, but it would be much more important during and just after major glaciations. The BQART model makes possible the quantification of the influencing factors (e.g., climate, basin area, ice cover) within individual basins, to better interpret the terrestrial signal in marine sedimentary records. The BQART model predicts the long‐term flux of sediment delivered by rivers; it does not predict the episodicity (e.g., typhoons, earthquakes) of this delivery.",2007,110,685,51,11,20,29,24,31,35,46,36,42,47
4fb004b0c5fe581602d89028d79ed87e51b96609,,1990,39,1139,106,0,1,6,13,22,33,20,25,26,25
5ea4661aa22572e2467ff3853449431923ec66d4,"I have deemed it advisable to preface this paper with a brief notice of the geographical position, extent, and more prominent physical features of Egypt, remarking at the same time that although much still remains to be done in this most interesting, and comparatively unexplored field for the geologist, yet it may not be wholly useless to lay before the Society a summary of what has already been effected.",1848,0,818,119,0,0,0,0,0,0,0,0,0,0
a2e480753323ae9df59cdbf5627fde339fe806ba,"In the Amazon Basin, substrate lithology and erosional regime (seen in terms of transport-limited and weathering-limited denudation) exert the most fundamental control on the chemistry of surface waters within a catchment. Secondary effects, such as the precipitation of salts within soils and in stream beds, biological uptake and release, and cyclic salt inputs, are more difficult to discern. Samples can be separated into four principal groupings based on relationships between total cation charge (TZ+) and geology. (1) Rivers with 0 3000 μeq/l drain massive evaporites. These rivers are rich in Na and Cl. In the third and fourth categories, rivers tend to have 1:1 (equivalent) ratios of Na:Cl and (Ca+Mg):(alkalinity+SO4), caused primarily by the weathering of carbonates and evaporites. Supplement available with entire article on microfiche. Order from the American Geophysical Union, 2000 Florida Avenue, N.W., Washington, DC 20009. Document C83-002; $2.50. Payment must accompany order.",1983,35,1125,77,0,2,4,4,9,13,9,14,5,12
6ce757bd7d3992f2ff3c9d90915b9ec47b19f0bb,"Does productivity increase with density? We revisit the issue usingFrench wage and TFP data. To deal with the ‘endogenous quantity of labour' bias (i.e., urban agglomeration is consequence of high local productivity rather than a cause), we take an instrumental variable approach and introduce a new set of geological instruments in addition to standard historical instruments. To dealwith the ‘endogenous quality of labour' bias (i.e., cities attract skilled workers so that the effects of skills and urban agglomeration are confounded), we take a worker fixed-effect approach with wage data. We find modest evidence about theendogenous quantity of labour bias and both sets of instruments give a similar answer. We find that the endogenous quality of labour bias is quantitatively more important.",2008,49,391,57,11,16,27,21,28,31,28,37,33,39
5a7b0f16bfb3acfeccca4c4f8c3aead1a9caf903,"The relationship between in-situ stress and fluid flow in fractured and faulted rock is examined by using data from detailed analyses of stress orientation and magnitude, fracture geometry, and precision temperature logs that indicate localized fluid flow. Data obtained from three boreholes that penetrate highly fractured and faulted crystalline rocks indicate that potentially active faults appear to be the most important hydraulic conduits in situ. The data indicate that the permeability of critically stressed faults is much higher than that of faults that are not optimally oriented for failure in the current stress field.",1995,2,876,37,0,3,11,4,13,14,13,23,29,28
579dbce2fe9decd8ad070af0e4baa209b2fc8039,"Terrestrial laser scanning, or lidar, is a recent innovation in spatial information data acquisition, which allows geological outcrops to be digitally captured with unprecedented resolution and accuracy. With point precisions and spacing of the order of a few centimetres, an enhanced quantitative element can now be added to geological fieldwork and analysis, opening up new lines of investigation at a variety of scales in all areas of field-based geology. Integration with metric imagery allows 3D photorealistic models to be created for interpretation, visualization and education. However, gaining meaningful results from lidar scans requires more than simply acquiring raw point data. Surveys require planning and, typically, a large amount of post-processing time. The contribution of this paper is to provide a more detailed insight into the technology, data collection and utilization techniques than is currently available. The paper focuses on the workflow for using lidar data, from the choice of field area and survey planning, to acquiring and processing data and, finally, extracting geologically useful data. Because manufacturer specifications for point precision are often optimistic when applied to real-world outcrops, the error sources associated with lidar data, and the implications of them propagating through the processing chain, are also discussed.",2008,35,373,19,5,12,26,20,30,36,28,43,45,24
901baa6ffd86f44fb55731242356c117fb6a801b,"One of the key works in the nineteenth-century battle between science and Scripture, Charles Lyell's Principles of Geology (1830-33) sought to explain the geological state of the modern Earth by considering the long-term effects of observable natural phenomena. Written with clarity and a dazzling intellectual passion, it is both a seminal work of modern geology and a compelling precursor to Darwinism, exploring the evidence for radical changes in climate and geography across the ages and speculating on the progressive development of life. A profound influence on Darwin, Principles of Geology also captured the imagination of contemporaries such as Melville, Emerson, Tennyson and George Eliot, transforming science with its depiction of the powerful forces that shape the natural world.",1969,1,695,0,1,7,0,1,3,4,2,9,4,8
e23873d41b9d09bf75b059d27c46943c9a662244,"THE twenty-third volume of the Memoirs of the Geological Survey of India, consisting of some 250 pages, is wholly taken up by an account of the geology of the Central Himalayas, by the Superintendent of the Survey, Mr. C. L. Griesbach, C.I.E. The carefully written text is illustrated by some of the most exquisite and instructive photographs of synclinals, folded beds, faults, glaciers, &c., which have ever been produced, to say nothing of the numerous maps and sections.",1892,0,256,35,0,0,0,0,0,0,0,0,0,0
ff1be9bad4448c3f6d7a1de7ae098d4f7d5b2b9c,"Abstract Amber (‘Burmite’) from the Hukawng Valley of Myanmar has been known since at least the 1st century AD. It is currently being produced from a hill known as Noije Bum, which was first documented as a source of amber in 1836. Several geologists visited the locality between 1892 and 1930. All of them believed that the host rocks to the amber are Tertiary (most said Eocene) in age, and this conclusion has been widely quoted in the literature. However, recent work indicates a Cretaceous age. Insect inclusions in amber are considered to be Turonian–Cenomanian, and a specimen of the ammonite Mortoniceras (of Middle-Upper Albian age) was discovered during the authors' visit. Palynomorphs in samples collected by the authors suggest that the amber-bearing horizon is Upper Albian to Lower Cenomanian. The preponderance of the evidence suggests that both rocks and amber are most probably Upper Albian. This determination is significant for the study of insect evolution, indicating that the oldest known definitive ants have been identified in this amber [American Museum Novitates 3361 (2002) 72]. This site occurs within the Hukawng Basin, which is comprised of folded sedimentary (±volcanic) rocks of Cretaceous and Cenozoic age. The mine exposes a variety of clastic sedimentary rocks, with thin limestone beds, and abundant carbonaceous material. The sediments were deposited in a nearshore marine environment, such as a bay or estuary. Amber is found in a fine clastic facies, principally as disk shaped clasts, oriented parallel to bedding. A minority occurs as runnels (stalactite shaped), with concentric layering caused by recurring flows of resin. An Upper Albian age is similar to that of Orbitolina limestones known from a number of locations in northern Myanmar. One of these, at Nam Sakhaw, 90 km SW of Noije Bum, has also been a source of amber.",2003,49,629,54,4,23,9,5,12,10,18,9,15,12
d8e5ce8d0e3a71d9677b5f6f38dc4695c9518715,"Magnetic anisotropy in sedimentary rocks is controlled by the processes of deposition and compaction, in volcanic rocks by the lava flow and in metamorphic and plutonic rocks by ductile deformation and mimetic crystallization. In massive ore it is due to processes associated with emplacement and consolidation of an ore body as well as to ductile deformation. Hence, it can be used as a tool of structural analysis for almost all rock types. Morcover, it can influence considerably the orientation of the remanent magnetization vector as well as the configuration of a magnetic anomaly over a magnetized body. For these reasons it should be investigated in palaeomagnetism and applied geophysics as well.",1982,68,995,56,0,7,5,9,6,15,20,11,15,20
f39abb87b261948e79888097fc2a1f92df1217ea,"THE second volume of Baron Ferdinand von Richthofen's great work on China has just appeared. Five years have elapsed since the publication of the first volume, and two additional volumes are promised to complete the work, which when its maps and full index have been supplied, will be a great storehouse of observations in almost every department of Geology. Few geologists have enjoyed such opportunities of extended travel as have fallen to the Baron's lot. Already familiar with the rocks of a large part of Central Europe, he carried his knowledge and experience to the far west of North America, and did admirable service there as a pioneer to those who have Jcome after him. Subsequently he set himself to explore the geological structure of the Chinese Empire, and he is now laboriously collecting and arranging the vast materials which he amassed in his wanderings through the almost unknown geological formations of that wide region.China: Ergebnisse eigener Reisen und darauf gegründeter Studien.Von F. Freiherrn von Richthofen. Zweiter Band. (Berlin: Reimer, 1882.)",1882,0,87,10,0,0,0,0,0,0,0,0,0,0
61830442b82ed21add6de4dfa768a94b7934c145,"IN commemoration of their jubilee, which took place on December 17, 1908, the council of the Geologists' Association decided to bring out a volume dealing with the geology of those parts of England and Wales which have been visited by the Association during the course of its excursions. The volume, which promises to attain a much larger size than was expected, is to be issued in four parts, the first of which is now before us. It is a well-printed work of 209 pages, with four plates and thirty-four text-illustrations; and it deals with the district north of the Thames from Oxfordshire to Bedfordshire and the eastern counties. It comprises seven articles, with the following titles: (1) Middlesex and Hertfordshire, by Mr. J. Hopkinson; (2) Essex, by Mr. T. V. Holmes; (3) The Pliocene Deposits of the Eastern Counties, by Mr. F. W. Harmer; (4) The Pleistocene Period in the Eastern Counties, by Mr. Harmer; (5) Cambridgeshire, Bed fordshire, and West Norfolk, by Mr. R. H. Rastall; (6) Buckinghamshire, by Dr. A. Morley Davies; and (7) The Oxford and Banbury District, by Mr. J. A. Douglas.Geology in the Field.The Jubilee Volume of the Geologists' Association (1858–1908). Edited by H. W. Monckton R. S. Herries. Part i. Pp. iv + 209. (London: Edward Stanford, 1909.) Price 5s. net.",1985,0,188,15,1,0,1,0,0,2,1,0,3,1
6e91ab3a6dd575032a603497b8b5933057faf018,"FOLLOWING the example of Mr. Chapman's Australian fossils—an outline of palaeonto logy based on Australian examples for Australian students—Mr. Howchin, of the University of Adelaide, has prepared a general text-book of geology based on Australian illustrations, followed by an account of the geology of South Australia, with shorter summaries of that of the other Australian States. The book should be very useful, as it fills a gap in Australian educational literature, while it supplies geologists in general with,an excellent and up-to-date compendium of the geo logy of South Australia. Mr. Howchin is exceptionally qualified for the work; he is well known for his discovery of the Australian Cambrian glacial deposits, his researches on fossil foraminifera, and his text-book, on the geography of South Australia. The first division of the work gives a clear summary of the general outlines of geology; it is especially good in the physiographic portions. The petrology. As comparatively ele mentary, since the book, being published by the South Australian Education Department, is probably intended more for secondary schools than for university students. Aus tralian petrologists may consider that there is inadequate notice of the alkaline igneous rocks and in an effort at simplification “pyroxene (augite)” is included in the hornblende group, a step which would lead students to overlook the important distinction between the pyroxenes and othe amphiboles. The parallelism of these series is also not indicated in the statement as to the com position of augite. There is not much informa tion about economic geology; for example, the author tells us nothing about the oil-fields of South Australia and their prospects. He follows those who extend the petrographiq use of the word “mineral” for mineral species into general geology, although mineralogists, such as Miers, adopt the more commonsense practice which does not refuse the term “mineral” to most economic minerals. The author, of course, cannot be consistent, for the term is not used in the latter part of the book in accordance with the restricted definition. In regard to the Australian artesian water, the author adduces evidence that the supply is dwindling from the reduction in size,of the mound springs; but those who hold that plutonic water is largely influential in the uplift,of the water in the wells do not consider, as is twice stated, that most of the water is plutonic in origin.The Geology of South Australia. (In two divisions.) Division 1, An Introduction to Geology, Physiographical and Structural, from the Australian Standpoint. Division 2, The Geology of South Australia, with Notes on the Chief Geological Systems and Occurrences in the other Australian States. By Walter Howchin. Pp. xvi + 543. (Adelaide: The Education Department, 1918.) Price 10s.",1919,0,68,9,0,0,0,0,0,0,0,0,0,0
1f13ef5107650a9e2b3c433a8b70285aefd661c6,"WHEN the writer of the obituary notice in NATURE of May 3 expressed regret that the late Prof. Grenville A. J. Cole was not spared to write a comprehensive work on the geology of Ireland, he seems to have overlooked the volume on the British Isles in the “Handbuch der Regionalen Geologie” Series, to which the late Prof. Cole contributed all the portions dealing with Ireland.",1924,0,112,4,0,0,0,0,0,0,0,0,0,0
cfb6c045d3292aba5221f76e2b2ddd1c3f814e13,"GENERAL GEOLOGY General Distribution, Succession, and Structure of Formations Probable Archean Basement Complex Post-Archean Erosion Period Probable Algonkian Caraca Quartzite Batatal Schist Itabira Iron Formation Piracicaba Formation Itacolumi Quartzite Summary Pre-Devonian Deformation Paleozoic-Early Mesozoic Erosion Period Mesozoic-Early Tertiary Deposition Diamantina Conglomerate Later erosion and deposition periods General Uplift Tertiary Canga Plains Conglomerate Formations Present Status of Erosion Present Topography",1915,0,73,3,0,0,0,0,0,0,0,0,0,0
699647b717986a9c5cac03e47a60d41eb0152d99,"THIS fifth edition of “Economic Geology,” like its predecessor the fourth, deals almost wholly with the economic geology of North America. Its excellent illustrations, numerous references, and good index, together with its balance of treatment as regards scope and descriptive detail, make it a useful and popular text-book so far as the deposits of economic minerals in North America are concerned. As regards deposits other than those of North America, however, it is much less satisfactory. Indeed, in this respect it needs both amplification and correction, and the author would do well in future editions either to make these amplifications or adopt a more appropriate title, as was done in the first three editions.Economic Geology.Prof.H.RiesBy. Fifth edition, revised. Pp. v + 843. (New York: John Wiley and Sons, Inc.; London: Chapman and Hall, Ltd., 1925.) 25s. net.",1926,0,52,3,0,0,0,0,0,0,0,0,0,0
b580ab4bf601610a344b123e0f91e39c3722b206,"The coastal sedimentary basin of Nigeria has been the scene of three depositional cycles. The first began with a marine incursion in the middle Cretaceous and was terminated by a mild folding phase in Santonian time. The second included the growth of a proto-Niger delta during the Late Cretaceous and ended in a major Paleocene marine transgression. The third cycle, from Eocene to Recent, marked the continuous growth of the main Niger delta. A new threefold lithostratigraphic subdivision is introduced for the Niger delta subsurface, comprising an upper sandy Benin Formation, an intervening unit of alternating sandstone and shale named the Agbada Formation, and a lower shaly Akata Formation. These three units extend across the whole delta and each ranges in age from early T rtiary to Recent. They are related to the present outcrops and environments of deposition. A separate member of the Benin Formation is recognized in the Port Harcourt area. This is the Afam Clay Member, which is interpreted to be an ancient valley fill formed in Miocene sediments. Subsurface structures are described as resulting from movement under the influence of gravity and their distribution is related to growth stages of the delta. Rollover anticlines in front of growth faults form the main objectives of oil exploration, the hydrocarbons being found in sandstone reservoirs of the Agbada Formation.",1967,7,1075,83,0,2,1,1,0,2,1,1,4,2
bd71265f1aa05b4689b82298a19086dbc3deec22,,2008,0,253,30,0,5,11,11,15,15,38,23,23,22
57ebc446914f21554e18e6818458768d23dd41e3,"The North America landscape north of about 40°N was extensively glaciated during the last Ice Age, which ended about 14,000 years ago. Thus, much of the landscapes that we see around us was created or modified by glacial erosion and/or glacial deposition. In addition, glacial deposits are a common material on which we build our houses, roads, and cities, and glacial outwash sand and gravel is a very common groundwater aquifer. The stratigraphic record of glacial deposits serves as a register of global climate change and a major branch of geology is focused on comparing records of glaciation in different parts of the world. Thus, a knowledge of glacial geology is of critical importance to geologists, civil engineers, groundwater hydrologists, and environmental scientists.",1971,0,1037,36,2,17,19,16,28,21,30,26,22,23
a05025c6601f9ef5a6a839a29f6b35b6903a920c,,2008,0,250,12,1,6,9,16,10,17,27,24,13,25
7bebbf907496045949f127dbcde4d9421a7172f4,"MAY I supplement Prof. Green's history of geological mapping in Scotland (NATURE, vol. xlvii. p. 49) by pointing out that Mr. Cruchley published, on March 23, 1840, “A Geological Map of Scotland by Dr. MacCulloch, F.R.S., &#38;c., published by order of the Lords of the Treasury by S. Arrowsmith, Hydrographer to the King.” This fine map is on the scale of four miles to an inch. From the omission of “the late” before MacCulloch's name, it seems possible that the plates were in course of engraving before his death in 1835.",1892,0,89,11,0,0,0,0,0,0,0,0,0,0
c79eac078d09808303c98cab0a5ac6393a8264a4,"Introduction to medical geology : , Introduction to medical geology : , کتابخانه دیجیتال جندی شاپور اهواز",2009,0,64,2,0,0,1,2,1,6,5,4,1,4
2cfd620128b62301624e36e7b4626072b151910d,"Williams textbook of endocrinology / , Williams textbook of endocrinology / , کتابخانه دیجیتال جندی شاپور اهواز",1985,0,3465,33,0,4,20,14,12,17,16,11,22,37
af06e7856d5b62fe8242d9e25b1cbbe63c77e449,"Clinical gynecologic endocrinology and infertility , Clinical gynecologic endocrinology and infertility , کتابخانه دیجیتال جندی شاپور اهواز",1983,0,2717,157,6,11,16,10,23,16,3,13,21,16
8c3a30dd7261f876c8dd211fde049a1e8dfbe8eb,"The stress response is subserved by the stress system, which is located both in the central nervous system and the periphery. The principal effectors of the stress system include corticotropin-releasing hormone (CRH); arginine vasopressin; the proopiomelanocortin-derived peptides alpha-melanocyte-stimulating hormone and beta-endorphin, the glucocorticoids; and the catecholamines norepinephrine and epinephrine. Appropriate responsiveness of the stress system to stressors is a crucial prerequisite for a sense of well-being, adequate performance of tasks, and positive social interactions. By contrast, inappropriate responsiveness of the stress system may impair growth and development and may account for a number of endocrine, metabolic, autoimmune, and psychiatric disorders. The development and severity of these conditions primarily depend on the genetic vulnerability of the individual, the exposure to adverse environmental factors, and the timing of the stressful events, given that prenatal life, infancy, childhood, and adolescence are critical periods characterized by increased vulnerability to stressors.",2005,111,1545,104,14,47,57,74,79,81,83,90,105,107
a1c6862fd3f82ffb6e4245eae37b8e139b87675d,"This report presents an algorithm to assist primary care physicians, endocrinologists, and others in the management of adult, nonpregnant patients with type 2 diabetes mellitus. In order to minimize the risk of diabetes-related complications, the goal of therapy is to achieve a hemoglobin A1c (A1C) of 6.5% or less, with recognition of the need for individualization to minimize the risks of hypoglycemia. We provide therapeutic pathways stratified on the basis of current levels of A1C, whether the patient is receiving treatment or is drug naïve. We consider monotherapy, dual therapy, and triple therapy, including 8 major classes of medications (biguanides, dipeptidyl-peptidase-4 inhibitors, incretin mimetics, thiazolidinediones, alpha-glucosidase inhibitors, sulfonylureas, meglitinides, and bile acid sequestrants) and insulin therapy (basal, premixed, and multiple daily injections), with or without orally administered medications. We prioritize choices of medications according to safety, risk of hypoglycemia, efficacy, simplicity, anticipated degree of patient adherence, and cost of medications. We recommend only combinations of medications approved by the US Food and Drug Administration that provide complementary mechanisms of action. It is essential to monitor therapy with A1C and self-monitoring of blood glucose and to adjust or advance therapy frequently (every 2 to 3 months) if the appropriate goal for each patient has not been achieved. We provide a flow-chart and table summarizing the major considerations. This algorithm represents a consensus of 14 highly experienced clinicians, clinical researchers, practitioners, and academicians and is based on the American Association of Clinical Endocrinologists/American College of Endocrinology Diabetes Guidelines and the recent medical literature.",2009,62,949,76,10,121,187,186,163,106,48,46,27,20
89f4a8ed61c1af384c895984ece7f07148747fdf,"Molecular Cloning has served as the foundation of technical expertise in labs worldwide for 30 years. No other manual has been so popular, or so influential. Molecular Cloning, Fourth Edition, by the celebrated founding author Joe Sambrook and new co-author, the distinguished HHMI investigator Michael Green, preserves the highly praised detail and clarity of previous editions and includes specific chapters and protocols commissioned for the book from expert practitioners at Yale, U Mass, Rockefeller University, Texas Tech, Cold Spring Harbor Laboratory, Washington University, and other leading institutions. The theoretical and historical underpinnings of techniques are prominent features of the presentation throughout, information that does much to help trouble-shoot experimental problems. For the fourth edition of this classic work, the content has been entirely recast to include nucleic-acid based methods selected as the most widely used and valuable in molecular and cellular biology laboratories. Core chapters from the third edition have been revised to feature current strategies and approaches to the preparation and cloning of nucleic acids, gene transfer, and expression analysis. They are augmented by 12 new chapters which show how DNA, RNA, and proteins should be prepared, evaluated, and manipulated, and how data generation and analysis can be handled. The new content includes methods for studying interactions between cellular components, such as microarrays, next-generation sequencing technologies, RNA interference, and epigenetic analysis using DNA methylation techniques and chromatin immunoprecipitation. To make sense of the wealth of data produced by these techniques, a bioinformatics chapter describes the use of analytical tools for comparing sequences of genes and proteins and identifying common expression patterns among sets of genes. Building on thirty years of trust, reliability, and authority, the fourth edition of Mol",2001,0,199651,12718,8,0,0,0,0,0,0,1,0,0
521027c14a7ef622a4f03106799706d0824e192a,"VMD is a molecular graphics program designed for the display and analysis of molecular assemblies, in particular biopolymers such as proteins and nucleic acids. VMD can simultaneously display any number of structures using a wide variety of rendering styles and coloring methods. Molecules are displayed as one or more ""representations,"" in which each representation embodies a particular rendering method and coloring scheme for a selected subset of atoms. The atoms displayed in each representation are chosen using an extensive atom selection syntax, which includes Boolean operators and regular expressions. VMD provides a complete graphical user interface for program control, as well as a text interface using the Tcl embeddable parser to allow for complex scripts with variable substitution, control loops, and function calls. Full session logging is supported, which produces a VMD command script for later playback. High-resolution raster images of displayed molecules may be produced by generating input scripts for use by a number of photorealistic image-rendering applications. VMD has also been expressly designed with the ability to animate molecular dynamics (MD) simulation trajectories, imported either from files or from a direct connection to a running MD simulation. VMD is the visualization component of MDScope, a set of tools for interactive problem solving in structural biology, which also includes the parallel MD program NAMD, and the MDCOMM software used to connect the visualization and simulation programs. VMD is written in C++, using an object-oriented design; the program, including source code and extensive documentation, is freely available via anonymous ftp and through the World Wide Web.",1996,15,34938,1410,0,0,0,0,0,0,0,0,0,0
7394dae1544c770625d8ead56df63433ca2be85b,"Molecular Genetics (Biology): An Overview | Sciencing Experiments in Molecular Genetics Experiments in molecular genetics (1972 edition) | Open ... Experimental Molecular Genetics | Biology | MIT OpenCourseWare DNA experiments you can perform at home | SBS Science Experiments in molecular genetics Jeffrey H. Miller ... DNA and Molecular Genetics Experiments in Molecular Biology: Biochemical Applications ... Molecular Genetics Biology Experiment Please help ... Molecular genetics | biology | Britannica Molecular Genetic Experiment : Biology Lab 1793 Words ... Miller, J.H. (1972) Experiments in Molecular Genetics ... Griffith's experiment Wikipedia DNA as genetic material: Revisiting classic experiments ... Experiments in molecular genetics (Book, 1972) [WorldCat.org] Measuring βGalactosidase Activity in Bacteria: Cell ... Classic Experiments in",1972,0,26378,2500,0,1,1,9,20,38,30,44,11,8
2d6f573c36c5e2153b65859fb080523fc4d842d0,"We announce the release of the fourth version of MEGA software, which expands on the existing facilities for editing DNA sequence data from autosequencers, mining Web-databases, performing automatic and manual sequence alignment, analyzing sequence alignments to estimate evolutionary distances, inferring phylogenetic trees, and testing evolutionary hypotheses. Version 4 includes a unique facility to generate captions, written in figure legend format, in order to provide natural language descriptions of the models and methods used in the analyses. This facility aims to promote a better understanding of the underlying assumptions used in analyses, and of the results generated. Another new feature is the Maximum Composite Likelihood (MCL) method for estimating evolutionary distances between all pairs of sequences simultaneously, with and without incorporating rate variation among sites and substitution pattern heterogeneities among lineages. This MCL method also can be used to estimate transition/transversion bias and nucleotide substitution pattern without knowledge of the phylogenetic tree. This new version is a native 32-bit Windows application with multi-threading and multi-user supports, and it is also available to run in a Linux desktop environment (via the Wine compatibility layer) and on Intel-based Macintosh computers under the Parallels program. The current version of MEGA is available free of charge at (http://www.megasoftware.net).",2007,10,28416,6041,0,0,0,0,1,18,2040,2152,1667,1255
6dea759b9e6d08b1e8ae7c3ac135234008e26aec,"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as 'Coot'.",2004,21,24373,1589,0,0,0,0,0,0,1,0,0,0
80935b370bac09ce615a002caabc30fbb26f029b,"With its theoretical basis firmly established in molecular evolutionary and population genetics, the comparative DNA and protein sequence analysis plays a central role in reconstructing the evolutionary histories of species and multigene families, estimating rates of molecular evolution, and inferring the nature and extent of selective forces shaping the evolution of genes and genomes. The scope of these investigations has now expanded greatly owing to the development of high-throughput sequencing techniques and novel statistical and computational methods. These methods require easy-to-use computer programs. One such effort has been to produce Molecular Evolutionary Genetics Analysis (MEGA) software, with its focus on facilitating the exploration and analysis of the DNA and protein sequence variation from an evolutionary perspective. Currently in its third major release, MEGA3 contains facilities for automatic and manual sequence alignment, web-based mining of databases, inference of the phylogenetic trees, estimation of evolutionary distances and testing evolutionary hypotheses. This paper provides an overview of the statistical methods, computational tools, and visual exploration modules for data input and the results obtainable in MEGA.",2004,73,12037,3302,1,0,26,1588,2150,1625,1243,879,678,475
7a253c839a797588065b4f843adffd4a7fa29b5f,"Abstract Three parallel algorithms for classical molecular dynamics are presented. The first assigns each processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute; the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models which can be difficult to parallelize efficiently—those with short-range forces where the neighbors of each atom change rapidly. They can be implemented on any distributed-memory parallel machine which allows for message-passing of data between independently executing processors. The algorithms are tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000 atoms on several parallel supercomputers--the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D. Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that the current generation of parallel machines is competitive with conventional vector supercomputers even for small problems. For large problems, the spatial algorithm achieves parallel efficiencies of 90% and a 1840-node Intel Paragon performs up to 165 faster than a single Cray C9O processor. Trade-offs between the three algorithms and guidelines for adapting them to more complex molecular dynamics simulations are also discussed.",1993,80,27072,688,0,0,0,0,0,0,0,0,0,0
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17519,724,0,0,0,0,0,0,0,1,1,2
c41c73c346652c2780d56372e2a332a480a2b332,"In molecular dynamics (MD) simulations the need often arises to maintain such parameters as temperature or pressure rather than energy and volume, or to impose gradients for studying transport properties in nonequilibrium MD. A method is described to realize coupling to an external bath with constant temperature or pressure with adjustable time constants for the coupling. The method is easily extendable to other variables and to gradients, and can be applied also to polyatomic molecules involving internal constraints. The influence of coupling time constants on dynamical variables is evaluated. A leap‐frog algorithm is presented for the general case involving constraints with coupling to both a constant temperature and a constant pressure bath.",1984,57,20664,573,0,0,0,0,0,0,0,0,0,0
06d49b0c74ebacde13bacabe9714b4825ea11ef4,"Recent developments of statistical methods in molecular phylogenetics are reviewed. It is shown that the mathematical foundations of these methods are not well established, but computer simulations and empirical data indicate that currently used methods such as neighbor joining, minimum evolution, likelihood, and parsimony methods produce reasonably good phylogenetic trees when a sufficiently large number of nucleotides or amino acids are used. However, when the rate of evolution varies exlensively from branch to branch, many methods may fail to recover the true topology. Solid statistical tests for examining'the accuracy of trees obtained by neighborjoining, minimum evolution, and least-squares method are available, but the methods for likelihood and parsimony trees are yet to be refined. Parsimony, likelihood, and distance methods can all be used for inferring amino acid sequences of the proteins of ancestral organisms that have become extinct.",1987,7,15493,3111,0,3,3,2,6,1,5,1,1,11
ee4a25c3b307792c695a77f4166191fabadc5d55,"We present here a framework for the study of molecular variation within a single species. Information on DNA haplotype divergence is incorporated into an analysis of variance format, derived from a matrix of squared-distances among all pairs of haplotypes. This analysis of molecular variance (AMOVA) produces estimates of variance components and F-statistic analogs, designated here as phi-statistics, reflecting the correlation of haplotypic diversity at different levels of hierarchical subdivision. The method is flexible enough to accommodate several alternative input matrices, corresponding to different types of molecular data, as well as different types of evolutionary assumptions, without modifying the basic structure of the analysis. The significance of the variance components and phi-statistics is tested using a permutational approach, eliminating the normality assumption that is conventional for analysis of variance but inappropriate for molecular data. Application of AMOVA to human mitochondrial DNA haplotype data shows that population subdivisions are better resolved when some measure of molecular differences among haplotypes is introduced into the analysis. At the intraspecific level, however, the additional information provided by knowing the exact phylogenetic relations among haplotypes or by a nonlinear translation of restriction-site change into nucleotide diversity does not significantly modify the inferred population genetic structure. Monte Carlo studies show that site sampling does not fundamentally affect the significance of the molecular variance components. The AMOVA treatment is easily extended in several different directions and it constitutes a coherent and flexible framework for the statistical analysis of molecular data.",1992,46,13113,3049,0,0,0,0,0,0,0,2,2,0
3c0c8388ec15c99dcfd3d696693cf1f88a1f3165,"Molecular simulation is an extremely useful, but computationally very expensive tool for studies of chemical and biomolecular systems. Here, we present a new implementation of our molecular simulation toolkit GROMACS which now both achieves extremely high performance on single processors from algorithmic optimizations and hand-coded routines and simultaneously scales very well on parallel machines. The code encompasses a minimal-communication domain decomposition algorithm, full dynamic load balancing, a state-of-the-art parallel constraint solver, and efficient virtual site algorithms that allow removal of hydrogen atom degrees of freedom to enable integration time steps up to 5 fs for atomistic simulations also in parallel. To improve the scaling properties of the common particle mesh Ewald electrostatics algorithms, we have in addition used a Multiple-Program, Multiple-Data approach, with separate node domains responsible for direct and reciprocal space interactions. Not only does this combination of algorithms enable extremely long simulations of large systems but also it provides that simulation performance on quite modest numbers of standard cluster nodes.",2008,0,12133,620,0,0,0,1,15,1006,1351,1298,1347,1251
04c69e602946dc56b985ad51aeeae403fbf04802,"NAMD is a parallel molecular dynamics code designed for high‐performance simulation of large biomolecular systems. NAMD scales to hundreds of processors on high‐end parallel platforms, as well as tens of processors on low‐cost commodity clusters, and also runs on individual desktop and laptop computers. NAMD works with AMBER and CHARMM potential functions, parameters, and file formats. This article, directed to novices as well as experts, first introduces concepts and methods used in the NAMD program, describing the classical molecular dynamics force field, equations of motion, and integration methods along with the efficient electrostatics evaluation algorithms employed and temperature and pressure controls used. Features for steering the simulation across barriers and for calculating both alchemical and conformational free energy differences are presented. The motivations for and a roadmap to the internal design of NAMD, implemented in C++ and based on Charm++ parallel objects, are outlined. The factors affecting the serial and parallel performance of a simulation are discussed. Finally, typical NAMD use is illustrated with representative applications to a small, a medium, and a large biomolecular system, highlighting particular features of NAMD, for example, the Tcl scripting language. The article also provides a list of the key features of NAMD and discusses the benefits of combining NAMD with the molecular graphics/sequence analysis software VMD and the grid computing/collaboratory software BioCoRE. NAMD is distributed free of charge with source code at www.ks.uiuc.edu. © 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1781–1802, 2005",2005,390,13572,738,0,1,0,0,0,0,1,5,513,1078
6e436b11eab90258229643cd47861f430f00080c,"Human breast tumours are diverse in their natural history and in their responsiveness to treatments. Variation in transcriptional programs accounts for much of the biological diversity of human cells and tumours. In each cell, signal transduction and regulatory systems transduce information from the cell's identity to its environmental status, thereby controlling the level of expression of every gene in the genome. Here we have characterized variation in gene expression patterns in a set of 65 surgical specimens of human breast tumours from 42 different individuals, using complementary DNA microarrays representing 8,102 human genes. These patterns provided a distinctive molecular portrait of each tumour. Twenty of the tumours were sampled twice, before and after a 16-week course of doxorubicin chemotherapy, and two tumours were paired with a lymph node metastasis from the same patient. Gene expression patterns in two tumour samples from the same individual were almost always more similar to each other than either was to any other sample. Sets of co-expressed genes were identified for which variation in messenger RNA levels could be related to specific features of physiological variation. The tumours could be classified into subtypes distinguished by pervasive differences in their gene expression patterns.",2000,43,13832,523,0,0,0,0,0,0,0,0,2,4
40c5441aad96b366996e6af163ca9473a19bb9ad,"The identification of maximally homologous subsequences among sets of long sequences is an important problem in molecular sequence analysis. The problem is straightforward only if one restricts consideration to contiguous subsequences (segments) containing no internal deletions or insertions. The more general problem has its solution in an extension of sequence metrics (Sellers 1974; Waterman et al., 1976) developed to measure the minimum number of “events” required to convert one sequence into another. These developments in the modern sequence analysis began with the heuristic homology algorithm of Needleman & Wunsch (1970) which first introduced an iterative matrix method of calculation. Numerous other heuristic algorithms have been suggested including those of Fitch (1966) and Dayhoff (1969). More mathematically rigorous algorithms were suggested by Sankoff (1972), Reichert et al. (1973) and Beyer et al. (1979) but these were generally not biologically satisfying or interpretable. Success came with Sellers (1974) development of a true metric measure of the distance between sequences. This metric was later generalized by Waterman et al. (1976) to include deletions/insertions of arbitrary length. This metric represents the minimum number of “mutational events” required to convert one sequence into another. It is of interest to note that Smith et al. (1980) have recently shown that under some conditions the generalized Sellers metric is equivalent to the original homology algorithm of Needleman & Wunsch (1970). In this letter we extend the above ideas to find a pair of segments, one from each of two long sequences, such that there is no other pair of segments with greater similarity (homology). The similarity measure used here allows for arbitrary length deletions and insertions.",1981,33,10026,656,3,2,6,12,9,7,11,14,22,25
c99628ea7b5fe2eb9dfbd6524af4c4b0dcfd9821,"New protein parameters are reported for the all-atom empirical energy function in the CHARMM program. The parameter evaluation was based on a self-consistent approach designed to achieve a balance between the internal (bonding) and interaction (nonbonding) terms of the force field and among the solvent-solvent, solvent-solute, and solute-solute interactions. Optimization of the internal parameters used experimental gas-phase geometries, vibrational spectra, and torsional energy surfaces supplemented with ab initio results. The peptide backbone bonding parameters were optimized with respect to data for N-methylacetamide and the alanine dipeptide. The interaction parameters, particularly the atomic charges, were determined by fitting ab initio interaction energies and geometries of complexes between water and model compounds that represented the backbone and the various side chains. In addition, dipole moments, experimental heats and free energies of vaporization, solvation and sublimation, molecular volumes, and crystal pressures and structures were used in the optimization. The resulting protein parameters were tested by applying them to noncyclic tripeptide crystals, cyclic peptide crystals, and the proteins crambin, bovine pancreatic trypsin inhibitor, and carbonmonoxy myoglobin in vacuo and in crystals. A detailed analysis of the relationship between the alanine dipeptide potential energy surface and calculated protein φ, χ angles was made and used in optimizing the peptide group torsional parameters. The results demonstrate that use of ab initio structural and energetic data by themselves are not sufficient to obtain an adequate backbone representation for peptides and proteins in solution and in crystals. Extensive comparisons between molecular dynamics simulations and experimental data for polypeptides and proteins were performed for both structural and dynamic properties. Energy minimization and dynamics simulations for crystals demonstrate that the latter are needed to obtain meaningful comparisons with experimental crystal structures. The presented parameters, in combination with the previously published CHARMM all-atom parameters for nucleic acids and lipids, provide a consistent set for condensed-phase simulations of a wide variety of molecules of biological interest.",1998,137,11258,290,0,0,0,0,0,0,0,5,190,404
41c6f229c4b149d502f89ffc386febb24116bd25,"Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.",1999,69,11717,931,1,0,1,10,102,737,793,790,786,675
c74bb1379abeec1f42b3cad633b95d19cfea69f4,"Molecular theory of gases and liquids , Molecular theory of gases and liquids , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1954,0,12199,294,0,0,0,0,0,0,0,0,0,0
725a3cbac5e12ee0dcd1dbd036929dc6d7783b58,"UNLABELLED
We have developed a new software package, Molecular Evolutionary Genetics Analysis version 2 (MEGA2), for exploring and analyzing aligned DNA or protein sequences from an evolutionary perspective. MEGA2 vastly extends the capabilities of MEGA version 1 by: (1) facilitating analyses of large datasets; (2) enabling creation and analyses of groups of sequences; (3) enabling specification of domains and genes; (4) expanding the repertoire of statistical methods for molecular evolutionary studies; and (5) adding new modules for visual representation of input data and output results on the Microsoft Windows platform.


AVAILABILITY
http://www.megasoftware.net.


CONTACT
s.kumar@asu.edu",2001,4,6259,1618,14,239,626,1021,1149,916,608,439,300,192
af08423e6305212023e2efa771c1858c0ef3ee93,A numerical algorithm integrating the 3N Cartesian equations of motion of a system of N points subject to holonomic constraints is formulated. The relations of constraint remain perfectly fulfilled at each step of the trajectory despite the approximate character of numerical integration. The method is applied to a molecular dynamics simulation of a liquid of 64 n-butane molecules and compared to a simulation using generalized coordinates. The method should be useful for molecular dynamics calculations on large molecules with internal degrees of freedom.,1977,14,14971,249,0,0,0,0,0,0,0,0,0,0
5b4fd2ede919f4ae0d101bb8fdaa8083632b5676,"A method is presented for the rapid isolation of high molecular weight plant DNA (50,000 base pairs or more in length) which is free of contaminants which interfere with complete digestion by restriction endonucleases. The procedure yields total cellular DNA (i.e. nuclear, chloroplast, and mitochondrial DNA). The technique is ideal for the rapid isolation of small amounts of DNA from many different species and is also useful for large scale isolations.",1980,8,9511,303,0,0,6,6,14,23,29,34,28,46
30eecc8a7b7346a5e0c3a6648b0e156faad3a786,"cellular viewpoint. That is all very well. In any case academics will swallow almost anything if it's their job. Even if the botanists of the 18th century, the well-intentioned of the 19th century and the latterday enthusiasts of this century believed in the cell, is it an approach that is truly relevant? Such must surely be the attitude of today's sceptical medical students, and of many of their now middle-aged predecessors. Well, let a battle-scarred predecessor speak. Even a conventional diet of comparative and classical morphology, combined with exposure to prefashionable Eltonian ecology, and a native distrust of big-business 'nouvelle vague' biology has not blinkered your reviewer to the virtues of Alberts' Molecular Biology of the Cell. This is undoubtedly a landmark for the student and his teacher in the field of preclinical medicine. Many American undergraduate texts in biochemistry and molecular genetics clearly outsell competing English publications. Only English textbooks in immunology remain popular, and effective sellers in this field. The successful texts in these areas eschew traditional academic contrapuntal argument. The new bestselling formula involves clarity of presentation, the effective use of multi-coloured diagrams and the detailed exposition of basic processes. Such didactic approaches suit undergraduates today, and certainly ease the hardship for their seniors needing a refresher",1983,0,8546,650,0,6,11,7,18,12,11,16,26,19
c697da0fd1e3c8be93aa2463a20085d767e7be67,,1977,0,9791,1023,0,1,0,1,4,2,19,86,101,128
b65fd09f71be715ab78bff5ac030fdde51225675,"We present ab initio quantum-mechanical molecular-dynamics calculations based on the calculation of the electronic ground state and of the Hellmann-Feynman forces in the local-density approximation at each molecular-dynamics step. This is possible using conjugate-gradient techniques for energy minimization, and predicting the wave functions for new ionic positions using subspace alignment. This approach avoids the instabilities inherent in quantum-mechanical molecular-dynamics calculations for metals based on the use of a fictitious Newtonian dynamics for the electronic degrees of freedom. This method gives perfect control of the adiabaticity and allows us to perform simulations over several picoseconds.",1993,0,22530,135,0,0,0,0,0,0,0,0,0,0
450ccba0a2951a7011b091d1b8bb6bb8be63505d,"Abstract Forty proteins with polypeptide chains of well characterized molecular weights have been studied by polyacrylamide gel electrophoresis in the presence of sodium dodecyl sulfate following the procedure of Shapiro, Vinuela, and Maizel (Biochem. Biophys. Res. Commun., 28, 815 (1967)). When the electrophoretic mobilities were plotted against the logarithm of the known polypeptide chain molecular weights, a smooth curve was obtained. The results show that the method can be used with great confidence to determine the molecular weights of polypeptide chains for a wide variety of proteins.",1969,2,16421,166,0,0,1,6,15,20,39,37,61,41
897172c2bff121370e4454cccb685a51dd2a2fba,"A new Lagrangian formulation is introduced. It can be used to make molecular dynamics (MD) calculations on systems under the most general, externally applied, conditions of stress. In this formulation the MD cell shape and size can change according to dynamical equations given by this Lagrangian. This new MD technique is well suited to the study of structural transformations in solids under external stress and at finite temperature. As an example of the use of this technique we show how a single crystal of Ni behaves under uniform uniaxial compressive and tensile loads. This work confirms some of the results of static (i.e., zero temperature) calculations reported in the literature. We also show that some results regarding the stress‐strain relation obtained by static calculations are invalid at finite temperature. We find that, under compressive loading, our model of Ni shows a bifurcation in its stress‐strain relation; this bifurcation provides a link in configuration space between cubic and hexagonal c...",1981,11,10305,222,0,0,0,0,0,0,0,0,0,0
27e5725be7e538f2329011e44481b68f4315f39e,"SummaryA new statistical method for estimating divergence dates of species from DNA sequence data by a molecular clock approach is developed. This method takes into account effectively the information contained in a set of DNA sequence data. The molecular clock of mitochondrial DNA (mtDNA) was calibrated by setting the date of divergence between primates and ungulates at the Cretaceous-Tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. A generalized leastsquares method was applied in fitting a model to mtDNA sequence data, and the clock gave dates of 92.3±11.7, 13.3±1.5, 10.9±1.2, 3.7±0.6, and 2.7±0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. Although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the bipedal creatureAustralopithecus afarensis, which lived some 3.7 million years ago at Laetoli in Tanzania and at Hadar in Ethiopia, was ancestral to man and evolved after the human-ape splitting. Another likelier possibility is that mtDNA was transferred through hybridization between a proto-human and a protochimpanzee after the former had developed bipedalism.",2005,119,6606,894,366,345,363,335,363,351,334,326,361,340
73d7d1adf83ee16e12e6b442ec15357da218c06d,"Diabetes-specific microvascular disease is a leading cause of blindness, renal failure and nerve damage, and diabetes-accelerated atherosclerosis leads to increased risk of myocardial infarction, stroke and limb amputation. Four main molecular mechanisms have been implicated in glucose-mediated vascular damage. All seem to reflect a single hyperglycaemia-induced process of overproduction of superoxide by the mitochondrial electron-transport chain. This integrating paradigm provides a new conceptual framework for future research and drug discovery.",2001,183,7809,418,3,39,172,208,285,315,374,423,421,445
86afe671ad3143796b57512859f71d594a44656b,"Second and revised edition 
 
Understanding Molecular Simulation: From Algorithms to Applications explains the physics behind the ""recipes"" of molecular simulation for materials science. Computer simulators are continuously confronted with questions concerning the choice of a particular technique for a given application. A wide variety of tools exist, so the choice of technique requires a good understanding of the basic principles. More importantly, such understanding may greatly improve the efficiency of a simulation program. The implementation of simulation methods is illustrated in pseudocodes and their practical use in the case studies used in the text. 
 
Since the first edition only five years ago, the simulation world has changed significantly -- current techniques have matured and new ones have appeared. This new edition deals with these new developments; in particular, there are sections on: 
 
· Transition path sampling and diffusive barrier crossing to simulaterare events 
· Dissipative particle dynamic as a course-grained simulation technique 
· Novel schemes to compute the long-ranged forces 
· Hamiltonian and non-Hamiltonian dynamics in the context constant-temperature and constant-pressure molecular dynamics simulations 
· Multiple-time step algorithms as an alternative for constraints 
· Defects in solids 
· The pruned-enriched Rosenbluth sampling, recoil-growth, and concerted rotations for complex molecules 
· Parallel tempering for glassy Hamiltonians 
 
Examples are included that highlight current applications and the codes of case studies are available on the World Wide Web. Several new examples have been added since the first edition to illustrate recent applications. Questions are included in this new edition. No prior knowledge of computer simulation is assumed.",1996,0,7330,435,1,10,24,37,42,81,146,182,187,232
ddf06cf0d375fb9404fe30c5f1d7858d74080e9c,The European Molecular Biology Open Software Suite (EMBOSS) is a mature package of software tools developed for the molecular biology community. It includes a comprehensive set of applications for molecular sequence analysis and other tasks and integrates popular third-party software packages under a consistent interface. EMBOSS includes extensive C programming libraries and is a platform to develop and release software in the true open source spirit.,2000,4,7763,588,3,10,22,85,127,216,255,308,348,346
82e12e48e4f1522dcee5c8c0334a7bae7d418c32,1. Molecular basis of evolution 2. Evolutionary changes of amino acid sequences 3. Evolutionary changes of DNA sequences 4. Synonymous and nonsynonymous nucleotide substitutions 5. Phylogenetic trees 6. Phylogenetic inference: Distance methods 7. Phylogenetic inference: Maximum parsimony methods 8. Phylogenetic inference: Maximum likelihood methods 9. Accuracies and statistical tests of phylogenetic trees 10. Molecular clocks and linearized trees 11. Ancestral nucleotide and amino acid sequences 12. Genetic polymorphism and evolution 13. Population trees from genetic markers 14. Perspectives Appendices A. Mathematical sumbols and notations B. Geological timescale C. Geological events in the Cenozoic and Meszoic eras D. Evolution of organisms based on the fossil record,2000,0,4081,389,3,38,77,112,128,156,157,179,175,182
5487f2e27071d24f9b545a3db39bce8bdc92f932,"The direct simulation Monte Carlo (or DSMC) method has, in recent years, become widely used in engineering and scientific studies of gas flows that involve low densities or very small physical dimensions. This method is a direct physical simulation of the motion of representative molecules, rather than a numerical solution of the equations that provide a mathematical model of the flow. These computations are no longer expensive and the period since the 1976 publication of the original Molecular Gas Dynamics has seen enormous improvements in the molecular models, the procedures, and the implementation strategies for the DSMC method. The molecular theory of gas flows is developed from first principles and is extended to cover the new models and procedures. Note: The disk that originally came with this book is no longer available. However, the same information is available from the author's website (http://gab.com.au/)",1994,0,4186,378,7,26,65,64,73,61,96,123,140,148
701d3e38292ccb8ef9441ab29a4ba66750757c7c,"The potassium channel from Streptomyces lividans is an integral membrane protein with sequence similarity to all known K+ channels, particularly in the pore region. X-ray analysis with data to 3.2 angstroms reveals that four identical subunits create an inverted teepee, or cone, cradling the selectivity filter of the pore in its outer end. The narrow selectivity filter is only 12 angstroms long, whereas the remainder of the pore is wider and lined with hydrophobic amino acids. A large water-filled cavity and helix dipoles are positioned so as to overcome electrostatic destabilization of an ion in the pore at the center of the bilayer. Main chain carbonyl oxygen atoms from the K+ channel signature sequence line the selectivity filter, which is held open by structural constraints to coordinate K+ ions but not smaller Na+ ions. The selectivity filter contains two K+ ions about 7.5 angstroms apart. This configuration promotes ion conduction by exploiting electrostatic repulsive forces to overcome attractive forces between K+ ions and the selectivity filter. The architecture of the pore establishes the physical principles underlying selective K+ conduction.",1998,41,6007,466,110,344,295,339,311,337,351,322,284,298
c2d5c55c485adaaf14654bee7b28fe6dd33fe155,,1989,0,6745,536,120,123,153,145,140,145,212,158,300,367
9eec798bd972a3d3968f5a9b3c915fddba4058a6,"A new molecular-replacement package is presented. It is an improvement on conventional methods, based on more powerful algorithms and a new conception that enables automation and rapid solution.",1994,12,4655,324,7,59,97,172,264,383,391,415,388,399
625ddfe164e3a25405672ecc1ca17d05353ff182,"Abstract A parallel message-passing implementation of a molecular dynamics (MD) program that is useful for bio(macro)molecules in aqueous environment is described. The software has been developed for a custom-designed 32-processor ring GROMACS (GROningen MAchine for Chemical Simulation) with communication to and from left and right neighbours, but can run on any parallel system onto which a a ring of processors can be mapped and which supports PVM-like block send and receive calls. The GROMACS software consists of a preprocessor, a parallel MD and energy minimization program that can use an arbitrary number of processors (including one), an optional monitor, and several analysis tools. The programs are written in ANSI C and available by ftp (information: gromacs@chem.rug.nl). The functionality is based on the GROMOS (GROningen MOlecular Simulation) package (van Gunsteren and Berendsen, 1987; BIOMOS B.V., Nijenborgh 4, 9747 AG Groningen). Conversion programs between GROMOS and GROMACS formats are included. The MD program can handle rectangular periodic boundary conditions with temperature and pressure scaling. The interactions that can be handled without modification are variable non-bonded pair interactions with Coulomb and Lennard-Jones or Buckingham potentials, using a twin-range cut-off based on charge groups, and fixed bonded interactions of either harmonic or constraint type for bonds and bond angles and either periodic or cosine power series interactions for dihedral angles. Special forces can be added to groups of particles (for non-equilibrium dynamics or for position restraining) or between particles (for distance restraints). The parallelism is based on particle decomposition. Interprocessor communication is largely limited to position and force distribution over the ring once per time step.",1995,38,6315,274,0,6,4,10,13,14,18,52,53,108
f6380f7f090f2cae9fe723dae1873adf57126400,"Abstract. GROMACS 3.0 is the latest release of a versatile and very well optimized package for molecular simulation. Much effort has been devoted to achieving extremely high performance on both workstations and parallel computers. The design includes an extraction of virial and periodic boundary conditions from the loops over pairwise interactions, and special software routines to enable rapid calculation of x–1/2. Inner loops are generated automatically in C or Fortran at compile time, with optimizations adapted to each architecture. Assembly loops using SSE and 3DNow! Multimedia instructions are provided for x86 processors, resulting in exceptional performance on inexpensive PC workstations. The interface is simple and easy to use (no scripting language), based on standard command line arguments with self-explanatory functionality and integrated documentation. All binary files are independent of hardware endian and can be read by versions of GROMACS compiled using different floating-point precision. A large collection of flexible tools for trajectory analysis is included, with output in the form of finished Xmgr/Grace graphs. A basic trajectory viewer is included, and several external visualization tools can read the GROMACS trajectory format. Starting with version 3.0, GROMACS is available under the GNU General Public License from http://www.gromacs.org.",2001,94,5348,244,1,33,68,144,243,294,338,381,387,346
e7148e7327e8445c736510c3681a999c6cf8203b,"BackgroundRecent advances in proteomics technologies such as two-hybrid, phage display and mass spectrometry have enabled us to create a detailed map of biomolecular interaction networks. Initial mapping efforts have already produced a wealth of data. As the size of the interaction set increases, databases and computational methods will be required to store, visualize and analyze the information in order to effectively aid in knowledge discovery.ResultsThis paper describes a novel graph theoretic clustering algorithm, ""Molecular Complex Detection"" (MCODE), that detects densely connected regions in large protein-protein interaction networks that may represent molecular complexes. The method is based on vertex weighting by local neighborhood density and outward traversal from a locally dense seed protein to isolate the dense regions according to given parameters. The algorithm has the advantage over other graph clustering methods of having a directed mode that allows fine-tuning of clusters of interest without considering the rest of the network and allows examination of cluster interconnectivity, which is relevant for protein networks. Protein interaction and complex information from the yeast Saccharomyces cerevisiae was used for evaluation.ConclusionDense regions of protein interaction networks can be found, based solely on connectivity data, many of which correspond to known protein complexes. The algorithm is not affected by a known high rate of false positives in data from high-throughput interaction techniques. The program is available from ftp://ftp.mshri.on.ca/pub/BIND/Tools/MCODE.",2003,61,3614,420,12,32,42,57,69,98,130,137,133,190
b34f0be6c739fbbf26714f62138559a11531fcd4,"In the past, basis sets for use in correlated molecular calculations have largely been taken from single configuration calculations. Recently, Almlof, Taylor, and co‐workers have found that basis sets of natural orbitals derived from correlated atomic calculations (ANOs) provide an excellent description of molecular correlation effects. We report here a careful study of correlation effects in the oxygen atom, establishing that compact sets of primitive Gaussian functions effectively and efficiently describe correlation effects i f the exponents of the functions are optimized in atomic correlated calculations, although the primitive (s p) functions for describing correlation effects can be taken from atomic Hartree–Fock calculations i f the appropriate primitive set is used. Test calculations on oxygen‐containing molecules indicate that these primitive basis sets describe molecular correlation effects as well as the ANO sets of Almlof and Taylor. Guided by the calculations on oxygen, basis sets for use in correlated atomic and molecular calculations were developed for all of the first row atoms from boron through neon and for hydrogen. As in the oxygen atom calculations, it was found that the incremental energy lowerings due to the addition of correlating functions fall into distinct groups. This leads to the concept of c o r r e l a t i o n c o n s i s t e n t b a s i s s e t s, i.e., sets which include all functions in a given group as well as all functions in any higher groups. Correlation consistent sets are given for all of the atoms considered. The most accurate sets determined in this way, [5s4p3d2f1g], consistently yield 99% of the correlation energy obtained with the corresponding ANO sets, even though the latter contains 50% more primitive functions and twice as many primitive polarization functions. It is estimated that this set yields 94%–97% of the total (HF+1+2) correlation energy for the atoms neon through boron.",1989,27,21215,51,0,0,0,0,0,0,0,0,0,0
0a21cd88817494b6101219c21b829c7d8aa1b541,"Three recently proposed constant temperature molecular dynamics methods by: (i) Nose (Mol. Phys., to be published); (ii) Hoover et al. [Phys. Rev. Lett. 48, 1818 (1982)], and Evans and Morriss [Chem. Phys. 77, 63 (1983)]; and (iii) Haile and Gupta [J. Chem. Phys. 79, 3067 (1983)] are examined analytically via calculating the equilibrium distribution functions and comparing them with that of the canonical ensemble. Except for effects due to momentum and angular momentum conservation, method (1) yields the rigorous canonical distribution in both momentum and coordinate space. Method (2) can be made rigorous in coordinate space, and can be derived from method (1) by imposing a specific constraint. Method (3) is not rigorous and gives a deviation of order N−1/2 from the canonical distribution (N the number of particles). The results for the constant temperature–constant pressure ensemble are similar to the canonical ensemble case.",1984,23,10639,145,0,0,0,0,0,0,0,0,0,0
aa274f1ed2996acababecde388db94a1b20604fa,"MOLREP is an automated program for molecular replacement which utilizes effective new approaches in data processing and rotational and translational searching. These include an automatic choice of all parameters, scaling by Patterson origin peaks and sott resolution cutoff. One of the cornerstones of the program is an original full-symmetry translation function combined with a packing function. Information from the model already placed in the cell is incorporated in both translation and packing functions. A number of tests using experimental data proved the ability of the program to find the correct solution in difficult cases.",1997,15,4365,262,0,1,4,8,17,33,67,166,200,279
2a193b9417a4aaf35bcad9152cf35f78dc5906a9,"The tools of molecular biology were used to solve an instance of the directed Hamiltonian path problem. A small graph was encoded in molecules of DNA, and the ""operations"" of the computation were performed with standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computations at the molecular level.",1994,37,4010,272,3,36,52,77,69,95,92,105,120,133
16fb5f02dac7f4ebcd8430b53df6e7e6ef6dab61,"THE major active ingredient of marijuana, Δ9-tetrahydrocannabi-nol (Δ9-THC), has been used as a psychoactive agent for thousands of years. Marijuana, and Δ9-THC, also exert a wide range of other effects including analgesia, anti-inflammation, immunosuppression, anticonvulsion, alleviation of intraocular pressure in glaucoma, and attenuation of vomiting1. The clinical application of cannabinoids has, however, been limited by their psychoactive effects, and this has led to interest in the biochemical bases of their action. Progress stemmed initially from the synthesis of potent derivatives of δ9-THC4,5, and more recently from the cloning of a gene encoding a G-protein-coupled receptor for cannabinoids6. This receptor is expressed in the brain but not in the periphery, except for a low level in testes. It has been proposed that the non-psychoactive effects of cannabinoids are either mediated centrally or through direct interaction with other, non-receptor proteins1,7,8. Here we report the cloning of a receptor for cannabinoids that is not expressed in the brain but rather in macrophages in the marginal zone of spleen.",1993,28,4460,296,1,21,31,57,52,102,105,93,99,129
a5a22f4adc9ae7ca2e7a2025ec297d1f5aa6b250,"A new parametric quantum mechanical molecular model, AM1 (Austin Model l), based on the NDDO approximation, is described. In it the major weaknesses of MNDO, in particular failure to reproduce hydrogen bonds, have been overcome without any increase in computing time. Results for 167 molecules are reported. Parameters are currently available for C, H, 0, and N.",1985,1,11082,115,0,0,0,0,1,1,2,78,431,486
292cd855004cc4b271cad0d13709124f28f69093,,2001,0,5784,272,128,182,225,278,366,351,374,354,382,356
020f8fd4c7d1519025e367632d436091d6fcf8f8,"Generation of Interleukin (IL)-1beta via cleavage of its proform requires the activity of caspase-1 (and caspase-11 in mice), but the mechanism involved in the activation of the proinflammatory caspases remains elusive. Here we report the identification of a caspase-activating complex that we call the inflammasome. The inflammasome comprises caspase-1, caspase-5, Pycard/Asc, and NALP1, a Pyrin domain-containing protein sharing structural homology with NODs. Using a cell-free system, we show that proinflammatory caspase activation and proIL-1beta processing is lost upon prior immunodepletion of Pycard. Moreover, expression of a dominant-negative form of Pycard in differentiated THP-1 cells blocks proIL-1beta maturation and activation of inflammatory caspases induced by LPS in vivo. Thus, the inflammasome constitutes an important arm of the innate immunity.",2002,49,4347,257,7,43,57,40,66,119,121,170,161,226
2a206b500ae70bbaf05ed692098871c92ec33527,,1992,0,9125,201,235,283,294,304,341,334,313,338,376,319
d0b0a759aa43f199501d9dcad4ee33014098aafe,"Plant responses to salinity stress are reviewed with emphasis on molecular mechanisms of signal transduction and on the physiological consequences of altered gene expression that affect biochemical reactions downstream of stress sensing. We make extensive use of comparisons with model organisms, halophytic plants, and yeast, which provide a paradigm for many responses to salinity exhibited by stress-sensitive plants. Among biochemical responses, we emphasize osmolyte biosynthesis and function, water flux control, and membrane transport of ions for maintenance and re-establishment of homeostasis. The advances in understanding the effectiveness of stress responses, and distinctions between pathology and adaptive advantage, are increasingly based on transgenic plant and mutant analyses, in particular the analysis of Arabidopsis mutants defective in elements of stress signal transduction pathways. We summarize evidence for plant stress signaling systems, some of which have components analogous to those that regulate osmotic stress responses of yeast. There is evidence also of signaling cascades that are not known to exist in the unicellular eukaryote, some that presumably function in intercellular coordination or regulation of effector genes in a cell-/tissue-specific context required for tolerance of plants. A complex set of stress-responsive transcription factors is emerging. The imminent availability of genomic DNA sequences and global and cell-specific transcript expression data, combined with determinant identification based on gain- and loss-of-function molecular genetics, will provide the infrastructure for functional physiological dissection of salt tolerance determinants in an organismal context. Furthermore, protein interaction analysis and evaluation of allelism, additivity, and epistasis allow determination of ordered relationships between stress signaling components. Finally, genetic activation and suppression screens will lead inevitably to an understanding of the interrelationships of the multiple signaling systems that control stress-adaptive responses in plants.",2000,370,4414,213,3,33,87,76,116,108,148,171,185,218
bb967168ead7a14adcb0121dcf24a930d1a383b3,"This paper describes the contents of the 2016 edition of the HITRAN molecular spectroscopic compilation. The new edition replaces the previous HITRAN edition of 2012 and its updates during the intervening years. The HITRAN molecular absorption compilation is composed of five major components: the traditional line-by-line spectroscopic parameters required for high-resolution radiative-transfer codes, infrared absorption cross-sections for molecules not yet amenable to representation in a line-by-line form, collision-induced absorption data, aerosol indices of refraction, and general tables such as partition sums that apply globally to the data. The new HITRAN is greatly extended in terms of accuracy, spectral coverage, additional absorption phenomena, added line-shape formalisms, and validity. Moreover, molecules, isotopologues, and perturbing gases have been added that address the issues of atmospheres beyond the Earth. Of considerable note, experimental IR cross-sections for almost 300 additional molecules important in different areas of atmospheric science have been added to the database. The compilation can be accessed through www.hitran.org. Most of the HITRAN data have now been cast into an underlying relational database structure that offers many advantages over the long-standing sequential text-based structure. The new structure empowers the user in many ways. It enables the incorporation of an extended set of fundamental parameters per transition, sophisticated line-shape formalisms, easy user-defined output formats, and very convenient searching, filtering, and plotting of data. A powerful application programming interface making use of structured query language (SQL) features for higher-level applications of HITRAN is also provided.",2005,1321,6840,199,42,179,232,287,328,379,371,405,446,483
a27087636a2708771f654fa175f406c63a9272be,"A description of the ab initio quantum chemistry package GAMESS is presented. Chemical systems containing atoms through radon can be treated with wave functions ranging from the simplest closed‐shell case up to a general MCSCF case, permitting calculations at the necessary level of sophistication. Emphasis is given to novel features of the program. The parallelization strategy used in the RHF, ROHF, UHF, and GVB sections of the program is described, and detailed speecup results are given. Parallel calculations can be run on ordinary workstations as well as dedicated parallel machines. © John Wiley & Sons, Inc.",1993,131,15511,135,0,0,0,0,0,0,0,0,0,0
e85fd090421d0ae5e5939ef51d6e3c44933cd08c,"Publisher Summary This chapter describes techniques concerned with classical and molecular genetics, cell biology, and biochemistry that can be used with Schizosaccharomyces pombe . Conjugation and sporulation cannot take place in S. pombe except under conditions of nutrient starvation. ME medium is generally used for genetic crosses. To cross two strains, a loopful of h – and a loopful of h + are mixed together on a ME plate. The cross is left to dry and is then incubated below 30°, as conjugation is severely reduced above this temperature. Fully formed four-spore asci can be seen after 2–3 days of incubation. A 2 day-old cross is usually used for tetrad analysis. Using a 3-day-old cross, one can check for the presence of asci under the light microscope. Random spore analysis allows many more spores to be examined than in tetrad analysis, and in this way recombination mapping and strain construction can be carried out. Diploid cells arise spontaneously in most S. pombe strains, this characteristic can be used to isolate homozygous diploids of any strain. For mutagenesis of yeast strains, ethylmethane sulfonate (EMS) and nitrosoguanidine is used.",1991,15,3389,278,5,17,22,51,60,69,119,123,133,124
f581036fe54ba182dc11696134a6d9182f68228b,"Mitochondria play a key part in the regulation of apoptosis (cell death). Their intermembrane space contains several proteins that are liberated through the outer membrane in order to participate in the degradation phase of apoptosis. Here we report the identification and cloning of an apoptosis-inducing factor, AIF, which is sufficient to induce apoptosis of isolated nuclei. AIF is a flavoprotein of relative molecular mass 57,000 which shares homology with the bacterial oxidoreductases; it is normally confined to mitochondria but translocates to the nucleus when apoptosis is induced. Recombinant AIF causes chromatin condensation in isolated nuclei and large-scale fragmentation of DNA. It induces purified mitochondria to release the apoptogenic proteins cytochrome c and caspase-9. Microinjection of AIF into the cytoplasm of intact cells induces condensation of chromatin, dissipation of the mitochondrial transmembrane potential, and exposure of phosphatidylserine in the plasma membrane. None of these effects is prevented by the wide-ranging caspase inhibitor known as Z-VAD.fmk. Overexpression of Bcl-2, which controls the opening of mitochondrial permeability transition pores, prevents the release of AIF from the mitochondrion but does not affect its apoptogenic activity. These results indicate that AIF is a mitochondrial effector of apoptotic cell death.",1999,32,4046,189,93,230,236,254,266,265,252,235,222,202
cc4422e00c3bbcbca00addbd19da65e6934e467b,"A new direct difference method for the computation of molecular interactions has been based on a bivariational transcorrelated treatment, together with special methods for the balancing of other errors. It appears that these new features can give a strong reduction in the error of the interaction energy, and they seem to be particularly suitable for computations in the important region near the minimum energy. It has been generally accepted that this problem is dominated by unresolved difficulties and the relation of the new methods to these apparent difficulties is analysed here.",1970,4,15843,78,0,0,0,0,0,0,0,0,0,0
b5aef2f50e0220c8e3117b6c23299d974772ee84,"In the following, the first results on ultraviolet laser desorption (UVLD) of bioorganic compounds in the mass range above 10000 daltons are reported. Strong molecular ion signals were registered by use of an organic matrix with strong absorption at the wavelength used for controlled energy deposition and soft desorption (7)",1988,10,4937,174,0,26,36,60,72,71,96,88,64,90
bcbca5c598a6fabaa580e3a1e4be071c243e1f69,"MICROPOROUS and mesoporous inorganic solids (with pore diameters of ≤20 Å and ∼20–500 Å respectively)1 have found great utility as catalysts and sorption media because of their large internal surface area. Typical microporous materials are the crystalline framework solids, such as zeolites2, but the largest pore dimensions found so far are ∼10–12 Å for some metallophosphates3–5 and ∼14 Å for the mineral cacoxenite6. Examples of mesoporous solids include silicas7 and modified layered materials8–11, but these are invariably amorphous or paracrystalline, with pores that are irregularly spaced and broadly distributed in size8,12. Pore size can be controlled by intercalation of layered silicates with a surfactant species9,13, but the final product retains, in part, the layered nature of the precursor material. Here we report the synthesis of mesoporous solids from the calcination of aluminosilicate gels in the presence of surfactants. The material14,15 possesses regular arrays of uniform channels, the dimensions of which can be tailored (in the range 16 Å to 100 Å or more) through the choice of surfactant, auxiliary chemicals and reaction conditions. We propose that the formation of these materials takes place by means of a liquid-crystal 'templating' mechanism, in which the silicate material forms inorganic walls between ordered surfactant micelles.",1992,16,12954,101,0,0,0,0,0,0,0,0,0,0
8bbe9dd9da329cf7bb311a5a0c3d53a0583491be,"The synthesis, characterization, and proposed mechanism of formation of a new family of silicatelaluminosilicate mesoporous molecular sieves designated as M41S is described. MCM-41, one member of this family, exhibits a hexagonal arrangement of uniform mesopores whose dimensions may be engineered in the range of - 15 A to greater than 100 A. Other members of this family, including a material exhibiting cubic symmetry, have ken synthesized. The larger pore M41S materials typically have surface areas above 700 m2/g and hydrocarbon sorption capacities of 0.7 cc/g and greater. A templating mechanism (liquid crystal templating-LCT) in which surfactant liquid crystal structures serve as organic templates is proposed for the formation of these materials. In support of this templating mechanism, it was demonstrated that the structure and pore dimensions of MCM-41 materials are intimately linked to the properties of the surfactant, including surfactant chain length and solution chemistry. The presence of variable pore size MCM-41, cubic material, and other phases indicates that M41S is an extensive family of materials.",1992,61,8954,93,0,9,60,74,132,149,182,187,271,285
2b6ede7b046c3da0e656c44215602c0487551a8b,"Microbe-associated molecular patterns (MAMPs) are molecular signatures typical of whole classes of microbes, and their recognition plays a key role in innate immunity. Endogenous elicitors are similarly recognized as damage-associated molecular patterns (DAMPs). This review focuses on the diversity of MAMPs/DAMPs and on progress to identify the corresponding pattern recognition receptors (PRRs) in plants. The two best-characterized MAMP/PRR pairs, flagellin/FLS2 and EF-Tu/EFR, are discussed in detail and put into a phylogenetic perspective. Both FLS2 and EFR are leucine-rich repeat receptor kinases (LRR-RKs). Upon treatment with flagellin, FLS2 forms a heteromeric complex with BAK1, an LRR-RK that also acts as coreceptor for the brassinolide receptor BRI1. The importance of MAMP/PRR signaling for plant immunity is highlighted by the finding that plant pathogens use effectors to inhibit PRR complexes or downstream signaling events. Current evidence indicates that MAMPs, DAMPs, and effectors are all perceived as danger signals and induce a stereotypic defense response.",2009,149,2496,246,28,105,137,182,251,264,243,243,222,224
ceedaf1403c1e5fb2faf6e005c380d0f588e2949,"Spend your few moment to read a book even only few pages. Reading book is not obligation and force for everybody. When you don't want to read, you can get punishment from the publisher. Read a book becomes a choice of your different characteristics. Many people with reading habit will always be enjoyable to read, or on the contrary. For some reasons, this molecular markers natural history and evolution tends to be the representative book in this website.",1994,0,3139,311,7,17,49,78,108,107,114,130,122,112
0fbe1c150884f85d22ee63a47bc6cbca30b4d4ec,"Methodology for General and Molecular Microbiology Morphology Light microscopy Determinative and cytological light microscopy Electron microscopy Cell fractionation Antigen-antibody reactions Growth: Physicochemical factors in growth Nutrition and media Enrichment and isolation Solid, liquid/solid and semisolid culture Liquid culture Growth measurement Culture preservation Molecular Genetics: Gene mutation Gene transfer in Gram-negative bacteria Gene transfer in Gram-positive bacteria Plasmids Transposon mutagenesis Gene cloning and expression Polymerase chain reaction Nucleic acid analysis Metabolism: Physical analysis Chemical analysis Enzymatic activity Permeability and transport Systematics: Phenotypic characterization DNA sequence similarities Ribosomal RNA hybridization and gene sequencing Nucleic acid probes General Methods: Laboratory safety Photography Records and reports",1994,0,3338,201,7,36,51,65,97,104,108,123,124,108
02a9b5080741eab5f6b0f1b1d357cc42fd2e8ad7,"One of the most remarkable aspects of an animal's behavior is the ability to modify that behavior by learning, an ability that reaches its highest form in human beings. For me, learning and memory have proven to be endlessly fascinating mental processes because they address one of the fundamental features of human activity: our ability to acquire new ideas from experience and to retain these ideas over time in memory. Moreover, unlike other mental processes such as thought, language, and consciousness, learning seemed from the outset to be readily accessible to cellular and molecular analysis. I, therefore, have been curious to know: What changes in the brain when we learn? And, once something is learned, how is that information retained in the brain? I have tried to address these questions through a reductionist approach that would allow me to investigate elementary forms of learning and memory at a cellular molecular level—as specific molecular activities within identified nerve cells.",2001,114,3397,186,1,53,127,134,142,190,178,213,192,190
58cb1e26e323ae41fad6f4638110e58116e9923a,"1. The Phase Equilibrium Problem. 2. Classical Thermodynamics of Phase Equilibria. 3. Thermodynamic Properties from Volumetric Data. 4. Intermolecular Forces, Corresponding States and Osmotic Systems. 5. Fugacities in Gas Mixtures. 6. Fugacities in Liquid Mixtures: Excess Functions. 7. Fugacities in Liquid Mixtures: Models and Theories of Solutions. 8. Polymers: Solutions, Blends, Membranes, and Gels. 9. Electrolyte Solutions. 10. Solubilities of Gases in Liquids. 11. Solubilities of Solids in Liquids. 12. High-Pressure Phase Equilibria. Appendix A. Uniformity of Intensive Potentials as a Criterion of Phase Equilibrium. Appendix B. A Brief Introduction to Statistical Thermodynamics. Appendix C. Virial Coefficients for Quantum Gases. Appendix D. The Gibbs-Duhem Equation. Appendix E. Liquid-Liquid Equilibria in Binary and Multicomponent Systems. Appendix F. Estimation of Activity Coefficients. Appendix G. A General Theorem for Mixtures with Associating or Solvating Molecules. Appendix H. Brief Introduction to Perturbation Theory of Dense Fluids. Appendix I. The Ion-Interaction Model of Pitzer for Multielectrolyte Solutions. Appendix J. Conversion Factors and Constants. Index.",1969,0,4433,200,0,7,6,10,4,5,16,8,12,16
b831ec95b6d1af66fd3d4f4a3a8a4b59381058a0,,2007,105,2509,815,11,73,238,343,427,380,295,186,171,141
e84cc71e71ee1d52df52b45ff6c95167fd447308,"Molecular cell biology , Molecular cell biology , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1986,0,5755,136,0,10,24,32,19,40,34,47,45,62
818fbce63180ab2fc3c344dd1bdf624fbfc37681,"A molecular dynamics simulation method which can generate configurations belonging to the canonical (T, V, N) ensemble or the constant temperature constant pressure (T, P, N) ensemble, is proposed. The physical system of interest consists of N particles (f degrees of freedom), to which an external, macroscopic variable and its conjugate momentum are added. This device allows the total energy of the physical system to fluctuate. The equilibrium distribution of the energy coincides with the canonical distribution both in momentum and in coordinate space. The method is tested for an atomic fluid (Ar) and works well.",1984,17,6394,141,0,9,20,17,15,18,36,29,36,30
31f53bb704c274df483aee602b290ae3e0202df4,"We present a unified scheme that, by combining molecular dynamics and density-functional theory, profoundly extends the range of both concepts. Our approach extends molecular dynamics beyond the usual pair-potential approximation, thereby making possible the simulation of both covalently bonded and metallic systems. In addition it permits the application of density-functional theory to much larger systems than previously feasible. The new technique is demonstrated by the calculation of some static and dynamic properties of crystalline silicon within a self-consistent pseudopotential framework.",1985,0,7489,122,1,7,24,37,52,83,104,106,142,147
b3c7a75a284844262a5b84658c85dd6ae3edb665,"Describes and discusses the use of theoretical models as an alternative to experiment in making accurate predictions of chemical phenomena. Addresses the formulation of theoretical molecular orbital models starting from quantum mechanics, and compares them to experimental results. Draws on a series of models that have already received widespread application and are available for new applications. A new and powerful research tool for the practicing experimental chemist.",1986,30,7893,110,10,41,75,77,98,117,144,153,212,324
c1cb8af97a5aecc3f78a3f0a6a285b841c6c404c,"In this article we present a new LINear Constraint Solver (LINCS) for molecular simulations with bond constraints. The algorithm is inherently stable, as the constraints themselves are reset instead of derivatives of the constraints, thereby eliminating drift. Although the derivation of the algorithm is presented in terms of matrices, no matrix matrix multiplications are needed and only the nonzero matrix elements have to be stored, making the method useful for very large molecules. At the same accuracy, the LINCS algorithm is 3 to 4 times faster than the SHAKE algorithm. Parallelization of the algorithm is straightforward.",1997,18,4383,138,0,1,1,3,5,18,20,41,51,69
dc155aa4381dad8d62057de57c83f5374abdaa8d,"The nervous system detects and interprets a wide range of thermal and mechanical stimuli, as well as environmental and endogenous chemical irritants. When intense, these stimuli generate acute pain, and in the setting of persistent injury, both peripheral and central nervous system components of the pain transmission pathway exhibit tremendous plasticity, enhancing pain signals and producing hypersensitivity. When plasticity facilitates protective reflexes, it can be beneficial, but when the changes persist, a chronic pain condition may result. Genetic, electrophysiological, and pharmacological studies are elucidating the molecular mechanisms that underlie detection, coding, and modulation of noxious stimuli that generate pain.",2009,184,2843,187,18,80,147,186,224,281,233,255,295,252
176cfe9afceb31dd6e768413e9ead95b5d34489b,"P2X receptors are membrane ion channels that open in response to the binding of extracellular ATP. Seven genes in vertebrates encode P2X receptor subunits, which are 40-50% identical in amino acid sequence. Each subunit has two transmembrane domains, separated by an extracellular domain (approximately 280 amino acids). Channels form as multimers of several subunits. Homomeric P2X1, P2X2, P2X3, P2X4, P2X5, and P2X7 channels and heteromeric P2X2/3 and P2X1/5 channels have been most fully characterized following heterologous expression. Some agonists (e.g., alphabeta-methylene ATP) and antagonists [e.g., 2',3'-O-(2,4,6-trinitrophenyl)-ATP] are strongly selective for receptors containing P2X1 and P2X3 subunits. All P2X receptors are permeable to small monovalent cations; some have significant calcium or anion permeability. In many cells, activation of homomeric P2X7 receptors induces a permeability increase to larger organic cations including some fluorescent dyes and also signals to the cytoskeleton; these changes probably involve additional interacting proteins. P2X receptors are abundantly distributed, and functional responses are seen in neurons, glia, epithelia, endothelia, bone, muscle, and hemopoietic tissues. The molecular composition of native receptors is becoming understood, and some cells express more than one type of P2X receptor. On smooth muscles, P2X receptors respond to ATP released from sympathetic motor nerves (e.g., in ejaculation). On sensory nerves, they are involved in the initiation of afferent signals in several viscera (e.g., bladder, intestine) and play a key role in sensing tissue-damaging and inflammatory stimuli. Paracrine roles for ATP signaling through P2X receptors are likely in neurohypophysis, ducted glands, airway epithelia, kidney, bone, and hemopoietic tissues. In the last case, P2X7 receptor activation stimulates cytokine release by engaging intracellular signaling pathways.",2002,735,2697,360,0,76,141,175,164,189,185,184,160,168
4e98dde420b63f896fc7e1ca79a5c5e3b15351b8,"We introduce the Bayesian skyline plot, a new method for estimating past population dynamics through time from a sample of molecular sequences without dependence on a prespecified parametric model of demographic history. We describe a Markov chain Monte Carlo sampling procedure that efficiently samples a variant of the generalized skyline plot, given sequence data, and combines these plots to generate a posterior distribution of effective population size through time. We apply the Bayesian skyline plot to simulated data sets and show that it correctly reconstructs demographic history under canonical scenarios. Finally, we compare the Bayesian skyline plot model to previous coalescent approaches by analyzing two real data sets (hepatitis C virus in Egypt and mitochondrial DNA of Beringian bison) that have been previously investigated using alternative coalescent methods. In the bison analysis, we detect a severe but previously unrecognized bottleneck, estimated to have occurred 10,000 radiocarbon years ago, which coincides with both the earliest undisputed record of large numbers of humans in Alaska and the megafaunal extinctions in North America at the beginning of the Holocene.",2005,43,2598,396,1,20,40,87,107,149,170,211,199,216
20aeb2357e9e215787c7e0d0acfe7a6b598c9103,"This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkisons Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, its easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot. This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and youll learn everything you need in the book. After reading this book youll be able to produce graphics customized precisely for your problems,and youll find it easy to get graphics out of your head and on to the screen or page.",2009,0,20834,1799,0,0,0,0,0,1,2,0,3,9
6dea759b9e6d08b1e8ae7c3ac135234008e26aec,"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as 'Coot'.",2004,21,24373,1589,0,0,0,0,0,0,1,0,0,0
412a0bb5a3baa91b62053d82c562bc172df0439f,"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems",2007,0,12593,918,0,0,0,0,0,0,0,0,5,46
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17519,724,0,0,0,0,0,0,0,1,1,2
d0a47c8c9c4213edd24a120a6912ebc08e348a19,"Abstract In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.",1996,7,10044,920,0,0,0,1,29,82,204,435,631,683
8e3f2b578ce2d2bb969ab64d4ca43eaa147f0674,"Publisher Summary This chapter discusses Raster3D, which is a suite of programs for molecular graphics. Crystallographers were among the first and most avid consumers of graphics workstations. Rapid advances in computer hardware, and particularly in the power of specialized computer graphics boards, have led to successive generations of personal workstations with ever more impressive capabilities for interactive molecular graphics. For many years, it was standard practice in crystallography laboratories to prepare figures by photographing directly from the workstation screen. No matter how beautiful the image on the screen, however, this approach suffers from several intrinsic limitations. Among these is the inherent limitation imposed by the effective resolution of the screen. Use of the graphics hardware in a workstation to generate images for later presentation can also impose other limitations. Designers of workstation hardware must compromise the quality of rendered images to achieve rendering speeds high enough for useful interactive manipulation of three-dimensional objects.",1997,13,3661,63,1,42,159,253,356,444,502,522,404,248
319bef16b09502a4c6d374a947ad74006cc1cda2,,1997,0,3410,0,195,200,182,208,208,197,199,198,178,186
793b4d2f5c2e3ceb53946e94f2ea2ff1d5dc1522,,1995,6,2472,120,2,22,49,88,86,104,131,133,171,202
3da75ffd1b1b7dacf37e0dd880e214fdb47980d5,"Raster3D Version 2.0 is a program suite for the production of photorealistic molecular graphics images. The code is hardware independent, and is particularly suited for use in producing large raster images of macromolecules for output to a film recorder or high-quality color printer. The Raster3D suite contains programs for composing illustrations of space-filling models, ball-and-stick models and ribbon-and-cylinder representations. It may also be used to render figures composed using other graphics tools, notably the widely used program Molscript [Kraulis (1991). J. Appl. Cryst. 24, 946-950].",1994,0,2417,61,1,39,73,147,196,223,238,221,192,208
459d0724afbcd591edde2dcc83e5e2750d878c0e,"These are the short notes for a two hour tutorial on principles and practice of computer graphics and scientific visualization. They are intended to summarize the contents of the tutorial transparencies and slides but they cannot completely replace them since restrictions in space and print quality do not permit the inclusion of figures and example images. For further reference the following standard text should be consulted: [3, 8, 5, 1, 6, 2, 9]",1997,4,2290,58,111,154,133,109,120,143,126,119,111,104
2bbf413f36f366fa73da4dc028a32131b5d205d6,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this �eld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.",2005,398,1911,121,16,67,147,188,202,206,228,165,158,136
356869aa0ae8d598e956c7f2ae884bbf5009c98c,"To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.",2008,16,1404,142,24,129,164,170,135,146,129,103,88,85
f8e1b3bcee316203b4aa843efd5f581bc16849dc,"From the Publisher: 
Visualization is a part of every day life. From weather map generation of financial modelling to MRI technology in medicine to 3D graphics used in movies like Jurassic Park, examples of visualization abound. The book/CD package offers readers the opportunity to practice visualization using a complete C++ programming environment developed by the authors.",1997,0,1949,122,23,34,57,71,83,65,77,108,103,101
f978d481fae83e57202d26d4fbd38e330889ea75,"Graphics processing units (GPUs), originally developed for rendering real-time effects in computer games, now provide unprecedented computational power for scientific applications. In this paper, we develop a general purpose molecular dynamics code that runs entirely on a single GPU. It is shown that our GPU implementation provides a performance equivalent to that of fast 30 processor core distributed memory cluster. Our results show that GPUs already provide an inexpensive alternative to such clusters and discuss implications for the future.",2008,26,1327,40,14,31,70,81,89,135,91,121,100,111
54dddebd5f6259d9f2f9b859007bb150da453535,"A model is developed of the human lower extremity to study how changes in musculoskeletal geometry and musculotendon parameters affect muscle force and its moment about the joints. The lines of action of 43 musculotendon actuators were defined based on their anatomical relationships to three-dimensional bone surface representations. A model for each actuator was formulated to compute its isometric force-length relation. The kinematics of the lower extremity were defined by modeling the hip, knee, ankle, subtalar, and metatarsophalangeal joints. Thus, the force and joint moment that each musculotendon actuator develops can be computed for any body position. The joint moments calculated with the model compare well with experimentally measured isometric joint moments. A graphical interface to the model has also been developed. It allows the user to visualize the musculoskeletal geometry and to manipulate the model parameters to study the biomechanical consequences of orthopaedic surgical procedures. For example, tendon transfer and lengthening procedures can be simulated by adjusting the model parameters according to various surgical techniques. Results of the simulated surgeries can be analyzed quickly in terms of postsurgery muscle forces and other biomechanical variables.<<ETX>>",1990,50,1679,139,2,3,5,7,22,18,15,25,17,24
6cae8a08979c34110a85428f06c973b38828eac6,,2004,0,1519,13,25,41,39,76,137,161,169,171,131,132
3c47192d40b1138f4e757948c64bb846c38c53ba,This paper presents a new reflectance model for rendering computer synthesized images. The model accounts for the relative brightness of different materials and light sources in the same scene. It describes the directional distribution of the reflected light and a color shift that occurs as the reflectance changes with incidence angle. The paper presents a method for obtaining the spectral energy distribution of the light reflected from an object made of a specific real material and discusses a procedure for accurately reproducing the color associated with the spectral energy distribution. The model is applied to the simulation of a metal and a plastic.,1987,30,1665,115,16,19,16,21,10,22,15,19,13,21
5ba7042c5220548c9d5636df3cc2c84bb8641e02,"Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.
 The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.",1980,8,1841,91,1,1,7,2,0,3,6,3,3,6
e80dc3681b94fbba79523201ab180852f901e618,"Abstract The role of computer graphics in different aspects of simulating matter on the atomic scale is discussed. The computer graphics is useful in specifying and examining chemical structures, since it is nowadays possible to study––with density functional theory––complex systems containing up to a few hundreds in-equivalent atoms. Furthermore, computer graphics is also an indispensable tool in analysing computed data and facilitates interpretation of results. In this context XCrySDen ( http://www.xcrysden.org/ ) is presented, a crystalline- and molecular-structure visualisation program, which aims at display of isosurfaces and contours, which can be superimposed on crystalline structures and interactively rotated and manipulated. Another aspect of computer utilisation in simulations that takes advantage of the computer’s graphics capabilities, is that it provides intuitive graphical user interfaces for the simulation setup. It is demonstrated how such interfaces are easily built using the developed GUIB software ( http://www-k3.ijs.si/kokalj/guib/ ).",2003,5,1305,17,0,1,4,18,31,53,43,44,60,64
4c231a788f20951fabc42abaa440c984a8f67661,"Fundamentals of interactive computer graphics , Fundamentals of interactive computer graphics , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,1773,18,12,44,51,91,117,125,146,126,121,90
6c73a2b81ba564a7f6fd89808f7629538adde6da,"1. Introduction. Image Processing as Picture Analysis. The Advantages of Interactive Graphics. Representative Uses of Computer Graphics. Classification of Applications. Development of Hardware and Software for Computer Graphics. Conceptual Framework for Interactive Graphics. 2. Programming in the Simple Raster Graphics Package (SRGP)/. Drawing with SRGP/. Basic Interaction Handling/. Raster Graphics Features/. Limitations of SRGP/. 3. Basic Raster Graphics Algorithms for Drawing 2d Primitives. Overview. Scan Converting Lines. Scan Converting Circles. Scan Convertiing Ellipses. Filling Rectangles. Fillign Polygons. Filling Ellipse Arcs. Pattern Filling. Thick Primiives. Line Style and Pen Style. Clipping in a Raster World. Clipping Lines. Clipping Circles and Ellipses. Clipping Polygons. Generating Characters. SRGP_copyPixel. Antialiasing. 4. Graphics Hardware. Hardcopy Technologies. Display Technologies. Raster-Scan Display Systems. The Video Controller. Random-Scan Display Processor. Input Devices for Operator Interaction. Image Scanners. 5. Geometrical Transformations. 2D Transformations. Homogeneous Coordinates and Matrix Representation of 2D Transformations. Composition of 2D Transformations. The Window-to-Viewport Transformation. Efficiency. Matrix Representation of 3D Transformations. Composition of 3D Transformations. Transformations as a Change in Coordinate System. 6. Viewing in 3D. Projections. Specifying an Arbitrary 3D View. Examples of 3D Viewing. The Mathematics of Planar Geometric Projections. Implementing Planar Geometric Projections. Coordinate Systems. 7. Object Hierarchy and Simple PHIGS (SPHIGS). Geometric Modeling. Characteristics of Retained-Mode Graphics Packages. Defining and Displaying Structures. Modeling Transformations. Hierarchical Structure Networks. Matrix Composition in Display Traversal. Appearance-Attribute Handling in Hierarchy. Screen Updating and Rendering Modes. Structure Network Editing for Dynamic Effects. Interaction. Additional Output Features. Implementation Issues. Optimizing Display of Hierarchical Models. Limitations of Hierarchical Modeling in PHIGS. Alternative Forms of Hierarchical Modeling. 8. Input Devices, Interaction Techniques, and Interaction Tasks. Interaction Hardware. Basic Interaction Tasks. Composite Interaction Tasks. 9. Dialogue Design. The Form and Content of User-Computer Dialogues. User-Interfaces Styles. Important Design Considerations. Modes and Syntax. Visual Design. The Design Methodology. 10. User Interface Software. Basic Interaction-Handling Models. Windows-Management Systems. Output Handling in Window Systems. Input Handling in Window Systems. Interaction-Technique Toolkits. User-Interface Management Systems. 11. Representing Curves and Surfaces. Polygon Meshes. Parametric Cubic Curves. Parametric Bicubic Surfaces. Quadric Surfaces. 12. Solid Modeling. Representing Solids. Regularized Boolean Set Operations. Primitive Instancing. Sweep Representations. Boundary Representations. Spatial-Partitioning Representations. Constructive Solid Geometry. Comparison of Representations. User Interfaces for Solid Modeling. 13. Achromatic and Colored Light. Achromatic Light. Chromatic Color. Color Models for Raster Graphics. Reproducing Color. Using Color in Computer Graphics. 14. The Quest for Visual Realism. Why Realism? Fundamental Difficulties. Rendering Techniques for Line Drawings. Rendering Techniques for Shaded Images. Improved Object Models. Dynamics. Stereopsis. Improved Displays. Interacting with Our Other Senses. Aliasing and Antialiasing. 15. Visible-Surface Determination. Functions of Two Variables. Techniques for Efficient Visible-Surface Determination. Algorithms for Visible-Line Determination. The z-Buffer Algorithm. List-Priority Algorithms. Scan-Line Algorithms. Area-Subdivision Algorithms. Algorithms for Octrees. Algorithms for Curved Surfaces. Visible-Surface Ray Tracing. 16. Illumination And Shading. Illumination Modeling. Shading Models for Polygons. Surface Detail. Shadows. Transparency. Interobject Reflections. Physically Based Illumination Models. Extended Light Sources. Spectral Sampling. Improving the Camera Model. Global Illumination Algorithms. Recursive Ray Tracing. Radiosity Methods. The Rendering Pipeline. 17. Image Manipulation and Storage. What Is an Image? Filtering. Image Processing. Geometric Transformations of Images. Multipass Transformations. Image Compositing. Mechanisms for Image Storage. Special Effects with Images. Summary. 18. Advanced Raster Graphic Architecture. Simple Raster-Display System. Display-Processor Systems. Standard Graphics Pipeline. Introduction to Multiprocessing. Pipeline Front-End Architecture. Parallel Front-End Architectures. Multiprocessor Rasterization Architectures. Image-Parallel Rasterization. Object-Parallel Rasterization. Hybrid-Parallel Rasterization. Enhanced Display Capabilities. 19. Advanced Geometric and Raster Algorithms. Clipping. Scan-Converting Primitives. Antialiasing. The Special Problems of Text. Filling Algorithms. Making copyPixel Fast. The Shape Data Structure and Shape Algebra. Managing Windows with bitBlt. Page Description Languages. 20. Advanced Modeling Techniques. Extensions of Previous Techniques. Procedural Models. Fractal Models. Grammar-Based Models. Particle Systems. Volume Rendering. Physically Based Modeling. Special Models for Natural and Synthetic Objects. Automating Object Placement. 21. Animation. Conventional and Computer-Assisted Animation. Animation Languages. Methods of Controlling Animation. Basic Rules of Animation. Problems Peculiar to Animation. Appendix: Mathematics for Computer Graphics. Vector Spaces and Affine Spaces. Some Standard Constructions in Vector Spaces. Dot Products and Distances. Matrices. Linear and Affine Transformations. Eigenvalues and Eigenvectors. Newton-Raphson Iteration for Root Finding. Bibliography. Index. 0201848406T04062001",1995,0,1285,93,34,46,48,56,47,74,49,43,81,77
a9918035dea348663c149a854ef92a9d222d3680,"A model building and refinement system is described for use with a Vector General 3400 display. The system allows the user to build models using guide atoms and angles to arrive at the final conformation. It has been used to assist in difference Fourier map interpretation at medium and high resolution, and to build a protein molecule into a multiple isomorphous replacement phased electron density map.",1978,7,1682,22,1,2,5,8,10,25,25,44,31,73
d57d75a95d1df5a421de98c34679ac719f374b76,"We design and implement Mars, a MapReduce framework, on graphics processors (GPUs). MapReduce is a distributed programming framework originally proposed by Google for the ease of development of web search applications on a large number of commodity CPUs. Compared with CPUs, GPUs have an order of magnitude higher computation power and memory bandwidth, but are harder to program since their architectures are designed as a special-purpose co-processor and their programming interfaces are typically for graphics applications. As the first attempt to harness GPU's power for MapReduce, we developed Mars on an NVIDIA G80 GPU, which contains over one hundred processors, and evaluated it in comparison with Phoenix, the state-of-the-art MapReduce framework on multi-core CPUs. Mars hides the programming complexity of the GPU behind the simple and familiar MapReduce interface. It is up to 16 times faster than its CPU-based counterpart for six common web applications on a quad-core machine.",2008,35,804,62,2,34,65,67,81,110,104,88,78,48
5c6b7b39f0ea433116f81cb7a3372684e6697b4a,"A liquid proportioning system includes two groups of positive metering tanks, each group consisting of at least two tanks each containing a supply of liquid to be blended and including outlet control device for selectively regulating the volume of liquid leaving the tank per unit of time. The liquids from each tank are fed together through a single conduit to a mixer which continuously blends the liquid components as they are flowing, and the blended liquids are fed to a reservoir tank, from which they are fed to a point of use in accordance with the variable requirements of the latter. One group of positive metering tanks feeds the liquid components to the mixer in exact proportions until these tanks are depleted, at which time the flow from these tanks is shut off automatically and the second group of tanks begins to feed the portioned liquid components to the mixer, while the first group of tanks are refilled. When the requirements of the point of use are decreased, sensors slow down the feeding of the liquid to the reservoir tank by increasing the time interval between the emptying of one group of metering tanks and the commencement of feeding from the other group of tanks.",1983,0,1456,92,0,1,1,3,1,7,5,16,14,22
fa8eaed21e0ff6cba18c8661d422c3be83880909,"BackgroundAnalyses of biomolecules for biodiversity, phylogeny or structure/function studies often use graphical tree representations. Many powerful tree editors are now available, but existing tree visualization tools make little use of meta-information related to the entities under study such as taxonomic descriptions or gene functions that can hardly be encoded within the tree itself (if using popular tree formats). Consequently, a tedious manual analysis and post-processing of the tree graphics are required if one needs to use external information for displaying or investigating trees.ResultsWe have developed TreeDyn, a tool using annotations and dynamic graphical methods for editing and analyzing multiple trees. The main features of TreeDyn are 1) the management of multiple windows and multiple trees per window, 2) the export of graphics to several standard file formats with or without HTML encapsulation and a new format called TGF, which enables saving and restoring graphical analysis, 3) the projection of texts or symbols facing leaf labels or linked to nodes, through manual pasting or by using annotation files, 4) the highlight of graphical elements after querying leaf labels (or annotations) or by selection of graphical elements and information extraction, 5) the highlight of targeted trees according to a source tree browsed by the user, 6) powerful scripts for automating repetitive graphical tasks, 7) a command line interpreter enabling the use of TreeDyn through CGI scripts for online building of trees, 8) the inclusion of a library of packages dedicated to specific research fields involving trees.ConclusionTreeDyn is a tree visualization and annotation tool which includes tools for tree manipulation and annotation and uses meta-information through dynamic graphical operators or scripting to help analyses and annotations of single trees or tree collections.",2006,45,934,82,2,16,31,32,63,73,70,77,84,94
e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1,"The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.
 In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.",2009,43,618,21,7,19,24,19,29,47,62,60,67,81
f74e04c23db19d746096eec7821f10112690f392,"We present a case study on the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. Graphics cards, containing multiple Graphics Processing Units (GPUs), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers and can be thought of as prototypes of the next generation of many-core processors. For certain classes of population-based Monte Carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multicore processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. On a canonical set of stochastic simulation examples including population-based Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we find speedups from 35- to 500-fold over conventional single-threaded computer code. Our findings suggest that GPUs have the potential to facilitate the growth of statistical modeling into complex data-rich domains through the availability of cheap and accessible many-core computation. We believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design. This article has supplementary material online.",2009,49,307,22,1,17,22,35,49,36,34,33,19,18
72418a969890621cfe99e470889ed0bedd0dba98,"We report a parallel Monte Carlo algorithm accelerated by graphics processing units (GPU) for modeling time-resolved photon migration in arbitrary 3D turbid media. By taking advantage of the massively parallel threads and low-memory latency, this algorithm allows many photons to be simulated simultaneously in a GPU. To further improve the computational efficiency, we explored two parallel random number generators (RNG), including a floating-point-only RNG based on a chaotic lattice. An efficient scheme for boundary reflection was implemented, along with the functions for time-resolved imaging. For a homogeneous semi-infinite medium, good agreement was observed between the simulation output and the analytical solution from the diffusion theory. The code was implemented with CUDA programming language, and benchmarked under various parameters, such as thread number, selection of RNG and memory access pattern. With a low-cost graphics card, this algorithm has demonstrated an acceleration ratio above 300 when using 1792 parallel threads over conventional CPU computation. The acceleration ratio drops to 75 when using atomic operations. These results render the GPU-based Monte Carlo simulation a practical solution for data analysis in a wide range of diffuse optical imaging applications, such as human brain or small-animal imaging.",2009,23,599,18,0,18,37,51,32,60,55,52,58,49
852e427df0329c0b9b033783246c3189b65b0a04,"Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. Lately, another property of DG has been growing in importance: The majority of a DG operator is applied in an element-local way, with weak penalty-based element-to-element coupling. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG's high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Using a single US$400 Nvidia GTX 280 GPU, we accelerate a solver for Maxwell's equations on a general 3D unstructured grid by a factor of around 50 relative to a serial computation on a current-generation CPU. In many cases, our algorithms exhibit full use of the device's available memory bandwidth. Example computations achieve and surpass 200gigaflops/s of net application-level floating point work. In this article, we describe and derive the techniques used to reach this level of performance. In addition, we present comprehensive data on the accuracy and runtime behavior of the method.",2009,31,426,37,8,33,45,40,47,42,50,55,35,33
d3c41281a5adb9626c1e05f5fc59ef2f8242438e,"We describe a complete implementation of all‐atom protein molecular dynamics running entirely on a graphics processing unit (GPU), including all standard force field terms, integration, constraints, and implicit solvent. We discuss the design of our algorithms and important optimizations needed to fully take advantage of a GPU. We evaluate its performance, and show that it can be more than 700 times faster than a conventional implementation running on a single CPU core. © 2009 Wiley Periodicals, Inc. J Comput Chem, 2009",2009,25,508,12,15,58,66,67,60,43,36,35,30,32
03aa649535c7e01ac2b3255f2f44131380dc93c7,"Graphics processors (GPUs) provide a vast number of simple, data-parallel, deeply multithreaded cores and high memory bandwidths. GPU architectures are becoming increasingly programmable, offering the potential for dramatic speedups for a variety of general-purpose applications compared to contemporary general-purpose processors (CPUs). This paper uses NVIDIA's C-like CUDA language and an engineering sample of their recently introduced GTX 260 GPU to explore the effectiveness of GPUs for a variety of application types, and describes some specific coding idioms that improve their performance on the GPU. GPU performance is compared to both single-core and multicore CPU performance, with multicore CPU implementations written using OpenMP. The paper also discusses advantages and inefficiencies of the CUDA programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body of applications.",2008,51,671,28,7,52,73,76,77,82,68,64,53,45
f74c526d01e8d7d910a41a3fdf6587e157868c4f,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general‐purpose computation to graphics hardware.",2007,302,753,9,48,60,91,70,93,64,81,51,49,30
c715fd808676921a84f248520925f8f5082cc83e,"1: Introduction.- 1.1 Graphics, Image Processing, and Pattern Recognition.- 1.2 Forms of Pictorial Data.- 1.2.1 Class 1: Full Gray Scale and Color Pictures.- 1.2.2 Class 2: Bilevel or ""Few Color"" pictures.- 1.2.3 Class 3: Continuous Curves and Lines.- 1.2.4 Class 4: Points or Polygons.- 1.3 Pictorial Input.- 1.4 Display Devices.- 1.5 Vector Graphics.- 1.6 Raster Graphics.- 1.7 Common Primitive Graphic Instructions.- 1.8 Comparison of Vector and Raster Graphics.- 1.9 Pictorial Editor.- 1.10 Pictorial Transformations.- 1.11 Algorithm Notation.- 1.12 A Few Words on Complexity.- 1.13 Bibliographical Notes.- 1.14 Relevant Literature.- 1.15 Problems.- 2: Digitization of Gray Scale Images.- 2.1 Introduction.- 2.2 A Review of Fourier and other Transforms.- 2.3 Sampling.- 2.3.1 One-dimensional Sampling.- 2.3.2 Two-dimensional Sampling.- 2.4 Aliasing.- 2.5 Quantization.- 2.6 Bibliographical Notes.- 2.7 Relevant Literature.- 2.8 Problems.- Appendix 2.A: Fast Fourier Transform.- 3: Processing of Gray Scale Images.- 3.1 Introduction.- 3.2 Histogram and Histogram Equalization.- 3.3 Co-occurrence Matrices.- 3.4 Linear Image Filtering.- 3.5 Nonlinear Image Filtering.- 3.5.1 Directional Filters.- 3.5.2 Two-part Filters.- 3.5.3 Functional Approximation Filters.- 3.6 Bibliographical Notes.- 3.7 Relevant Literature.- 3.8 Problems.- 4: Segmentation.- 4.1 Introduction.- 4.2 Thresholding.- 4.3 Edge Detection.- 4.4 Segmentation by Region Growing.- 4.4.1 Segmentation by Average Brightness Level.- 4.4.2 Other Uniformity Criteria.- 4.5 Bibliographical Notes.- 4.6 Relevant Literature.- 4.7 Problems.- 5: Projections.- 5.1 Introduction.- 5.2 Introduction to Reconstruction Techniques.- 5.3 A Class of Reconstruction Algorithms.- 5.4 Projections for Shape Analysis.- 5.5 Bibliographical Notes.- 5.6 Relevant Literature.- 5.7 Problems.- Appendix 5.A: An Elementary Reconstruction Program.- 6: Data Structures.- 6.1 Introduction.- 6.2 Graph Traversal Algorithms.- 6.3 Paging.- 6.4 Pyramids or Quad Trees.- 6.4.1 Creating a Quad Tree.- 6.4.2 Reconstructing an Image from a Quad Tree.- 6.4.3 Image Compaction with a Quad Tree.- 6.5 Binary Image Trees.- 6.6 Split-and-Merge Algorithms.- 6.7 Line Encodings and the Line Adjacency Graph.- 6.8 Region Encodings and the Region Adjacency Graph.- 6.9 Iconic Representations.- 6.10 Data Structures for Displays.- 6.11 Bibliographical Notes.- 6.12 Relevant Literature.- 6.13 Problems.- Appendix 6.A: Introduction to Graphs.- 7: Bilevel Pictures.- 7.1 Introduction.- 7.2 Sampling and Topology.- 7.3 Elements of Discrete Geometry.- 7.4 A Sampling Theorem for Class 2 Pictures.- 7.5 Contour Tracing.- 7.5.1 Tracing of a Single Contour.- 7.5.2 Traversal of All the Contours of a Region.- 7.6 Curves and Lines on a Discrete Grid.- 7.6.1 When a Set of Pixels is not a Curve.- 7.6.2 When a Set of Pixels is a Curve.- 7.7 Multiple Pixels.- 7.8 An Introduction to Shape Analysis.- 7.9 Bibliographical Notes.- 7.10 Relevant Literature.- 7.11 Problems.- 8: Contour Filling.- 8.1 Introduction.- 8.2 Edge Filling.- 8.3 Contour Filling by Parity Check.- 8.3.1 Proof of Correctness of Algorithm 8.3.- 8.3.2 Implementation of a Parity Check Algorithm.- 8.4 Contour Filling by Connectivity.- 8.4.1 Recursive Connectivity Filling.- 8.4.2 Nonrecursive Connectivity Filling.- 8.4.3 Procedures used for Connectivity Filling.- 8.4.4 Description of the Main Algorithm.- 8.5 Comparisons and Combinations.- 8.6 Bibliographical Notes.- 8.7 Relevant Literature.- 8.8 Problems.- 9: Thinning Algorithms.- 9.1 Introduction.- 9.2 Classical Thinning Algorithms.- 9.3 Asynchronous Thinning Algorithms.- 9.4 Implementation of an Asynchronous Thinning Algorithm.- 9.5 A Quick Thinning Algorithm.- 9.6 Structural Shape Analysis.- 9.7 Transformation of Bilevel Images into Line Drawings.- 9.8 Bibliographical Notes.- 9.9 Relevant Literature.- 9.10 Problems.- 10: Curve Fitting and Curve Displaying.- 10.1 Introduction.- 10.2 Polynomial Interpolation.- 10.3 Bezier Polynomials.- 10.4 Computation of Bezier Polynomials.- 10.5 Some Properties of Bezier Polynomials.- 10.6 Circular Arcs.- 10.7 Display of Lines and Curves.- 10.7.1 Display of Curves through Differential Equations.- 10.7.2 Effect of Round-off Errors in Displays.- 10.8 A Point Editor.- 10.8.1 A Data Structure for a Point Editor.- 10.8.2 Input and Output for a Point Editor.- 10.9 Bibliographical Notes.- 10.10 Relevant Literature.- 10.11 Problems.- 11: Curve Fitting with Splines.- 11.1 Introduction.- 11.2 Fundamental Definitions.- 11.3 B-Splines.- 11.4 Computation with B-Splines.- 11.5 Interpolating B-Splines.- 11.6 B-Splines in Graphics.- 11.7 Shape Description and B-splines.- 11.8 Bibliographical Notes.- 11.9 Relevant Literature.- 11.10 Problems.- 12: Approximation of Curves.- 12.1 Introduction.- 12.2 Integral Square Error Approximation.- 12.3 Approximation Using B-Splines.- 12.4 Approximation by Splines with Variable Breakpoints.- 12.5 Polygonal Approximations.- 12.5.1 A Suboptimal Line Fitting Algorithm.- 12.5.2 A Simple Polygon Fitting Algorithm.- 12.5.3 Properties of Algorithm 12.2.- 12.6 Applications of Curve Approximation in Graphics.- 12.6.1 Handling of Groups of Points by a Point Editor.- 12.6.2 Finding Some Simple Approximating Curves.- 12.7 Bibliographical Notes.- 12.8 Relevant Literature.- 12.9 Problems.- 13: Surface Fitting and Surface Displaying.- 13.1 Introduction.- 13.2 Some Simple Properties of Surfaces.- 13.3 Singular Points of a Surface.- 13.4 Linear and Bilinear Interpolating Surface Patches.- 13.5 Lofted Surfaces.- 13.6 Coons Surfaces.- 13.7 Guided Surfaces.- 13.7.1 Bezier Surfaces.- 13.7.2 B-Spline Surfaces.- 13.8 The Choice of a Surface Partition.- 13.9 Display of Surfaces and Shading.- 13.10 Bibliographical Notes.- 13.11 Relevant Literature.- 13.12 Problems.- 14: The Mathematics of Two-Dimensional Graphics.- 14.1 Introduction.- 14.2 Two-Dimensional Transformations.- 14.3 Homogeneous Coordinates.- 14.3.1 Equation of a Line Defined by Two Points.- 14.3.2 Coordinates of a Point Defined as the Intersection of Two Lines.- 14.3.3 Duality.- 14.4 Line Segment Problems.- 14.4.1 Position of a Point with respect to a Line.- 14.4.2 Intersection of Line Segments.- 14.4.3 Position of a Point with respect to a Polygon.- 14.4.4 Segment Shadow.- 14.5 Bibliographical Notes.- 14.6 Relevant Literature.- 14.7 Problems.- 15: Polygon Clipping.- 15.1 Introduction.- 15.2 Clipping a Line Segment by a Convex Polygon.- 15.3 Clipping a Line Segment by a Regular Rectangle.- 15.4 Clipping an Arbitrary Polygon by a Line.- 15.5 Intersection of Two Polygons.- 15.6 Efficient Polygon Intersection.- 15.7 Bibliographical Notes.- 15.8 Relevant Literature.- 15.9 Problems.- 16: The Mathematics of Three-Dimensional Graphics.- 16.1 Introduction.- 16.2 Homogeneous Coordinates.- 16.2.1 Position of a Point with respect to a Plane.- 16.2.2 Intersection of Triangles.- 16.3 Three-Dimensional Transformations.- 16.3.1 Mathematical Preliminaries.- 16.3.2 Rotation around an Axis through the Origin.- 16.4 Orthogonal Projections.- 16.5 Perspective Projections.- 16.6 Bibliographical Notes.- 16.7 Relevant Literature.- 16.8 Problems.- 17: Creating Three-Dimensional Graphic Displays.- 17.1 Introduction.- 17.2 The Hidden Line and Hidden Surface Problems.- 17.2.1 Surface Shadow.- 17.2.2 Approaches to the Visibility Problem.- 17.2.3 Single Convex Object Visibility.- 17.3 A Quad Tree Visibility Algorithm.- 17.4 A Raster Line Scan Visibility Algorithm.- 17.5 Coherence.- 17.6 Nonlinear Object Descriptions.- 17.7 Making a Natural Looking Display.- 17.8 Bibliographical Notes.- 17.9 Relevant Literature.- 17.10 Problems.- Author Index.- Algorithm Index.",1981,0,1207,40,0,2,8,14,20,29,42,52,49,68
25e80b49c3546876eaed9b1437ffe7cac32ca00f,"The purpose of this article is to illustrate the steps involved in testing for multigroup invariance using Amos Graphics. Based on analysis of covariance (ANCOV) structures, 2 applications are demonstrated, each of which represents a different set of circumstances. Application 1 focuses on the equivalence of a measuring instrument and tests for its invariance across 3 teacher panels, given baseline models that are identical across groups. Application 2 centers on the equivalence of a postulated theoretical structure across adolescent boys and girls in light of baseline models that are differentially specified across groups. Taken together, these illustrated examples should be of substantial assistance to researchers interested in testing for multigroup invariance using the Amos program.",2004,22,829,111,1,2,12,19,35,40,51,60,46,41
b999766e1685da444ab997b6a21a741a169ad7b1,"Molecular mechanics simulations offer a computational approach to study the behavior of biomolecules at atomic detail, but such simulations are limited in size and timescale by the available computing resources. State‐of‐the‐art graphics processing units (GPUs) can perform over 500 billion arithmetic operations per second, a tremendous computational resource that can now be utilized for general purpose computing as a result of recent advances in GPU hardware and software architecture. In this article, an overview of recent advances in programmable GPUs is presented, with an emphasis on their application to molecular mechanics simulations and the programming techniques required to obtain optimal performance in these cases. We demonstrate the use of GPUs for the calculation of long‐range electrostatics and nonbonded forces for molecular dynamics simulations, where GPU‐based calculations are typically 10–100 times faster than heavily optimized CPU‐based implementations. The application of GPU acceleration to biomolecular simulation is also demonstrated through the use of GPU‐accelerated Coulomb‐based ion placement and calculation of time‐averaged potentials from molecular dynamics trajectories. A novel approximation to Coulomb potential calculation, the multilevel summation method, is introduced and compared with direct Coulomb summation. In light of the performance obtained for this set of calculations, future applications of graphics processors to molecular dynamics simulations are discussed. © 2007 Wiley Periodicals, Inc. J Comput Chem, 2007",2007,143,690,25,2,39,54,64,88,86,85,52,46,38
f6ac5b2dab21d5d9b395a3c8bb7b92dbf996443d,"In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.",2004,36,830,79,11,38,40,53,82,121,85,93,74,70
d7316857b373a5b8dfc9008408dd76ffd8456542,"A typical PC has at least three cooling fans...and one case heater. That "" heater "" would be the graphics-processing unit (GPU) — usually a separate, highly specialized microprocessor chip dedicated to graphics. It does a brilliant job when the PC is running a graphics-intensive game or playing a video. At other times, it's largely underutilized, radiating unused power as heat. In fact, a discrete GPU is the most underutilized component in a PC. Although it's capable of amazing things, it spends much of its time performing routine chores, like scrolling the screen. Yet it has the potential to be the swiftest processing engine in the system — and it's already there, just waiting for something to do. For four years, NVIDIA has waged a campaign to redefine the role of GPUs. Not that graphics aren't important. Pushing pixels has been good business for NVIDIA, the world's leading graphics-processor company. Since the early 1990s, NVIDIA's GPUs have offloaded most of the graphics processing from CPUs in hundreds of millions of personal computers and videogame machines. NVIDIA continues to design its GPUs for superb graphics performance. But since 2005, NVIDIA has cultivated another fast-growing market — GPUs for computing applications beyond graphics. At first, these applications were called "" general-purpose GPU "" (GPGPU) computing. Today, NVIDIA prefers to call it "" GPU computing. "" In the professional market, it's often called high-performance computing. By harnessing the massively parallel-processing resources originally designed for 3D graphics, clever programmers can apply GPUs to a much broader range of computing applications. Some of those applications, such as video transcoding, still involve graphics to some degree. Whereas the CPU might spend an hour or more converting a recorded video for uploading to YouTube or burning a DVD, a GPU can tear through the job in minutes. The GPU can also enhance the video frame-by-frame, using consumer versions of sophisticated software once available only to NASA, law-enforcement agencies, and surveillance experts. Other GPU-computing applications are data-intensive tasks having little or nothing to do with graphics. Examples are stock-trading calculations and seismic-data analysis for oil exploration. Additional examples, such as high-resolution medical imaging, combine graphics with heavy-duty number crunching. On these kinds of workloads, an ordinary GPU can blow away the latest multicore CPUs. Link several GPUs together in a workstation or a cluster of systems, and you've got a "" desktop",2009,10,333,9,3,24,37,44,56,35,43,29,29,13
dbab18be6e8cd820dbbe666a3feec509cac0cb71,"BackgroundThe Smith-Waterman algorithm is one of the most widely used tools for searching biological sequence databases due to its high sensitivity. Unfortunately, the Smith-Waterman algorithm is computationally demanding, which is further compounded by the exponential growth of sequence databases. The recent emergence of many-core architectures, and their associated programming interfaces, provides an opportunity to accelerate sequence database searches using commonly available and inexpensive hardware.FindingsOur CUDASW++ implementation (benchmarked on a single-GPU NVIDIA GeForce GTX 280 graphics card and a dual-GPU GeForce GTX 295 graphics card) provides a significant performance improvement compared to other publicly available implementations, such as SWPS3, CBESW, SW-CUDA, and NCBI-BLAST. CUDASW++ supports query sequences of length up to 59K and for query sequences ranging in length from 144 to 5,478 in Swiss-Prot release 56.6, the single-GPU version achieves an average performance of 9.509 GCUPS with a lowest performance of 9.039 GCUPS and a highest performance of 9.660 GCUPS, and the dual-GPU version achieves an average performance of 14.484 GCUPS with a lowest performance of 10.660 GCUPS and a highest performance of 16.087 GCUPS.ConclusionCUDASW++ is publicly available open-source software. It provides a significant performance improvement for Smith-Waterman-based protein sequence database searches by fully exploiting the compute capability of commonly used CUDA-enabled low-cost GPUs.",2009,24,290,31,8,22,42,46,34,38,32,21,14,10
895860c6083736508d2541900cdf0960eb11592f,"The design, implementation, and capabilities of an extensible visualization system, UCSF Chimera, are discussed. Chimera is segmented into a core that provides basic services and visualization, and extensions that provide most higher level functionality. This architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features. Two unusual extensions are presented: Multiscale, which adds the ability to visualize large‐scale molecular assemblies such as viral coats, and Collaboratory, which allows researchers to share a Chimera session interactively despite being at separate locales. Other extensions include Multalign Viewer, for showing multiple sequence alignments and associated structures; ViewDock, for screening docked ligand orientations; Movie, for replaying molecular dynamics trajectories; and Volume Viewer, for display and analysis of volumetric data. A discussion of the usage of Chimera in real‐world situations is given, along with anticipated future directions. Chimera includes full user documentation, is free to academic and nonprofit users, and is available for Microsoft Windows, Linux, Apple Mac OS X, SGI IRIX, and HP Tru64 Unix from http://www.cgl.ucsf.edu/chimera/. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1605–1612, 2004",2004,70,28262,1711,0,0,0,0,1,0,0,0,0,0
7e8f728c62b49afeb5b4b17c94d9991c671a1d36,A package of computer programs for analysis and visualization of three-dimensional human brain functional magnetic resonance imaging (FMRI) results is described. The software can color overlay neural activation maps onto higher resolution anatomical scans. Slices in each cardinal plane can be viewed simultaneously. Manual placement of markers on anatomical landmarks allows transformation of anatomical and functional scans into stereotaxic (Talairach-Tournoux) coordinates. The techniques for automatically generating transformed functional data sets from manually labeled anatomical data sets are described. Facilities are provided for several types of statistical analyses of multiple 3D functional data sets. The programs are written in ANSI C and Motif 1.2 to run on Unix workstations.,1996,18,8866,1572,4,6,27,43,65,94,92,133,164,183
dd1b3a3793619cec8994cc7cca10e6dee656fb7b,"UNLABELLED
Research over the last few years has revealed significant haplotype structure in the human genome. The characterization of these patterns, particularly in the context of medical genetic association studies, is becoming a routine research activity. Haploview is a software package that provides computation of linkage disequilibrium statistics and population haplotype patterns from primary genotype data in a visually appealing and interactive interface.


AVAILABILITY
http://www.broad.mit.edu/mpg/haploview/


CONTACT
jcbarret@broad.mit.edu",2005,12,13385,1397,0,0,0,1,78,1112,1159,1137,1018,973
9cbe87b5b1d41cf34ac18a586a4957c539e8aa2b,"The program Mercury, developed by the Cambridge Crystallographic Data Centre, is designed primarily as a crystal structure visualization tool. A new module of functionality has been produced, called the Materials Module, which allows highly customizable searching of structural databases for intermolecular interaction motifs and packing patterns. This new module also includes the ability to perform packing similarity calculations between structures containing the same compound. In addition to the Materials Module, a range of further enhancements to Mercury has been added in this latest release, including void visualization and links to ConQuest, Mogul and IsoStar.",2008,10,6815,630,6,68,187,390,480,545,580,644,704,743
3040ac52b3154de53a831715f2ce7f4e7d3d723b,"SUMMARY
We present here Blast2GO (B2G), a research tool designed with the main purpose of enabling Gene Ontology (GO) based data mining on sequence data for which no GO annotation is yet available. B2G joints in one application GO annotation based on similarity searches with statistical analysis and highlighted visualization on directed acyclic graphs. This tool offers a suitable platform for functional genomics research in non-model species. B2G is an intuitive and interactive desktop application that allows monitoring and comprehension of the whole annotation and analysis process.


AVAILABILITY
Blast2GO is freely available via Java Web Start at http://www.blast2go.de.


SUPPLEMENTARY MATERIAL
http://www.blast2go.de -> Evaluation.",2005,9,8838,1007,1,18,40,76,85,164,249,421,581,765
373ed76c4bcc231c16f4b800768f3387b84808ef,"Since its original release, the popular crystal structure visualization program Mercury has undergone continuous further development. Comparisons between crystal structures are facilitated by the ability to display multiple structures simultaneously and to overlay them. Improvements have been made to many aspects of the visual display, including the addition of depth cueing, and highly customizable lighting and background effects. Textual and numeric data associated with structures can be shown in tables or spreadsheets, the latter opening up new ways of interacting with the visual display. Atomic displacement ellipsoids, calculated powder diffraction patterns and predicted morphologies can now be shown. Some limited molecular-editing capabilities have been added. The object-oriented nature of the C++ libraries underlying Mercury makes it easy to re-use the code in other applications, and this has facilitated three-dimensional visualization in several other programs produced by the Cambridge Crystallographic Data Centre.",2006,6,5298,381,70,214,272,332,253,363,320,372,336,358
80e394ee3e1834091596e8b55c9ad9bf11456e09,"The distributed nature of biological knowledge poses a major challenge to the interpretation of genome-scale datasets, including those derived from microarray and proteomic studies. This report describes DAVID, a web-accessible program that integrates functional genomic annotations with intuitive graphical summaries. Lists of gene or protein identifiers are rapidly annotated and summarized according to shared categorical data for Gene Ontology, protein domain, and biochemical pathway membership. DAVID assists in the interpretation of genome-scale datasets by facilitating the transition from data collection to biological meaning.",2003,15,7654,806,9,42,113,171,308,432,595,719,773,634
bf81aff7e1b1294e9ed53c991fbcfed2d4c7c5e0,1. Information Visualization 2. Space 3. Interaction 4. Focus + Context 5. Data Mapping: Text 6. Higher-Level Visualization 7. Using Vision to Think 8. Applications and Innovations 9. Conclusion Bibliography Index,1999,16,4543,308,39,98,124,187,170,222,215,224,222,245
fc1f637dff1d4d18e8ebdd8fe9963deacebe11a5,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/.",2009,11,5648,67,0,3,24,38,55,141,219,354,518,761
b989b192be547648594e5d7f8474f6da99ca4840,"Most designers know that yellow text presented against a blue background reads clearly and easily, but how many can explain why, and what really are the best ways to help others and ourselves clearly see key patterns in a bunch of data? When we use software, access a website, or view business or scientific graphics, our understanding is greatly enhanced or impeded by the way the information is presented. 
 
This book explores the art and science of why we see objects the way we do. Based on the science of perception and vision, the author presents the key principles at work for a wide range of applications--resulting in visualization of improved clarity, utility, and persuasiveness. The book offers practical guidelines that can be applied by anyone: interaction designers, graphic designers of all kinds (including web designers), data miners, and financial analysts. 
 
 
 
Complete update of the recognized source in industry, research, and academic for applicable guidance on information visualizing. 
 
Includes the latest research and state of the art information on multimedia presentation. 
 
More than 160 explicit design guidelines based on vision science. 
 
A new final chapter that explains the process of visual thinking and how visualizations help us to think about problems. 
 
Packed with over 400 informative full color illustrations, which are key to understanding of the subject. 
 
Table of Contents 
 
 
Chapter 1. Foundations for an Applied Science of Data Visualization 
 
Chapter 2. The Environment, Optics, Resolution, and the Display 
 
Chapter 3. Lightness, Brightness, Contrast and Constancy 
 
Chapter 4. Color 
 
Chapter 5. Visual Salience and Finding Information 
 
Chapter 6. Static and Moving Patterns 
 
Chapter 7. Space Perception 
 
Chapter 8. Visual Objects and Data Objects 
 
Chapter 9. Images, Narrative, and Gestures for Explanation 
 
Chapter 10. Interacting with Visualizations 
 
Chapter 11. Visual Thinking Processes",2000,0,4034,340,18,41,75,74,125,152,158,190,202,214
aae7c875fc7531233c2a3ebefa31a33f1a0d7f49,Representation and Geometry of Multivariate Data. Nonparametric Estimation Criteria. Histograms: Theory and Practice. Frequency Polygons. Averaged Shifted Histograms. Kernel Density Estimators. The Curse of Dimensionality and Dimension Reduction. Nonparametric Regression and Additive Models. Special Topics. Appendices. Indexes.,1992,0,4196,414,1,9,10,41,49,49,45,72,61,64
306d4932b20a5ef616a2d6fec6fc60d2f37aa757,"We have developed a computer software package, IMOD, as a tool for analyzing and viewing three-dimensional biological image data. IMOD is useful for studying and modeling data from tomographic, serial section, and optical section reconstructions. The software allows image data to be visualized by several different methods. Models of the image data can be visualized by volume or contour surface rendering and can yield quantitative information.",1996,20,4072,308,2,4,5,5,7,12,17,22,19,48
cb19024f90d759cad27ca1301311d3c250c62a49,"Open source software encourages innovation by allowing users to extend the functionality of existing applications. Treeview is a popular application for the visualization of microarray data, but is closed-source and platform-specific, which limits both its current utility and suitability as a platform for further development. Java Treeview is an open-source, cross-platform rewrite that handles very large datasets well, and supports extensions to the file format that allow the results of additional analysis to be visualized and compared. The combination of a general file format and open source makes Java Treeview an attractive choice for solving a class of visualization problems. An applet version is also available that can be used on any website with no special server-side setup.",2004,9,2834,349,2,18,36,65,93,111,122,178,200,226
444e76c1d69dbc38c49bb307590b8bd04e311278,"SUMMARY
Artemis is a DNA sequence visualization and annotation tool that allows the results of any analysis or sets of analyses to be viewed in the context of the sequence and its six-frame translation. Artemis is especially useful in analysing the compact genomes of bacteria, archaea and lower eukaryotes, and will cope with sequences of any size from small genes to whole genomes. It is implemented in Java, and can be run on any suitable platform. Sequences and annotation can be read and written directly in EMBL, GenBank and GFF format. AVAILABITLTY: Artemis is available under the GNU General Public License from http://www.sanger.ac.uk/Software/Artemis",2000,8,3054,235,0,13,42,55,101,99,101,99,103,119
4e28d137444632ac3816f26b684a8de06c591b9e,,1997,1,2487,304,0,5,13,31,52,62,89,128,147,159
fc30910a8bf8daa3256284580b1dfe98b32d3559,"BackgroundSince the inception of the GO annotation project, a variety of tools have been developed that support exploring and searching the GO database. In particular, a variety of tools that perform GO enrichment analysis are currently available. Most of these tools require as input a target set of genes and a background set and seek enrichment in the target set compared to the background set. A few tools also exist that support analyzing ranked lists. The latter typically rely on simulations or on union-bound correction for assigning statistical significance to the results.ResultsGOrilla is a web-based application that identifies enriched GO terms in ranked lists of genes, without requiring the user to provide explicit target and background sets. This is particularly useful in many typical cases where genomic data may be naturally represented as a ranked list of genes (e.g. by level of expression or of differential expression). GOrilla employs a flexible threshold statistical approach to discover GO terms that are significantly enriched at the top of a ranked gene list. Building on a complete theoretical characterization of the underlying distribution, called mHG, GOrilla computes an exact p-value for the observed enrichment, taking threshold multiple testing into account without the need for simulations. This enables rigorous statistical analysis of thousand of genes and thousands of GO terms in order of seconds. The output of the enrichment analysis is visualized as a hierarchical structure, providing a clear view of the relations between enriched GO terms.ConclusionGOrilla is an efficient GO analysis tool with unique features that make a useful addition to the existing repertoire of GO enrichment tools. GOrilla's unique features and advantages over other threshold free enrichment tools include rigorous statistics, fast running time and an effective graphical representation. GOrilla is publicly available at: http://cbl-gorilla.cs.technion.ac.il",2009,25,2755,147,9,20,63,77,154,175,253,273,309,355
05b90d85f160feb9e46d7e87ca89aa5b919c705e,"SummaryNMR View is a computer program designed for the visualization and analysis of NMR data. It allows the user to interact with a practically unlimited number of 2D, 3D and 4D NMR data files. Any number of spectral windows can be displayed on the screen in any size and location. Automatic peak picking and facilitated peak analysis features are included to aid in the assignment of complex NMR spectra. NMR View provides structure analysis features and data transfer to and from structure generation programs, allowing for a tight coupling between spectral analysis and structure generation. Visual correlation between structures and spectra can be done with the Molecular Data Viewer, a molecular graphics program with bidirectional communication to NMR View. The user interface can be customized and a command language is provided to allow for the automation of various tasks.",1994,19,2637,126,1,4,3,5,21,20,30,61,72,86
c572e58b9dd4abf559046fc4ce42a7c42df4ba34,"A cross-platform program, VESTA, has been developed to visualize both structural and volumetric data in multiple windows with tabs. VESTA represents crystal structures by ball-and-stick, space-filling, polyhedral, wireframe, stick, dot-surface and thermal-ellipsoid models. A variety of crystal-chemical information is extractable from fractional coordinates, occupancies and oxidation states of sites. Volumetric data such as electron and nuclear densities, Patterson functions, and wavefunctions are displayed as isosurfaces, bird's-eye views and two-dimensional maps. Isosurfaces can be colored according to other physical quantities. Translucent isosurfaces and/or slices can be overlapped with a structural model. Collaboration with external programs enables the user to locate bonds and bond angles in the `graphics area', simulate powder diffraction patterns, and calculate site potentials and Madelung energies. Electron densities determined experimentally are convertible into their Laplacians and electronic energy densities.",2008,131,2975,35,18,88,166,228,225,224,214,207,226,267
2a4d01ccf5661721f562b18a0fcfd6ca61f78ee7,"A new paper-and-pencil test of spatial visualization was constructed from the figures used in the Chronometric study of Shepard and Metzler (1971). In large samples, the new test displayed substantial internal consistency (Kuder-Richardson 20 = .88), a test-retest reliability (.83), and consistent sex differences over the entire range of ages investigated. Correlations with other measures indicated strong association with tests of spatial visualization and virtually no association with tests of verbal ability.",1978,17,2236,223,0,5,2,1,3,4,4,3,9,6
95cf34d93f792cf7c31f35bb98bdf5cf7ed6fa94,"The SPIDER system has evolved into a comprehensive tool set for image processing, making use of modern graphics interfacing in the VMS and UNIX environment. SPIDER and WEB handle the complementary tasks of batch processing and visualization of the results. The emphasis of the SPIDER system remains in the area of single particle averaging and reconstruction, although a variety of other application areas have been added. Novel features are a suite of operations relating to the determination, modeling, and correction of the contrast transfer function and the availability of the entire documentation in hypertext format.",1996,0,2113,159,6,10,20,34,42,59,71,68,78,87
75a6fe7a278ff0c60343af9ed5228b5f4cf38bed,"Never before in history has data been generated at such high volumes as it is today. Exploring and analyzing the vast volumes of data is becoming increasingly difficult. Information visualization and visual data mining can help to deal with the flood of information. The advantage of visual data exploration is that the user is directly involved in the data mining process. There are a large number of information visualization techniques which have been developed over the last decade to support the exploration of large data sets. In this paper, we propose a classification of information visualization and visual data mining techniques which is based on the data type to be visualized, the visualization technique, and the interaction and distortion technique. We exemplify the classification using a few examples, most of them referring to techniques and systems presented in this special section.",2002,59,1754,125,6,30,52,57,74,68,93,104,108,105
f8e1b3bcee316203b4aa843efd5f581bc16849dc,"From the Publisher: 
Visualization is a part of every day life. From weather map generation of financial modelling to MRI technology in medicine to 3D graphics used in movies like Jurassic Park, examples of visualization abound. The book/CD package offers readers the opportunity to practice visualization using a complete C++ programming environment developed by the authors.",1997,0,1949,122,23,34,57,71,83,65,77,108,103,101
10bb96d680650fa7aed27950a19d567eeeea7e22,"A multi-purpose pattern-fitting system, RIETAN-2000, has been extensively utilized to contribute to many structural studies. It offers a sophisticated structure-refinement technique of whole-pattern fitting based on the maximum-entropy method (MEM) in combination with a MEM analysis program PRIMA. We have recently completed a successor system, RIETAN-FP, to RIETAN-2000, adding new features such as standardization of crystal-structure data, an extended March-Dollase preferred-orientation function, and automation of imposing restraints on bond lengths and angles. Further, we have been developing a new three-dimensional visualization system, VESTA, using wxWidgets as a C++ application framework. VESTA excels in visualization, rendering, and manipulation of crystal structures and electron/nuclear densities determined by X-ray/ neutron diffraction and electronic-structure calculations. VESTA also enables us to display wave functions and electrostatic potentials calculated with part of these programs.",2007,5,1687,17,2,12,31,41,89,104,149,155,151,185
0204e53a26cec30e656d3dfcac0a5222fd14e557,,1992,0,2112,66,1,8,19,39,39,46,52,64,43,46
8098c2189725d6a27407249376d1ae416fc31005,1 Engine Types and Their Operations 2 Engine Design and Operating Parameters 3 Thermochemistry of Fuel-Air Mixtures 4 Properties of Working Fluids 5 Ideal Models of Engine Cycles 6 Gas Exchange Processes 7 SI Engine Fuel Metering and Manifold Phenomena 8 Charge Motion within the Cylinder 9 Combustion in Ignition Engines 10 Combustion in Compression Ignition Engines 11 Pollutant Formation and Control 12 Engine Heat Transfer 13 Engine Friction and Lubrication 14 Modeling Real Engine Flow and Combustion Processes 15 Engine Operating Characteristics Appendixes,1988,0,14034,1013,0,0,0,0,0,0,0,0,0,0
f7216b952af5bb89b11039ea5a5ebc0edf2f45ca,,2001,0,2801,305,1,21,27,42,65,79,119,111,133,153
c54d65d53d7ada275d34e1a7c53643ff0c82eb93,"Abstract The increasing industrialization and motorization of the world has led to a steep rise for the demand of petroleum-based fuels. Petroleum-based fuels are obtained from limited reserves. These finite reserves are highly concentrated in certain regions of the world. Therefore, those countries not having these resources are facing energy/foreign exchange crisis, mainly due to the import of crude petroleum. Hence, it is necessary to look for alternative fuels which can be produced from resources available locally within the country such as alcohol, biodiesel, vegetable oils etc. This paper reviews the production, characterization and current statuses of vegetable oil and biodiesel as well as the experimental research work carried out in various countries. This paper touches upon well-to-wheel greenhouse gas emissions, well-to-wheel efficiencies, fuel versatility, infrastructure, availability, economics, engine performance and emissions, effect on wear, lubricating oil etc. Ethanol is also an attractive alternative fuel because it is a renewable bio-based resource and it is oxygenated, thereby providing the potential to reduce particulate emissions in compression-ignition engines. In this review, the properties and specifications of ethanol blended with diesel and gasoline fuel are also discussed. Special emphasis is placed on the factors critical to the potential commercial use of these blends. The effect of the fuel on engine performance and emissions (SI as well as compression ignition (CI) engines), and material compatibility is also considered. Biodiesel is methyl or ethyl ester of fatty acid made from virgin or used vegetable oils (both edible and non-edible) and animal fat. The main resources for biodiesel production can be non-edible oils obtained from plant species such as Jatropha curcas (Ratanjyot), Pongamia pinnata (Karanj), Calophyllum inophyllum (Nagchampa), Hevca brasiliensis (Rubber) etc. Biodiesel can be blended in any proportion with mineral diesel to create a biodiesel blend or can be used in its pure form. Just like petroleum diesel, biodiesel operates in compression ignition (diesel) engine, and essentially require very little or no engine modifications because biodiesel has properties similar to mineral diesel. It can be stored just like mineral diesel and hence does not require separate infrastructure. The use of biodiesel in conventional diesel engines result in substantial reduction in emission of unburned hydrocarbons, carbon monoxide and particulate. This review focuses on performance and emission of biodiesel in CI engines, combustion analysis, wear performance on long-term engine usage, and economic feasibility.",2007,97,2763,89,5,44,115,145,190,170,250,224,240,240
da65f4970806ab2724947247e8aa498203443f13,"Preface Preface to the Second Edition Preface to the First Edition 1: Introduction 2: Combustion and Thermochemistry 3: Introduction to Mass Transfer 4: Chemical Kinetics 5: Some Important Chemical Mechanisms 6: Coupling Chemical and Thermal Analyses of Reacting Systems 7: Simplifed Conversation Equations for Reacting Flows 8: Laminar Premixed Flames 9: Laminar Diffusion Flames 10: Droplet Evaporation and Burning 11: Introduction to Turbulent Flows 12: Turbulent Premixed Flames 13: Turbulent Nonpremixed Flames 14: Burning of Solids 15: Pollutant Emissions 16: Detonations Appendix A: Selected Thermodynamic Propertiesof Gases Comprising C-H-O-N System Appendix B: Fuel Properties Appendix C: Selected Properties of Air, Nitrogen, and Oxygen Appendix D: Diffusion Coefficients and Methodology for their Estimation Appendix E: Generalized Newton's Method for the Solution of Nonlinear Equations Appendix F: Computer Codes for Equilibrium Products of Hydrocarbon-Air Combustion",2000,0,2105,187,12,30,26,33,55,62,63,74,68,96
3e0725a32c83ee06419edafccacd6f3768ab5da7,"[1] We present a global tabulation of black carbon (BC) and primary organic carbon (OC) particles emitted from combustion. We include emissions from fossil fuels, biofuels, open biomass burning, and burning of urban waste. Previous ‘‘bottom-up’’ inventories of black and organic carbon have assigned emission factors on the basis of fuel type and economic sector alone. Because emission rates are highly dependent on combustion practice, we consider combinations of fuel, combustion type, and emission controls and their prevalence on a regional basis. Central estimates of global annual emissions are 8.0 Tg for black carbon and 33.9 Tg for organic carbon. These estimates are lower than previously published estimates by 25–35%. The present inventory is based on 1996 fuel-use data, updating previous estimates that have relied on consumption data from 1984. An offset between decreased emission factors and increased energy use since the base year of the previous inventory prevents the difference between this work and previous inventories from being greater. The contributions of fossil fuel, biofuel, and open burning are estimated as 38%, 20%, and 42%, respectively, for BC, and 7%, 19%, and 74%, respectively, for OC. We present a bottom-up estimate of uncertainties in source strength by combining uncertainties in particulate matter emission factors, emission characterization, and fuel use. The total uncertainties are about a factor of 2, with uncertainty ranges of 4.3–22 Tg/yr for BC and 17–77 Tg/yr for OC. Low-technology combustion contributes greatly to both the emissions and the uncertainties. Advances in emission characterization for small residential, industrial, and mobile sources and topdown analysis combining field measurements and transport modeling with iterative inventory development will be required to reduce the uncertainties further. INDEX TERMS: 0305 Atmospheric Composition and Structure: Aerosols and particles (0345, 4801); 0322 Atmospheric Composition and Structure: Constituent sources and sinks; 0345 Atmospheric Composition and Structure: Pollution—urban and regional (0305); 0360 Atmospheric Composition and Structure: Transmission and scattering of radiation; 0365 Atmospheric Composition and Structure: Troposphere—composition and chemistry; KEYWORDS: emission, black carbon, organic carbon, fossil fuel, biofuel, biomass burning",2004,394,2042,178,16,63,49,72,80,122,98,160,145,154
6019b33d51af92a113766ee73ba4d1eb53d9e50e,INTRODUCTION ....................................................................................................................................... 465 LIGNIN AS A SUBSTRATE ............................................................................................................................... 466 MICROBIOLOGY OF LIGNIN BIODEGRADATI ON ........................................................................... 468 Anaerobic Conditions ............................................................................................................................... 469 Aerobic Conditions ................................................................................................................................... 469 LIGNIN DEGRADATION BY WHITEROT FUNGI ............................................................................. 471 Physiology .......................................................................................................................................................... 472 Biochemistry ............................................................................................................................................ 475 Genetics ..............................................................................................................................................................486 Molecular Biology .................................................................................................................................... 489 CONCLUSIONS AND RECOMMENDATIONS ..................................................................................... 491 ENZYMATIC “COMBUSTION” ........................................................................................................................ 493,1987,0,2528,129,2,16,45,73,64,69,74,80,54,51
5fbfddb1efed4355b620f689e441619e3008d945,"Our current understanding of the mechanisms and rate parameters for the gas-phase reactions of nitrogen compounds that are applicable to combustion-generated air pollution is discussed and illustrated by comparison of results from detailed kinetics calculations with experimental data. In particular, the mechanisms and rate parameters for thermal and prompt NO formation, for fuel nitrogen conversion, for the Thermal De-NOx and RAPRENOx processes, and for NO2 and N2O formation and removal processes are considered. Sensitivity and rate-of-production analyses are applied in the calculations to determine which elementary reactions are of greatest importance in the nitrogen conversion process. Available information on the rate parameters for these important elementary reactions has been surveyed, and recommendations for the rate coefficients for these reactions are provided. The principal areas of uncertainty in nitrogen reaction mechanisms and rate parameters are outlined.",1989,94,2522,91,0,12,31,49,63,89,77,110,87,107
02d2616821d5a2fb99fa8de1f3dcc72e2fe9f3c3,"Principles of mathematical models as tools in engineering and science are discussed in relation to turbulent combustion modeling. A model is presented for the rate of combustion which takes into account the intermittent appearance of reacting species in turbulent flames. This model relates the rate of combustion to the rate of dissipation of eddies and expresses the rate of reaction by the mean concentration of a reacting specie, the turbulent kinetic energy and the rate of dissipation of this energy. The essential features of this model are that it does not call for predictions of fluctuations of reacting species and that it is applicable to premixed as well as diffusion flames. The combustion model is tested on both premixed and diffusion flames with good results. Special attention is given to soot formation and combustion in turbulent flames. Predictions are made for two C 2 H 2 turbulent diffusion flames by incorporating both the above combustion model and the model for the rate of soot formation developed by Tesner et al., as well as previous observations by Magnussen concerning the behavior of soot in turbulent flames. The predicted results are in close agreement with the experimental data. All predictions in the present paper have been made by modeling turbulence by the k -∈ model. Buoyancy is taken into consideration in the momentum equations. Effects of terms containing density fluctuations have not been included.",1977,6,2443,119,1,0,2,1,1,4,3,6,7,5
22a8709b338bf1eddcfebddc1c16982951d1f4b5,Review of Chemical Thermodynamics. Review of Chemical Kinetics. Conservation Equations for Multi--Component Reacting Systems. Rankine--Hugoniot Relations of Detonation and Deflagration Waves of Premixed Gases. Premixed Laminar Flames. Diffusion Flames and Combustion of a Single Liquid Fuel Droplet. Turbulent Flames. Turbulent Reacting Flows with Premixed Reactants. Chemically Reacting Boundary--Layer Flows. Ignition. Appendix. Index.,1986,0,1995,141,1,2,6,6,11,14,15,23,21,24
7be9620770607f4f08ec341438bdc52cff53f79b,,2009,0,1388,55,3,15,65,95,114,148,207,185,173,136
9a919541129b3dbb88dd3f8e0244779271701b54,"Properties of biomass relevant to combustion are briefly reviewed. The compositions of biomass among fuel types are variable, especially with respect to inorganic constituents important to the critical problems of fouling and slagging. Alkali and alkaline earth metals, in combination with other fuel elements such as silica and sulfur, and facilitated by the presence of chlorine, are responsible for many undesirable reactions in combustion furnaces and power boilers. Reductions in the concentrations of alkali metals and chlorine, created by leaching the elements from the fuel with water, yield remarkable improvements in ash fusion temperatures and confirm much of what is suggested regarding the nature of fouling by biomass fuels. Other influences of biomass composition are observed for the rates of combustion and pollutant emissions. Standardized engineering practices setting out protocols of analysis and interpretation may prove useful in reducing unfavorable impacts and industry costs, and further development is encouraged.",1998,40,1644,113,2,0,4,5,6,10,17,15,33,31
edb07725544317e529582309c7ce05cf9e0349c2,"In this article, the status of fat and oil derived diesel fuels with respect to fuel properties, engine performance, and emissions is reviewed. The fuels considered are primarily the methyl esters of fatty acids derived from a variety of vegetable oils and animal fats, and referred to as biodiesel. The major obstacle to widespread use of biodiesel is the high cost relative to petroleum. Economics of biodiesel production are discussed, and it is concluded that the price of the feedstock fat or oil is the major factor determining biodiesel price.Biodiesel is completely miscible with petroleum diesel fuel, and is generally tested as a blend. The use of biodiesel in neat or blended form has no effect on the energy based engine fuel economy. The lubricity of these fuels is superior to conventional diesel, and this property is imparted to blends at levels above 20 vol%. Emissions of PM can be reduced dramatically through use of biodiesel in engines that are not high lube oil emitters. Emissions of NOx increase significantly for both neat and blended fuels in both two- and four-stroke engines. The increase may be lower in newer, lower NOx emitting four-strokes, but additional data are needed to confirm this conclusion. A discussion of available data on unregulated air toxins is presented, and it is concluded that definitive studies have yet to be performed in this area. A detailed discussion of important biodiesel properties and recommendations for future research is presented. Among the most important recommendations is the need for all future studies to employ biodiesel of well-known composition and purity, and to report detailed analyses. The purity levels necessary for achieving adequate engine endurance, compatibility with coatings and elastomers, cold flow properties, stability, and emissions performance must be better defined.",1998,16,1819,55,0,4,12,9,8,26,18,44,44,84
07acf3c7e635c7970894192da585672e2bb51c42,"The awareness of the increase in greenhouse gas emissions has resulted in the development of new technologies with lower emissions and technologies that can accommodate capture and sequestration of carbon dioxide. For existing coal-fired combustion plants there are two main options for CO2 capture: removal of nitrogen from flue gases or removal of nitrogen from air before combustion to obtain a gas stream ready for geo-sequestration. In oxy-fuel combustion, fuel is combusted in pure oxygen rather than air. This technology recycles flue gas back into the furnace to control temperature and makeup the volume of the missing N2 to ensure there is sufficient gas to maintain the temperature and heat flux profiles in the boiler. A further advantage of the technology revealed in pilot-scale tests is substantially reduced NOx emissions. For coal-fired combustion, the technology was suggested in the eighties, however, recent developments have led to a renewed interest in the technology. This paper provides a comprehensive review of research that has been undertaken, gives the status of the technology development and assessments providing comparisons with other power generation options, and suggests research needs.",2005,42,1374,41,1,4,9,23,41,84,138,121,146,116
e1b96d80e424f4a1976f4abd07eabef6fa52afc7,"The first use of microalgae by humans dates back 2000 years to the Chinese, who used Nostoc to survive during famine. However, microalgal biotechnology only really began to develop in the middle of the last century. Nowadays, there are numerous commercial applications of microalgae. For example, (i) microalgae can be used to enhance the nutritional value of food and animal feed owing to their chemical composition, (ii) they play a crucial role in aquaculture and (iii) they can be incorporated into cosmetics. Moreover, they are cultivated as a source of highly valuable molecules. For example, polyunsaturated fatty acid oils are added to infant formulas and nutritional supplements and pigments are important as natural dyes. Stable isotope biochemicals help in structural determination and metabolic studies. Future research should focus on the improvement of production systems and the genetic modification of strains. Microalgal products would in that way become even more diversified and economically competitive.",2006,105,3421,171,1,15,45,71,134,161,217,290,284,361
8a7f8f9227f8b843fbbe6c0c991670bbc8f9832f,"Using bank-level data for 80 countries in the year's 1988-95, this article shows that differences in interest margins and bank profitability reflect a variety of determinants: bank characteristics, macroeconomic conditions, explicit and implicit bank taxation, deposit insurance regulation, overall financial structure, and underlying legal and institutional indicators. A larger ratio of bank assets to gross domestic product and a lower market concentration ratio lead to lower margins and profits, controlling for differences in bank activity, leverage, and the macroeconomic environment. Foreign banks have higher margins and profits than domestic banks in developing countries, while the opposite holds in industrial countries. Also, there is evidence that the corporate tax burden is fully passed onto bank customers, while higher reserve requirements are not, especially in developing countries.",1999,53,2226,211,10,6,26,42,32,35,51,51,49,54
1ffbf5ada9068d39951c01689978ca5adae9eed2,Entrepreneurship has been the engine propelling much of the growth of the business sector as well as a driving force behind the rapid expansion of the social sector. This article offers a comparative analysis of commercial and social entrepreneurship using a prevailing analytical model from commercial entrepreneurship. The analysis highlights key similarities and differences between these two forms of entrepreneurship and presents a framework on how to approach the social entrepreneurial process more systematically and effectively. We explore the implications of this analysis of social entrepreneurship for both practitioners and researchers.,2006,30,2040,160,8,13,31,48,81,104,117,133,160,163
3e2ab599c6a7fb19db5e3ccd8c2aaf5ea61678ba,"Many microorganisms, especially bacteria, produce biosurfactants when grown on water-immiscible substrates. Biosurfactants are more effective, selective, environmentally friendly, and stable than many synthetic surfactants. Most common biosurfactants are glycolipids in which carbohydrates are attached to a long-chain aliphatic acid, while others, like lipopeptides, lipoproteins, and heteropolysaccharides, are more complex. Rapid and reliable methods for screening and selection of biosurfactant-producing microorganisms and evaluation of their activity have been developed. Genes involved in rhamnolipid synthesis (rhlAB) and regulation (rhlI and rhlR) in Pseudomonas aeruginosa are characterized, and expression of rhlAB in heterologous hosts is discussed. Genes for surfactin production (sfp, srfA, and comA) in Bacillus spp. are also characterized. Fermentative production of biosurfactants depends primarily on the microbial strain, source of carbon and nitrogen, pH, temperature, and concentration of oxygen and metal ions. Addition of water-immiscible substrates to media and nitrogen and iron limitations in the media result in an overproduction of some biosurfactants. Other important advances are the use of water-soluble substrates and agroindustrial wastes for production, development of continuous recovery processes, and production through biotransformation. Commercialization of biosurfactants in the cosmetic, food, health care, pulp- and paper-processing, coal, ceramic, and metal industries has been proposed. However, the most promising applications are cleaning of oil-contaminated tankers, oil spill management, transportation of heavy crude oil, enhanced oil recovery, recovery of crude oil from sludge, and bioremediation of sites contaminated with hydrocarbons, heavy metals, and other pollutants. Perspectives for future research and applications are also discussed.",1997,229,1403,95,1,8,12,17,20,26,24,36,34,36
f42239347b34560fdedad44f302e52cb7144b1e4,"Abstract Surfactants are surface-active compounds capable of reducing surface and interfacial tension at the interfaces between liquids, solids and gases, thereby allowing them to mix or disperse readily as emulsions in water or other liquids. The enormous market demand for surfactants is currently met by numerous synthetic, mainly petroleum-based, chemical surfactants. These compounds are usually toxic to the environment and non-biodegradable. They may bio-accumulate and their production, processes and by-products can be environmentally hazardous. Tightening environmental regulations and increasing awareness for the need to protect the ecosystem have effectively resulted in an increasing interest in biosurfactants as possible alternatives to chemical surfactants. Biosurfactants are amphiphilic compounds of microbial origin with considerable potential in commercial applications within various industries. They have advantages over their chemical counterparts in biodegradability and effectiveness at extreme temperature or pH and in having lower toxicity. Biosurfactants are beginning to acquire a status as potential performance-effective molecules in various fields. At present biosurfactants are mainly used in studies on enhanced oil recovery and hydrocarbon bioremediation. The solubilization and emulsification of toxic chemicals by biosurfactants have also been reported. Biosurfactants also have potential applications in agriculture, cosmetics, pharmaceuticals, detergents, personal care products, food processing, textile manufacturing, laundry supplies, metal treatment and processing, pulp and paper processing and paint industries. Their uses and potential commercial applications in these fields are reviewed.",2000,187,1459,31,0,5,21,22,28,32,38,56,79,67
62cf465bbf50dae7906de5492d721452086c9744,"Abstract. In response to the rapidly growing field of proteomics, the use of recombinant proteins has increased greatly in recent years. Recombinant hybrids containing a polypeptide fusion partner, termed affinity tag, to facilitate the purification of the target polypeptides are widely used. Many different proteins, domains, or peptides can be fused with the target protein. The advantages of using fusion proteins to facilitate purification and detection of recombinant proteins are well-recognized. Nevertheless, it is difficult to choose the right purification system for a specific protein of interest. This review gives an overview of the most frequently used and interesting systems: Arg-tag, calmodulin-binding peptide, cellulose-binding domain, DsbA, c-myc-tag, glutathione S-transferase, FLAG-tag, HAT-tag, His-tag, maltose-binding protein, NusA, S-tag, SBP-tag, Strep-tag, and thioredoxin.",2002,174,1307,53,2,7,38,50,59,58,75,77,97,81
1204f9d5f1a1ad3f78d44cd06712e033305f9a96,"The commercial culture of microalgae is now over 30 years old with the main microalgal species grown being Chlorella and Spirulina for health food, Dunaliella salina for β-carotene, Haematococcus pluvialis for astaxanthin and several species for aquaculture. The culture systems currently used to grow these algae are generally fairly unsophisticated. For example, Dunaliella salina is cultured in large (up to approx. 250 ha) shallow open-air ponds with no artificial mixing. Similarly, Chlorella and Spirulina also are grown outdoors in either paddle-wheel mixed ponds or circular ponds with a rotating mixing arm of up to about 1 ha in area per pond. The production of microalgae for aquaculture is generally on a much smaller scale, and in many cases is carried out indoors in 20-40 1 carboys or in large plastic bags of up to approximately 1000 1 in volume. More recently, a helical tubular photobioreactor system, the BIOCOIL™, has been developed which allows these algae to be grown reliably outdoors at high cell densities in semi-continuous culture. Other closed photobioreactors such as fiat panels are also being developed. The main problem facing the commercialisation of new microalgae and microalgal products is the need for closed culture systems and the fact that these are very capital intensive. The high cost of microalgal culture systems relates to the need for light and the relatively slow growth rate of the algae. Although this problem has been avoided in some instances by growing the algae heterotrophically, not all algae or algal products can be produced this way.",1999,32,1184,73,0,0,13,9,16,8,14,22,16,39
b60577c146c6d3492d8200dafe462f89864f060d,"Summary The foregoing examples illustrate how the theory developed here may be employed to make estimates concerning the condition of a commercial marine fishery. The examples employed, although having perhaps as complete information as any available for this purpose, leave something to be desired. In particular, in both of these examples, very little or no data are available concerning intensity of fishing and abundance for the early period of development of the fishery, well before the maximum catches are reached. A great deal of precision would be added to the estimate if such information were available. We may emphasize, therefore, the desirability of obtaining detailed information on the total catch and catch-per-unit-of-effort from as early in the development of a commercial fishery as may be possible. Measurements of fishing mortality rates at more than one level of population would also be desirable, since they would make possible verification of the adequacy of the form of equation (13a) for describing the changes in population under the joint influences of growth and fishing. In order to apply the theory developed here to the tropical tuna fishery, it will be necessary to compile statistics of catch, abundance and intensity of fishing over a considerable series of years, beginning as early in the history of the fishery as possible. This task is well under way. It will also be necessary to obtain some estimate of the rate of fishing mortality, or to devise some other means of estimating the constant k 2 . Estimation of fishing mortality from tagging promises to be a difficult problem for the tunas. Exploration of other means of obtaining the relationship between U and P appears, therefore, to constitute an important line of investigation.",1991,8,1382,84,16,16,8,12,19,18,15,25,24,27
590c9339a1849f363dde3e99bcfb67c24ae5896c,"Groundwater geochemistry is an interdisciplinary science concerned with the chemistry in the subsurface environment. The chemical composition of groundwater is the combined result of the quality of water that enters the groundwater reservoir and reactions with minerals and organic matter of the aquifer matrix may modify the water quality. Apart from natural processes as controlling factors on the groundwater quality, in recent years the effect of pollution, such as nitrate from fertilizers and acid rain, also influences the groundwater chemistry. Due to the long residence time of groundwater in the invisible subsurface environment, the effect of pollution may first become apparent tens to hundreds of years afterwards. A proper understanding of the processes occurring in aquifers is required in order to predict what the effect of present day human activities will be on that scale. This book presents a comprehensive and quantitative approach to the study of groundwater quality. Practical examples of application are presented throughout the text.",1993,0,4138,332,2,6,3,11,20,38,51,59,79,82
1cb1946049da4cff926028e4821d7cdb25d68f41,"Prentice Hall, 1997. Book Condition: New. Brand New, Unread Copy in Perfect Condition. A+ Customer Service! Summary: 1. Thermochemical Principles. 2. Chemical Kinetics. 3. Aqueous Complexes. 4. Activity Coefficients of Dissolved Species. 5. Acids and Bases. 6. Carbonate Chemistry. 7. Chemical Weathering. 8. General Controls on Natural Water Chemistry. 9. The Geochemistry of Clay Minerals. 10. Adsorption-Desorption Reactions. 11. Oxidation-Reduction Concepts. 12. Iron and Sulfur Geochemistry. 13. Actinides and Their Daughter and Fission Products. Geochemical Computer Models. References. Index.",1997,0,2359,343,1,10,35,52,47,58,79,113,103,141
60c71b175ad3a2861e2f5ed4b3eda85022f4171f,"This book is written as a reference on organic substances in natural waters and as a supplementary text for graduate students in water chemistry. The chapters address five topics: amount, origin, nature, geochemistry, and characterization of organic carbon. Of these topics, the main themes are the amount and nature of dissolved organic carbon in natural waters (mainly fresh water, although seawater is briefly discussed). It is hoped that the reader is familiar with organic chemistry, but it is not necessary. The first part of the book is a general overview of the amount and general nature of dissolved organic carbon. Over the past 10 years there has been an exponential increase in knowledge on organic substances in water, which is the result of money directed toward the research of organic compounds, of new methods of analysis (such as gas chromatography and mass spectrometry), and most importantly, the result of more people working in this field. Because of this exponential increase in knowledge, there is a need to pull together and summarize the data that has accumulated from many disciplines over the last decade.",1985,789,2700,281,1,6,32,35,45,51,49,58,40,51
569114559b424f536ef2133c2627740109ffc012,"Analytical data for Sr, Rb, Cs, Ba, Pb, rare earth elements, Y, Th, U, Zr, Hf, Sn, Nb, Mo, Ni, Co, V, Cr, Sc, Cu and major elements are reported for eocene volcanic rocks cropping out in the Kastamonu area, Pontic chain of Northern Turkey. SiO2% versus K2O% relationship shows that the analyzed samples belong to two major groups: the basaltic andesitic and the andesitic ones. High-K basaltic andesites and low-K andesites occur too. Although emplaced on continental type basement (the North Anatolian Crystalline Swell), the Pontic eocene volcanics show elemental abundances closely comparable with typical island arc calc-alkaline suites, e.g. low SiO2% range, low to moderate K2O% and large cations (Cs, Rb, Sr, Ba, Pb) contents and REE patterns with fractionated light and almost flat heavy REE patterns. ΣREE and highly charged cations (Th, U, Hf, Sn, Zr) are slightly higher than typical calc-alkaline values. Ferromagnesian elements show variable values. Within the basaltic andesite group the increase of K%, large cations, ΣREE, La/Yb ratio and high valency cations and the decrease of ferromagnesian element abundances with increasing SiO2% content indicate that the rock types making up this group developed by crystalliquid fractionation of olivine and clinopyroxene from a basic parent magma. Trace element concentration suggest that the andesite group was not derived by crystal-liquid fractionation processes from the basaltic andesites, but could represent a distinct group of rocks derived from a different parent magma.",1976,18,3798,98,0,0,3,7,9,9,9,7,20,8
416e13e8326f25de552aa6b49944252ca32d31e7,"Abstract We report analyses of the176Hf/177Hf ratio for 25 chondrites from different classes of meteorites (C, O, and E) and the176Lu/177Hf ratio for 23 of these as measured by plasma source mass spectrometry. We have obtained a new set of present-day mean values in chondrites of176Hf/177Hf= 0.282772 ± 29 and176Lu/177Hf= 0.0332 ± 2. The176Hf/177Hf ratio of the Solar System material 4.56 Ga ago was 0.279742 ± 29. Because the mantle array lies above the Bulk Silicate Earth in a143Nd/144Nd versus176Hf/177Hf plot, no terrestrial basalt seems to have formed from a primitive undifferentiated mantle, thereby casting doubt on the significance of high3He/4He ratios. Comparison of observedHf/Nd ratios with those inferred from isotopic plots indicates that, in addition to the two most prominent components at the surface of the Earth, the depleted mantle and the continental crust, at least one more reservoir, which is not a significant component in the source of oceanic basalts, is needed to account for the Bulk Silicate Earth Hf-Nd geochemistry. This unaccounted for component probably consists of subducted basalts, representing ancient oceanic crust and plateaus. The lower continental crust and subducted pelagic sediments are found to be unsuitable candidates. Although it would explain the Lu-Hf systematics of oceanic basalts, perovskite fractionation from an early magma ocean does not account for the associated Nd isotopic signature. Most basalts forming the mantle array tap a mantle source which corresponds to residues left by ancient melting events with garnet at the liquidus.",1997,62,2481,277,2,9,11,13,13,26,22,27,36,50
c0f22604d7137aabe1d6d0f03499dbbcd0635260,"The development of petroleum geochemistry and geology carbon and origin of life petroleum and its products how oil forms - natural hydrocarbons how oil forms - generated hydrocarbons modeling petroleum generation the origin of natural gas migration and accumulation abnormal pressures the source rock coals, shales, and other terrestrial source rocks petroleum in the reservoir seeps and surface prospecting a geochemical program for petroleum exploration crude oil correlation prospect evaluation.",1995,0,2405,208,41,18,35,45,48,39,41,53,56,52
055cda652a732def8f3fe95ec165bc8c17600c4a,"Abstract With the aim to link zircon composition with paragenesis and thus metamorphic conditions, zircons from eclogite- and granulite-facies rocks were analysed for trace elements using LA-ICP-MS and SHRIMP ion microprobe. Metamorphic zircons from these different settings display a large variation in trace element composition. In the granulites, zircon overgrowths formed in equilibrium with partial melt and are similar to magmatic zircon in terms of high Y, Hf and P content, steep heavy-enriched REE pattern, positive Ce anomaly and negative Eu anomaly. They are distinguishable from magmatic zircon because of their low Th/U ratio. Independently of whole rock composition, metamorphic zircon domains in eclogite-facies rocks have low Th/U ratio and reduced HREE enrichment and Eu anomaly. In a low grade metamorphic vein, zircon has low Th/U ratio but is extremely enriched in Y, Nb and HREE. Petrological and geochronological data demonstrate that metamorphic zircon overgrowths crystallised at granulite-facies conditions in equilibrium with unzoned garnet. It is thus possible for the first time to calculate trace element distribution coefficients between zircon and garnet. Hf is the elements that most strongly partition into zircon. Y, Nb and REE have distribution coefficients between 90 and 0.9 with minimum values for the MREE. In eclogite-facies rocks, the HREE depletion in metamorphic zircon domains is attributed to concurrent formation of garnet under sub-solidus conditions. In one sample, the zircon/garnet trace elements partitioning indicates that metamorphic zircon formed in equilibrium with the garnet rim, i.e. at the eclogitic peak. The reduced Eu anomaly in the metamorphic zircon is interpreted as indicating absence of feldspars and thus supports zircon formation in eclogite facies. In a metamorphic vein within the eclogite-facies rocks, zircons have larger Eu anomaly with respect to high-pressure zircon. Together with geochronological evidence, the Eu anomaly suggests that these zircons formed during prograde metamorphism, before the break down of feldspars at high pressure. The REE composition of zircon can therefore relate zircon formation to specific metamorphic stages such as eclogite, granulite or greenschist facies. This allows linking zircon U–Pb ages with pressure–temperature conditions, a fundamental step in constraining rates of metamorphic processes.",2002,46,1938,280,4,30,34,36,34,47,58,68,70,93
88e5fa11e77f8b9d37dedf455df04edb2d0ad25a,"Basaltic volcanism 'samples' the Earth's mantle to great depths, because solid-state convection transports deep material into the (shallow) melting region. The isotopic and trace-element chemistry of these basalts shows that the mantle contains several isotopically and chemically distinct components, which reflect its global evolution. This evolution is characterized by upper-mantle depletion of many trace elements, possible replenishment from the deeper, less depleted mantle, and the recycling of oceanic crust and lithosphere, but of only small amounts of continental material.",1997,109,2174,166,12,34,55,43,52,64,78,89,77,71
b912f87e69796129dc4d0723a516ad5ad87c1de7,"1. The Hydrologic Cycle. 2. Chemical Background. 3. Organic Compounds in Natural Waters. 4. The Carbonate System and pH Control. 5. Clay Minerals and Ion Exchange. 6. Stability Relationships and Silicate Equilibria. 7. Kinetics. 8. Weathering and Water Chemistry, I: Principles. 9. Weathering and Water Chemistry, II: Examples. 10. Acid Deposition and Surface Water Chemistry. 11. Evaporation and Saline Waters. 12. The Oceans. 13. Redox Equilibria. 14. Redox Conditions in Natural Waters. 15. Trace Elements. 16. Mathematical and Numerical Models. 17. Isotopes. Appendices.",1988,0,2390,124,20,24,45,33,52,51,52,59,47,69
ff80f7f75ee30403b2a9a23bde406b08c9214901,Theoretical and Experimental Principles.- Isotope Fractionation Processes of Selected Elements.- Variations of Stable Isotope Ratios in Nature.,1973,0,1930,161,0,0,0,2,0,0,2,0,4,2
f01c83c9df2f71b1471bc154ee0964a7d6050b3c,"Partial table of contents: Hydrothermal Mineral Deposits: What We Do and Don't Know (B. Skinner). Magmas and Hydrothermal Fluids (C. Burnham). Thermal Aspects of Ore Formation (L. Cathles). Oxygen and Hydrogen Isotope Relationships in Hydrothermal Mineral Deposits (H. Taylor). Hydrothermal Alteration and Its Relationship to Ore Fluid Composition (M. Reed). Sulfide Ore Mineral Stabilities, Morphologies, and Intergrowth Textures (D. Vaughan & J. Craig). Gangue Mineral Transport and Deposition (J. Rimstidt). Fluid Inclusion Studies of Hydrothermal Ore Deposits (E. Roedder & R. Bodnar). Geothermal Systems and Mercury Deposits (H. Barnes & T. Seward). Submarine Hydrothermal Systems and Deposits (S. Scott). Ore--Forming Brines in Active Continental Rifts (M. McKibben & L. Hardie). Appendix. Index.",1968,0,2362,22,1,4,3,7,2,6,10,6,3,8
965bedad12934e33dcb7164a3abe3f289d62a112,"The factors affecting the amounts and types of organic matter in lacustrine sediments are summarized in this review, and synthesis, of published studies. Biota hving in the lake and in its watershed are the sources of the organic compounds initially contributed to the lake system. Microbial reworking of these materials during sinking and early sedimentation markedly diminishes the total amount of organic matter while replacing many of the primary compounds with secondary ones. Much of the organic matter content of sediments is the product of this microbial reprocessing. Various organic matter components of lake sediments nonetheless retain source information and thereby contribute to the paleohmnological record. Carbon/nitrogen ratios of total organic matter reflect original proportions of algal and land- derived material. Carbon isotopic compositions indicate the history of lake productivity and carbon recycling. Biomarker compounds provide important information about contributions from different biota. Sterol compositions and chainlength distributions of n-alkanes, n-alkanoic acids, and n-alkanols help distinguish different algal and watershed sources and also record diagenetic alterations. Stabilization of functional-group-containing biomarkers by conversion into saturated or aromatic hydrocarbons or by incorporation into bound forms improves their preservation and hence record of source information. Lignin components provide important evidence of watershed plant cover, and pigments reflect algal assemblages. The interplay of the factors influencing the organic matter content of lake sediments is illustrated by overviews of sedimentary records of four lake systems--Lake Biwa (Japan), Lake Greifen (Switzerland), Lake Washington (Pacific Northwest), and the Great Lakes (American Midwest).",1993,172,1472,104,0,3,8,13,8,14,16,22,15,25
e1ca0329e5e1489a913646e10146d34ac1bbffc4,"Chemical equilibrium aqueous solutions solution-mineral equilibria silicates crystal chemistry surface chemistry surface chemistry - the solution-mineral interface chemical thermodynamics chemical thermodynamics - phase equilibria oxidation and reduction siotope geochemistry reaction rates and mass transfer the fluid envelopes weathering and soils sedimentation and diagenesis - inorganic geochemistry, organic geochemistry metamorphism formation and crystallization of magmas volatiles and magmas hydrothermal ore deposits distribution of the elements historical geochemistry.",1967,0,1833,75,1,4,4,9,9,17,18,10,23,12
7d46fb931847cfeb88768ffd63be4d0ecf23b911,1. The Hydrologic Cycle. 2. Chemical Background. 3. The Carbonate System and pH Control 4. Clay Minerals and Cation Exchange. 5. Adsorption. 6. Organic Compounds in Natural Waters. 7. Redox Equilibria. 8. Redox Conditions in Natural Waters. 9. Heavy Metals and Metalloids. 10. Stability Relationships and Silicate Equilibria. 11. Kinetics. 12. Weathering and Water Chemistry. 13. Acid Water. 14. Isotopes. 15. Evaporation and Saline Waters. 16. Transport and Reaction Modeling References. Glossary of Geologic Terms. Appendix I: Piper and Stiff Diagrams. Appendix II: Standard-State Thermodynamic Data for Some Common Species. Appendix III: Equilibrium Constants at 25 C and Enthalpies of Reaction for Selected Reactions. Answers to Problems. Author Index. Subject Index.,1997,0,1214,176,1,3,11,20,20,37,31,44,63,59
1095964c675aab8a15b163c4278b7bb93d48cb61,"The organic matter content of lake sediments contains information that helps to reconstruct past environmental conditions, evaluate histories of climate change, and assess impacts of humans on local ecosystems. The elemental, isotopic, and molecular compositions of organic matter buried in sediment provide evidence of the biota that have lived in a lake and its catchment area, and they serve as proxies of organic matter delivery and accumulation. Sedimentary records from the North American Great Lakes provide examples of applications of organic geochemistry to paleolimnological reconstructions. The records of these lakes date from retreat of the Laurentide ice sheet around 12 ka, include the mid-Holocene Hypsithermal, and show consequences of recent human changes. Low Corg/Ntotal ratios indicate that most of the sediment organic matter in the Great Lakes is from algal production, yet changes in biomarker molecule compositions also show that varying amounts of land-plant organic matter have been delivered to the lakes. Elevated algal productivity that accompanies nutrient enrichments of lake waters is recorded as excursions to less negative δ13C values in the organic matter of sediments that were deposited in the 1960s and 1970s. Increased organic carbon mass accumulation rates mirror the isotopic excursions in most parts of the Great Lakes. Accumulations of petroleum residues and pyrogenic polycyclic aromatic hydrocarbons in sediments identify fluvial and eolian delivery of organic matter components to different parts of the Great Lakes. Emerging applications of multiple and novel organic geochemical proxies to paleolimnological reconstructions are promising, yet some potentially important measurements remain underutilized.",2003,166,1124,146,1,11,15,25,25,36,43,46,60,52
38e78cce7ecb5e0ff2b004235c92ac61b68b316c,,1984,0,1535,35,0,3,15,10,12,9,4,13,19,16
c06fd0f58ae6ec7aadbfde7611d0a358817d2d02,,1958,0,1895,78,5,7,7,7,9,13,12,11,10,14
492d00b32087f7aeb59fc703113fff48f0cfc60f,"Abstract The distributions of certain minor and trace elements in marine sediments should potentially provide forensic tools for determining the redox conditions of the bottom waters at the time of deposition. The ability to identify such conditions in the geological past is important because (1) current models of the conditions of formation of organic-rich rocks require reexamination, (2) a method to determine whether the areal extent of anoxic waters expanded or retracted in response to palaeoceanographic changes is required, and (3) the effects of such environmental changes on the geochemical balance of these elements in the ocean need to be understood. Recent research has suggested that some minor and trace elements are precipitated where free dissolved sulphide is present (Cu, Cd, Ni, Zn) without undergoing a valency change, whereas others undergo a change in valency and are either more efficiently adsorbed onto solid surfaces under oxic (I) or anoxic (V) conditions or are precipitated under anoxic conditions (Cr, Mn, Mo, Re, U, V). Hence, the enrichment of these minor and trace elements relative to their crustal abundances indicates that the host sediments accumulated under anoxic conditions, although not necessarily under anoxic bottom waters. Examination of the chemical composition of the sediments of anoxic basins, continental margin sediments and oxidized deepsea sediments shows that I and Mn enrichments are reliable indicators of bottom water oxygenation, whereas enrichments of the remaining elements reflect either bottom water anoxia or element uptake by subsurface anoxic sediments below a relatively thin surficial oxic veneer. Hence, the absence of metal enrichment in these cases can be taken as firm evidence that the bottom waters of a basin of sedimentation were not anoxic. These behaviours may be used to propose, for example, that the Holocene sapropel in the Black Sea accumulated under oxic bottom waters, whereas the modern facies reflects its formation under the prevailing intensely anoxic conditions, and that the Panama Basin bottom waters were not anoxic during the Last Glacial Maximum when the rate of accumulation of organic carbon increased. Likewise, the enrichment of Mn as a mixed carbonate phase in some ancient black shales strongly suggests that they formed under oxic bottom waters rather than anoxic conditions as is commonly assumed.",1993,132,1234,78,0,1,6,16,15,10,21,22,26,20
62972351669150c84574853f49519bcf3c9c3e38,"Abstract Several hundred samples of carbon from various geologic sources have been analyzed in a new survey of the variation of the ratio C13/C12 in nature. Mass spectrometric determinations were made on the instruments developed by H. C. Urey and his co-workers utilizing two complete feed systems with magnetic switching to determine small differences in isotope ratios between samples and a standard gas. With this procedure variations of the ratio C13/C12 can be determined with an accuracy of ±0.01% of the ratio. The results confirm previous work with a few exceptions. The range of variation in the ratio is 4.5%. Terrestrial organic carbon and carbonate rocks constitute two well defined groups, the carbonates being richer in C13 by some 2%. Marine organic carbon lies in a range intermediate between these groups. Atmospheric CO2 is richer in C13 than was formerly believed. Fossil wood, coal and limestones show no correlation of C13/C12 ratio with age. If petroleum is of marine organic origin a considerable change in isotopic composition has probably occurred. Such a change seems to have occurred in carbon from black shales and carbonaceous schists. Samples of graphites, diamonds, igneous rocks and gases from Yellowstone Park have been analyzed. The origin of graphite cannot be determined from C13/C12 ratios. The terrestrial distribution of carbon isotopes between igneous rocks and sediments is discussed with reference to the available meteoritic determinations. Isotopic fractionation between iron carbide and graphite in meteorites may indicate the mechanism by which early fractionation between deep seated and surface terrestrial carbon may have occurred.",1953,51,1733,94,3,10,2,4,8,2,9,7,9,8
222e7fe0bbe103490d4f522f199f774b06bbfc89,"Suspended sediments from large and middle size Chinese estuaries, including the Yalujiang, Shuangtaizihe, Luanhe, Jiaojiang and Zhujiang, were analysed to understand trace metal transport in the coastal zone. The determinations of 13 major and trace elements plus organic carbon were made of total concentrations and were fully validated by certified reference materials (CRMs). The combination of the data sets with other Chinese estuaries, such as Changjiang and Huanghe, provides an overview of particulate trace metal geochemistry in this region. Trace metal levels in Chinese rivers are relatively low compared with those draining industrialized regions of Europe and North America. In the estuaries, most particulate elements illustrate stable distribution in the mixing zone until a salinity of 30, especially when absolute concentrations are normalized to aluminium, although the total suspended matter (TSM) is quite different in time and space. Using Al as a reference, it was estimated that 25–40% for Cu, and 5–20% for Pb could remain in labile part in the Jiaojiang, Shuangtaizihe and Zhujiang, whereas different features of labile elements were found in the Changjiang and Luanhe. The mean enrichment factor (EFm) increases with higher sewage to river runoff ratio (S/R) over the drainage basin and EFm for suspended matter is higher than that for bottom sediments. Finally, inputs of particulate trace metals to the coast are estimated based on the riverine sediment load and chemical compositions.",2002,37,839,240,0,0,1,1,10,13,12,10,21,44
c1445d3bb3728189cc10750efc64c4f53b1978fb,"Submission information at the series homepage and springer.com/authors Order online at springer.com ▶ or for the Americas call (toll free) 1-800-SPRINGER ▶ or email us at: customerservice@springer.com. ▶ For outside the Americas call +49 (0) 6221-345-4301 ▶ or email us at: customerservice@springer.com. Handbook of Geochemistry Editor-in-chief: K.H. Wedepohl Series Editors: C.W. Correns, D.M. Shaw, K.K. Turekian, J. Zemann",1969,0,1338,17,0,4,3,14,6,6,8,8,10,10
987014db9f8a7826065a53ac34009e43d3291b50,"C.A.J. APPELO and D. POSTMA (ed.) A.A. Balkema Publishers, Leiden, The Netherlands. 2005. 2nd ed. Hardcover, 649 pp. $43.95. ISBN 0-41-536428-0.

This book presents important fundamental concepts and current knowledge of groundwater geochemistry and the interaction of water, minerals, gases,",2006,0,681,88,13,22,18,15,21,30,39,32,39,25
f61ddae58a261329045df077a5b19b822e9cc1bc,Preface. The CO2-Carbonic Acid System and Solution Chemistry. Interactions Between Carbonate Minerals and Solutions. Coprecipitation Reactions and Solid Solutions of Carbonate Minerals. The Oceanic Carbonate System and Calcium Carbonate Accumulation in Deep Sea Sediments. Composition and Source of Shoal-Water Carbonate Sediments. Early Marine Diagenesis of Shoal-Water Carbonate Sediments. Early Non-Marine Diagenesis of Sedimentary Carbonates. Carbonates as Sedimentary Rocks in Subsurface Processes. Current Carbon Cycle and Human Impact. Sedimentary Carbonates in the Evolution of Earth's Surface Environment. References. Index.,1990,0,1086,84,0,3,10,29,24,23,24,32,25,32
66ac28f0807c399b0c309bbe6fc30065ea6c167e,,2006,0,658,101,2,10,23,27,30,46,42,66,52,51
5b71c8f5408fed955ab454a871f6ad8af7a0d0eb,"Abstract We analyzed the redox sensitive elements V, Mo, U, Re and Cd in surface sediments from the Northwest African margin, the U.S. Northwest margin and the Arabian Sea to determine their response under a range of redox conditions. Where oxygen penetrates 1 cm or less into the sediments, Mo and V diffuse to the overlying water as Mn is reduced and remobilized. Authigenic enrichments of U, Re and Cd are evident under these redox conditions. With the onset of sulfate reduction, all of the metals accumulate authigenically with Re being by far the most enriched. General trends in authigenic metal accumulation are described by calculating authigenic fluxes for the 3 main redox regimes: oxic, reducing where oxygen penetrates ≤1 cm, and anoxic conditions. Using a simple diagenesis model and global estimates of organic carbon rain rate and bottom water oxygen concentrations, we calculate the area of sediments below 1000 m water depth in which oxygen penetration is ≤1 cm to be 4% of the ocean floor. We conclude that sediments where oxygen penetrates ≤1 cm release Mn, V and Mo to seawater at rates of 140%–260%, 60%–150% and 5%–10% of their respective riverine fluxes, using the authigenic metal concentrations and accumulation rates from this work and other literature. These sediments are sinks for Re, Cd and U, with burial fluxes of 70%–140%, 30%–80% and 20%–40%, respectively, of their dissolved riverine inputs. We modeled the sensitivity of the response of seawater Re, Cd and V concentrations to changes in the area of reducing sediments where oxygen penetrates ≤1 cm. Our analysis suggests a negligible change in seawater Re concentration, whereas seawater concentrations of Cd and V could have decreased and increased, respectively, by 5%–10% over 20 kyr if the area of reducing sediments increased by a factor of 2 and by 10%–20% if the area increased by a factor of 3. The concentration variations for a factor of 2 increase in the area of reducing sediments are at about the level of uncertainty of Cd/Ca and V/Ca ratios observed in foraminifera shells over the last 40 kyr. This implies that the area of reducing sediments in the ocean deeper than 1000 m (4%) has not been greater than twice the present value in the recent past.",1999,84,906,66,0,7,11,19,18,24,26,27,20,34
504b24a796b2ae23203912cc6d37af7eaf5681cc,"A comprehensive review of the single and sequential extraction schemes for metal fractionation in environmental samples such as soil and industrially contaminated soils, sewage sludge and sludge amended soils, road dust and run off, waste and miscellaneous materials along with other approaches of sequential extraction methods are being presented. A discussion on the application of chemometric methods in sequential extraction analysis is also being given. The study of single and sequential extraction methods for various reference materials are also being looked into. The review covers several aspects of the single and sequential extraction methodologies. The use of each reagents involved in these schemes are also discussed briefly. Finally the present upto date information by different workers in various fields of environmental geochemistry along with the possible future developments are also being outlined.",2008,286,416,28,3,13,17,26,32,39,41,34,39,39
c61aa5ed1fd4c1ebef2798cb7efa828054844a1b,"Concentrations of naturally occurring arsenic in ground water vary regionally due to a combination of climate and geology. Although slightly less than half of 30,000 arsenic analyses of ground water in the United States were 1 μg/L, about 10% exceeded 10 μg/L. At a broad regional scale, arsenic concentrations exceeding 10 μg/L appear to be more frequently observed in the western United States than in the eastern half. Arsenic concentrations in ground water of the Appalachian Highlands and the Atlantic Plain generally are very low ( 1 μg/L). Concentrations are somewhat greater in the Interior Plains and the Rocky Mountain System. Investigations of ground water in New England, Michigan, Minnesota, South Dakota, Oklahoma, and Wisconsin within the last decade suggest that arsenic concentrations exceeding 10 μg/L are more widespread and common than previously recognized. 
 
Arsenic release from iron oxide appears to be the most common cause of widespread arsenic concentrations exceeding 10 μg/L in ground water. This can occur in response to different geochemical conditions, including release of arsenic to ground water through reaction of iron oxide with either natural or anthropogenic (i.e., petroleum products) organic carbon. Iron oxide also can release arsenic to alkaline ground water, such as that found in some felsic volcanic rocks and alkaline aquifers of the western United States. Sulfide minerals are both a source and sink for arsenic. Geothermal water and high evaporation rates also are associated with arsenic concentrations 10g/L in ground and surface water, particularly in the west.",2000,138,866,55,1,5,23,24,28,45,47,40,61,48
d999add354aceccbe8b815949a77dfbee53a5f9b,"Clumped isotope geochemistry is concerned with the state of ordering of rare isotopes in natural materials. That is, it examines the extent to which rare isotopes (D, ^(13)C, ^(15)N, ^(18)O, etc.) bond with or near each other rather than with the sea of light isotopes in which they swim. Abundances of isotopic ‘clumps’ in natural materials are influenced by a wide variety of factors. In most cases, their concentrations approach (within ca. 1%, relative) the amount expected for a random distribution of isotopes. Deviations from this stochastic distribution result from: enhanced thermodynamic stability of heavy-isotope ‘clumps’; slower kinetics of reactions requiring the breakage of bonds between heavy isotopes; the mass dependence of diffusive and thermo-gravitational fractionations; mixing between components that differ from one another in bulk isotopic composition; biochemical and photochemical fractionations that may reflect combinations of these simpler physical mechanisms; and, in some cases, other processes we do not yet understand. Although clumped isotope geochemistry is a young field, several seemingly promising applications have already emerged. Most importantly, it appears that proportions of ^(13)C–^(18)O bonds in carbonate minerals are sensitive to their growth temperatures, independent of bulk isotopic composition. Thus, ‘clumped isotope’ analysis of ancient carbonates can be used as a quantitative paleothermometer that requires no assumptions about the δ^(18)O of waters from which carbonates grew. This approach has been used to reconstruct marine temperatures across the Phanerozoic (reaching back to the Silurian), terrestrial ground temperatures across the Cenozoic, thermal histories of aqueously altered meteorites, among other applications. Clumped isotope geochemistry is also placing new constraints on the atmospheric budget and stratospheric photochemistry of CO_2, and should be capable of placing analogous new constraints on the budgets of other atmospheric gases. Finally, this field could be extended to encompass sulfates, volatile hydrocarbons, organic moieties and other materials.",2007,75,515,41,0,2,9,13,27,25,34,56,40,53
af10426046ca1410ed652a64ca6c1976ac1e519f,"This document serves as an update of the North American Society for Pediatric Gastroenterology, Hepatology, and Nutrition (NASPGHAN) and the European Society for Pediatric Gastroenterology, Hepatology, and Nutrition (ESPGHAN) 2009 clinical guidelines for the diagnosis and management of gastroesophageal reflux disease (GERD) in infants and children and is intended to be applied in daily practice and as a basis for clinical trials. Eight clinical questions addressing diagnostic, therapeutic and prognostic topics were formulated. A systematic literature search was performed from October 1, 2008 (if the question was addressed by 2009 guidelines) or from inception to June 1, 2015 using Embase, MEDLINE, the Cochrane Database of Systematic Reviews and the Cochrane Central Register of Controlled Clinical Trials. The approach of the Grading of Recommendations Assessment, Development and Evaluation (GRADE) was applied to define and prioritize outcomes. For therapeutic questions, the quality of evidence was also assessed using GRADE. Grading the quality of evidence for other questions was performed according to the Quality Assessment of Studies of Diagnostic Accuracy (QUADAS) and Quality in Prognostic Studies (QUIPS) tools. During a three-day consensus meeting, all recommendations were discussed and finalized. In cases where no randomized controlled trials (RCT; therapeutic questions) or diagnostic accuracy studies were available to support the recommendations, expert opinion was used. The group members voted on each recommendation, using the nominal voting technique.With this approach, recommendations regarding evaluation and management of infants and children with GERD to standardize and improve quality of care were formulated. Additionally, two algorithms were developed, one for infants < 12 months of age and the other for older infants and children.",2009,963,407,2,0,0,0,0,0,0,1,62,96,55
e8d197a8bc142117006fcc060eed75af52998dec,"The discovery of a series of genetic and serological markers associated with disease susceptibility and phenotype in inflammatory bowel disease has led to the prospect of an integrated classification system involving clinical, serological and genetic parameters. The Working Party has reviewed current clinical classification systems in Crohn's disease, ulcerative colitis and indeterminate colitis, and provided recommendations for clinical classification in practice. Progress with respect to integrating serological and genetic markers has been examined in detail, and the implications are discussed. While an integrated system is not proposed for clinical use at present, the introduction of a widely acceptable clinical subclassification is strongly advocated, which would allow detailed correlations among serotype, genotype and clinical phenotype to be examined and confirmed in independent cohorts of patients and, thereby, provide a vital foundation for future work.",2005,347,2653,81,3,30,80,114,119,155,156,152,172,201
a80b9786319a6e16a904f103d5daf965f4df3335,"This document is the first update of the American College of Gastroenterology (ACG) colorectal cancer (CRC) screening recommendations since 2000. The CRC screening tests are now grouped into cancer prevention tests and cancer detection tests. Colonoscopy every 10 years, beginning at age 50, remains the preferred CRC screening strategy. It is recognized that colonoscopy is not available in every clinical setting because of economic limitations. It is also realized that not all eligible persons are willing to undergo colonoscopy for screening purposes. In these cases, patients should be offered an alternative CRC prevention test (flexible sigmoidoscopy every 5–10 years, or a computed tomography (CT) colonography every 5 years) or a cancer detection test (fecal immunochemical test for blood, FIT).",2009,155,1314,17,21,88,110,150,153,163,135,133,120,77
126913d7e19bf80ea31f20172cd7629e8728a287,"Research on hepatic encephalopathy is hampered by the imprecise definition of this disabling complication of liver disease. Under this light, the Organisation Mondiale de Gastroentérologie commissioned a Working Party to reach a consensus in this area and to present it at the 11th World Congress of Gastroenterology in Vienna (1998). The Working Party continued its work thereafter and now present their final report. In summary, the Working Party has suggested a modification of current nomenclature for clinical diagnosis of hepatic encephalopathy; proposed guidelines for the performance of future clinical trials in hepatic encephalopathy; and felt the need for a large study to redefine neuropsychiatric abnormalities in liver disease, which would allow the diagnosis of minimal (subclinical) encephalopathy to be made on firm statistical grounds. In the interim, it proposes the use of a psychometric hepatic encephalopathy score, based on the result of 5 neuropsychologic tests. Finally, the need for a careful evaluation of the newer neuroimaging modalities for the diagnosis of hepatic encephalopathy was stressed.",2002,35,1822,42,6,32,30,50,50,60,78,79,112,123
01e81325095bfe4e9d371941314b9a0974532749,"Helicobacter pylori (H. pylori) remains a prevalent, worldwide, chronic infection. Though the prevalence of this infection appears to be decreasing in many parts of the world, H. pylori remains an important factor linked to the development of peptic ulcer disease, gastric malignanc and dyspeptic symptoms. Whether to test for H. pylori in patients with functional dyspepsia, gastroesophageal reflux disease (GERD), patients taking nonsteroidal antiinflammatory drugs, with iron deficiency anemia, or who are at greater risk of developing gastric cancer remains controversial. H. pylori can be diagnosed by endoscopic or nonendoscopic methods. A variety of factors including the need for endoscopy, pretest probability of infection, local availability, and an understanding of the performance characteristics and cost of the individual tests influences choice of evaluation in a given patient. Testing to prove eradication should be performed in patients who receive treatment of H. pylori for peptic ulcer disease, individuals with persistent dyspeptic symptoms despite the test-and-treat strategy, those with H. pylori-associated MALT lymphoma, and individuals who have undergone resection of early gastric cancer. Recent studies suggest that eradication rates achieved by first-line treatment with a proton pump inhibitor (PPI), clarithromycin, and amoxicillin have decreased to 70–85%, in part due to increasing clarithromycin resistance. Eradication rates may also be lower with 7 versus 14-day regimens. Bismuth-containing quadruple regimens for 7–14 days are another first-line treatment option. Sequential therapy for 10 days has shown promise in Europe but requires validation in North America. The most commonly used salvage regimen in patients with persistent H. pylori is bismuth quadruple therapy. Recent data suggest that a PPI, levofloxacin, and amoxicillin for 10 days is more effective and better tolerated than bismuth quadruple therapy for persistent H. pylori infection, though this needs to be validated in the United States.",2007,217,1255,50,9,45,66,90,102,114,125,128,128,138
52b7bf3ba59b31f362aa07f957f1543a29a4279e,The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.,1995,26,32227,2921,0,0,0,0,0,0,0,0,0,0
0b7c3ad049492e9337f376ec46aea040598afad6,"An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. We present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each mobile host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic self starting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.",1999,38,12415,1293,0,0,2,1,3,4,3,729,822,786
6716697767fc601efc7690f40820d9ea7a7bf57c,"The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w...",1998,44,13261,1218,0,0,1,0,0,0,1,2,3,210
5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53,"From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.",2000,1,13174,1036,0,1,0,0,0,1,0,6,465,804
6f3a06207ac5a86841e3044e89e2464ed442d74a,"A logging instrument contains a pulsed neutron source and a pair of radiation detectors spaced along the length of the instrument. The radiation detectors are gated differently from each other to provide an indication of formation porosity which is substantially independent of the formation salinity. In the preferred embodiment, the electrical signals indicative of radiation detected by the long-spaced detector are gated for almost the entire interval between neutron pulses and the short-spaced signals are gated for a significantly smaller time interval which commences soon after the termination of a given neutron burst. The signals from the two detectors are combined in a ratio circuit for determination of porosity.",2001,12,12447,1429,2,0,1,2,1,4,694,771,798,873
012f7d74a24ba58e0d965295e4a2eea4dc33531f,"This paper contains the likelihood analysis of vector autoregressive models allowing for cointegration. The author derives the likelihood ratio test for cointegrating rank and finds it asymptotic distribution. He shows that the maximum likelihood estimator of the cointegrating relations can be found by reduced rank regression and derives the likelihood ratio test of structural hypotheses about these relations. The author shows that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian, allowing inference for hypotheses on the cointegrating relation to be conducted using the Chi("" squared"") distribution. Copyright 1991 by The Econometric Society.",1991,50,10591,796,0,0,0,0,0,0,63,189,211,221
06bb5771e6b8a9356c5f4ae28c98b4397c043349,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.",2004,285,8997,719,128,142,184,265,276,392,390,427,421,513
7abeda3a20c13bfee416d94efa313ff870656fec,"Support vector machine (SVM) is a popular technique for classication. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signicant steps. In this guide, we propose a simple procedure, which usually gives reasonable results.",2008,16,6929,438,235,315,354,377,480,536,632,696,614,617
04b23f577c20d1a0e2a67aadda555f58e6d23d6e,"This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.",2008,57,4227,606,42,80,110,160,183,219,261,288,338,414
70e3da6c426ca384f78f77474cbbf00a436038a2,"Contents: Introduction: Differential Equations and Dynamical Systems.- An Introduction to Chaos: Four Examples.- Local Bifurcations.- Averaging and Perturbation from a Geometric Viewpoint.- Hyperbolic Sets, Sympolic Dynamics, and Strange Attractors.- Global Bifurcations.- Local Codimension Two Bifurcations of Flows.- Appendix: Suggestions for Further Reading. Postscript Added at Second Printing. Glossary. References. Index.",1983,1,13387,693,0,0,0,0,0,0,0,1,2,1
40212e9474c3ddf3d8c6ffd13dd3211ec9406c49,"This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.",1998,48,8549,660,26,43,75,120,208,272,332,363,449,391
ccce1cf96f641b3581fba6f4ce2545f4135a15e3,"In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM's. The approach is illustrated on a two-spiral benchmark classification problem.",1999,15,7732,704,0,0,0,16,51,72,104,145,178,238
7c46799502bebfe6a9ae0f457b7b8b92248ec260,"An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",1980,47,7945,298,3,14,27,9,28,44,61,54,102,110
fb6b4b57f431a0cfbb83bb2af8beab4ee694e94c,"DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer.In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98% accurate, while the baseline method is only 86% accurate.",2002,98,5320,484,16,65,82,125,177,224,200,253,247,288
4de39c94e340a108fff01a90a67b0c17c86fb981,"This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm.",1999,0,5914,580,16,31,81,127,154,176,228,332,310,335
8ff61b8e097ccdb784a35b466ba9e130c2502513,"Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.",2003,10,4727,553,36,71,102,134,152,208,190,246,242,241
ecc8fc05c5cc22b86cac987245b840a6c2021f1e,"This book gives a detailed mathematical and statistical analysis of the cointegrated vector autoregresive model. This model had gained popularity because it can at the same time capture the short-run dynamic properties as well as the long-run equilibrium behaviour of many non-stationary time series. It also allows relevant economic questions to be formulated in a consistent statistical framework. Part I of the book is planned so that it can be used by those who want to apply the methods without going into too much detail about the probability theory. The main emphasis is on the derivation of estimators and test statistics through a consistent use of the Guassian likelihood function. It is shown that many different models can be formulated within the framework of the autoregressive model and the interpretation of these models is discussed in detail. In particular, models involving restrictions on the cointegration vectors and the adjustment coefficients are discussed, as well as the role of the constant and linear drift. In Part II, the asymptotic theory is given the slightly more general framework of stationary linear processes with i.i.d. innovations. Some useful mathematical tools are collected in Appendix A, and a brief summary of weak convergence in given in Appendix B. The book is intended to give a relatively self-contained presentation for graduate students and researchers with a good knowledge of multivariate regression analysis and likelihood methods. The asymptotic theory requires some familiarity with the theory of weak convergence of stochastic processes. The theory is treated in detail with the purpose of giving the reader a working knowledge of the techniques involved. Many exercises are provided. The theoretical analysis is illustrated with the empirical analysis of two sets of economic data. The theory has been developed in close contract with the application and the methods have been implemented in the computer package CATS in RATS as a result of a rcollaboation with Katarina Juselius and Henrik Hansen.",1996,0,4586,442,10,26,60,77,80,111,157,158,172,191
7f755d620b57acf27a16ff95923c5677ff8198bb,"Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such ""all-together"" methods. We then compare their performance with three methods based on binary classifications: ""one-against-all,"" ""one-against-one,"" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the ""one-against-one"" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.",2002,32,6276,326,14,47,89,144,215,231,266,332,342,378
d5f169880e30e1f76827d72f862555d00b01bed9,"In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.",1975,7,7590,563,1,3,1,1,3,1,2,2,0,1
c564aa7639a08c280423489e52b6e32055c9aa7f,"1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems.",1991,0,7080,472,6,31,90,141,187,255,235,268,276,315
9c4da62e9e89e65ac78ee271e424e8b498053e8c,"Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.",1999,259,5700,256,38,61,98,150,218,310,375,417,482,488
d611f303ebab48a9cb9e265ba42c538be9f198ba,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.",1994,28,6633,708,3,2,10,21,29,53,76,114,190,261
d68725804eadecf83d707d89e12c5132bf376187,,2001,22,4213,909,1,15,39,78,82,114,132,145,178,200
384bb3944abe9441dcd2cede5e7cd7353e9ee5f7,,1999,0,4915,367,2,12,15,40,55,91,110,158,175,190
bd3a6b06537db6e66aaa2e345ab3af7f307640ad,"This paper shows how we can estimate VAR's formulated in levels and test general restrictions on the parameter matrices even if the processes may be integrated or cointegrated of an arbitrary order. We can apply a usual lag selection procedure to a possibly integrated or cointegrated VAR since the standard asymptotic theory is valid (as far as the order of integration of the process does not exceed the true lag length of the model). Having determined a lag length k, we then estimate a (k + dmax)th-order VAR where dmax is the maximal order of integration that we suspect might occur in the process. The coefficient matrices of the last dmax lagged vectors in the model are ignored (since these are regarded as zeros), and we can test linear or nonlinear restrictions on the first k coefficient matrices using the standard asymptotic theory.",1995,21,4350,546,2,5,8,16,20,30,29,37,35,50
e52fb14e4beccc5e88a33c1fe5c7d6e780831ae1,"A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.",1996,12,3463,285,0,2,6,9,10,12,13,13,30,36
2d86e239a9e9741f22be1d8c1feed7a44da1bdc1,"This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software.",2000,0,3311,280,11,50,97,124,187,245,250,279,280,276
e2bbea031af4e0aab292323c6dcd128050b26540,"From the Publisher: 
Engineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book.",1968,0,5473,352,0,1,8,17,23,39,36,49,46,28
fe84db9e87a513b285ab32147cd901782e66616d,"The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.",1998,77,5674,131,0,2,0,2,93,125,168,193,237,284
00f66c12904a003f46077f444e6e0c64b8115bb0,"Several polycations possessing substantial buffering capacity below physiological pH, such as lipopolyamines and polyamidoamine polymers, are efficient transfection agents per se--i.e., without the addition of cell targeting or membrane-disruption agents. This observation led us to test the cationic polymer polyethylenimine (PEI) for its gene-delivery potential. Indeed, every third atom of PEI is a protonable amino nitrogen atom, which makes the polymeric network an effective ""proton sponge"" at virtually any pH. Luciferase reporter gene transfer with this polycation into a variety of cell lines and primary cells gave results comparable to, or even better than, lipopolyamines. Cytotoxicity was low and seen only at concentrations well above those required for optimal transfection. Delivery of oligonucleotides into embryonic neurons was followed by using a fluorescent probe. Virtually all neurons showed nuclear labeling, with no toxic effects. The optimal PEI cation/anion balance for in vitro transfection is only slightly on the cationic side, which is advantageous for in vivo delivery. Indeed, intracerebral luciferase gene transfer into newborn mice gave results comparable (for a given amount of DNA) to the in vitro transfection of primary rat brain endothelial cells or chicken embryonic neurons. Together, these properties make PEI a promising vector for gene therapy and an outstanding core for the design of more sophisticated devices. Our hypothesis is that its efficiency relies on extensive lysosome buffering that protects DNA from nuclease degradation, and consequent lysosomal swelling and rupture that provide an escape mechanism for the PEI/DNA particles.",1995,4,5631,186,2,16,37,73,124,130,124,152,151,159
74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,"This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.",1999,28,3028,403,4,18,39,49,75,93,109,141,148,163
0e48afa17a9508792c6dabf95c2b3a7171366256,"A retroviral vector system based on the human immunodeficiency virus (HIV) was developed that, in contrast to a murine leukemia virus-based counterpart, transduced heterologous sequences into HeLa cells and rat fibroblasts blocked in the cell cycle, as well as into human primary macrophages. Additionally, the HIV vector could mediate stable in vivo gene transfer into terminally differentiated neurons. The ability of HIV-based viral vectors to deliver genes in vivo into nondividing cells could increase the applicability of retroviral vectors in human gene therapy.",1996,83,4772,131,29,92,122,165,225,191,218,217,218,190
fc4c9a3177f53de98f0ebb2d070629d10f646917,,2005,0,2960,453,55,93,113,104,153,161,189,215,253,214
6434a32dfa090fd5e5ae3809958dd68bacb5839c,"This paper addresses the problem of the classification of hyperspectral remote sensing images by support vector machines (SVMs). First, we propose a theoretical discussion and experimental analysis aimed at understanding and assessing the potentialities of SVM classifiers in hyperdimensional feature spaces. Then, we assess the effectiveness of SVMs with respect to conventional feature-reduction-based approaches and their performances in hypersubspaces of various dimensionalities. To sustain such an analysis, the performances of SVMs are compared with those of two other nonparametric classifiers (i.e., radial basis function neural networks and the K-nearest neighbor classifier). Finally, we study the potentially critical issue of applying binary SVMs to multiclass problems in hyperspectral data. In particular, four different multiclass strategies are analyzed and compared: the one-against-all, the one-against-one, and two hierarchical tree-based strategies. Different performance indicators have been used to support our experimental studies in a detailed and accurate way, i.e., the classification accuracy, the computational time, the stability to parameter setting, and the complexity of the multiclass architecture. The results obtained on a real Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral dataset allow to conclude that, whatever the multiclass strategy adopted, SVMs are a valid and effective alternative to conventional pattern recognition approaches (feature-reduction procedures combined with a classification method) for the classification of hyperspectral remote sensing data.",2004,54,2902,284,4,16,36,61,70,92,102,115,134,135
a8797f1d253c75669d96e6fcceda2be3f8534e1d,"Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.",2002,57,3072,239,13,26,62,79,106,97,125,141,152,161
830107cb2b579b300cb2c2dddae94191bd6d508a,"This paper considers estimation and testing of vector autoregressio n coefficients in panel data, and applies the techniques to analyze the dynamic relationships between wages an d hours worked in two samples of American males. The model allows for nonstationary individual effects and is estimated by applying instrumental variables to the quasi-differenced autoregressive equations. The empirical results suggest the absence of lagged hours in the wage forecasting equation. The results also show that lagged hours is important in the hours equation. Copyright 1988 by The Econometric Society.",1988,0,3435,114,2,3,6,6,7,12,11,14,18,27
53fcc056f79e04daf11eb798a7238e93699665aa,"This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On realworld sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.",1998,23,2786,273,5,13,12,18,43,47,69,70,96,92
58931b142b6c01869cf22f4e74ce967230fe6069,"We have developed a new expression vector which allows efficient selection for transfectants that express foreign genes at high levels. The vector is composed of a ubiquitously strong promoter based on the beta-actin promoter, a 69% subregion of the bovine papilloma virus genome, and a mutant neomycin phosphotransferase II-encoding gene driven by a weak promoter, which confers only marginal resistance to G418. Thus, high concentrations of G418 (approx. 800 micrograms/ml) effectively select for transfectants containing a high vector copy number (greater than 300). We tested this system by producing human interleukin-2 (IL-2) in L cells and Chinese hamster ovary (CHO) cells, and the results showed that high concentrations of G418 efficiently yielded L cell and CHO cell transfectants stably producing IL-2 at levels comparable with those previously attained using gene amplification. The vector sequences were found to have integrated into the host chromosome, and were stably maintained in the transfectants for several months.",1991,0,3872,96,0,2,3,6,10,24,44,67,135,150
4ead4d868721e8b38c266415a1f6201cd147a78a,"SUMMARY This paper is concerned with the representation of a multivariate sample of size n as points P1, P2, ..., PI in a Euclidean space. The interpretation of the distance A(Pi, Pj) between the ith andjth members of the sample is discussed for some commonly used types of analysis, including both Q and R techniques. When all the distances between n points are known a method is derived which finds their co-ordinates referred to principal axes. A set of necessary and sufficient conditions for a solution to exist in real Euclidean space is found. Q and R techniques are defined as being dual to one another when they both lead to a set of n points with the same inter-point distances. Pairs of dual techniques are derived. In factor analysis the distances between points whose co-ordinates are the estimated factor scores can be interpreted as D2 with a singular dispersion matrix.",1966,25,3613,214,3,8,4,12,13,15,22,24,19,24
3367b98c29988f21a7ac06b76297bc925e13c1d9,"The paper describes a general methodology for the fitting of measured or calculated frequency domain responses with rational function approximations. This is achieved by replacing a set of starting poles with an improved set of poles via a scaling procedure. A previous paper (Gustavsen et al., 1997) described the application of the method to smooth functions using real starting poles. This paper extends the method to functions with a high number of resonance peaks by allowing complex starting poles. Fundamental properties of the method are discussed and details of its practical implementation are described. The method is demonstrated to be very suitable for fitting network equivalents and transformer responses. The computer code is in the public domain, available from the first author.",1999,8,2614,290,2,6,13,20,28,56,77,81,127,139
33e938103ecc37e4e9e286d63ac6ef6fd14e0796,"We have developed a new expression vector which allows efficient selection for transfectants that express foreign genes at high levels. The vector is composed of a ubiquitously strong promoter based on the beta-actin promoter, a 69% subregion of the bovine papilloma virus genome, and a mutant neomycin phosphotransferase II-encoding gene driven by a weak promoter, which confers only marginal resistance to G418. Thus, high concentrations of G418 (approx. 800 micrograms/ml) effectively select for transfectants containing a high vector copy number (greater than 300). We tested this system by producing human interleukin-2 (IL-2) in L cells and Chinese hamster ovary (CHO) cells, and the results showed that high concentrations of G418 efficiently yielded L cell and CHO cell transfectants stably producing IL-2 at levels comparable with those previously attained using gene amplification. The vector sequences were found to have integrated into the host chromosome, and were stably maintained in the transfectants for several months.",1991,39,4312,25,0,3,4,4,24,43,85,126,162,206
3ee73f2bf6cd154efd65783be0761cb6b7478196,"Support vector machines (SVMs) are becoming popular in a wide variety of biological applications. But, what exactly are SVMs and how do they work? And what are their most promising applications in the life sciences?",2006,31,2331,231,6,17,37,62,52,72,83,113,118,141
24e6cf0796237f21c780a3f0c996817f57b3a1bd,Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.,2004,24,3497,5,0,0,0,0,0,0,0,0,1,0
69d6c6305c2cc2283f5a8a92316803800ecb6b7f,,1985,0,3075,193,1,0,0,2,3,2,1,4,12,10
c7822d12beec5cfa81c48ced04d34d4ecf2654b5,"1. Introduction 2. The space-phasor model of A.C. machines 3. Vector and direct torque control of synchronous machines 4. Vector and direct torque control of induction machines 5. Torque control of switched reluctance motors 6. Effects of magnetic saturation 7. Artificial intelligence-based steady-state and transient analysis of electrical machines, estimators 8. Self-commissioning Index",1998,0,2402,241,2,7,30,31,57,59,86,84,145,126
8d73c0d0c92446102fdb6cc728b5d69674a1a387,"We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results.",2000,75,2572,207,12,36,28,45,62,96,97,110,102,143
282ec09590a275606dee13a76b269ce3b3c5a88c,"Four new antibiotic-resistant derivatives of the broad-host-range (bhr) cloning vector pBBR1MCS have been constructed. These new plasmids have several advantages over many of the currently available bhr vectors in that: (i) they are relatively small (< 5.3 kb), (ii) they possess an extended multiple cloning site (MCS), (iii) they allow direct selection of recombinant plasmid molecules in Escherichia coli via disruption of the LacZ alpha peptide, (iv) they are mobilizable when the RK2 transfer functions are provided in trans and (v) they are compatible with IncP, IncQ and IncW group plasmids, as well as with ColE1- and P15a-based replicons.",1995,10,3007,143,0,0,4,13,30,43,60,59,68,87
cfc6d0c8260594ebc5dd20ee558d29b1014ed41a,"In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.",2002,30,2169,327,6,16,24,54,69,81,95,126,126,163
28ec1fe81dfc6eebe359898ff79960e24876032e,"Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries. Problems associated with initialization and poor convergence to boundary concavities, however, have limited their utility. This paper presents a new external force for active contours, largely solving both problems. This external force, which we call gradient vector flow (GVF), is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image. It differs fundamentally from traditional snake external forces in that it cannot be written as the negative gradient of a potential function, and the corresponding snake is formulated directly from a force balance condition rather than a variational formulation. Using several two-dimensional (2-D) examples and one three-dimensional (3-D) example, we show that GVF has a large capture range and is able to move snakes into boundary concavities.",1998,41,2804,167,4,12,28,43,55,73,107,117,163,190
88ebd3cd81e9745433fc25d312322cc6b609e4d9,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.",1994,8,3503,64,0,2,2,8,15,29,49,69,97,158
da9df5d83da5f948e6bbc4e8ae3fe144044bf0e7,"Monetary policy and the private sector behaviour of the U.S. economy are modelled as a time varying structural vector autoregression, where the sources of time variation are both the coefficients and the variance covariance matrix of the innovations. The paper develops a new, simple modelling strategy for the law of motion of the variance covariance matrix and proposes an efficient Markov chain Monte Carlo algorithm for the model likelihood/posterior numerical evaluation. The main empirical conclusions are: (1) both systematic and non-systematic monetary policy have changed during the last 40 years—in particular, systematic responses of the interest rate to inflation and unemployment exhibit a trend toward a more aggressive behaviour, despite remarkable oscillations; (2) this has had a negligible effect on the rest of the economy. The role played by exogenous non-policy shocks seems more important than interest rate policy in explaining the high inflation and unemployment episodes in recent U.S. economic history.",2003,62,1733,415,8,7,9,11,31,37,55,62,69,84
9008cdacbdcff8a218a6928e94fe7c6dfc237b24,"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.",1997,20,2850,92,4,41,52,55,77,135,125,184,184,169
d8b68dadb16dc9b95b91d3776f3231c9754d919f,"ABSTRACT Vectors derived from human immunodeficiency virus (HIV) are highly efficient vehicles for in vivo gene delivery. However, their biosafety is of major concern. Here we exploit the complexity of the HIV genome to provide lentivirus vectors with novel biosafety features. In addition to the structural genes, HIV contains two regulatory genes, tat and rev, that are essential for HIV replication, and four accessory genes that encode critical virulence factors. We previously reported that the HIV type 1 accessory open reading frames are dispensable for efficient gene transduction by a lentivirus vector. We now demonstrate that the requirement for the tat gene can be offset by placing constitutive promoters upstream of the vector transcript. Vectors generated from constructs containing such a chimeric long terminal repeat (LTR) transduced neurons in vivo at very high efficiency, whether or not they were produced in the presence of Tat. When the rev gene was also deleted from the packaging construct, expression of gag and pol was strictly dependent on Rev complementation in trans. By the combined use of a separate nonoverlapping Rev expression plasmid and a 5′ LTR chimeric transfer construct, we achieved optimal yields of vector of high transducing efficiency (up to 107transducing units [TU]/ml and 104 TU/ng of p24). This third-generation lentivirus vector uses only a fractional set of HIV genes: gag, pol, and rev. Moreover, the HIV-derived constructs, and any recombinant between them, are contingent on upstream elements and trans complementation for expression and thus are nonfunctional outside of the vector producer cells. This split-genome, conditional packaging system is based on existing viral sequences and acts as a built-in device against the generation of productive recombinants. While the actual biosafety of the vector will ultimately be proven in vivo, the improved design presented here should facilitate testing of lentivirus vectors.",1998,63,2808,91,3,17,65,48,76,85,67,95,111,123
f368f95a533c910cb3ef71c8f4aa45df22a31168,,1964,0,3589,82,0,1,1,3,0,1,3,0,6,5
43ffa2c1a06a76e58a333f2e7d0bd498b24365ca,"The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems.",1996,4,2502,120,3,8,25,19,19,23,33,40,75,75
626a5da1bfc0f4b38be27f867f95daa061655f94,"The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs using a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based on exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental results assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate an improvement of generalization performance.",2002,33,2238,130,26,37,81,112,119,125,146,130,155,105
76f96dadd80b19bde49e0e1f07bfa9fe8485eeec,"From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",2001,0,2663,73,2,10,26,60,70,66,77,81,119,133
f640afd3488eb756451592b93dc5b060852511fd,"The current challenge, now that two plant genomes have been sequenced, is to assign a function to the increasing number of predicted genes. In Arabidopsis, approximately 55% of genes can be assigned a putative function, however, less than 8% of these have been assigned a function by direct experimental evidence. To identify these functions, many genes will have to undergo comprehensive analyses, which will include the production of chimeric transgenes for constitutive or inducible ectopic expression, for antisense or dominant negative expression, for subcellular localization studies, for promoter analysis, and for gene complementation studies. The production of such transgenes is often hampered by laborious conventional cloning technology that relies on restriction digestion and ligation. With the aim of providing tools for high throughput gene analysis, we have produced a Gateway-compatible Agrobacterium sp. binary vector system that facilitates fast and reliable DNA cloning. This collection of vectors is freely available, for noncommercial purposes, and can be used for the ectopic expression of genes either constitutively or inducibly. The vectors can be used for the expression of protein fusions to the Aequorea victoria green fluorescent protein and to the β-glucuronidase protein so that the subcellular localization of a protein can be identified. They can also be used to generate promoter-reporter constructs and to facilitate efficient cloning of genomic DNA fragments for complementation experiments. All vectors were derived from pCambia T-DNA cloning vectors, with the exception of a chemically inducible vector, for Agrobacterium sp.-mediated transformation of a wide range of plant species.",2003,31,2347,144,1,5,17,45,88,79,92,117,128,179
8da26b27d092807f704a91446494cc1a53ad2d15,"This paper describes the joint dynamics of bond yields and macroeconomic variables in a Vector Autoregression, where identifying restrictions are based on the absence of arbitrage. Using a term structure model with inflation and economic growth factors, we investigate how macro variables affect bond prices and the dynamics of the yield curve. The setup accommodates higher order autoregressive lags for the macro factors. The macro variables are augmented by traditional unobserved term structure factors. We find that the forecasting performance of a VAR improves when no-arbitrage restrictions are imposed. Models that incorporate macro factors forecast better than traditional term structure models with only unobservable factors. Variance decompositions show that macro factors explain up to 85% of the variation in bond yields. Macro factors primarily explain movements at the short end and middle of the yield curve while unobservable factors still account for most of the movement at the long end of the yield curve.",2001,109,1808,369,11,12,24,39,52,94,90,124,110,111
81a5952532cdd48eec5e3dc326907c36a70e0a24,"A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.",1984,88,2920,82,6,21,35,33,75,77,80,124,151,124
90902e16f4e8ff5e3c4bf0f971380af5753aacdd,Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.,2004,40,1711,226,5,17,19,34,49,81,69,95,89,117
33dedd088742944d3f7bc1159f296123a66a9f15,"Designer Stem Cells Despite their promise for use as disease models and in regenerative medicine, the generation of human-induced pluripotent stem (iPS) cells has been hindered by the integration of vector and transgenes in the host cell genome. Recent studies using the Cre/LoxP recombination strategy and the piggyBac transposon approach have approached this objective. However, Yu et al. (p. 797, published online 26 March) now show the derivation of human iPS cells from postnatal foreskin fibroblasts using the nonintegrating oriP/EBNA1-based episomal vectors. The resultant iPS cells show characteristics of human embryonic stem cells and are free of vector and transgenes. Human induced pluripotent stem cells can be generated without integration of exogenous DNA into their genomes. Reprogramming differentiated human cells to induced pluripotent stem (iPS) cells has applications in basic biology, drug development, and transplantation. Human iPS cell derivation previously required vectors that integrate into the genome, which can create mutations and limit the utility of the cells in both research and clinical applications. We describe the derivation of human iPS cells with the use of nonintegrating episomal vectors. After removal of the episome, iPS cells completely free of vector and transgene sequences are derived that are similar to human embryonic stem (ES) cells in proliferative and developmental potential. These results demonstrate that reprogramming human somatic cells does not require genomic integration or the continued presence of exogenous reprogramming factors and removes one obstacle to the clinical application of human iPS cells.",2009,43,2178,33,118,222,259,251,239,216,174,175,131,120
22ec7d2f1d4f802a135e0ec6611e0c508f1fca47,"This chapter presents the most basic results on topological vector spaces. With the exception of the last section, the scalar field over which vector spaces are defined can be an arbitrary, non-discrete valuated field K; K is endowed with the uniformity derived from its absolute value. The purpose of this generality is to clearly identify those properties of the commonly used real and complex number field that are essential for these basic results. Section 1 discusses the description of vector space topologies in terms of neighborhood bases of 0, and the uniformity associated with such a topology. Section 2 gives some means for constructing new topological vector spaces from given ones. The standard tools used in working with spaces of finite dimension are collected in Section 3, which is followed by a brief discussion of affine subspaces and hyperplanes (Section 4). Section 5 studies the extremely important notion of boundedness. Metrizability is treated in Section 6. This notion, although not overly important for the general theory, deserves special attention for several reasons; among them are its connection with category, its role in applications in analysis, and its role in the history of the subject (cf. Banach [1]). Restricting K to subfields of the complex numbers, Section 7 discusses the transition from real to complex fields and vice versa.",1967,0,3172,35,8,7,37,36,42,60,57,77,68,56
529cf7a716e6c9da99c6a468730f22398f75c1a4,"A real-time obstacle avoidance method for mobile robots which has been developed and implemented is described. This method, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously with range data sampled by onboard range sensors. The VFH method subsequently uses a two-stage data-reduction process to compute the desired control commands for the vehicle. Experimental results from a mobile robot traversing densely cluttered obstacle courses in smooth and continuous motion and at an average speed of 0.6-0.7 m/s are shown. A comparison of the VFN method to earlier methods is given. >",1991,35,2370,130,8,14,19,30,27,36,40,36,48,37
93aa298b40bb3ec23c25239089284fdf61ded917,"Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.",2004,16,1456,304,3,60,68,65,74,94,107,108,104,131
ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36,"Support Vector Machines (SVM’s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM’s introduce the notion of a “kernel induced feature space” which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM’s is that the higher-dimensional space doesn’t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system’s likelihood to perform well on unseen data) of SVM’s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM’s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM’s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than “yes/no” classification).",2002,20,1959,162,57,84,145,152,160,160,155,129,107,112
42371fda17462ef1ce69ddabe42f923195d5762f,"Structural vector autoregressions (VARs) are widely used to trace out the effect of monetary policy innovations on the economy. However, the sparse information sets typically used in these empirical models lead to at least two potential problems with the results. First, to the extent that central banks and the private sector have information not reflected in the VAR, the measurement of policy innovations is likely to be contaminated. A second problem is that impulse responses can be observed only for the included variables, which generally constitute only a small subset of the variables that the researcher and policymaker care about. In this paper we investigate one potential solution to this limited information problem, which combines the standard structural VAR analysis with recent developments in factor analysis for large data sets. We find that the information that our factor-augmented VAR (FAVAR) methodology exploits is indeed important to properly identify the monetary transmission mechanism. Overall, our results provide a comprehensive and coherent picture of the effect of monetary policy on the economy.",2004,77,1595,229,7,19,30,32,48,70,91,81,91,103
d8fcfd198038d418c30e007fb858ed19e45f4681,"We develop an on-demand multipath distance vector protocol for mobile ad hoc networks. Specifically, we propose multipath extensions to a well-studied single path routing protocol known as ad hoc on-demand distance vector (AODV). The resulting protocol is referred to as ad hoc on-demand multipath distance vector (AOMDV). The protocol computes multiple loop-free and link-disjoint paths. Loop-freedom is guaranteed by using a notion of ""advertised hopcount"". Link-disjointness of multiple paths is achieved by using a particular property of flooding. Performance comparison of AOMDV with AODV using ns-2 simulations shows that AOMDV is able to achieve a remarkable improvement in the end-to-end delay-often more than a factor of two, and is also able to reduce routing overheads by about 20%.",2001,51,1701,179,1,8,34,61,61,84,98,104,123,107
455d9a4ff96561d543acbcb2aa81d6cd8fcd20df,"My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently.",1998,84,2437,38,3,7,12,20,23,40,45,60,55,59
12500b532f6f76750d249c7db0f7c678fe731e5d,"An overview of the recent developments in the field of cylindrical vector beams is
 provided. As one class of spatially variant polarization, cylindrical vector beams
 are the axially symmetric beam solution to the full vector electromagnetic wave
 equation. These beams can be generated via different active and passive methods.
 Techniques for manipulating these beams while maintaining the polarization symmetry
 have also been developed. Their special polarization symmetry gives rise to unique
 high-numerical-aperture focusing properties that find important applications in
 nanoscale optical imaging and manipulation. The prospects for cylindrical vector
 beams and their applications in other fields are also briefly discussed.",2009,96,1885,29,28,64,85,102,127,121,161,165,221,198
7c854f6ad964decd450acefbc52ea96b7570dce7,"A method has been described for the isolation of DNA from micro-organisms which yields stable, biologically active, highly polymerized preparations relatively free from protein and RNA. Alternative methods of cell disruption and DNA isolation have been described and compared. DNA capable of transforming homologous strains has been used to test various steps in the procedure and preparations have been obtained possessing high specific activities. Representative samples have been characterized for their thermal stability and sedimentation behaviour.",1961,22,9927,226,5,37,58,86,111,150,154,191,175,211
37126de1bbf70ba0349d8b72af8bb9357c3b12cf,"Today's surface ocean is saturated with respect to calcium carbonate, but increasing atmospheric carbon dioxide concentrations are reducing ocean pH and carbonate ion concentrations, and thus the level of calcium carbonate saturation. Experimental evidence suggests that if these trends continue, key marine organisms—such as corals and some plankton—will have difficulty maintaining their external calcium carbonate skeletons. Here we use 13 models of the ocean–carbon cycle to assess calcium carbonate saturation under the IS92a ‘business-as-usual’ scenario for future emissions of anthropogenic carbon dioxide. In our projections, Southern Ocean surface waters will begin to become undersaturated with respect to aragonite, a metastable form of calcium carbonate, by the year 2050. By 2100, this undersaturation could extend throughout the entire Southern Ocean and into the subarctic Pacific Ocean. When live pteropods were exposed to our predicted level of undersaturation during a two-day shipboard experiment, their aragonite shells showed notable dissolution. Our findings indicate that conditions detrimental to high-latitude ecosystems could develop within decades, not centuries as suggested previously.",2005,63,4073,358,11,62,108,153,218,253,259,337,360,335
f32b0b1ea1e0282ab5c0f8ca23c295f3c3b69198,"Molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes (particularly so among microorganisms). Consequently, the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level. Molecular comparisons show that life on this planet divides into three primary groupings, commonly known as the eubacteria, the archaebacteria, and the eukaryotes. The three are very dissimilar, the differences that separate them being of a more profound nature than the differences that separate typical kingdoms, such as animals and plants. Unfortunately, neither of the conventionally accepted views of the natural relationships among living systems--i.e., the five-kingdom taxonomy or the eukaryote-prokaryote dichotomy--reflects this primary tripartite division of the living world. To remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a ""domain."" Life on this planet would then be seen as comprising three domains, the Bacteria, the Archaea, and the Eucarya, each containing two or more kingdoms. (The Eucarya, for example, contain Animalia, Plantae, Fungi, and a number of others yet to be defined). Although taxonomic structure within the Bacteria and Eucarya is not treated herein, Archaea is formally subdivided into the two kingdoms Euryarchaeota (encompassing the methanogens and their phenotypically diverse relatives) and Crenarchaeota (comprising the relatively tight clustering of extremely thermophilic archaebacteria, whose general phenotype appears to resemble most the ancestral phenotype of the Archaea.",1990,50,5773,374,4,57,89,118,106,112,143,127,151,146
05509d207766c7bac327e55ddca03212c48faca0,"Multicellular organisms live, by and large, harmoniously with microbes. The cornea of the eye of an animal is almost always free of signs of infection. The insect flourishes without lymphocytes or antibodies. A plant seed germinates successfully in the midst of soil microbes. How is this accomplished? Both animals and plants possess potent, broad-spectrum antimicrobial peptides, which they use to fend off a wide range of microbes, including bacteria, fungi, viruses and protozoa. What sorts of molecules are they? How are they employed by animals in their defence? As our need for new antibiotics becomes more pressing, could we design anti-infective drugs based on the design principles these molecules teach us?",2002,101,7070,271,57,133,191,218,263,241,295,347,382,416
6dd84bd0edbafc92d03c57d6bea2d4e07093ea80,,1963,0,4357,459,0,4,4,5,3,8,5,12,9,11
816d9b199d07340238fd263a6369271c556b6ca8,"Interactions between organisms are a major determinant of the distribution and abundance of species. Ecology textbooks (e.g., Ricklefs 1984, Krebs 1985, Begon et al. 1990) summarise these important interactions as intra- and interspecific competition for abiotic and biotic resources, predation, parasitism and mutualism. Conspicuously lacking from the list of key processes in most text books is the role that many organisms play in the creation, modification and maintenance of habitats. These activities do not involve direct trophic interactions between species, but they are nevertheless important and common. The ecological literature is rich in examples of habitat modification by organisms, some of which have been extensively studied (e.g. Thayer 1979, Naiman et al. 1988).",1994,89,5139,165,2,2,14,27,39,61,56,63,64,92
af47ec845f1b60e2117bb1a093de224429786e8c,"Functional partnerships between proteins are at the core of complex cellular phenotypes, and the networks formed by interacting proteins provide researchers with crucial scaffolds for modeling, data reduction and annotation. STRING is a database and web resource dedicated to protein–protein interactions, including both physical and functional interactions. It weights and integrates information from numerous sources, including experimental repositories, computational prediction methods and public text collections, thus acting as a meta-database that maps all interaction evidence onto a common set of genomes and proteins. The most important new developments in STRING 8 over previous releases include a URL-based programming interface, which can be used to query STRING from other resources, improved interaction prediction via genomic neighborhood in prokaryotes, and the inclusion of protein structures. Version 8.0 of STRING covers about 2.5 million proteins from 630 organisms, providing the most comprehensive view on protein–protein interactions currently available. STRING can be reached at http://string-db.org/.",2008,39,2214,201,0,96,239,282,225,195,179,201,221,124
36e2c49a23cb62bc5f47c82603373970e975b87d,"The rhizosphere encompasses the millimeters of soil surrounding a plant root where complex biological and ecological processes occur. This review describes recent advances in elucidating the role of root exudates in interactions between plant roots and other plants, microbes, and nematodes present in the rhizosphere. Evidence indicating that root exudates may take part in the signaling events that initiate the execution of these interactions is also presented. Various positive and negative plant-plant and plant-microbe interactions are highlighted and described from the molecular to the ecosystem scale. Furthermore, methodologies to address these interactions under laboratory conditions are presented.",2006,209,3163,137,9,38,63,86,108,122,169,242,244,261
dddb532a1fa61531bf24b3a4bd0f9a81b747aa43,"Physical ecosystem engineers are organisms that directly or indirectly control the availability of resources to other organisms by causing physical state changes in biotic or abiotic materials. Physical ecosystem engineering by organisms is the physical modification, maintenance, or creation of habitats. Ecological effects of engineers on many other species occur in virtually all ecosystems because the physical state changes directly create nonfood resources such as living space, directly control abiotic resources, and indirectly modulate abiotic forces that, in turn, affect resource use by other organisms. Trophic interactions and resource competition do not constitute engineering. Engineering can have significant or trivial effects on other species, may involve the physical structure of an organism (like a tree) or structures made by an organism (like a beaver dam), and can, but does not invariably, have feedback effects on the engineer. We argue that engineering has both negative and positive effects on species richness and abundances at small scales, but the net effects are probably positive at larger scales encompassing engineered and nonengineered environments in ecological and evolutionary space and time. Models of the population dynamics of engineers suggest that the engineer/habitat equilibrium is often, but not always, locally stable and may show long-term cycles, with potential ramifications for community and ecosystem stability. As yet, data adequate to parameterize such a model do not exist for any engineer species. Because engineers control flows of energy and materials but do not have to participate in these flows, energy, mass, and stoichiometry do not appear to be useful in predicting which engineers will have big effects. Empirical observations suggest some potential generalizations about which species will be important engineers in which ecosystems. We point out some of the obvious, and not so obvious, ways in which engineering and trophic relations interact, and we call for greater research on physical ecosystem engineers, their impacts, and their interface with trophic relations.",1997,69,2035,73,2,6,26,28,34,41,55,59,45,101
2105cf67e643f73b0812ddd3252d23b812330fb3,"Investigating diversity in asexual organisms using molecular markers involves the assignment of individuals to clonal lineages and the subsequent analysis of clonal diversity. Assignment is possible using a distance matrix in combination with a user-specified threshold, defined as the maximum distance between two individuals that are considered to belong to the same clonal lineage. Analysis of clonal diversity requires tests for differences in diversity and clonal composition between populations. We developed two programs, GENOTYPE and GENODIVE for such analyses of clonal diversity in asexually reproducing organisms. Additionally, genotype can be used for detecting genotyping errors in studies of sexual organisms.",2004,9,1704,73,0,3,10,11,16,30,33,60,101,124
279f02e2e821df40bb7dea6f9afc1bd0e2d530e4,"The helix-loop-helix (HLH) family of transcriptional regulatory proteins are key players in a wide array of developmental processes. Over 240 HLH proteins have been identified to date in organisms ranging from the yeast Saccharomyces cerevisiae to humans (6). Studies in Xenopus laevis, Drosophila melanogaster, and mice have convincingly demonstrated that HLH proteins are intimately involved in developmental events such as cellular differentiation, lineage commitment, and sex determination. In yeast, HLH proteins regulate several important metabolic pathways, including phosphate uptake and phospholipid biosynthesis (19, 67, 112). In multicellular organisms, HLH factors are required for a multitude of important developmental processes, including neurogenesis, myogenesis, hematopoiesis, and pancreatic development (12, 86, 127, 179). The purpose of this review is to examine the structure and functional properties of HLH proteins. 
 
 
 
E-box sites: elements mediating cell-type-specific gene transcription. 
Gene transcription of the immunoglobulin heavy-chain (IgH) gene has long been known to be regulated, in part, by a cis-acting DNA element known as the IgH intronic enhancer (109, 156). By in vivo methylation protection assays, a number of sites were identified in both the IgH and the kappa light-chain gene enhancers which were specifically protected in B cells but not in nonlymphoid cells (41). These elements shared a signature motif which consisted of the core hexanucleotide sequence, CANNTG, and were subsequently dubbed E boxes (41). A total of five E-box elements are present in the IgH gene enhancer: μE1, μE2, μE3, μE4, and μE5. The Ig kappa enhancer also contains three cannonical E boxes, designated κE1, κE2, and κE3. E-box sites have been subsequently found in B-cell-specific promoter and enhancer elements, including a subset of Ig light-chain gene promoters, the IgH and Ig light-chain 3′ enhancers, and, more recently, the λ5 promoter (110, 118, 156). 
 
E-box elements have also been identified in promoter and enhancer elements that regulate muscle-, neuron-, and pancreas-specific gene expression. For example, in muscle, the muscle creatine kinase gene, acetylcholine receptor genes α and δ, and the myosin light-chain gene all require E-box elements for full activity (27, 51, 85). A number of genes whose expression is limited to the pancreas also require E-box sites for proper expression. The insulin and somatostatin genes, for example, contain E-box sites that, when multimerized, are sufficient to regulate pancreatic β-cell-specific gene expression (168). More recently, E-box regulatory sites have been identified in a number of neuron-specific genes, including the opsin, hippocalcin, beta 2 subunit of the neuronal nicotinic acetylcholine receptor, and muscarinic acetylcholine receptor genes (1, 21, 52, 125). 
 
 
 
 
E-box sites: cognate recognition sequence for HLH proteins. 
Two proteins, termed E12 and E47, were originally identified as binding to the κE2/μE5 site (65, 102). They have a region of homology with the Drosophila Daughterless protein, the myogenic differentiation factor MyoD, members of the achaete-scute gene complex, and the Myc family of transcription factors (102). This stretch of conserved residues, known as the Myc homology region, appeared to be critical for the DNA binding properties of E12 and E47 (102). The E12 and E47 proteins, which differ only within this Myc homology region, arise by alternative splicing of the E2A gene (157). This conserved sequence, which was modeled as two amphipathic alpha helices separated by a flexible loop structure, was named the HLH motif and shown to function as a dimerization domain. 
 
 
 
 
The HLH structure. 
The solution structure of the basic HLH (bHLH)-leucine zipper (LZ) factor Max first confirmed the existence of the HLH motif (44). Subsequently, the three-dimensional structure of the E47 bHLH polypeptide bound to its E-box recognition site, CACCTG, has been solved at 2.8-Å resolution (38). A number of interesting features were revealed from analysis of the E47 crystal structure. The E47 dimer forms a parallel, four-helix bundle which allows the basic region to contact the major groove (38). In addition to the basic region, residues in the loop and helix 2 also make contact with DNA (38). Stable interaction of the HLH domain is favored by van der Waals interactions between conserved hydrophobic residues (38). The E47 dimer is centered over the E box, with each monomer interacting with either a CAC or CAG half-site. A glutamate present in the basic region of each subunit makes contact with the cytosine and adenine bases in the E-box half-site. An adjacent arginine residue stabilizes the position of the glutamate by direct interaction with these nucleotides and additionally the phosphodiester backbone. Both the glutamate and the arginine residues are conserved in most bHLH proteins, consistent with a role in specific DNA binding (6, 38, 102). 
 
 
 
 
Classification of the HLH proteins. 
Owing to the large number of HLH proteins that have been described, a classification scheme that was based upon tissue distribution, dimerization capabilities, and DNA-binding specificities was devised (Fig. ​(Fig.1)1) (101). Class I HLH proteins, also known as the E proteins, include E12, E47, HEB, E2-2, and Daughterless. These proteins are expressed in many tissues and capable of forming either homo- or heterodimers (103). The DNA-binding specificity of class I proteins is limited to the E-box site. Class II HLH proteins, which include members such as MyoD, myogenin, Atonal, NeuroD/BETA2, and the achaete-scute complex, show a tissue-restricted pattern of expression. With few exceptions, they are incapable of forming homodimers and preferentially heterodimerize with the E proteins. Class I-class II heterodimers can bind both canonical and noncanonical E-box sites (103). Class III HLH proteins include the Myc family of transcription factors, TFE3, SREBP-1, and the microphthalmia-associated transcription factor, Mi. Proteins of this class contain an LZ adjacent to the HLH motif (66, 177). Class IV HLH proteins define a family of molecules, including Mad, Max, and Mxi, that are capable of dimerizing with the Myc proteins or with one another (7, 22, 174). A group of HLH proteins that lack a basic region, including Id and emc, define the class V HLH proteins (18, 39, 47). Class V members are negative regulators of class I and class II HLH proteins (18, 39, 47). Class VI HLH proteins have as their defining feature a proline in their basic region. This group includes the Drosophila proteins Hairy and Enhancer of split (76, 141). Finally, the class VII HLH proteins are categorized by the presence of the bHLH-PAS domain and include members such as the aromatic hydrocarbon receptor (AHR), the AHR nuclear-translocator (Arnt), hypoxia-inducible factor 1α, and the Drosophila Single-minded and Period proteins (34). 
 
 
 
FIG. 1 
 
Multiple sequence alignment and classification of some representative members of the HLH family of transcription factors. Shown is a dendrogram created by aligning the sequences of the indicated HLH proteins by the Clustal W algorithm (160). 
 
 
 
Recently, another classification method of HLH proteins has been described (6). Based on the amino acid sequences of 242 HLH proteins, a phylogenetic tree was created to group family members according to evolutionary relationships (6). Four major groups, A through D, which comprise more than 24 protein families were identified (6). The groupings were based upon DNA-binding specificity as well as conservation of amino acids at certain positions (6). As the number of HLH proteins continues to grow, this evolutionary or “natural” classification may provide a more accurate and convenient means of categorization.",2000,193,1642,121,24,58,74,77,104,71,64,70,83,88
b39c51b5e5f106430dfa6a234fcc671b7782e45e,"In the last 10 years, a large family of secreted signaling molecules has been discovered that appear to mediate many key events in normal growth and development. The family is known as the TGF-p superfamily (Massague 1990), a name taken from the first member of the family to be isolated (transforming growth factor-^l). This name is somewhat misleading, because TGF-p 1 has a large number of effects in different systems (Spom and Roberts 1992). It actually inhibits the proliferation of many different cell lines, and its original ""transforming"" activity may be due to secondary effects on matrix pro­ duction and synthesis of other growth factors (Moses et al. 1990). The two dozen other members of the TGF-p superfamily have a remarkable range of activities. In Diosophila, a TGF-p-related gene is required for dorsoventral axis formation in early embryos, communication between tissue layers in gut development, and correct proximal distal patterning of adult appendages. In Xenopus, a TGF-p-related gene is expressed specifically at one end of fertilized eggs and may function in early signaling events that lay out the basic body plan. In mammals, TGF-p-related molecules have been found that control sexual development, pituitary hormone production, and the creation of bones and cartilage. The recognition of TGF-p superfamily members in many different organ­ isms and contexts provides one of the major unifying themes in recent molecular studies of animal growth and development. The rough outlines of the TGF-p family were first rec­ ognized in the 1980s. Since that time, a number of ex­ cellent reviews have appeared that summarize the prop­ erties of different family members (Ying 1989; Massague 1990; Lyons et al. 1991; Spom and Roberts 1992). Here, I will focus on four areas that have seen major progress in the last 3 years: structural characterization of the signal­ ing molecule, isolation of new family members, cloning of receptor molecules, and new genetic tests of the func­ tions of these factors in different organisms.",1994,106,1983,45,31,91,159,151,161,143,121,109,86,71
3fa772c422d9bd85e451822e4fcc58c98d5c481d,"Reactive oxygen species (ROS) have multifaceted roles in the orchestration of plant gene expression and gene-product regulation. Cellular redox homeostasis is considered to be an ""integrator"" of information from metabolism and the environment controlling plant growth and acclimation responses, as well as cell suicide events. The different ROS forms influence gene expression in specific and sometimes antagonistic ways. Low molecular antioxidants (e.g., ascorbate, glutathione) serve not only to limit the lifetime of the ROS signals but also to participate in an extensive range of other redox signaling and regulatory functions. In contrast to the low molecular weight antioxidants, the ""redox"" states of components involved in photosynthesis such as plastoquinone show rapid and often transient shifts in response to changes in light and other environmental signals. Whereas both types of ""redox regulation"" are intimately linked through the thioredoxin, peroxiredoxin, and pyridine nucleotide pools, they also act independently of each other to achieve overall energy balance between energy-producing and energy-utilizing pathways. This review focuses on current knowledge of the pathways of redox regulation, with discussion of the somewhat juxtaposed hypotheses of ""oxidative damage"" versus ""oxidative signaling,"" within the wider context of physiological function, from plant cell biology to potential applications.",2009,440,1176,70,16,73,98,122,126,107,109,109,81,90
5f74244d62bdc42a8ef6606495508e40d647366f,"The potential of oxygen free radicals and other reactive oxygen species (ROS) to damage tissues and cellular components, called oxidative stress, in biological systems has become a topic of significant interest for environmental toxicology studies. The balance between prooxidant endogenous and exogenous factors (i.e., environmental pollutants) and antioxidant defenses (enzymatic and nonenzymatic) in biological systems can be used to assess toxic effects under stressful environmental conditions, especially oxidative damage induced by different classes of chemical pollutants. The role of these antioxidant systems and their sensitivity can be of great importance in environmental toxicology studies. In the past decade, numerous studies on the effects of oxidative stress caused by some environmental pollutants in terrestrial and aquatic species were published. Increased numbers of agricultural and industrial chemicals are entering the aquatic environment and being taken up into tissues of aquatic organisms. Transition metals, polycyclic aromatic hydrocarbons, organochlorine and organophosphate pesticides, polychlorinated biphenyls, dioxins, and other xenobiotics play important roles in the mechanistic aspects of oxidative damage. Such a diverse array of pollutants stimulate a variety of toxicity mechanisms, such as oxidative damage to membrane lipids, DNA, and proteins and changes to antioxidant enzymes. Although there are considerable gaps in our knowledge of cellular damage, response mechanisms, repair processes, and disease etiology in biological systems, free radical reactions and the production of toxic ROS are known to be responsible for a variety of oxidative damages leading to adverse health effects and diseases. In the past decade, mammalian species were used as models for the study of molecular biomarkers of oxidative stress caused by environmental pollutants to elucidate the mechanisms underlying cellular oxidative damage and to study the adverse effects of some environmental pollutants with oxidative potential in chronic exposure and/or sublethal concentrations. This review summarizes current knowledge and advances in the understanding of such oxidative processes in biological systems. This knowledge is extended to specific applications in aquatic organisms because of their sensitivity to oxidative pollutants, their filtration capacity, and their potential for environmental toxicology studies.",2006,164,1365,71,7,10,33,40,63,71,72,96,104,128
175152bdf0eeeab0cc4fa457784dd8ebdda132a6,"The paper that follows is based on notes taken by Dr. R. S. Pierce on five lectures given by the author at the California Institute of Technology in January 1952. They have been revised by the author but they reflect, apart from minor changes, the lectures as they were delivered. The subject-matter, as the title suggests, is the role of error in logics, or in the physical implementation of logics—–in automatasynthesis. Error is viewed, therefore, not as an extraneous and misdirected or misdirecting accident, but as an essential part of the process under consideration—–its importance in the synthesis of automata being fully comparable to that of the factor which is normally considered, the intended and correct logical structure. Our present treatment of error is unsatisfactory and ad hoc. It is the author’s conviction, voiced over many years, that error should be treated by thermodynamical methods, and be the subject of a thermodynamical theory, as information has been, by the work of L. Szilard and C. E. Shannon (cf. 5.2). The present treatment falls far short of achieving this, but it assembles, it is hoped, some of the building materials, which will have to enter into the final structure. The author wants to express his thanks to K. A. Brueckner and M. Gell-Mann, then at the University of Illinois, to whose discussions in 1951 he owes some important stimuli on this subject; to Dr. R. S. Pierce at the California Institute of Technology, on whose excellent notes this exposition is based; and to the California Institute of Technology, whose invitation to deliver these lectures combined with the very warm reception by the audience, caused him to write this paper in its present form, and whose cooperation in connection with the present publication is much appreciated.",1956,7,2192,147,0,2,8,4,2,9,7,7,2,4
9932ff03877ee134b4fa753c7555ae6281b6ad3e,"Ocean-going ships carry, as ballast, seawater that is taken on in port and released at subsequent ports of call. Plankton samples from Japanese ballast water released in Oregon contained 367 taxa. Most taxa with a planktonic phase in their life cycle were found in ballast water, as were all major marine habitat and trophic groups. Transport of entire coastal planktonic assemblages across oceanic barriers to similar habitats renders bays, estuaries, and inland waters among the most threatened ecosystems in the world. Presence of taxonomically difficult or inconspicuous taxa in these samples suggests that ballast water invasions are already pervasive.",1993,29,1626,42,4,11,22,31,25,47,37,63,52,60
1666fad6fbd651cada1d4ad8eb5831d7f5fbafb0,"Fractal-like networks effectively endow life with an additional fourth spatial dimension. This is the origin of quarter-power scaling that is so pervasive in biology. Organisms have evolved hierarchical branching networks that terminate in size-invariant units, such as capillaries, leaves, mitochondria, and oxidase molecules. Natural selection has tended to maximize both metabolic capacity, by maximizing the scaling of exchange surface areas, and internal efficiency, by minimizing the scaling of transport distances and times. These design principles are independent of detailed dynamics and explicit models and should apply to virtually all organisms.",1999,6,1428,86,3,28,33,50,43,71,66,89,60,61
c18600920e1b9bfd04a6c7baa82c0ad239aea803,,2001,106,1477,80,0,14,12,20,31,42,43,63,72,86
9233c0090a4a0f804f4a294775d395bb55400e2d,"Many ecosystem services are delivered by organisms that depend on habitats that are segregated spatially or temporally from the location where services are provided. Management of mobile organisms contributing to ecosystem services requires consideration not only of the local scale where services are delivered, but also the distribution of resources at the landscape scale, and the foraging ranges and dispersal movements of the mobile agents. We develop a conceptual model for exploring how one such mobile-agent-based ecosystem service (MABES), pollination, is affected by land-use change, and then generalize the model to other MABES. The model includes interactions and feedbacks among policies affecting land use, market forces and the biology of the organisms involved. Animal-mediated pollination contributes to the production of goods of value to humans such as crops; it also bolsters reproduction of wild plants on which other services or service-providing organisms depend. About one-third of crop production depends on animal pollinators, while 60-90% of plant species require an animal pollinator. The sensitivity of mobile organisms to ecological factors that operate across spatial scales makes the services provided by a given community of mobile agents highly contextual. Services vary, depending on the spatial and temporal distribution of resources surrounding the site, and on biotic interactions occurring locally, such as competition among pollinators for resources, and among plants for pollinators. The value of the resulting goods or services may feed back via market-based forces to influence land-use policies, which in turn influence land management practices that alter local habitat conditions and landscape structure. Developing conceptual models for MABES aids in identifying knowledge gaps, determining research priorities, and targeting interventions that can be applied in an adaptive management context.",2007,199,1172,51,7,36,59,52,62,80,98,107,85,95
04b68c1b6ee148d7b1332198d03b571728792bc8,"We have carried out detailed statistical analyses of integral membrane proteins of the helix‐bundle class from eubacterial, archaean, and eukaryotic organisms for which genome‐wide sequence data are available. Twenty to 30% of all ORFs are predicted to encode membrane proteins, with the larger genomes containing a higher fraction than the smaller ones. Although there is a general tendency that proteins with a smaller number of transmembrane segments are more prevalent than those with many, uni‐cellular organisms appear to prefer proteins with 6 and 12 transmembrane segments, whereas Caenorhabditis elegansandHomo sapienshave a slight preference for proteins with seven transmembrane segments. In all organisms, there is a tendency that membrane proteins either have manytransmembrane segments with short connecting loops or few transmembrane segments with large extra‐membraneous domains. Membrane proteins from all organisms studied, except possibly the archaeon Methanococcus jannaschii, follow the so‐called “positive‐inside” rule; i.e., they tend to have a higher frequency of positively charged residues in cytoplasmic than in extra‐cytoplasmic segments.",1998,45,1444,26,7,20,35,32,34,31,42,48,72,79
42eb47797fe541dd1e9ebe46f31df3cbda9e66ec,"Searches for genes involved in the ageing process have been made in genetically tractable model organisms such as yeast, the nematode Caenorhabditis elegans , Drosophila melanogaster fruitflies and mice. These genetic studies have established that ageing is indeed regulated by specific genes, and have allowed an analysis of the pathways involved, linking physiology, signal transduction and gene regulation. Intriguing similarities in the phenotypes of many of these mutants indicate that the mutations may also perturb regulatory systems that control ageing in higher organisms.",2000,104,1290,31,0,42,108,86,100,93,65,65,62,71
830fdb7b17cfa7b96693c41f04416c05ccbf34e6,"A full description of a protein's function requires knowledge of all partner proteins with which it specifically associates. From a functional perspective, ‘association’ can mean direct physical binding, but can also mean indirect interaction such as participation in the same metabolic pathway or cellular process. Currently, information about protein association is scattered over a wide variety of resources and model organisms. STRING aims to simplify access to this information by providing a comprehensive, yet quality-controlled collection of protein–protein associations for a large number of organisms. The associations are derived from high-throughput experimental data, from the mining of databases and literature, and from predictions based on genomic context analysis. STRING integrates and ranks these associations by benchmarking them against a common reference set, and presents evidence in a consistent and intuitive web interface. Importantly, the associations are extended beyond the organism in which they were originally described, by automatic transfer to orthologous protein pairs in other organisms, where applicable. STRING currently holds 730 000 proteins in 180 fully sequenced organisms, and is available at http://string.embl.de/.",2004,22,1220,64,3,37,63,80,47,43,55,57,60,64
9c82f82047b5dd52fda35fbca3622cbe11ce3b8e,"Large proteins are usually expressed in a eukaryotic system while smaller ones are expressed in prokaryotic systems. For proteins that require glycosylation, mammalian cells, fungi or the baculovirus system is chosen. The least expensive, easiest and quickest expression of proteins can be carried out in Escherichia coli. However, this bacterium cannot express very large proteins. Also, for S-S rich proteins, and proteins that require post-translational modifications, E. coli is not the system of choice. The two most utilized yeasts are Saccharomyces cerevisiae and Pichia pastoris. Yeasts can produce high yields of proteins at low cost, proteins larger than 50 kD can be produced, signal sequences can be removed, and glycosylation can be carried out. The baculoviral system can carry out more complex post-translational modifications of proteins. The most popular system for producing recombinant mammalian glycosylated proteins is that of mammalian cells. Genetically modified animals secrete recombinant proteins in their milk, blood or urine. Similarly, transgenic plants such as Arabidopsis thaliana and others can generate many recombinant proteins.",2009,141,818,52,6,31,49,55,67,83,74,89,99,84
24971a32453170ec29c7335e2afb72f426bfce24,"The organization of biological activities into daily cycles is universal in organisms as diverse as cyanobacteria, fungi, algae, plants, flies, birds and man. Comparisons of circadian clocks in unicellular and multicellular organisms using molecular genetics and genomics have provided new insights into the mechanisms and complexity of clock systems. Whereas unicellular organisms require stand-alone clocks that can generate 24-hour rhythms for diverse processes, organisms with differentiated tissues can partition clock function to generate and coordinate different rhythms. In both cases, the temporal coordination of a multi-oscillator system is essential for producing robust circadian rhythms of gene expression and biological activity.",2005,166,1216,55,7,52,63,71,63,65,69,83,69,79
71c5df87d5e6e7e5a00dc7c3668ac7248814a33f,"Models that describe the spread of invading organisms often assume that the dispersal distances of propagules are normally distributed. In contrast, measured dispersal curves are typically leptokurtic, not normal. In this paper, we consider a class of models, integrodifference equations, that directly incorporate detailed dispersal data as well as population growth dynamics. We provide explicit formulas for the speed of invasion for compensatory growth and for different choices of the propagule redistribution kernel and apply these formulas to the spread of D. pseudoobscura. We observe that: (1) the speed of invasion of a spreading population is extremely sensitive to the precise shape of the redistribution kernel and, in particular, to the tail of the distribution; (2) fat-tailed kernels can generate accelerating invasions rather than constant-speed travelling waves; (3) normal redistribution kernels (and by inference, many reaction-diffusion models) may grossly underestimate rates of spread of invading populations in comparison with models that incorporate more realistic leptokurtic distributions; and (4) the relative superiority of different redistribution kernels depends, in general, on the precise magnitude of the net reproductive rate. The addition of an Allee effect to an integrodifference equation may decrease the overall rate of spread. An Allee effect may also introduce a critical range; the population must surpass this spatial thresh-old in order to invade successfully. Fat-tailed kernels and Allee effects provide alternative explanations for the accelerating rates of spread observed for many invasions.",1996,0,1298,97,0,8,11,23,25,33,38,54,50,47
f9df3e2cc08af8119d6234f81147628080bc343f,"Choices of synonymous codons in unicellular organisms are here reviewed, and differences in synonymous codon usages between Escherichia coli and the yeast Saccharomyces cerevisiae are attributed to differences in the actual populations of isoaccepting tRNAs. There exists a strong positive correlation between codon usage and tRNA content in both organisms, and the extent of this correlation relates to the protein production levels of individual genes. Codon-choice patterns are believed to have been well conserved during the course of evolution. Examination of silent substitutions and tRNA populations in Enterobacteriaceae revealed that the evolutionary constraint imposed by tRNA content on codon usage decelerated rather than accelerated the silent-substitution rate, at least insofar as pairs of taxonomically related organisms were examined. Codon-choice patterns of multicellular organisms are briefly reviewed, and diversity in G+C percentage at the third position of codons in vertebrate genes--as well as a possible causative factor in the production of this diversity--is discussed.",1985,38,1605,70,0,8,16,20,29,24,25,13,29,30
216fce085f7112eeec2a9b9252b7f8735d2b846f,,1991,25,2444,28,28,46,62,56,49,28,47,33,29,31
d2167c06eaa23893331ab0ae062e57ce22b89e29,"UNLABELLED
Biologists and other scientists routinely need to know times of divergence between species and to construct phylogenies calibrated to time (timetrees). Published studies reporting time estimates from molecular data have been increasing rapidly, but the data have been largely inaccessible to the greater community of scientists because of their complexity. TimeTree brings these data together in a consistent format and uses a hierarchical structure, corresponding to the tree of life, to maximize their utility. Results are presented and summarized, allowing users to quickly determine the range and robustness of time estimates and the degree of consensus from the published literature.


AVAILABILITY
TimeTree is available at http://www.timetree.net",2006,12,1039,115,2,2,9,11,38,55,79,82,130,123
b6297363c4b33f0b882170add35c8e5cfa25a79d,"Unprecedented development along tropical shorelines is causing severe degradation of coral reefs primarily from increases in sedimentation. Sediment particles smother reef organisms and reduce light available for photosynthesis. Excessive sedmentation can adversely affect the structure and function of the coral reef ecosystem by altering both physical and biological processes. Mean sediment rates and suspended sediment concentrations for reefs not subject to stresses from human activities are < 1 to ca 10 mg cm-* d-' and < 10 mg I-', respectively. Chronic rates and concentrations above these values are 'hlgh'. Heavy sedmentation is associated with fewer coral species, less live coral, lower coral growth rates, greater abundance of branching forms, reduced coral recruitment, decreased calcification, decreased net productivity of corals, and slower rates of reef accretion. Coral species have different capabilities of clearing themselves of sediment particles or surviving lower light levels. Sedlment rejection is a function of morphology, orientation, growth habit, and behavior; and of the amount and type of se lment . Coral growth rates are not simple indicators of sediment levels. Decline of tropical fisheries is partially attributable to deterioration of coral reefs, seagrass beds, and mangroves from sedimentation. Sedimentation can alter the complex interactions between fish and their reef habitat. For example, sedimentation can lull major reef-building corals, leading to eventual collapse of the reef framework. A decline in the amount of shelter the reef provides leads to reductions in both number of individuals and number of species of fish. Currently, we are unable to rigorously predict the responses of coral reefs and reef organisms to excessive sedimentation from coastal development and other sources. Given information on the amount of sediment which will be introduced into the reef environment, the coral community composition, the depth of the reef, the percent coral cover, and the current patterns, we should be able to predict the consequences of a particular activity. Models of physical processes (e.g. sediment transport) must be complemented with better understanding of organism and ecosystem responses to sediment stress. Specifically, we need data on the threshold levels for reef orgarusms and for the reef ecosystem as a whole the levels above which sedimentation has lethal effects for particular species and above which normal functioning of the reef ceases. Additional field studies on the responses of reef organisms to both temgenous and calcium carbonate sediments are necessary. To effectively assess trends on coral reefs, e.g. changes in abundance and spatial arrangement of dominant benthic organisms, scientists must start using standardized monitoring methods. Long-term data sets are critical for tracking these complex ecosystems.",1990,109,1322,116,1,3,9,6,9,8,9,18,17,19
d6dc753be567413c42f02f02f35da476221575ca,"(2002). The Effects of Harmful Algal Blooms on Aquatic Organisms. Reviews in Fisheries Science: Vol. 10, No. 2, pp. 113-390.",2002,1964,1156,85,0,10,24,37,49,54,64,59,64,66
f90500d0f00a6c20303c28076eb0818f90e70ec2,"Although the nonlinear optical effect known as second-harmonic generation (SHG) has been recognized since the earliest days of laser physics and was demonstrated through a microscope over 25 years ago, only in the past few years has it begun to emerge as a viable microscope imaging contrast mechanism for visualization of cell and tissue structure and function. Only small modifications are required to equip a standard laser-scanning two-photon microscope for second-harmonic imaging microscopy (SHIM). Recent studies of the three-dimensional in vivo structures of well-ordered protein assemblies, such as collagen, microtubules and muscle myosin, are beginning to establish SHIM as a nondestructive imaging modality that holds promise for both basic research and clinical pathology. Thus far the best signals have been obtained in a transmitted light geometry that precludes in vivo measurements on large living animals. This drawback may be addressed through improvements in the collection of SHG signals via an epi-illumination microscope configuration. In addition, SHG signals from certain membrane-bound dyes have been shown to be highly sensitive to membrane potential. Although this indicates that SHIM may become a valuable tool for probing cell physiology, the small signal size would limit the number of photons that could be collected during the course of a fast action potential. Better dyes and optimized microscope optics could ultimately lead to the imaging of neuronal electrical activity with SHIM.",2003,31,1194,23,1,17,26,47,71,66,66,84,62,97
ddf280021cb2ebce1d42dc28d6b6897fd3163eaa,"Metallic nanoparticles are among the most widely used types of engineered nanomaterials; however, little is known about their environmental fate and effects. To assess potential environmental effects of engineered nanometals, it is important to determine which species are sensitive to adverse effects of various nanomaterials. In the present study, zebrafish, daphnids, and an algal species were used as models of various trophic levels and feeding strategies. To understand whether observed effects are caused by dissolution, particles were characterized before testing, and particle concentration and dissolution were determined during exposures. Organisms were exposed to silver, copper, aluminum, nickel, and cobalt as both nanoparticles and soluble salts as well as to titanium dioxide nanoparticles. Our results indicate that nanosilver and nanocopper cause toxicity in all organisms tested, with 48-h median lethal concentrations as low as 40 and 60 microg/L, respectively, in Daphnia pulex adults, whereas titanium dioxide did not cause toxicity in any of the tests. Susceptibility to nanometal toxicity differed among species, with filter-feeding invertebrates being markedly more susceptible to nanometal exposure compared with larger organisms (i.e., zebrafish). The role of dissolution in observed toxicity also varied, being minor for silver and copper but, apparently, accounting for most of the toxicity with nickel. Nanoparticulate forms of metals were less toxic than soluble forms based on mass added, but other dose metrics should be developed to accurately assess concentration-response relationships for nanoparticle exposures.",2008,21,793,52,1,21,39,72,69,87,84,81,66,76
dbc447956c16e27cfb030e40552359d3c79bc690,"Whole-genome association studies (WGAS) bring new computational, as well as analytic, challenges to researchers. Many existing genetic-analysis tools are not designed to handle such large data sets in a convenient manner and do not necessarily exploit the new opportunities that whole-genome data bring. To address these issues, we developed PLINK, an open-source C/C++ WGAS tool set. With PLINK, large data sets comprising hundreds of thousands of markers genotyped for thousands of individuals can be rapidly manipulated and analyzed in their entirety. As well as providing tools to make the basic analytic steps computationally efficient, PLINK also supports some novel approaches to whole-genome data that take advantage of whole-genome coverage. We introduce PLINK and describe the five main domains of function: data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation. In particular, we focus on the estimation and use of identity-by-state and identity-by-descent information in the context of population-based whole-genome studies. This information can be used to detect and correct for population stratification and to identify extended chromosomal segments that are shared identical by descent between very distantly related individuals. Analysis of the patterns of segmental sharing has the potential to map disease loci that contain multiple rare variants in a population-based linkage analysis.",2007,47,22270,2671,0,0,0,0,0,1,0,2,4,57
a2893118e14c29a23472b02249b4641b9971786b,"Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.",2005,85,26853,2905,0,0,0,0,0,0,0,0,0,0
ed73eb5b0fcb517d1e13feaa1e09be163e7f7cde,"We present an efficient scheme for calculating the Kohn-Sham ground state of metallic systems using pseudopotentials and a plane-wave basis set. In the first part the application of Pulay's DIIS method (direct inversion in the iterative subspace) to the iterative diagonalization of large matrices will be discussed. Our approach is stable, reliable, and minimizes the number of order ${\mathit{N}}_{\mathrm{atoms}}^{3}$ operations. In the second part, we will discuss an efficient mixing scheme also based on Pulay's scheme. A special ``metric'' and a special ``preconditioning'' optimized for a plane-wave basis set will be introduced. Scaling of the method will be discussed in detail for non-self-consistent and self-consistent calculations. It will be shown that the number of iterations required to obtain a specific precision is almost independent of the system size. Altogether an order ${\mathit{N}}_{\mathrm{atoms}}^{2}$ scaling is found for systems containing up to 1000 electrons. If we take into account that the number of k points can be decreased linearly with the system size, the overall scaling can approach ${\mathit{N}}_{\mathrm{atoms}}$. We have implemented these algorithms within a powerful package called VASP (Vienna ab initio simulation package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semiconducting surfaces, phonons in simple metals, transition metals, and semiconductors) and turned out to be very reliable. \textcopyright{} 1996 The American Physical Society.",1996,1,56171,430,0,0,0,0,0,0,0,0,0,0
bad1be638b556812f6a08b277c0fa2d2e92d5e96,"We present a detailed description and comparison of algorithms for performing ab-initio quantum-mechanical calculations using pseudopotentials and a plane-wave basis set. We will discuss: (a) partial occupancies within the framework of the linear tetrahedron method and the finite temperature density-functional theory, (b) iterative methods for the diagonalization of the Kohn-Sham Hamiltonian and a discussion of an efficient iterative method based on the ideas of Pulay's residual minimization, which is close to an order Natoms2 scaling even for relatively large systems, (c) efficient Broyden-like and Pulay-like mixing methods for the charge density including a new special ‘preconditioning’ optimized for a plane-wave basis set, (d) conjugate gradient methods for minimizing the electronic free energy with respect to all degrees of freedom simultaneously. We have implemented these algorithms within a powerful package called VAMP (Vienna ab-initio molecular-dynamics package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semi-conducting surfaces, phonons in simple metals, transition metals and semiconductors) and turned out to be very reliable.",1996,56,36814,242,0,0,0,0,0,0,0,0,0,0
32d662d196223bb46f8a970f6df68ce69a9a6c2f,The University of Wisconsin Genetics Computer Group (UWGCG) has been organized to develop computational tools for the analysis and publication of biological sequence data. A group of programs that will interact with each other has been developed for the Digital Equipment Corporation VAX computer using the VMS operating system. The programs available and the conditions for transfer are described.,1984,9,13500,577,1,0,0,7,18,44,47,86,100,904
5d54560c6c88eecccf18cdce4255ce63cc91cb36,"The thermodynamic properties of 154 mineral end-members, 13 silicate liquid end-members and 22 aqueous fluid species are presented in a revised and updated data set. The use of a temperature-dependent thermal expansion and bulk modulus, and the use of high-pressure equations of state for solids and fluids, allows calculation of mineral-fluid equilibria to 100 kbar pressure or higher. A pressure-dependent Landau model for order-disorder permits extension of disordering transitions to high pressures, and, in particular, allows the alpha-beta quartz transition to be handled more satisfactorily. Several melt end- members have been included to enable calculation of simple phase equilibria and as a first stage in developing melt mixing models in NCKFMASH. The simple aqueous species density model has been extended to enable speciation calculations and mineral solubility determination involving minerals and aqueous species at high temperatures and pressures. The data set has also been improved by incorporation of many new phase equilibrium constraints, calorimetric studies and new measurements of molar volume, thermal expansion and compressibility. This has led to a significant improvement in the level of agreement with the available experimental phase equilibria, and to greater flexibility in calculation of complex mineral equilibria. It is also shown that there is very good agreement between the data set and the most recent available calorimetric data. kinetics which apply to determining directly the greatest majority of such equilibria in the laboratory, for forming solid solutions, and inclusion of aqueous and silicate melt species), and provides uncertainties especially at lower temperatures, as well as the diYculty of establishing reversals of reactions involving solid allowing the likely uncertainties on the results of thermodynamic calculations to be estimated. This is a solutions. The levels of precision and accuracy required of thermodynamic data in order to be able to forward- critical issue in that calculations using data sets should always involve uncertainty propagation to help evalu- model synthetic and natural mineral assemblages mean that the continuing upgrading and expansion of the ate the results. Because the experimental phase equilib- ria involve overlapping subsets of compositional space, data set by incorporation of new phase equilibrium constraints, calorimetry and new measurements of the derived thermodynamic data are highly correlated, and it is only the inclusion of the correlations which molar volume, thermal expansion and compressibility are more than justified. enables the reliable calculation of uncertainties on mineral reactions to be performed. Earlier work on mineral thermodynamic data sets for rock-forming minerals includes compilations of The thermodynamic data extraction involves using weighted least squares on the diVerent types of data",2004,350,4003,392,139,122,146,177,188,211,195,227,238,227
fcd71a2ca1a15d3b6171670ce82177c81cd9745e,"This article discusses the conduct and evaluatoin of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflect and debate on the important subject of grounding interpretive research methodology.",1999,97,5446,451,23,44,70,98,137,151,167,230,276,291
c896b6c0e8e6be8ba28142e096d705241ce94b9a,"This book is an introduction to level set methods and dynamic implicit surfaces. These are powerful techniques for analyzing and computing moving fronts in a variety of different settings. While it gives many examples of the utility of the methods to a diverse set of applications, it also gives complete numerical analysis and recipes, which will enable users to quickly apply the techniques to real problems. The book begins with a description of implicit surfaces and their basic properties, then devises the level set geometry and calculus toolbox, including the construction of signed distance functions. Part II adds dynamics to this static calculus. Topics include the level set equation itself, Hamilton-Jacobi equations, motion of a surface normal to itself, re-initialization to a signed distance function, extrapolation in the normal direction, the particle level set method and the motion of co-dimension two (and higher) objects. Part III is concerned with topics taken from the fields of Image Processing and Computer Vision. These include the restoration of images degraded by noise and blur, image segmentation with active contours (snakes), and reconstruction of surfaces from unorganized data points. Part IV is dedicated to Computational Physics. It begins with one phase compressible fluid dynamics, then two-phase compressible flow involving possibly different equations of state, detonation and deflagration waves, and solid/fluid structure interaction. Next it discusses incompressible fluid dynamics, including a computer graphics simulation of smoke, free surface flows, including a computer graphics simulation of water, and fully two-phase incompressible flow. Additional related topics include incompressible flames with applications to computer graphics and coupling a compressible and incompressible fluid. Finally, heat flow and Stefan problems are discussed. A student or researcher working in mathematics, computer graphics, science, or engineering interested in any dynamic moving front, which might change its topology or develop singularities, will find this book interesting and useful.",2002,0,5399,423,13,69,140,220,244,318,322,338,362,367
974334c336e87267704cb41e748fc24e5bc0e8e6,"Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.",1996,30,5972,638,27,130,218,233,292,285,332,341,304,351
144adacded5ed56c35a5f157fe231a0459620ec8,"In this article we present a standardized set of 260 pictures for use in experiments investigating differences and similarities in the processing of pictures and words. The pictures are black-and-white line drawings executed according to a set of rules that provide consistency of pictorial representation. The pictures have been standardized on four variables of central relevance to memory and cognitive processing: name agreement, image agreement, familiarity, and visual complexity. The intercorrelations among the four measures were low, suggesting that they are indices of different attributes of the pictures. The concepts were selected to provide exemplars from several widely studied semantic categories. Sources of naming variance, and mean familiarity and complexity of the exemplars, differed significantly across the set of categories investigated. The potential significance of each of the normative variables to a number of semantic and episodic memory tasks is discussed.",1980,39,4690,351,0,0,1,5,8,7,11,14,16,17
b9e43395663f74c581982e9ca97a0d7057a0008c,"LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)≥z(S⋃T)+z(S⋂T) for allS, T inN. Such a function is called submodular. We consider the problem maxS⊂N{a(S):|S|≤K,z(S) submodular}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a “greedy” heuristic always produces a solution whose value is at least 1 −[(K − 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e − 1)/e, where e is the base of the natural logarithm.",1978,13,3951,470,2,3,7,6,6,2,5,6,2,3
981e4615bba324b40b5ef5cac710eab594d5009a,"Abstract The soft set theory offers a general mathematical tool for dealing with uncertain, fuzzy, not clearly defined objects. The main purpose of this paper is to introduce the basic notions of the theory of soft sets, to present the first results of the theory, and to discuss some problems of the future.",1999,11,3328,653,0,0,0,1,3,0,3,1,3,14
b9fdcf9cafc0ef7d66475029e37dbed64ab2ab45,"This document describes release 2.0 of the SimpleScalar tool set, a suite of free, publicly available simulation tools that offer both detailed and high-performance simulation of modern microprocessors. The new release offers more tools and capabilities, precompiled binaries, cleaner interfaces, better documentation, easier installation, improved portability, and higher performance. This paper contains a complete description of the tool set, including retrieval and installation instructions, a description of how to use the tools, a description of the target SimpleScalar architecture, and many details about the internals of the tools and how to customize them. With this guide, the tool set can be brought up and generating results in under an hour (on supported platforms).",1997,15,3460,359,6,31,69,166,212,218,316,290,308,308
eb46bda7b1f60fff84265fec045b473df170caea,"Fuzzy Set Theory - And Its Applications, Third Edition is a textbook for courses in fuzzy set theory. It can also be used as an introduction to the subject. The character of a textbook is balanced with the dynamic nature of the research in the field by including many useful references to develop a deeper understanding among interested readers. The book updates the research agenda (which has witnessed profound and startling advances since its inception some 30 years ago) with chapters on possibility theory, fuzzy logic and approximate reasoning, expert systems, fuzzy control, fuzzy data analysis, decision making and fuzzy set models in operations research. All chapters have been updated. Exercises are included.",1985,194,6446,277,2,5,9,19,27,35,45,55,74,107
dfdb7324a90b5bc11b5c8b39bff6cfa498587c86,,1999,0,4151,347,6,28,82,103,161,196,280,320,331,299
ef4481cbc18c91e7bf0e53693bb77f3608743626,"A family of new measures of point and graph centrality based on early intuitions of Bavelas (1948) is introduced. These measures define centrality in terms of the degree to which a point falls on the shortest path between others and there fore has a potential for control of communication. They may be used to index centrality in any large or small network of symmetrical relations, whether connected or unconnected.",1977,4,7421,301,0,6,2,6,6,2,4,3,1,3
f97ba43adfd5f6a6641c879b854149c4a7df7ca4,"The Penn World Table displays a set of national accounts economic time series covering many countries. Its expenditure entries are denominated in a common set of prices in a common currency so that real quantity comparisons can be made, both between countries and over time. It also provides information about relative prices within and between countries, as well as demographic data and capital stock estimates. This updated, revised, and expanded Mark 5 version of the table includes more countries, years, and variables of interest to economic researchers. The Table is available on personal computer diskettes and through BITNET.",1991,9,3478,236,7,51,64,95,150,164,176,183,224,236
c62015ab9a87bdf67425e916cc14cfac2fe7455c,"A level set method for capturing the interface between two fluids is combined with a variable density projection method to allow for computation of two-phase flow where the interface can merge/break and the flow can have a high Reynolds number. A distance function formulation of the level set method enables one to compute flows with large density ratios (1000/1) and flows that are surface tension driven; with no emotional involvement. Recent work has improved the accuracy of the distance function formulation and the accuracy of the advection scheme. We compute flows involving air bubbles and water drops, to name a few. We validate our code against experiments and theory.",1994,0,3945,186,0,12,18,21,29,54,38,68,75,116
126900d94473a2f06a4b2a265c77fa44df1fd8d1,"Given a collection<inline-equation><f> <sc>F</sc></f></inline-equation> of subsets of <?Pub Fmt italic>S<?Pub Fmt /italic> ={1,…,<?Pub Fmt italic>n<?Pub Fmt /italic>}, <?Pub Fmt italic>setcover<?Pub Fmt /italic> is the problem of selecting as few as possiblesubsets from <inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union covers<?Pub Fmt italic>S,<?Pub Fmt /italic>, and <?Pub Fmt italic>maxk-cover<?Pub Fmt /italic> is the problem of selecting<?Pub Fmt italic>k<?Pub Fmt /italic> subsets from<inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union has maximum cardinality. Both these problems areNP-hard.   We prove that (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) ln<?Pub Fmt italic>n<?Pub Fmt /italic> is a threshold below   which setcover cannot be approximated efficiently, unless NP has slightlysuperpolynomial time algorithms. This closes the gap (up to low-orderterms) between the ratio of approximation achievable by the greedyalogorithm (which is (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) lnn), and provious results of Lund and Yanakakis, that showed hardness ofapproximation within a ratio of <inline-equation><f><fen lp=""par""><lim align=""r""><op><rf>log</rf></op><ll>2</ll></lim>n<rp post=""par""></fen>/2≃0.72</f></inline-equation> ln <?Pub Fmt italic>n<?Pub Fmt /italic>. For max<?Pub Fmt italic>k<?Pub Fmt /italic>-cover, we show an approximationthreshold of (1 - 1/<?Pub   Fmt italic>e<?Pub Fmt /italic>)(up tolow-order terms), under assumption that <inline-equation><f>P≠NP</f><?Pub   Caret></inline-equation>.",1998,35,3199,250,34,31,35,36,48,67,77,121,110,123
fc3eb090e39d71295c362458b8a0c48d2c5d8377,"During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.",2009,29,2472,276,2,15,50,84,127,154,182,189,257,314
0ca21629c5f2da8a8a704335ab09e9b33f73f133,"This article presents a new data set on inequality in the distribution of income. The authors explain the criteria they applied in selecting data on Gini coefficients and on individual quintile groups' income shares. Comparison of the new data set with existing compilations reveals that the data assembled here represent an improvement in quality and a significant expansion in coverage, although differences in the definition of the underlying data might still affect inter temporal and international comparability. Based on this new data set, the authors do not find a systematic link between growth and changes in aggregate inequality. They do find a strong positive relationship between growth and reduction of poverty.",1996,70,3149,299,6,31,67,128,133,140,203,163,183,168
2c276714c18231f1ff2c7f7727cc479506a5b27a,"We examine explanations for corporate financing-, dividend-, and compensation-policy issues. We document robust empirical relations among corporate policy decisions and various firm characteristics. Our evidence suggests contracting theories are more important in explaining cross-sectional variation in observed financial, dividend, and compensation policies than either tax-based or signaling theories.",1992,33,3813,188,2,12,23,29,31,40,61,80,86,112
2805537bec87a6177037b18f9a3a9d3f1038867b,"The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.",1997,71,3542,215,4,18,24,38,66,50,73,72,72,70
c5561bda7ff1438096f784e553cf60385bddd09a,"Communities have the potential to function effectively and adapt successfully in the aftermath of disasters. Drawing upon literatures in several disciplines, we present a theory of resilience that encompasses contemporary understandings of stress, adaptation, wellness, and resource dynamics. Community resilience is a process linking a network of adaptive capacities (resources with dynamic attributes) to adaptation after a disturbance or adversity. Community adaptation is manifest in population wellness, defined as high and non-disparate levels of mental and behavioral health, functioning, and quality of life. Community resilience emerges from four primary sets of adaptive capacities—Economic Development, Social Capital, Information and Communication, and Community Competence—that together provide a strategy for disaster readiness. To build collective resilience, communities must reduce risk and resource inequities, engage local people in mitigation, create organizational linkages, boost and protect social supports, and plan for not having a plan, which requires flexibility, decision-making skills, and trusted sources of information that function in the face of unknowns.",2008,191,3058,240,10,29,59,99,121,196,251,304,315,326
e48f4d8dedc8ef449edf3d5917bc285901b42bf7,"A set of face stimuli called the NimStim Set of Facial Expressions is described. The goal in creating this set was to provide facial expressions that untrained individuals, characteristic of research participants, would recognize. This set is large in number, multiracial, and available to the scientific community online. The results of psychometric evaluations of these stimuli are presented. The results lend empirical support for the validity and reliability of this set of facial expressions as determined by accurate identification of expressions and high intra-participant agreement across two testing sessions, respectively.",2009,63,2698,195,43,82,127,205,220,219,244,276,273,284
e5a9fb3d49ef54f049509231e5883a14bef070f0,"A fast marching level set method is presented for monotonically advancing fronts, which leads to an extremely fast scheme for solving the Eikonal equation. Level set methods are numerical techniques for computing the position of propagating fronts. They rely on an initial value partial differential equation for a propagating level set function and use techniques borrowed from hyperbolic conservation laws. Topological changes, corner and cusp development, and accurate determination of geometric properties such as curvature and normal direction are naturally obtained in this setting. This paper describes a particular case of such methods for interfaces whose speed depends only on local position. The technique works by coupling work on entropy conditions for interface motion, the theory of viscosity solutions for Hamilton-Jacobi equations, and fast adaptive narrow band level set methods. The technique is applicable to a variety of problems, including shape-from-shading problems, lithographic development calculations in microchip manufacturing, and arrival time problems in control theory.",1996,27,3010,266,9,12,13,31,34,40,44,89,84,106
5f2f9cd5f3d0a2693a86b74cacb70e4a1c71ebc1,A contracted Gaussian basis set (6‐311G**) is developed by optimizing exponents and coefficients at the Mo/ller–Plesset (MP) second‐order level for the ground states of first‐row atoms. This has a triple split in the valence s and p shells together with a single set of uncontracted polarization functions on each atom. The basis is tested by computing structures and energies for some simple molecules at various levels of MP theory and comparing with experiment.,1980,7,10920,8,0,0,0,0,0,0,0,0,0,0
348cd9726be5e5740ab751c15fbad4b60d98246d,"Whereas much of organic chemistry has classically dealt with the preparation and study of the properties of individual molecules, an increasingly significant portion of the activity in chemical research involves understanding and utilizing the nature of the interactions between molecules. Two representative areas of this evolution are supramolecular chemistry and molecular recognition. The interactions between molecules are governed by intermolecular forces whose energetic and geometric properties are much less well understood than those of classical chemical bonds between atoms. Among the strongest of these interactions, however, are hydrogen bonds, whose directional properties are better understood on the local level (that is, for a single hydrogen bond) than many other types of non-bonded interactions. Nevertheless, the means by which to characterize, understand, and predict the consequences of many hydrogen bonds among molecules, and the resulting formation of molecular aggregates (on the microscopic scale) or crystals (on the macroscopic scale) has remained largely enigmatic. One of the most promising systematic approaches to resolving this enigma was initially developed by the late M. C. Etter, who applied graph theory to recognize, and then utilize, patterns of hydrogen bonding for the understanding and design of molecular crystals. In working with Etter's original ideas the power and potential utility of this approach on one hand, and on the other, the need to develop and extend the initial Etter formalism was generally recognized. It with that latter purpose that we originally undertook the present review.",1995,0,7111,156,1,14,17,24,36,66,68,132,147,171
a31dfa5eb97fced9494dfa1d88578da6827bf78d,"Spend your time even for only few minutes to read a book. Reading a book will never reduce and waste your time to be useless. Reading, for some people become a need that is to do every day such as spending time for eating. Now, what about you? Do you like to read a book? Now, we will show you a new book enPDFd fuzzy set theory and its applications that can be a new way to explore the knowledge. When reading this book, you can get one thing to always remember in every reading time, even step by step.",1993,0,3808,110,32,80,70,92,98,103,84,107,97,104
35f87841b45e820bc1e1bfc66ac85b6d313d6f05,"We propose a new multiphase level set framework for image segmentation using the Mumford and Shah model, for piecewise constant and piecewise smooth optimal approximations. The proposed method is also a generalization of an active contour model without edges based 2-phase segmentation, developed by the authors earlier in T. Chan and L. Vese (1999. In Scale-Space'99, M. Nilsen et al. (Eds.), LNCS, vol. 1682, pp. 141–151) and T. Chan and L. Vese (2001. IEEE-IP, 10(2):266–277). The multiphase level set formulation is new and of interest on its own: by construction, it automatically avoids the problems of vacuum and overlap; it needs only log n level set functions for n phases in the piecewise constant case; it can represent boundaries with complex topologies, including triple junctions; in the piecewise smooth case, only two level set functions formally suffice to represent any partition, based on The Four-Color Theorem. Finally, we validate the proposed models by numerical results for signal and image denoising and segmentation, implemented using the Osher and Sethian level set method.",2002,72,2581,279,2,16,56,77,110,121,131,159,157,177
e4c2d802cf9fe8de8f213727512e18bbfe3dc631,"Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which retains some of the attractive features of existing methods and overcomes some of their limitations. The authors' techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori assumption about the object's topology is made. A single instance of the authors' model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian (1988) to model propagating solid/liquid interfaces with curvature-dependent speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient field with constant speed or a speed that depends on the curvature. It is moved by solving a ""Hamilton-Jacobi"" type equation written for a function in which the interface is a particular level set. A speed term synthesized from the image is used to stop the interface in the vicinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. The authors present a variety of ways of computing the evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images. >",1995,94,3515,159,25,42,51,52,76,99,106,119,133,171
f991a2a0e09f442a2ad528bbcab8fc0992f7cf3b,"[1] The historical surface temperature data set HadCRUT provides a record of surface temperature trends and variability since 1850. A new version of this data set, HadCRUT3, has been produced, benefiting from recent improvements to the sea surface temperature data set which forms its marine component, and from improvements to the station records which provide the land data. A comprehensive set of uncertainty estimates has been derived to accompany the data: Estimates of measurement and sampling error, temperature bias effects, and the effect of limited observational coverage on large-scale averages have all been made. Since the mid twentieth century the uncertainties in global and hemispheric mean temperatures are small, and the temperature increase greatly exceeds its uncertainty. In earlier periods the uncertainties are larger, but the temperature increase over the twentieth century is still significantly larger than its uncertainty.",2006,55,2008,315,16,77,141,153,214,233,263,246,151,111
65c260fce4fb6fe44ac44562b75d38b0ac5c38f5,"Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods.",2009,60,1883,341,0,21,42,84,103,138,192,204,218,195
cf9f2a48528e50a5625eb8b8200d86a93e098b7a,"We describe the construction of a 10' latitude/longitude data set of mean monthly sur- face climate over global land areas, excluding Antarctica. The climatology includes 8 climate ele- ments —precipitation, wet-day frequency, temperature, diurnal temperature range, relative humid- ity, sunshine duration, ground frost frequency and windspeed—and was interpolated from a data set of station means for the period centred on 1961 to 1990. Precipitation was first defined in terms of the parameters of the Gamma distribution, enabling the calculation of monthly precipitation at any given return period. The data are compared to an earlier data set at 0.5o latitude/longitude resolution and show added value over most regions. The data will have many applications in applied climatology, biogeochemical modelling, hydrology and agricultural meteorology and are available through the International Water Management Institute World Water and Climate Atlas (http://www.iwmi.org) and the Climatic Research Unit (http://www.cru.uea.ac.uk).",2002,34,2127,359,1,8,21,46,69,89,109,153,139,176
0ac187c25dc630aca84751947f01879e479196e0,"The 6‐31G* and 6‐31G** basis sets previously introduced for first‐row atoms have been extended through the second‐row of the periodic table. Equilibrium geometries for one‐heavy‐atom hydrides calculated for the two‐basis sets and using Hartree–Fock wave functions are in good agreement both with each other and with the experimental data. HF/6‐31G* structures, obtained for two‐heavy‐atom hydrides and for a variety of hypervalent second‐row molecules, are also in excellent accord with experimental equilibrium geometries. No large deviations between calculated and experimental single bond lengths have been noted, in contrast to previous work on analogous first‐row compounds, where limiting Hartree–Fock distances were in error by up to a tenth of an angstrom. Equilibrium geometries calculated at the HF/6‐31G level are consistently in better agreement with the experimental data than are those previously obtained using the simple split‐valance 3‐21G basis set for both normal‐ and hypervalent compounds. Normal‐mode vibrational frequencies derived from 6‐31G* level calculations are consistently larger than the corresponding experimental values, typically by 10%–15%; they are of much more uniform quality than those obtained from the 3‐21G basis set. Hydrogenation energies calculated for normal‐ and hypervalent compounds are in moderate accord with experimental data, although in some instances large errors appear. Calculated energies relating to the stabilities of single and multiple bonds are in much better accord with the experimental energy differences.",1982,13,5402,6,0,8,14,26,42,30,47,41,33,46
9fb7b636edeaf344394fdf37481d7b83eec75358,Recently Viola et al. [2001] have introduced a rapid object detection. scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce a novel set of rotated Haar-like features. These novel features significantly enrich the simple features of Viola et al. and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%.,2002,9,3141,167,2,16,55,79,103,155,197,201,224,236
9c6f95c53af4a60acef42aa38529ab91dcce7351,"The relatively small diffuse function‐augmented basis set, 3‐21+G, is shown to describe anion geometries and proton affinities adequately. The diffuse sp orbital exponents are recommended for general use to augment larger basis sets.",1983,24,4512,1,0,3,12,20,20,16,31,31,39,44
63c659fc8a2a8238bb8952b00d1128450a7cce4b,"Light synchronizes mammalian circadian rhythms with environmental time by modulating retinal input to the circadian pacemaker—the suprachiasmatic nucleus (SCN) of the hypothalamus. Such photic entrainment requires neither rods nor cones, the only known retinal photoreceptors. Here, we show that retinal ganglion cells innervating the SCN are intrinsically photosensitive. Unlike other ganglion cells, they depolarized in response to light even when all synaptic input from rods and cones was blocked. The sensitivity, spectral tuning, and slow kinetics of this light response matched those of the photic entrainment mechanism, suggesting that these ganglion cells may be the primary photoreceptors for this system.",2002,52,2793,152,48,77,76,95,82,111,89,112,111,129
60c6bc0aafe922a8a1a1fb16e76ce0bab6cfdc98,"Descriptive set theory has been one of the main areas of research in set theory for almost a century. This text attempts to present a largely balanced approach, which combines many elements of the different traditions of the subject. It includes a wide variety of examples, exercises (over 400), and applications, in order to illustrate the general concepts and results of the theory. This text provides a first basic course in classical descriptive set theory and covers material with which mathematicians interested in the subject for its own sake or those that wish to use it in their field should be familiar. Over the years, researchers in diverse areas of mathematics, such as logic and set theory, analysis, topology, probability theory, etc., have brought to the subject of descriptive set theory their own intuitions, concepts, terminology and notation.",1987,0,2231,247,1,0,0,0,0,0,0,2,7,14
5ae073986408c9931bf6887fafb85e253866f7cc,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",2001,0,2010,237,8,28,38,55,62,62,86,86,116,97
b4e0ca86d00efc6c548939e3ed614bf64dd9d0ab,"Let A be a binary matrix of size m × n, let cT be a positive row vector of length n and let e be the column vector, all of whose m components are ones. The set-covering problem is to minimize cTx subject to Ax ≥ e and x binary. We compare the value of the objective function at a feasible solution found by a simple greedy heuristic to the true optimum. It turns out that the ratio between the two grows at most logarithmically in the largest column sum of A. When all the components of cT are the same, our result reduces to a theorem established previously by Johnson and Lovasz.",1979,4,2540,188,0,3,3,7,6,9,2,6,10,7
d243b5eb81a8501cc0477c47a4ce7d4feb524aee,"In this paper, we present a new variational formulation for geometric active contours that forces the level set function to be close to a signed distance function, and therefore completely eliminates the need of the costly re-initialization procedure. Our variational formulation consists of an internal energy term that penalizes the deviation of the level set function from a signed distance function, and an external energy term that drives the motion of the zero level set toward the desired image features, such as object boundaries. The resulting evolution of the level set function is the gradient flow that minimizes the overall energy functional. The proposed variational level set formulation has three main advantages over the traditional level set formulations. First, a significantly larger time step can be used for numerically solving the evolution partial differential equation, and therefore speeds up the curve evolution. Second, the level set function can be initialized with general functions that are more efficient to construct and easier to use in practice than the widely used signed distance function. Third, the level set evolution in our formulation can be easily implemented by simple finite difference scheme and is computationally more efficient. The proposed algorithm has been applied to both simulated and real images with promising results.",2005,28,2042,191,2,12,51,92,141,172,176,210,221,181
9ceedeb5e2ebc39a08c70e2d42d69bca0e064bc1,"A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hierarchical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the performance of some of the criteria may be data dependent.",1985,56,2759,108,4,2,14,10,10,16,10,18,27,37
afdf0c581ad8e194eb6a58a0f7582e49e77bf85d,"We have expanded the reference set of proteins used in SELCON3 by including 11 additional proteins (selected from the reference sets of Yang and co-workers and Keiderling and co-workers). Depending on the wavelength range and whether or not denatured proteins are included in the reference set, five reference sets were constructed with the number of reference proteins varying from 29 to 48. The performance of three popular methods for estimating protein secondary structure fractions from CD spectra (implemented in software packages CONTIN, SELCON3, and CDSSTR) and a variant of CONTIN, CONTIN/LL, that incorporates the variable selection method in the locally linearized model in CONTIN, were examined using the five reference sets described here, and a 22-protein reference set. Secondary structure assignments from DSSP were used in the analysis. The performances of all three methods were comparable, in spite of the differences in the algorithms used in the three software packages. While CDSSTR performed the best with a smaller reference set and larger wavelength range, and CONTIN/LL performed the best with a larger reference set and smaller wavelength range, the performances for individual secondary structures were mixed. Analyzing protein CD spectra using all three methods should improve the reliability of predicted secondary structural fractions. The three programs are provided in CDPro software package and have been modified for easier use with the different reference sets described in this paper. CDPro software is available at the website: http://lamar.colostate.edu/ approximately sreeram/CDPro.",2000,48,2541,68,0,8,34,52,66,85,102,106,118,141
5cee6b7fb97778fb9343a308f5b6982ef9c97135,"Abstract In this paper, the authors study the theory of soft sets initiated by Molodtsov. The authors define equality of two soft sets, subset and super set of a soft set, complement of a soft set, null soft set, and absolute soft set with examples. Soft binary operations like AND, OR and also the operations of union, intersection are defined. De Morgan's laws and a number of results are verified in soft set theory.",2003,10,1946,157,0,1,2,0,3,12,20,43,90,123
166641a1c482e2aadbafed7d5f7e637924442237,This paper describes an extension to the set of Basic Linear Algebra Subprograms. The extensions are targeted at matrix-vector operations that should provide for efficient and portable implementations of algorithms for high-performance computers,1990,53,1933,177,24,27,49,53,58,56,80,86,59,48
b72b370e9494b7ad715969292ff1476df1b1e2fb,"The study of sets is important and thus popular in the business and economic world for three major reasons: Basic understanding of concepts in sets and set algebra provides a form of logical language through which business specialists can communicate important concepts and ideas. Set algebra is used in solving counting problems of a logical nature. The study of set algebra provides a solid background to understanding of probability and statistics, which are important business decision-making tools.",2007,86,1685,131,99,90,88,74,89,100,110,105,91,108
475bbf493d8246031a5152c8005a5c567231307c,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",2007,54,2190,9,3,27,75,83,133,145,189,190,197,190
6e50b1cb4131104e40dc18b4de5398e7bb892e03,"In the context of structural optimization we propose a new numerical method based on a combination of the classical shape derivative and of the level-set method for front propagation. We implement this method in two and three space dimensions for a model of linear or nonlinear elasticity. We consider various objective functions with weight and perimeter constraints. The shape derivative is computed by an adjoint method. The cost of our numerical algorithm is moderate since the shape is captured on a fixed Eulerian mesh. Although this method is not specifically designed for topology optimization, it can easily handle topology changes. However, the resulting optimal shape is strongly dependent on the initial guess.",2004,40,1825,141,11,26,45,48,54,58,75,76,85,94
7d23f522cf17ac3de97d6f16fa8f616d716fb383,"This paper presents a new approach to structural topology optimization. We represent the structural boundary by a level set model that is embedded in a scalar function of a higher dimension. Such level set models are flexible in handling complex topological changes and are concise in describing the boundary shape of the structure. Furthermore, a well-founded mathematical procedure leads to a numerical algorithm that describes a structural optimization as a sequence of motions of the implicit boundaries converging to an optimum solution and satisfying specified constraints. The result is a 3D topology optimization technique that demonstrates outstanding flexibility of handling topological changes, fidelity of boundary representation and degree of automation. We have implemented the algorithm with the use of several robust and efficient numerical techniques of level set methods. The benefit and the advantages of the proposed method are illustrated with several 2D examples that are widely used in the recent literature of topology optimization, especially in the homogenization based methods.",2003,28,1957,64,2,18,29,33,36,60,54,72,85,75
0eb6c9584cb52d91ef202e9bfa0e92676eb0ecf4,"The air–sea fluxes of momentum, heat, freshwater and their components have been computed globally from 1948 at frequencies ranging from 6-hourly to monthly. All fluxes are computed over the 23 years from 1984 to 2006, but radiation prior to 1984 and precipitation before 1979 are given only as climatological mean annual cycles. The input data are based on NCEP reanalysis only for the near surface vector wind, temperature, specific humidity and density, and on a variety of satellite based radiation, sea surface temperature, sea-ice concentration and precipitation products. Some of these data are adjusted to agree in the mean with a variety of more reliable satellite and in situ measurements, that themselves are either too short a duration, or too regional in coverage. The major adjustments are a general increase in wind speed, decrease in humidity and reduction in tropical solar radiation. The climatological global mean air–sea heat and freshwater fluxes (1984–2006) then become 2 W/m2 and −0.1 mg/m2 per second, respectively, down from 30 W/m2 and 3.4 mg/m2 per second for the unaltered data. However, decadal means vary from 7.3 W/m2 (1977–1986) to −0.3 W/m2 (1997–2006). The spatial distributions of climatological fluxes display all the expected features. A comparison of zonally averaged wind stress components across ocean sub-basins reveals large differences between available products due both to winds and to the stress calculation. Regional comparisons of the heat and freshwater fluxes reveal an alarming range among alternatives; typically 40 W/m2 and 10 mg/m2 per second, respectively. The implied ocean heat transports are within the uncertainty of estimates from ocean observations in both the Atlantic and Indo-Pacific basins. They show about 2.4 PW of tropical heating, of which 80% is transported to the north, mostly in the Atlantic. There is similar good agreement in freshwater transport at many latitudes in both basins, but neither in the South Atlantic, nor at 35°N.",2009,134,1286,94,14,38,47,73,121,123,124,124,121,113
901b357c7d4ab59298bd0872554f3f34091c40ff,"I argue that research on organizational configurations has been limited by a mismatch between theory and methods and introduce set-theoretic methods as a viable alternative for overcoming this mismatch. I demonstrate the value of such methods for studying organizational configurations and discuss their applicability for examining equifinality and limited diversity among configurations, as well as their relevance to other research fields such as complementarities theory, complexity theory, and the resource-based view",2007,142,1345,140,13,22,29,40,49,60,85,72,78,155
f0a8e0bf935ae8cb55c09841483dd671d46add28,"Abstract A generalized model of rough sets called variable precision model (VP-model), aimed at modelling classification problems involving uncertain or imprecise information, is presented. The generalized model inherits all basic mathematical properties of the original model introduced by Pawlak. The main concepts are introduced formally and illustrated with simple examples. The application of the model to analysis of knowledge representation systems is also discussed.",1993,53,1893,154,7,10,12,23,22,30,23,41,37,44
71cd76a801da510b3714043629e3bc5e13a3e1ce,"Summary. To systematically identify and analyze the 15 HA and 9 NA subtypes of influenza A virus, we need reliable, simple methods that not only characterize partial sequences but analyze the entire influenza A genome. We designed primers based on the fact that the 15 and 21 terminal segment specific nucleotides of the genomic viral RNA are conserved between all influenza A viruses and unique for each segment. The primers designed for each segment contain influenza virus specific nucleotides at their 3′-end and non-influenza virus nucleotides at the 5′-end. With this set of primers, we were able to amplify all eight segments of N1, N2, N4, N5, and N8 subtypes. For N3, N6, N7, and N9 subtypes, the segment specific sequences of the neuraminidase genes are different. Therefore, we optimized the primer design to allow the amplification of those neuraminidase genes as well. The resultant primer set is suitable for all influenza A viruses to generate full-length cDNAs, to subtype viruses, to sequence their DNA, and to construct expression plasmids for reverse genetics systems.",2001,38,1758,77,2,3,7,14,24,32,52,68,104,145
5d54560c6c88eecccf18cdce4255ce63cc91cb36,"The thermodynamic properties of 154 mineral end-members, 13 silicate liquid end-members and 22 aqueous fluid species are presented in a revised and updated data set. The use of a temperature-dependent thermal expansion and bulk modulus, and the use of high-pressure equations of state for solids and fluids, allows calculation of mineral-fluid equilibria to 100 kbar pressure or higher. A pressure-dependent Landau model for order-disorder permits extension of disordering transitions to high pressures, and, in particular, allows the alpha-beta quartz transition to be handled more satisfactorily. Several melt end- members have been included to enable calculation of simple phase equilibria and as a first stage in developing melt mixing models in NCKFMASH. The simple aqueous species density model has been extended to enable speciation calculations and mineral solubility determination involving minerals and aqueous species at high temperatures and pressures. The data set has also been improved by incorporation of many new phase equilibrium constraints, calorimetric studies and new measurements of molar volume, thermal expansion and compressibility. This has led to a significant improvement in the level of agreement with the available experimental phase equilibria, and to greater flexibility in calculation of complex mineral equilibria. It is also shown that there is very good agreement between the data set and the most recent available calorimetric data. kinetics which apply to determining directly the greatest majority of such equilibria in the laboratory, for forming solid solutions, and inclusion of aqueous and silicate melt species), and provides uncertainties especially at lower temperatures, as well as the diYculty of establishing reversals of reactions involving solid allowing the likely uncertainties on the results of thermodynamic calculations to be estimated. This is a solutions. The levels of precision and accuracy required of thermodynamic data in order to be able to forward- critical issue in that calculations using data sets should always involve uncertainty propagation to help evalu- model synthetic and natural mineral assemblages mean that the continuing upgrading and expansion of the ate the results. Because the experimental phase equilib- ria involve overlapping subsets of compositional space, data set by incorporation of new phase equilibrium constraints, calorimetry and new measurements of the derived thermodynamic data are highly correlated, and it is only the inclusion of the correlations which molar volume, thermal expansion and compressibility are more than justified. enables the reliable calculation of uncertainties on mineral reactions to be performed. Earlier work on mineral thermodynamic data sets for rock-forming minerals includes compilations of The thermodynamic data extraction involves using weighted least squares on the diVerent types of data",2004,350,4003,392,139,122,146,177,188,211,195,227,238,227
4cc29c9fe3c2d0381649465a7be3ab0a662fbcc4,"An improved dynamic programming algorithm is reported for RNA secondary structure prediction by free energy minimization. Thermodynamic parameters for the stabilities of secondary structure motifs are revised to include expanded sequence dependence as revealed by recent experiments. Additional algorithmic improvements include reduced search time and storage for multibranch loop free energies and improved imposition of folding constraints. An extended database of 151,503 nt in 955 structures? determined by comparative sequence analysis was assembled to allow optimization of parameters not based on experiments and to test the accuracy of the algorithm. On average, the predicted lowest free energy structure contains 73 % of known base-pairs when domains of fewer than 700 nt are folded; this compares with 64 % accuracy for previous versions of the algorithm and parameters. For a given sequence, a set of 750 generated structures contains one structure that, on average, has 86 % of known base-pairs. Experimental constraints, derived from enzymatic and flavin mononucleotide cleavage, improve the accuracy of structure predictions.",1999,117,3639,378,15,94,170,234,227,228,236,269,250,230
41078349f5d8c2f32518bcccf4a0cb1f577789a1,"In 1995, the International Association for the Properties of Water and Steam (IAPWS) adopted a new formulation called “The IAPWS Formulation 1995 for the Thermodynamic Properties of Ordinary Water Substance for General and Scientific Use”, which we abbreviate to IAPWS-95 formulation or IAPWS-95 for short. This IAPWS-95 formulation replaces the previous formulation adopted in 1984. This work provides information on the selected experimental data of the thermodynamic properties of water used to develop the new formulation, but information is also given on newer data. The article presents all details of the IAPWS-95 formulation, which is in the form of a fundamental equation explicit in the Helmholtz free energy. The function for the residual part of the Helmholtz free energy was fitted to selected data for the following properties: (a) thermal properties of the single-phase region (pρT) and of the vapor–liquid phase boundary (pσρ′ρ″T), including the phase-equilibrium condition (Maxwell criterion), and (b) t...",2002,7,3342,161,11,39,62,90,83,101,106,117,153,158
943c326ce0555c66d68b4955e1f7f489b7fc8f35,"We demonstrate in this work that the surface tension, water‐organic solvent, transfer‐free energies and the thermodynamics of melting of linear alkanes provide fundamental insights into the nonpolar driving forces for protein folding and protein binding reactions. We first develop a model for the curvature dependence of the hydrophobic effect and find that the macroscopic concept of interfacial free energy is applicable at the molecular level. Application of a well‐known relationship involving surface tension and adhesion energies reveals that dispersion forces play little or no net role in hydrophobic interactions; rather, the standard model of disruption of water structure (entropically driven at 25°C) is correct. The hydrophobic interaction is found, in agreement with the classical picture, to provide a major driving force for protein folding. Analysis of the melting behavior of hydrocarbons reveals that close packing of the protein interior makes only a small free energy contribution to folding because the enthalpic gain resulting from increased dispersion interactions (relative to the liquid) is countered by the freezing of side chain motion. The identical effect should occur in association reactions, which may provide an enormous simplification in the evaluation of binding energies. Protein binding reactions, even between nearly planar or concave/convex interfaces, are found to have effective hydrophobicities considerably smaller than the prediction based on macroscopic surface tension. This is due to the formation of a concave collar region that usually accompanies complex formation. This effect may preclude the formation of complexes between convex surfaces.",1991,34,5119,147,0,9,14,41,110,164,234,335,401,466
2267b5833829a82c017a6a9aa907a775d724748e,"A critical discussion is given of the use of local compositions for representation of excess Gibbs energies of liquid mixtures. A new equation is derived, based on Scott's two-liquid model and on an assumption of nonrandomness similar to that used by Wilson. For the same activity coefficients at infinite dilution, the Gibbs energy of mixing is calculated with the new equation as well as the equations of van Laar, Wilson, and Heil; these four equations give similar results for mixtures of moderate nonideality but they differ appreciably for strongly nonideal systems, especially for those with limited miscibility. The new equation contains a nonrandomness parameter α12 which makes it applicable to a large variety of mixtures. By proper selection of α12, the new equation gives an excellent representation of many types of liquid mixtures while other local composition equations appear to be limited to specific types. Consideration is given to prediction of ternary vapor-liquid and ternary liquid-liquid equilibria based on binary data alone.",1968,0,4945,138,0,0,2,1,2,4,2,7,3,11
0fa6ad7b5fcef5e18c205182a2096cf29cd8e775,,2007,0,2935,171,18,35,62,105,123,194,231,267,247,285
98d6e196543b719f98acb4aa1b2c28e09a63b84e,"A revised regular solution-type thermodynamic model for twelve-component silicate liquids in the system SiO2-TiO2-Al2O3-Fe2O3-Cr2O3-FeO-MgO-CaO-Na2O-K2O-P2O5-H2O is calibrated. The model is referenced to previously published standard state thermodynamic properties and is derived from a set of internally consistent thermodynamic models for solid solutions of the igneous rock forming minerals, including: (Mg,Fe2+,Ca)-olivines, (Na,Mg,Fe2+,Ca)M2 (Mg,Fe2+, Ti, Fe3+, Al)M1 (Fe3+, Al,Si)2TETO6-pyroxenes, (Na,Ca,K)-feldspars, (Mg,Fe2+) (Fe3+, Al, Cr)2O4-(Mg,Fe2+)2 TiO4 spinels and (Fe2+, Mg, Mn2+)TiO3-Fe2O3 rhombohedral oxides. The calibration utilizes over 2,500 experimentally determined compositions of silicate liquids coexisting at known temperatures, pressures and oxygen fugacities with apatite ±feldspar ±leucite ±olivine ±pyroxene ±quartz ±rhombohedral oxides ±spinel ±whitlockite ±water. The model is applicable to natural magmatic compositions (both hydrous and anhydrous), ranging from potash ankaratrites to rhyolites, over the temperature (T) range 900°–1700°C and pressures (P) up to 4 GPa. The model is implemented as a software package (MELTS) which may be used to simulate igneous processes such as (1) equilibrium or fractional crystallization, (2) isothermal, isenthalpic or isochoric assimilation, and (3) degassing of volatiles. Phase equilibria are predicted using the MELTS package by specifying bulk composition of the system and either (1) T and P, (2) enthalpy (H) and P, (3) entropy (S) and P, or (4) T and volume (V). Phase relations in systems open to oxygen are determined by directly specifying the fo2 or the T-P-fo2 (or equivalently H-P-fo2, S-P-fo2, T-V-fo2) evolution path. Calculations are performed by constrained minimization of the appropriate thermodynamic potential. Compositions and proportions of solids and liquids in the equilibrium assemblage are computed.",1995,118,2316,219,9,20,21,43,42,45,50,71,72,77
82b9504d35dc4739a1cd3bd2b4f90ac645659116,"Abstract A thermodynamic theory of “weak” ferromagnetism of α-Fe 2 O 3 , MnCO 3 and CoCO 3 is developed on the basis of landau's theory of phase transitions of the second kind. It is shown that the “weak” ferromagnetism is due to the relativistic spin-lattice and the magnetic dipole interactions. A strong dependence of the properties of “weak” ferromagnetics on the magnetic crystalline symmetry is noted and the behaviour of these ferromagnetics in a magnetic field is studied.",1958,6,3469,56,2,2,4,8,11,13,5,8,7,11
f00b6c02a96dab7a27ab3d69570360d85b5f04ba,,1948,0,4595,85,0,0,0,1,0,0,2,0,0,1
879e8d7778c0ab1479339fe29d3cc4ded78fe4e5,"The probability of a configuration is given in classical theory by the Boltzmann formula exp [— V/hT] where V is the potential energy of this configuration. For high temperatures this of course also holds in quantum theory. For lower temperatures, however, a correction term has to be introduced, which can be developed into a power series of h. The formula is developed for this correction by means of a probability function and the result discussed.",1932,0,5250,45,0,0,0,0,1,0,1,1,0,0
d40fb11afbe101d34f78b27bc448fea20dd3f23f,"A report about values for the entropy, molar volume, and for the enthalpy and Gibbs energy of formation for the elements and minerals and substances at 298.15 K.",1995,0,2583,150,81,69,65,63,58,68,75,65,67,119
5f9604e8f60943e397cebc107f06acc0a5169074,"Abstract : This volume, together with its companion, Selected Values of Thermodynamic Properties of the Elements, represents a complete revision of the work, Selected Values of Thermodynamic Properties of Metals and Alloys, by Hultgren, Orr, Anderson, and Kelley, published in 1963 by John Wiley and Sons, New York. The work should cover pertinent data available at the date printed on the first page of each system. Inspection will show that many or most of the selected values differ from the 1963 edition; many of the differences are substantial. This shows progress in measurement and, at the same time, hints of uncertainties still present.",1973,0,2730,101,4,3,16,32,37,37,48,52,59,65
3c2c03f28eac064da546f33d3372e1088471b3b0,"Recent advances in theoretical geochemistry permit calculation of the standard molal thermodynamic properties of a wide variety of minerals, gases, aqueous species, and reactions from 1 to 5000 bar and 0 to 1000°C. The SUPCRT92 software package facilitates practical application of these recent theories, equations, and data to define equilibrium constraints on geochemical processes in a wide variety of geologic systems. 
 
The SUPCRT92 package is composed of three interactive FORTRAN 77 programs, SUPCRT92, MPRONS92, and CPRONS92, and a sequential-access thermodynamic database, SPRONS92.DAT. The SUPCRT92 program reads or permits user-generation of its two input files, CON and RXN, retrieves data from the direct-access equivalent of SPRONS92.DAT, calculates the standard molal Gibbs free energy, enthalpy, entropy, heat capacity, and volume of each reaction specified on the RXN file through a range of conditions specified on the CON file, and writes the calculated reaction properties to the output TAB file and, optionally, to PLT files that facilitate their graphical depiction. Calculations can be performed along the liquid side of the H2O vaporization boundary by specifying either temperature (T) or pressure (P), and in the single-phase regions of fluid H2O by specifying either T and P, T and H2O density, T and log K, or P and log K. SPRONS92.DAT, which contains standard molal thermodynamic properties at 25°C and 1 bar, equation-of-state parameters, heat capacity coefficients, and phase transition data for approximately 500 minerals, gases, and aqueous species, can be augmented or otherwise modified using MPRONS92, and converted to its direct-access equivalent using CPRONS92.",1992,41,2070,104,11,22,31,38,25,42,52,32,56,57
0b9193580334b1529287aed6a91d1221f09126dd,"An updated genome‐scale reconstruction of the metabolic network in Escherichia coli K‐12 MG1655 is presented. This updated metabolic reconstruction includes: (1) an alignment with the latest genome annotation and the metabolic content of EcoCyc leading to the inclusion of the activities of 1260 ORFs, (2) characterization and quantification of the biomass components and maintenance requirements associated with growth of E. coli and (3) thermodynamic information for the included chemical reactions. The conversion of this metabolic network reconstruction into an in silico model is detailed. A new step in the metabolic reconstruction process, termed thermodynamic consistency analysis, is introduced, in which reactions were checked for consistency with thermodynamic reversibility estimates. Applications demonstrating the capabilities of the genome‐scale metabolic model to predict high‐throughput experimental growth and gene deletion phenotypic screens are presented. The increased scope and computational capability using this new reconstruction is expected to broaden the spectrum of both basic biology and applied systems biology studies of E. coli metabolism.",2007,86,1404,120,6,67,86,122,120,137,122,142,125,99
09cae7408f2ded45a53bcea325e8b3480a71009d,,1988,0,1893,158,1,11,25,46,47,47,59,69,54,54
c5cfe026fce5d5626f876dc8f465f2224c4a22d8,"The models recognize that ZrSiO4, ZrTiO4,  and TiSiO4, but not ZrO2 or TiO2, are independently variable phase components in zircon. Accordingly, the equilibrium controlling the Zr content of rutile coexisting with zircon is ZrSiO4 = ZrO2 (in rutile) +  SiO2. The equilibrium controlling the Ti content of zircon is either ZrSiO4 + TiO2 = ZrTiO4 + SiO2 or TiO2 + SiO2 = TiSiO4, depending whether Ti substitutes for Si or Zr. The Zr content of rutile thus depends on the activity of SiO2$$(a_{\text{SiO}_{2}})$$ as well as T, and the Ti content of zircon depends on $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ as well as T. New and published experimental data confirm the predicted increase in the Zr content of rutile with decreasing $$a_{\text{SiO}_{2}},$$ and unequivocally demonstrate that the Ti content of zircon increases with decreasing $$a_{\text{SiO}_{2}}$$. The substitution of Ti in zircon therefore is primarily for Si. Assuming a constant effect of P, unit $$a_{\text{ZrSiO}_{4}},$$ and that $$a_{\text{ZrO}_{2}}$$ and $$a_{\text{ZrTiO}_{4}}$$ are proportional to ppm Zr in rutile and ppm Ti in zircon, [log(ppm Zr-in-rutile) + log$$a_{\text{SiO}_{2}}$$] = A1 + B1/T(K) and  [log(ppm Ti-in-zircon) + log$$a_{\text{SiO}_{2}}$$ − log$$a_{\text{TiO}_{2}}$$] = A2 + B2/T, where the A and B are constants. The constants were derived from published and new data from experiments with $$a_{\text{SiO}_{2}}$$ buffered by either quartz or zircon + zirconia, from experiments with $$a_{\text{SiO}_{2}}$$ defined by the Zr content of rutile, and from well-characterized natural samples. Results are A1 = 7.420 ± 0.105;  B1 = −4,530 ± 111;  A2 = 5.711 ± 0.072;  B2 = −4,800 ± 86 with activity referenced to α-quartz and rutile at P and T of interest. The zircon thermometer may now be applied to rocks without quartz and/or rutile, and the rutile thermometer applied to rocks without quartz, provided that $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ are estimated. Maximum uncertainties introduced to zircon and rutile thermometry by unconstrained $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ can be quantitatively assessed and are ≈60 to 70°C at 750°C. A preliminary assessment of the dependence of the two thermometers on P predicts that an uncertainty of ±1 GPa introduces an additional uncertainty at 750°C of ≈50°C for the Ti-in-zircon thermometer and of ≈70 to 80°C for the Zr-in-rutile thermometer.",2007,14,1241,138,7,17,37,44,44,37,48,82,86,101
c3ccd84ec1d1453bc5766234a00c4d899dd5ca31,,1971,0,2415,102,17,11,31,37,38,42,52,48,62,48
683fb521e4e1a08954db267952f9878a37adbca9,"Abstract A numerical model for the simulation of sea ice circulation and thickness over a seasonal cycle is presented. This model is used to investigate the effects of ice dynamics on Arctic ice thickness and air-sea heat flux characteristics by carrying out several numerical simulations over the entire Arctic Ocean region. The essential idea in the model is to couple the dynamics to the ice thickness characteristics by allowing the ice interaction to become stronger as the ice becomes thicker and/or contains a lower areas percentage of thin ice. The dynamics in turn causes high oceanic heat losses in regions of ice divergence and reduced heat losses in regions of convergence. TO model these effects consistently the ice is considered to interact in a plastic manner with the plastic strength chosen to depend on the ice thickness and concentration. The thickness and concentration, in turn, evolve according to continuity equations which include changes in ice mass and percent of open water due to advection, ...",1979,0,1891,143,5,6,14,10,14,14,9,12,20,14
e821a03f897328fb0589f47c3014a1d4d1d87469,"The structural properties of free nanoclusters are reviewed. Special attention is paid to the interplay of energetic, thermodynamic, and kinetic factors in the explanation of cluster structures that are actually observed in experiments. The review starts with a brief summary of the experimental methods for the production of free nanoclusters and then considers theoretical and simulation issues, always discussed in close connection with the experimental results. The energetic properties are treated first, along with methods for modeling elementary constituent interactions and for global optimization on the cluster potential-energy surface. After that, a section on cluster thermodynamics follows. The discussion includes the analysis of solid-solid structural transitions and of melting, with its size dependence. The last section is devoted to the growth kinetics of free nanoclusters and treats the growth of isolated clusters and their coalescence. Several specific systems are analyzed.",2005,467,1338,17,6,45,76,90,87,67,103,109,109,101
f1ce2a74f0fa4bb31690efcfa62dabf9fee7ff58,"Abstract : Contents: Methods of evaluation, atomic weights, fundamental constants, symbols and units, general references, and properties of the elements.",1973,0,1981,29,0,5,18,29,47,43,65,68,63,44
f3f6b18aa6349d904a719688f605e836dd3a4ec8,"The behavior of any system at high enough temperatures approaches that of its classical counterpart. The probability of any configurational position is then proportional to exp—U/kT, with U the potential energy. Wigner has shown that quantum‐mechanical deviations, as the temperature is lowered, may be approximated by multiplication of this with 1–f, where f is a function proportional to h2 and having terms in T−2 and terms in T−3. This type of approximation is unsatisfactory for a system of many degrees of freedom, that is, one of many dependent molecules. For such a system it is shown that instead of Wigner's approximation we may replace the classical potential U in the exponent by U—kTf, where f is the same as Wigner's function.",1947,1,2049,149,0,2,2,0,2,0,0,0,1,0
bbe9992b30d352501716cf3978a79e6026704bc0,,1991,0,1557,9,0,17,45,63,59,78,57,65,82,63
fa8787db8f8d438383b10293a947a4e42f97eca2,"Motivated by the computation of scattering amplitudes at strong coupling, we consider minimal area surfaces in AdS5 which end on a null polygonal contour at the boundary. We map the classical problem of finding the surface into an SU(4) Hitchin system. The polygon with six edges is the first non-trivial example. For this case, we write an integral equation which determines the area as a function of the shape of the polygon. The equations are identical to those of the Thermodynamics Bethe Ansatz. Moreover, the area is given by the free energy of this TBA system. The high temperature limit of the TBA system can be exactly solved. It leads to an explicit expression for a special class of hexagonal contours.",2009,83,215,25,5,47,18,18,20,14,11,22,10,14
6f826d2863352923695839a35530043e6de2faed,,1947,0,1986,100,0,0,1,1,2,0,5,4,3,4
baf71c23b505f4f3bcf0e45d737087be13514116,"This study presents ISORROPIA II, a thermo- dynamic equilibrium model for the K + -Ca 2+ -Mg 2+ -NH + - Na + -SO 2 -NO 3 -Cl -H2O aerosol system. A comprehen- sive evaluation of its performance is conducted against water uptake measurements for laboratory aerosol and predictions of the SCAPE2 thermodynamic module over a wide range of atmospherically relevant conditions. The two models agree well, to within 13% for aerosol water content and total PM mass, 16% for aerosol nitrate and 6% for aerosol chloride and ammonium. Largest discrepancies were found under condi- tions of low RH, primarily from differences in the treatment of water uptake and solid state composition. In terms of com- putational speed, ISORROPIA II was more than an order of magnitude faster than SCAPE2, with robust and rapid con- vergence under all conditions. The addition of crustal species does not slow down the thermodynamic calculations (com- pared to the older ISORROPIA code) because of optimiza- tions in the activity coefficient calculation algorithm. Based on its computational rigor and performance, ISORROPIA II appears to be a highly attractive alternative for use in large scale air quality and atmospheric transport models.",2007,91,1058,40,4,8,16,14,33,49,42,54,73,100
1ebb1c0989f5e437e7222ad0ebd4808b62bd358a,,1985,0,1561,14,0,17,27,43,43,46,54,52,40,48
5588a41780e2f397d1d8f5b7195f3426ea863581,"The application of the conventional permeability equations to the study of biological membranes leads often to contradictions. It is shown that the equations generally used, based on two permeability coefficients—the solute permeability coefficient and the water permeability coefficient—are incompatible with the requirements of thermodynamics of irreversible processes. 
 
The inconsistencies are removed by a thermodynamic treatment, following the approach of Staverman, which leads to a three coefficient system taking into account the interactions: solute-solvent, solute-membrane and solvent-membrane. 
 
The equations derived here have been applied to various permeability measurements found in the literature, such as: the penetration of heavy water into animal cells, permeability of blood vessels, threshold concentration of plasmolysis and relaxation experiments with artificial membranes. 
 
It is shown how the pertinent coefficients may be derived from the experimental data and how to choose suitable conditions in order to obtain all the required information on the permeability of the membranes. 
 
The significance of these coefficients for the elucidation of membrane structure is pointed out.",1958,10,1848,72,0,1,4,0,3,5,8,7,14,14
d8c6a43ba76ee72c7dd3fc5911a3e565e1ad472d,The temporal correlations of thermodynamic concentration fluctuations have been measured in a chemically reactive system at equilibrium by observing fluctuations of the fluorescence of a reaction product. The experiment yields the chemical rate constants and diffusion coefficients and shows the coupling among them. Data are reported for binding of ethidium bromide to DNA.,1972,0,1553,62,0,4,8,4,5,4,6,4,2,4
ba5c27fbbb26094b3fc844bb0afb0491e0dfca83,"Abstract : Recommended values are provided for chemical thermodynamic properties of inorganic substances and for organic substances usually containing only one or two carbon atoms. Where available, values are given for the enthalpy of formation, Gibbs energy of formation, entropy, and heat capacity at 298.15 K (25 C), the enthalpy difference between 298.15 and 0 K and the enthalpy of formation at 0 K. All values are given in SI units and are for a standard state pressure of 100 000 pascal. This volume is a new collective edition of 'Selected Values of Chemical Thermodynamic Properties,' which was issued serially as National Bureau of Standards Technical Notes 270-1 (1965) to 270-8 (1981). Values are given for properties of gaseous, liquid and crystalline substances, for solutions in water, and for mixed aqueous and organic solutions. Values are not given for alloys or other solid solutions, fused salts or for substances of undefined composition. Compounds of the transuranium elements are not included. (Author)",1982,0,1286,70,3,2,4,6,6,8,12,12,13,10
a43f2b4724d9e4a38de0ef8f3bbf51337a71d5fe,An equation of state is proposed for the mixture of hard spheres based on an averaging process over the two results of the solution of the Percus–Yevick integral equation for the mixture of hard spheres. Compressibility and other equilibrium properties of the binary mixtures of hard spheres are calculated and they are compared with the related machine‐calculated (Monte Carlo and molecular dynamics) data. The comparison shows excellent agreement between the proposed equation of state and the machine‐calculated data.,1971,15,1621,11,1,4,4,4,4,3,12,4,3,12
01f6eab4e5853e7b6085f3417fa6c637218f166a,"A computationally efficient and rigorous thermodynamic model that predicts the physical state and composition of inorganic atmospheric aerosol is presented. One of the main features of the model is the implementation of mutual deliquescence of multicomponent salt particles, which lowers the deliquescence point of the aerosol phase.The model is used to examine the behavior of four types of tropospheric aerosol (marine, urban, remote continental and non-urban continental), and the results are compared with the predictions of two other models currently in use. The results of all three models were generally in good agreement. Differences were found primarily in the mutual deliquescence humidity regions, where the new model predicted the existence of water, and the other two did not. Differences in the behavior (speciation and water absorbing properties) between the aerosol types are pointed out. The new model also needed considerably less CPU time, and always shows stability and robust convergence.",1998,36,1135,116,1,6,3,9,19,23,22,64,46,60
a27bff788a802242f1196246cae8b6e9e0e5dd6e,"v. 1. Elements O, HD, T, F, Cl, Br, I, He, Ne, Ar, Kr, Xe, Rn, S, N, P, and their compounds. pt. 1. Methods and computation. pt. 2. Tables v. 2. Elements C, Si, Ge, Sn, Pb, and their compounds. pt. 1. Methods and computation. pt. 2. Tables v. 3. Elements B, Al, Ga, In, Tl, Be, Mg, Ca, Sr, Ba, and their compounds. pt. 1. Methods and computation. pt. 2. Tables.",1994,0,1167,18,29,27,26,42,41,31,39,47,42,40
ff7302c1a174654b7912ccc18a6c2cbdc0320613,"We use the string hypothesis for the mirror theory to derive the Thermodynamic Bethe Ansatz equations for the AdS5 × S5 mirror model. We further demonstrate how these equations can be used to construct the associated Y-system recently discussed in the literature, putting particular emphasis on the assumptions and the range of validity of the corresponding construction.",2009,66,418,14,35,57,43,37,31,29,40,36,28,29
d891e37d4c5b6490a0832b9da4c027464db6641c,"The composition of the phase assemblage and pore solution of Portland cements hydrated between 0-60°C were modeled as a function of time and temperature. Results of thermodynamic modeling showed good agreement with experimental data gained at 5, 20, and 50°C. At 5 and 20°C, a similar phase assemblage was calculated to be present, while at ~50°C, thermodynamic calculations predicted conversion of ettringite and monocarbonate to monosulphate. Modeling showed that in Portland cements having an Al2O3/SO3 ratio of > 1.3 (bulk weight), above 50°C monosulphate and monocarbonate are present. In Portland cements containing less Al (Al2O3/SO3 < 1.3), above 50°C monosulphate and small amounts of ettringite are expected to persist. A good correlation between calculated porosity and measured compressive strength was observed.",2008,83,652,26,8,15,33,35,32,43,46,60,62,77
85071fc496825e30a60ede2eebde09717071b679,"Iron oxides occur ubiquitously in environmental, geological, planetary, and technological settings. They exist in a rich variety of structures and hydration states. They are commonly fine-grained (nanophase) and poorly crystalline. This review summarizes recently measured thermodynamic data on their formation and surface energies. These data are essential for calculating the thermodynamic stability fields of the various iron oxide and oxyhydroxide phases and understanding their occurrence in natural and anthropogenic environments. The competition between surface enthalpy and the energetics of phase transformation leads to the general conclusion that polymorphs metastable as micrometer-sized or larger crystals can often be thermodynamically stabilized at the nanoscale. Such size-driven crossovers in stability help to explain patterns of occurrence of different iron oxides in nature.",2008,29,599,16,11,24,34,49,47,53,39,52,50,47
71ea4c3aeb896a8af6eccce9eba9a0a6cfe7a6d4,"The production of functional molecular architectures through self-assembly is commonplace in biology, but despite advances, it is still a major challenge to achieve similar complexity in the laboratory. Self-assembled structures that are reproducible and virtually defect free are of interest for applications in three-dimensional cell culture, templating, biosensing and supramolecular electronics. Here, we report the use of reversible enzyme-catalysed reactions to drive self-assembly. In this approach, the self-assembly of aromatic short peptide derivatives provides a driving force that enables a protease enzyme to produce building blocks in a reversible and spatially confined manner. We demonstrate that this system combines three features: (i) self-correction--fully reversible self-assembly under thermodynamic control; (ii) component-selection--the ability to amplify the most stable molecular self-assembly structures in dynamic combinatorial libraries; and (iii) spatiotemporal confinement of nucleation and structure growth. Enzyme-assisted self-assembly therefore provides control in bottom-up fabrication of nanomaterials that could ultimately lead to functional nanostructures with enhanced complexities and fewer defects.",2009,30,412,1,11,35,32,41,32,44,41,32,35,33
dbe4b7e3dcfb57ab565bbd90474e77e154e3b2e8,"Improved thermodynamic parameters for prediction of RNA duplex formation are derived from optical melting studies of 90 oligoribonucleotide duplexes containing only Watson-Crick base pairs. To test end or base composition effects, new sets of duplexes are included that have identical nearest neighbors, but different base compositions and therefore different ends. Duplexes with terminal GC pairs are more stable than duplexes with the same nearest neighbors but terminal AU pairs. Penalizing terminal AU base pairs by 0.45 kcal/mol relative to terminal GC base pairs significantly improves predictions of DeltaG degrees37 from a nearest-neighbor model. A physical model is suggested in which the differential treatment of AU and GC ends accounts for the dependence of the total number of Watson-Crick hydrogen bonds on the base composition of a duplex. On average, the new parameters predict DeltaG degrees37, DeltaH degrees, DeltaS degrees, and TM within 3.2%, 6.0%, 6.8%, and 1.3 degreesC, respectively. These predictions are within the limit of the model, based on experimental results for duplexes predicted to have identical thermodynamic parameters.",1998,135,1021,48,0,6,23,18,20,23,34,44,58,60
6c2cb0efb697ac1805d8736dd70a9b07a02d958d,"Using tabulated thermodynamic data, a comprehensive investigation of the thermodynamic stability of binary oxides in contact with silicon at 1000 K was conducted. Reactions between silicon and each binary oxide at 1000 K, including those involving ternary phases, were considered. Sufficient data exists to conclude that all binary oxides except the following are thermodynamically unstable in contact with silicon at 1000 K: Li{sub 2}O, most of the alkaline earth oxides (BeO, MgO, CaO, and SrO), the column IIIB oxides (Sc{sub 2}O{sub 3}, Y{sub 2}O{sub 3}, and {ital Re}{sub 2}O{sub 3}, where {ital Re} is a rare earth), ThO{sub 2}, UO{sub 2}, ZrO{sub 2}, HfO{sub 2}, and Al{sub 2}O{sub 3}. Of these remaining oxides, sufficient data exists to conclude that BeO, MgO, CaO, and ZrO{sub 2} are thermodynamically stable in contact with silicon at 1000 K. Our results are consistent with reported investigations of silicon/binary oxide interfaces and identify candidate materials for future investigations. {copyright} {ital 1996 Materials Research Society.}",1996,109,1129,15,0,0,0,16,26,63,98,88,80,92
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
fc40d567e4434e96fc1c1437d16f4855d3280abb,"The foreign body reaction composed of macrophages and foreign body giant cells is the end-stage response of the inflammatory and wound healing responses following implantation of a medical device, prosthesis, or biomaterial. A brief, focused overview of events leading to the foreign body reaction is presented. The major focus of this review is on factors that modulate the interaction of macrophages and foreign body giant cells on synthetic surfaces where the chemical, physical, and morphological characteristics of the synthetic surface are considered to play a role in modulating cellular events. These events in the foreign body reaction include protein adsorption, monocyte/macrophage adhesion, macrophage fusion to form foreign body giant cells, consequences of the foreign body response on biomaterials, and cross-talk between macrophages/foreign body giant cells and inflammatory/wound healing cells. Biomaterial surface properties play an important role in modulating the foreign body reaction in the first two to four weeks following implantation of a medical device, even though the foreign body reaction at the tissue/material interface is present for the in vivo lifetime of the medical device. An understanding of the foreign body reaction is important as the foreign body reaction may impact the biocompatibility (safety) of the medical device, prosthesis, or implanted biomaterial and may significantly impact short- and long-term tissue responses with tissue-engineered constructs containing proteins, cells, and other biological components for use in tissue engineering and regenerative medicine. Our perspective has been on the inflammatory and wound healing response to implanted materials, devices, and tissue-engineered constructs. The incorporation of biological components of allogeneic or xenogeneic origin as well as stem cells into tissue-engineered or regenerative approaches opens up a myriad of other challenges. An in depth understanding of how the immune system interacts with these cells and how biomaterials or tissue-engineered constructs influence these interactions may prove pivotal to the safety, biocompatibility, and function of the device or system under consideration.",2008,161,3439,153,17,70,126,163,160,186,277,318,316,382
39d931673c5f50a98b2165e8ff25ebcf4fbd3f8f,"The field of biomaterials has become a vital area, as these materials can enhance the quality and longevity of human life and the science and technology associated with this field has now led to multi-million dollar business. The paper focuses its attention mainly on titanium-based alloys, even though there exists biomaterials made up of ceramics, polymers and composite materials. The paper discusses the biomechanical compatibility of many metallic materials and it brings out the overall superiority of Ti based alloys, even though it is costlier. As it is well known that a good biomaterial should possess the fundamental properties such as better mechanical and biological compatibility and enhanced wear and corrosion resistance in biological environment, the paper discusses the influence of alloy chemistry, thermomechanical processing and surface condition on these properties. In addition, this paper also discusses in detail the various surface modification techniques to achieve superior biocompatibility, higher wear and corrosion resistance. Overall, an attempt has been made to bring out the current scenario of Ti based materials for biomedical applications.",2009,156,3417,69,13,59,135,138,228,269,308,302,351,366
257305088dd5c98284436ce991dfcd96df446428,"Biomass represents an abundant carbon-neutral renewable resource for the production of bioenergy and biomaterials, and its enhanced use would address several societal needs. Advances in genetics, biotechnology, process chemistry, and engineering are leading to a new manufacturing concept for converting renewable biomass to valuable fuels and products, generally referred to as the biorefinery. The integration of agroenergy crops and biorefinery manufacturing technologies offers the potential for the development of sustainable biopower and biomaterials that will lead to a new manufacturing paradigm.",2006,59,4830,76,37,117,177,181,260,363,350,414,423,460
b21b5a36acad5be6dd47f3332dd0a05333287001,"New generations of synthetic biomaterials are being developed at a rapid pace for use as three-dimensional extracellular microenvironments to mimic the regulatory characteristics of natural extracellular matrices (ECMs) and ECM-bound growth factors, both for therapeutic applications and basic biological studies. Recent advances include nanofibrillar networks formed by self-assembly of small building blocks, artificial ECM networks from protein polymers or peptide-conjugated synthetic polymers that present bioactive ligands and respond to cell-secreted signals to enable proteolytic remodeling. These materials have already found application in differentiating stem cells into neurons, repairing bone and inducing angiogenesis. Although modern synthetic biomaterials represent oversimplified mimics of natural ECMs lacking the essential natural temporal and spatial complexity, a growing symbiosis of materials engineering and cell biology may ultimately result in synthetic materials that contain the necessary signals to recapitulate developmental processes in tissue- and organ-specific differentiation and morphogenesis.",2005,141,3965,71,23,90,147,144,200,254,293,320,314,368
fbd2a5392926f69ef17fb1093c40874b055e8c18,"As a lightweight metal with mechanical properties similar to natural bone, a natural ionic presence with significant functional roles in biological systems, and in vivo degradation via corrosion in the electrolytic environment of the body, magnesium-based implants have the potential to serve as biocompatible, osteoconductive, degradable implants for load-bearing applications. This review explores the properties, biological performance, challenges and future directions of magnesium-based biomaterials.",2006,55,3257,69,6,23,40,70,115,152,175,236,282,333
e3c98b087826c8bf032d1934fe96130ae7f39c03,"Abstract During the past two decades significant advances have been made in the development of biodegradable polymeric materials for biomedical applications. Degradable polymeric biomaterials are preferred candidates for developing therapeutic devices such as temporary prostheses, three-dimensional porous structures as scaffolds for tissue engineering and as controlled/sustained release drug delivery vehicles. Each of these applications demands materials with specific physical, chemical, biological, biomechanical and degradation properties to provide efficient therapy. Consequently, a wide range of natural or synthetic polymers capable of undergoing degradation by hydrolytic or enzymatic route are being investigated for biomedical applications. This review summarizes the main advances published over the last 15 years, outlining the synthesis, biodegradability and biomedical applications of biodegradable synthetic and natural polymers.",2007,222,3340,77,3,30,70,121,196,229,253,288,359,314
0416e7ec53a62833045bd3e25845b69c947089ff,,2008,0,3412,96,194,225,247,300,267,373,439,398,277,131
e6d528559f803caa1577247c6be8dd15c9580d1c,"Two complementary strategies can be used in the fabrication of molecular biomaterials. In the 'top-down' approach, biomaterials are generated by stripping down a complex entity into its component parts (for example, paring a virus particle down to its capsid to form a viral cage). This contrasts with the 'bottom-up' approach, in which materials are assembled molecule by molecule (and in some cases even atom by atom) to produce novel supramolecular architectures. The latter approach is likely to become an integral part of nanomaterials manufacture and requires a deep understanding of individual molecular building blocks and their structures, assembly properties and dynamic behaviors. Two key elements in molecular fabrication are chemical complementarity and structural compatibility, both of which confer the weak and noncovalent interactions that bind building blocks together during self-assembly. Using natural processes as a guide, substantial advances have been achieved at the interface of nanomaterials and biology, including the fabrication of nanofiber materials for three-dimensional cell culture and tissue engineering, the assembly of peptide or protein nanotubes and helical ribbons, the creation of living microlenses, the synthesis of metal nanowires on DNA templates, the fabrication of peptide, protein and lipid scaffolds, the assembly of electronic materials by bacterial phage selection, and the use of radiofrequency to regulate molecular behaviors.",2003,105,2753,29,0,27,60,97,115,135,130,142,215,181
62343a260e8d476cb49491ee4b1e9c66e8b8c773,"Silk from the silkworm, Bombyx mori, has been used as biomedical suture material for centuries. The unique mechanical properties of these fibers provided important clinical repair options for many applications. During the past 20 years, some biocompatibility problems have been reported for silkworm silk; however, contamination from residual sericin (glue-like proteins) was the likely cause. More recent studies with well-defined silkworm silk fibers and films suggest that the core silk fibroin fibers exhibit comparable biocompatibility in vitro and in vivo with other commonly used biomaterials such as polylactic acid and collagen. Furthermore, the unique mechanical properties of the silk fibers, the diversity of side chain chemistries for 'decoration' with growth and adhesion factors, and the ability to genetically tailor the protein provide additional rationale for the exploration of this family of fibrous proteins for biomaterial applications. For example, in designing scaffolds for tissue engineering these properties are particularly relevant and recent results with bone and ligament formation in vitro support the potential role for this biomaterial in future applications. To date, studies with silks to address biomaterial and matrix scaffold needs have focused on silkworm silk. With the diversity of silk-like fibrous proteins from spiders and insects, a range of native or bioengineered variants can be expected for application to a diverse set of clinical needs.",2003,117,2645,36,8,28,38,49,93,110,138,143,182,169
2ba2bf0bf72e415f4124d3e485019f659b2708ee,"The development of tissue engineering in the field of orthopaedic surgery is now booming. Two fields of research in particular are emerging: the association of osteo-inductive factors with implantable materials; and the association of osteogenic stem cells with these materials (hybrid materials). In both cases, an understanding of the phenomena of cell adhesion and, in particular, understanding of the proteins involved in osteoblast adhesion on contact with the materials is of crucial importance. The proteins involved in osteoblast adhesion are described in this review (extracellular matrix proteins, cytoskeletal proteins, integrins, cadherins, etc.). During osteoblast/material interactions, their expression is modified according to the surface characteristics of materials. Their involvement in osteoblastic response to mechanical stimulation highlights the significance of taking them into consideration during development of future biomaterials. Finally, an understanding of the proteins involved in osteoblast adhesion opens up new possibilities for the grafting of these proteins (or synthesized peptide) onto vector materials, to increase their in vivo bioactivity or to promote cell integration within the vector material during the development of hybrid materials.",2000,158,2238,64,2,19,44,47,54,83,73,105,114,122
a42d6065d0b1a31c597bcc3b7c0252881a2a2e33,"Fluorescence methods are being used increasingly in biochemical, medical, and chemical research. This is because of the inherent sensitivity of this technique. and the favorable time scale of the phenomenon of fluorescence. 8 Fluorescence emission occurs about 10- sec (10 nsec) after light absorp tion. During this period of time a wide range of molecular processes can occur, and these can effect the spectral characteristics of the fluorescent compound. This combination of sensitivity and a favorable time scale allows fluorescence methods to be generally useful for studies of proteins and membranes and their interactions with other macromolecules. This book describes the fundamental aspects of fluorescence. and the biochemical applications of this methodology. Each chapter starts with the -theoreticalbasis of each phenomenon of fluorescence, followed by examples which illustrate the use of the phenomenon in the study of biochemical problems. The book contains numerous figures. It is felt that such graphical presentations contribute to pleasurable reading and increased understand ing. Separate chapters are devoted to fluorescence polarization, lifetimes, quenching, energy transfer, solvent effects, and excited state reactions. To enhance the usefulness of this work as a textbook, problems are included which illustrate the concepts described in each chapter. Furthermore, a separate chapter is devoted to the instrumentation used in fluorescence spectroscopy. This chapter will be especially valuable for those perform ing or contemplating fluorescence measurements. Such measurements are easily compromised by failure to consider a number of simple principles.""",1983,96,26279,1664,0,0,0,0,0,0,0,0,0,0
6165d59e158c88267b1154c167da68bfca644f4a,"X-ray photoelectron spectroscopy (XPS) is a dedicated surface The surface of steel disc was examined by means of XPS technique, and its chemical Moulder J.F., Stickle W.F., Sobol P.E., Bomben K.D.: Handbook of X-ray. X-ray photoelectron spectroscopy of films that were exposed to air revealed surface oxidation, with titanium Figure 2 shows the XPS titanium 2p ionization region of an 82 nm thick film. Handbook of Chemistry and Physics, 92nd Ed., CRC. The valences of Ni and Ti ions were determined by X-ray photoelectron First of all, the analysis of XPS spectra have shown that there is no influence Riggs W, Davis L, Moulder J, Muilenberg 1979 Handbook of X-ray Photoelectron. Moulder, J. F. & Chastain, J. Handbook of X-ray Photoelectron Spectroscopy: a Reference Book of Standard Spectra for Identification and Interpretation of XPS. X-ray photoelectron spectroscopy confirmed the coordination of HP with",1992,0,15371,466,0,0,0,0,0,0,0,0,0,0
6f5e15196862d888a296afc942b72848590976e4,"A software package for the analysis of X-ray absorption spectroscopy (XAS) data is presented. This package is based on the IFEFFIT library of numerical and XAS algorithms and is written in the Perl programming language using the Perl/Tk graphics toolkit. The programs described here are: (i) ATHENA, a program for XAS data processing, (ii) ARTEMIS, a program for EXAFS data analysis using theoretical standards from FEFF and (iii) HEPHAESTUS, a collection of beamline utilities based on tables of atomic absorption data. These programs enable high-quality data analysis that is accessible to novices while still powerful enough to meet the demands of an expert practitioner. The programs run on all major computer platforms and are freely available under the terms of a free software license.",2005,13,9396,214,7,51,106,149,238,259,358,456,614,729
ffdedd94afc7278b4348186df980f9ae2df80ad6,"1. Introduction 2. Quantum Dynamics in Hilbert Space 3. The Density Operator and Quantum Dynamics in Liouville Space 4. Quantum Electrodynamics, Optical Polarization, and Nonlinear Spectroscopy 5. Nonlinear Response Functions and Optical Susceptibilities 6. The Optical Response Functions of a Multilevel System with Relaxation 7. Semiclassical Simulation of the Optical Response Functions 8. The Cumulant Expansion and the Multimode Brownian Oscillator Model 9. Fluorescence, Spontaneous-Raman and Coherent-Raman Spectroscopy 10. Selective Elimination of Inhomogeneous Broadening Photon Echoes 11. Resonant Gratings, Pump-Probe, and Hole Burning Spectroscopy 12. Wavepacket Dynamics in Liouville Space The Wigner Representation 13. Wavepacket Analysis of Nonimpulsive Measurements 14. Off-Resonance Raman Scattering 15. Polarization Spectroscopy Birefringence and Dichroism 16. Nonlinear Response of Molecular Assemblies The Local-Field Approximation 17. Many Body and Cooperative Effects in the Nonlinear Response",1995,0,3761,239,14,37,79,112,103,113,140,135,157,178
8cb01b154ae81d71a82bfcbacd6de6761891b208,"This paper describes a new NMR imaging modality--MR diffusion tensor imaging. It consists of estimating an effective diffusion tensor, Deff, within a voxel, and then displaying useful quantities derived from it. We show how the phenomenon of anisotropic diffusion of water (or metabolites) in anisotropic tissues, measured noninvasively by these NMR methods, is exploited to determine fiber tract orientation and mean particle displacements. Once Deff is estimated from a series of NMR pulsed-gradient, spin-echo experiments, a tissue's three orthotropic axes can be determined. They coincide with the eigenvectors of Deff, while the effective diffusivities along these orthotropic directions are the eigenvalues of Deff. Diffusion ellipsoids, constructed in each voxel from Deff, depict both these orthotropic axes and the mean diffusion distances in these directions. Moreover, the three scalar invariants of Deff, which are independent of the tissue's orientation in the laboratory frame of reference, reveal useful information about molecular mobility reflective of local microstructure and anatomy. Inherently tensors (like Deff) describing transport processes in anisotropic media contain new information within a macroscopic voxel that scalars (such as the apparent diffusivity, proton density, T1, and T2) do not.",1994,38,5169,371,1,12,13,23,23,46,33,54,84,71
f58236e6f1e94196c57b9a29e79eeb2e8f175059,"Preface. Preface to the First Edition. Contributors. Contributors to the First Edition. Chapter 1. Fundamentals of Impedance Spectroscopy (J.Ross Macdonald and William B. Johnson). 1.1. Background, Basic Definitions, and History. 1.1.1 The Importance of Interfaces. 1.1.2 The Basic Impedance Spectroscopy Experiment. 1.1.3 Response to a Small-Signal Stimulus in the Frequency Domain. 1.1.4 Impedance-Related Functions. 1.1.5 Early History. 1.2. Advantages and Limitations. 1.2.1 Differences Between Solid State and Aqueous Electrochemistry. 1.3. Elementary Analysis of Impedance Spectra. 1.3.1 Physical Models for Equivalent Circuit Elements. 1.3.2 Simple RC Circuits. 1.3.3 Analysis of Single Impedance Arcs. 1.4. Selected Applications of IS. Chapter 2. Theory (Ian D. Raistrick, Donald R. Franceschetti, and J. Ross Macdonald). 2.1. The Electrical Analogs of Physical and Chemical Processes. 2.1.1 Introduction. 2.1.2 The Electrical Properties of Bulk Homogeneous Phases. 2.1.2.1 Introduction. 2.1.2.2 Dielectric Relaxation in Materials with a Single Time Constant. 2.1.2.3 Distributions of Relaxation Times. 2.1.2.4 Conductivity and Diffusion in Electrolytes. 2.1.2.5 Conductivity and Diffusion-a Statistical Description. 2.1.2.6 Migration in the Absence of Concentration Gradients. 2.1.2.7 Transport in Disordered Media. 2.1.3 Mass and Charge Transport in the Presence of Concentration Gradients. 2.1.3.1 Diffusion. 2.1.3.2 Mixed Electronic-Ionic Conductors. 2.1.3.3 Concentration Polarization. 2.1.4 Interfaces and Boundary Conditions. 2.1.4.1 Reversible and Irreversible Interfaces. 2.1.4.2 Polarizable Electrodes. 2.1.4.3 Adsorption at the Electrode-Electrolyte Interface. 2.1.4.4 Charge Transfer at the Electrode-Electrolyte Interface. 2.1.5 Grain Boundary Effects. 2.1.6 Current Distribution, Porous and Rough Electrodes- the Effect of Geometry. 2.1.6.1 Current Distribution Problems. 2.1.6.2 Rough and Porous Electrodes. 2.2. Physical and Electrochemical Models. 2.2.1 The Modeling of Electrochemical Systems. 2.2.2 Equivalent Circuits. 2.2.2.1 Unification of Immitance Responses. 2.2.2.2 Distributed Circuit Elements. 2.2.2.3 Ambiguous Circuits. 2.2.3 Modeling Results. 2.2.3.1 Introduction. 2.2.3.2 Supported Situations. 2.2.3.3 Unsupported Situations: Theoretical Models. 2.2.3.4 Unsupported Situations: Equivalent Network Models. 2.2.3.5 Unsupported Situations: Empirical and Semiempirical Models. Chapter 3. Measuring Techniques and Data Analysis. 3.1. Impedance Measurement Techniques (Michael C. H. McKubre and Digby D. Macdonald). 3.1.1 Introduction. 3.1.2 Frequency Domain Methods. 3.1.2.1 Audio Frequency Bridges. 3.1.2.2 Transformer Ratio Arm Bridges. 3.1.2.3 Berberian-Cole Bridge. 3.1.2.4 Considerations of Potentiostatic Control. 3.1.2.5 Oscilloscopic Methods for Direct Measurement. 3.1.2.6 Phase-Sensitive Detection for Direct Measurement. 3.1.2.7 Automated Frequency Response Analysis. 3.1.2.8 Automated Impedance Analyzers. 3.1.2.9 The Use of Kramers-Kronig Transforms. 3.1.2.10 Spectrum Analyzers. 3.1.3 Time Domain Methods. 3.1.3.1 Introduction. 3.1.3.2 Analog-to-Digital (A/D) Conversion. 3.1.3.3 Computer Interfacing. 3.1.3.4 Digital Signal Processing. 3.1.4 Conclusions. 3.2. Commercially Available Impedance Measurement Systems (Brian Sayers). 3.2.1 Electrochemical Impedance Measurement Systems. 3.2.1.1 System Configuration. 3.2.1.2 Why Use a Potentiostat? 3.2.1.3 Measurements Using 2, 3 or 4-Terminal Techniques. 3.2.1.4 Measurement Resolution and Accuracy. 3.2.1.5 Single Sine and FFT Measurement Techniques. 3.2.1.6 Multielectrode Techniques. 3.2.1.7 Effects of Connections and Input Impedance. 3.2.1.8 Verification of Measurement Performance. 3.2.1.9 Floating Measurement Techniques. 3.2.1.10 Multichannel Techniques. 3.2.2 Materials Impedance Measurement Systems. 3.2.2.1 System Configuration. 3.2.2.2 Measurement of Low Impedance Materials. 3.2.2.3 Measurement of High Impedance Materials. 3.2.2.4 Reference Techniques. 3.2.2.5 Normalization Techniques. 3.2.2.6 High Voltage Measurement Techniques. 3.2.2.7 Temperature Control. 3.2.2.8 Sample Holder Considerations. 3.3. Data Analysis (J. Ross Macdonald). 3.3.1 Data Presentation and Adjustment. 3.3.1.1 Previous Approaches. 3.3.1.2 Three-Dimensional Perspective Plotting. 3.3.1.3 Treatment of Anomalies. 3.3.2 Data Analysis Methods. 3.3.2.1 Simple Methods. 3.3.2.2 Complex Nonlinear Least Squares. 3.3.2.3 Weighting. 3.3.2.4 Which Impedance-Related Function to Fit? 3.3.2.5 The Question of ""What to Fit"" Revisited. 3.3.2.6 Deconvolution Approaches. 3.3.2.7 Examples of CNLS Fitting. 3.3.2.8 Summary and Simple Characterization Example. Chapter 4. Applications of Impedance Spectroscopy. 4.1. Characterization of Materials (N. Bonanos, B. C. H. Steele, and E. P. Butler). 4.1.1 Microstructural Models for Impedance Spectra of Materials. 4.1.1.1 Introduction. 4.1.1.2 Layer Models. 4.1.1.3 Effective Medium Models. 4.1.1.4 Modeling of Composite Electrodes. 4.1.2 Experimental Techniques. 4.1.2.1 Introduction. 4.1.2.2 Measurement Systems. 4.1.2.3 Sample Preparation-Electrodes. 4.1.2.4 Problems Associated With the Measurement of Electrode Properties. 4.1.3 Interpretation of the Impedance Spectra of Ionic Conductors and Interfaces. 4.1.3.1 Introduction. 4.1.3.2 Characterization of Grain Boundaries by IS. 4.1.3.3 Characterization of Two-Phase Dispersions by IS. 4.1.3.4 Impedance Spectra of Unusual Two-phase Systems. 4.1.3.5 Impedance Spectra of Composite Electrodes. 4.1.3.6 Closing Remarks. 4.2. Characterization of the Electrical Response of High Resistivity Ionic and Dielectric Solid Materials by Immittance Spectroscopy (J. Ross Macdonald). 4.2.1 Introduction. 4.2.2 Types of Dispersive Response Models: Strengths and Weaknesses. 4.2.2.1 Overview. 4.2.2.2 Variable-slope Models. 4.2.2.3 Composite Models. 4.2.3 Illustration of Typical Data Fitting Results for an Ionic Conductor. 4.3. Solid State Devices (William B. Johnson and Wayne L. Worrell). 4.3.1 Electrolyte-Insulator-Semiconductor (EIS) Sensors. 4.3.2 Solid Electrolyte Chemical Sensors. 4.3.3 Photoelectrochemical Solar Cells. 4.3.4 Impedance Response of Electrochromic Materials and Devices (Gunnar A. Niklasson, Anna Karin Johsson, and Maria Stromme). 4.3.4.1 Introduction. 4.3.4.2 Materials. 4.3.4.3 Experimental Techniques. 4.3.4.4 Experimental Results on Single Materials. 4.3.4.5 Experimental Results on Electrochromic Devices. 4.3.4.6 Conclusions and Outlook. 4.3.5 Time-Resolved Photocurrent Generation (Albert Goossens). 4.3.5.1 Introduction-Semiconductors. 4.3.5.2 Steady-State Photocurrents. 4.3.5.3 Time-of-Flight. 4.3.5.4 Intensity-Modulated Photocurrent Spectroscopy. 4.3.5.5 Final Remarks. 4.4. Corrosion of Materials (Digby D. Macdonald and Michael C. H. McKubre). 4.4.1 Introduction. 4.4.2 Fundamentals. 4.4.3 Measurement of Corrosion Rate. 4.4.4 Harmonic Analysis. 4.4.5 Kramer-Kronig Transforms. 4.4.6 Corrosion Mechanisms. 4.4.6.1 Active Dissolution. 4.4.6.2 Active-Passive Transition. 4.4.6.3 The Passive State. 4.4.7 Point Defect Model of the Passive State (Digby D. Macdonald). 4.4.7.1 Introduction. 4.4.7.2 Point Defect Model. 4.4.7.3 Electrochemical Impedance Spectroscopy. 4.4.7.4 Bilayer Passive Films. 4.4.8 Equivalent Circuit Analysis (Digby D. Macdonald and Michael C. H. McKubre). 4.4.8.1 Coatings. 4.4.9 Other Impedance Techniques. 4.4.9.1 Electrochemical Hydrodynamic Impedance (EHI). 4.4.9.2 Fracture Transfer Function (FTF). 4.4.9.3 Electrochemical Mechanical Impedance. 4.5. Electrochemical Power Sources. 4.5.1 Special Aspects of Impedance Modeling of Power Sources (Evgenij Barsoukov). 4.5.1.1 Intrinsic Relation Between Impedance Properties and Power Sources Performance. 4.5.1.2 Linear Time-Domain Modeling Based on Impedance Models, Laplace Transform. 4.5.1.3 Expressing Model Parameters in Electrical Terms, Limiting Resistances and Capacitances of Distributed Elements. 4.5.1.4 Discretization of Distributed Elements, Augmenting Equivalent Circuits. 4.5.1.5 Nonlinear Time-Domain Modeling of Power Sources Based on Impedance Models. 4.5.1.6 Special Kinds of Impedance Measurement Possible with Power Sources-Passive Load Excitation and Load Interrupt. 4.5.2 Batteries (Evgenij Barsoukov). 4.5.2.1 Generic Approach to Battery Impedance Modeling. 4.5.2.2 Lead Acid Batteries. 4.5.2.3 Nickel Cadmium Batteries. 4.5.2.4 Nickel Metal-hydride Batteries. 4.5.2.5 Li-ion Batteries. 4.5.3 Impedance Behavior of Electrochemical Supercapacitors and Porous Electrodes (Brian E. Conway). 4.5.3.1 Introduction. 4.5.3.2 The Time Factor in Capacitance Charge or Discharge. 4.5.3.3 Nyquist (or Argand) Complex-Plane Plots for Representation of Impedance Behavior. 4.5.3.4 Bode Plots of Impedance Parameters for Capacitors. 4.5.3.5 Hierarchy of Equivalent Circuits and Representation of Electrochemical Capacitor Behavior. 4.5.3.6 Impedance and Voltammetry Behavior of Brush Electrode Models of Porous Electrodes. 4.5.3.7 Impedance Behavior of Supercapacitors Based on Pseudocapacitance. 4.5.3.8 Deviations of Double-layer Capacitance from Ideal Behavior: Representation by a Constant-phase Element (CPE). 4.5.4 Fuel Cells (Norbert Wagner). 4.5.4.1 Introduction. 4.5.4.2 Alkaline Fuel Cells (AFC). 4.5.4.3 Polymer Electrolyte Fuel Cells (PEFC). 4.5.4.4 Solid Oxide Fuel Cells (SOFC). Appendix. Abbreviations and Definitions of Models. References. Index.",2005,395,4760,173,7,35,88,145,170,224,282,351,386,381
bd10d9b0934c8aabfa8a1aa6e59d8d76f2ad0aae,,1968,0,8279,206,1,2,5,9,10,17,28,27,35,48
d6e447a92dafed2271364117fc9b76cbf5f09a64,"In the Preface to Identification of Essential Oil Components by Gas Chromatography/Quadrupole Mass Spectroscopy [sic], Robert P. Adams, the author, states that he began research on essential oils in 1966. This may account for the reason that he persistently uses the term mass spectroscopy rather than mass spectrometry (Robert, there are no light bulbs inside those mass spectrometers). Again, as was the case with the second edition, there is no indication that this is a third edition; but, as in the second edition, the author clearly acknowledges this fact in the Preface. This edition has spectra for 1606 compounds. Adams takes some liberty in saying that this is 400 more than the previous edition, which actually had spectra for 1252 compounds. Because the spectra were acquired on an HP 5970 Mass Selective Detector (spectra in the previous editions were acquired using a Finnigan ITD800 internal ionization quadrupole ion trap instrument), the display format of the spectra corresponds to that of the Agilent (formerly Hewlett-Packard) ChemStation mass spectral display rather than the Finnigan ITD-800 (now Thermo Electron). The ChemStation has the ordinate of the spectrum labeled in ion abundance rather than percent abundance or relative percent intensity. One of the curiosities of these spectra is that all exhibit exactly the same maximum abundance. As with the previous edition, each spectrum has a structure showing chirality were appropriate, retention time on a DB-5 column, and Kovat’s index. Details are provided about the GC column and the operating conditions of the GC as well as the sample injected into the GC (volume, split ratio, and the internal standard used for retention time). The only important parameter missing from the operation of the mass spectrometer is the rate at which the data are acquired (the number of spectra per second). Another important factor would be the identification of the ChemStation version used. ChemStation has two very different versions of Autotune, and the only way to distinguish between which version was used is by the ChemStation version number. Adams states that data were acquired after using the instrument’s “Autotune” to set the operating conditions. As was criticized in the review of the second edition, this book has some significant shortcomings in the brief text portion. They go beyond the criticism of calling a mass spectral peak an ion. In the discussion of whether or not mass spectra acquired with a quadrupole ion trap (QIT) and a transmission quadrupole are comparable, the author displays what he says are spectra of 3-methyl4-heptone obtained on both instruments. The spectrum displayed that was reported to have been obtained on the transmission quadrupole is not that of 3-methyl-4heptone but that of 2-methyl-4-heptone. The telltale peak at m/z 58, which is 30% of the intensity of the peak at m/z 57 (the base peak), is the giveaway. This m/z 58 peak is obviously missing in the spectrum obtained with the QIT and is also missing from a spectrum for 3-methyl-4-heptone obtained on a transmission quadrupole that appeared in the second edition. Another area exhibiting a limited understanding of electron ionization mass spectrometry is the comparison of spectra of tricyclene. The author points to the fact that the peak at m/z 77 is 102% of the intensity of the peak at m/z 79 in the spectrum obtained with the QIT, whereas the peak at m/z 77 is 98% of the peak at m/z 79 in the spectrum obtained with the transmission quadrupole. These relative intensity differences are insignificant, especially when the intensity of the peak at m/z 77 relative to the intensity of the base peak is about the same in both spectra ( 40%). More effort could have been put into the proofing of these few pages of text. It is stated that a compound is represented by a chromatographic peak that has a retention-time range of 5.589 to 5.67. Then, in an attempt to show how the recorded retention time is determined using the equation “5.89 min 0.03 min 5.62 min” is presented, it is obvious the “5.89” in the equation should have been “5.589”, but this should have been caught in the proofing stage. The references to journal articles would be more valuable if they included titles, which is now the requirement for the Journal of the American Society for Mass Spectrometry. This is especially true for the M. P. Clay reference that cites an unnamed article that appeared in Mass Spec Source, an obscure organ published by Scientific Instrument Services in Ringoes, NJ (http://www.sisweb.com). As was the case with the second edition, the “Appendices” consist of an alphabetical listing of compounds (mostly common names) with their retention times and Kovat’s index on a DB-5 capillary GC column Published online September 28, 2005",2001,1,3283,192,1,4,18,41,72,129,147,198,236,268
2ef3a171b1565c07d323f84559b454c2790aeff8,"Abstract Recent Raman scattering studies in different types of graphene samples are reviewed here. We first discuss the first-order and the double resonance Raman scattering mechanisms in graphene, which give rise to the most prominent Raman features. The determination of the number of layers in few-layer graphene is discussed, giving special emphasis to the possibility of using Raman spectroscopy to distinguish a monolayer from few-layer graphene stacked in the Bernal (AB) configuration. Different types of graphene samples produced both by exfoliation and using epitaxial methods are described and their Raman spectra are compared with those of 3D crystalline graphite and turbostratic graphite, in which the layers are stacked with rotational disorder. We show that Resonance Raman studies, where the energy of the excitation laser line can be tuned continuously, can be used to probe electrons and phonons near the Dirac point of graphene and, in particular allowing a determination to be made of the tight-binding parameters for bilayer graphene. The special process of electron–phonon interaction that renormalizes the phonon energy giving rise to the Kohn anomaly is discussed, and is illustrated by gated experiments where the position of the Fermi level can be changed experimentally. Finally, we discuss the ability of distinguishing armchair and zig-zag edges by Raman spectroscopy and studies in graphene nanoribbons in which the Raman signal is enhanced due to resonance with singularities in the density of electronic states.",2009,97,3851,78,4,75,116,222,289,337,350,370,402,398
04e7b197209d40c5479400fc5a3c7332ec2ef605,"Abstract We review recent work on Raman spectroscopy of graphite and graphene. We focus on the origin of the D and G peaks and the second order of the D peak. The G and 2 D Raman peaks change in shape, position and relative intensity with number of graphene layers. This reflects the evolution of the electronic structure and electron–phonon interactions. We then consider the effects of doping on the Raman spectra of graphene. The Fermi energy is tuned by applying a gate-voltage. We show that this induces a stiffening of the Raman G peak for both holes and electrons doping. Thus Raman spectroscopy can be efficiently used to monitor number of layers, quality of layers, doping level and confinement.",2007,176,5551,98,3,20,52,114,196,258,369,456,460,548
98e5e3e09c7a5dfee943934c807120fa266a9525,"Localized surface plasmon resonance (LSPR) spectroscopy of metallic nanoparticles is a powerful technique for chemical and biological sensing experiments. Moreover, the LSPR is responsible for the electromagnetic-field enhancement that leads to surface-enhanced Raman scattering (SERS) and other surface-enhanced spectroscopic processes. This review describes recent fundamental spectroscopic studies that reveal key relationships governing the LSPR spectral location and its sensitivity to the local environment, including nanoparticle shape and size. We also describe studies on the distance dependence of the enhanced electromagnetic field and the relationship between the plasmon resonance and the Raman excitation energy. Lastly, we introduce a new form of LSPR spectroscopy, involving the coupling between nanoparticle plasmon resonances and adsorbate molecular resonances. The results from these fundamental studies guide the design of new sensing experiments, illustrated through applications in which researchers use both LSPR wavelength-shift sensing and SERS to detect molecules of chemical and biological relevance.",2007,131,4668,81,11,82,113,151,235,304,342,417,443,437
a3d40fc31c613028c55e9c175305c2410e3f4d50,"Abstract High-resolution fluorescence spectroscopy was used to characterize dissolved organic matter (DOM) in concentrated and unconcentrated water samples from a wide variety of freshwater, coastal and marine environments. Several types of fluorescent signals were observed, including humic-like, tyrosine-like, and tryptophan-like. Humic-like fluorescence consisted of two peaks, one stimulated by UV excitation (peak A) and one by visible excitation (peak C). For all samples, the positions of both excitation and emission maxima for peak C were dependent upon wavelength of observation, with a shift towards longer wavelength emission maximum at longer excitation wavelength and longer wavelength excitation maximum at longer emission wavelength. A trend was observed in the position of wavelength-independent maximum fluorescence ( Ex max Em max ) for peak C, with maximum at shorter excitation and emission wavelengths for marine samples than for freshwater samples. Mean positions of these maxima were: rivers Ex max Em max = 340 448 nm; coastal water Ex max Em max = 342 442 nm; marine shallow transitional Ex max Em max = 310 423 nm; marine shallow eutrophic Ex max Em max = 299 389 nm; and marine deep Ex max Em max = 340 438 nm. Differences suggest that the humic material in marine surface waters is chemically different from humic material in the other environments sampled. These results explain previous conflicting reports regarding fluorescence properties of DOM from natural waters and also provide a means of distinguishing between water mass sources in the ocean.",1996,37,2603,353,2,6,6,13,18,17,21,24,45,35
b8422d7ac1085191b7b52479bdc69d0f07178d7a,This review discusses the application of infrared spectroscopy to the study of proteins. The focus is on the mid-infrared spectral region and the study of protein reactions by reaction-induced infrared difference spectroscopy.,2007,469,2874,118,2,20,41,62,81,94,164,144,202,252
7a5dd48a99595df65b929ec95ef40bc93a7e0b0b,"Raman spectroscopy has historically played an important role in the structural characterization of graphitic materials, in particular providing valuable information about defects, stacking of the graphene layers and the finite sizes of the crystallites parallel and perpendicular to the hexagonal axis. Here we review the defect-induced Raman spectra of graphitic materials from both experimental and theoretical standpoints and we present recent Raman results on nanographites and graphenes. The disorder-induced D and D' Raman features, as well as the G'-band (the overtone of the D-band which is always observed in defect-free samples), are discussed in terms of the double-resonance (DR) Raman process, involving phonons within the interior of the 1st Brillouin zone of graphite and defects. In this review, experimental results for the D, D' and G' bands obtained with different laser lines, and in samples with different crystallite sizes and different types of defects are presented and discussed. We also present recent advances that made possible the development of Raman scattering as a tool for very accurate structural analysis of nano-graphite, with the establishment of an empirical formula for the in- and out-of-plane crystalline size and even fancier Raman-based information, such as for the atomic structure at graphite edges, and the identification of single versus multi-graphene layers. Once established, this knowledge provides a powerful machinery to understand newer forms of sp(2) carbon materials, such as the recently developed pitch-based graphitic foams. Results for the calculated Raman intensity of the disorder-induced D-band in graphitic materials as a function of both the excitation laser energy (E(laser)) and the in-plane size (L(a)) of nano-graphites are presented and compared with experimental results. The status of this research area is assessed, and opportunities for future work are identified.",2007,116,3151,46,9,31,46,83,129,190,244,314,260,318
7e237afb27010a033a0c5823e787c4aab8e5882e,,1983,0,4424,57,0,2,13,23,45,50,81,85,90,105
3c7fcd2d18209100ccc46857bb33e9b7e1d31be2,"The use of Raman spectroscopy to reveal the remarkable structure and the unusual electronic and phonon properties of single wall carbon nanotubes (SWNTs) is reviewed comprehensively. The various types of Raman scattering processes relevant to carbon nanotubes are reviewed, and the theoretical foundations for these topics are presented. The most common experimental techniques used to probe carbon nanotubes are summarized, followed by a review of the novel experimental findings for each of the features in the first order and second order Raman spectra for single wall carbon nanotubes. These results are presented and discussed in connection with theoretical considerations. Raman spectra for bundles of SWNTs, for SWNTs surrounded by various common wrapping agents, and for isolated SWNTs at the single nanotube level are reviewed. Some of the current research challenges facing the field are briefly summarized.",2005,121,3087,71,7,36,83,103,124,159,217,247,259,280
7cc8ed07c0b4637029c9f595b2dda59c37e2fdfd,"Recently, we have shown that net magnetization transfer among scalar coupled homonuclear spins can be obtained by the application of a spin-lock field (I), or more effectively, by the application of a phase-alternated spin-lock field (2, 3). Analogous methods to accomplish net homonuclear magnetization transfer, based on different rfirradiation schemes have previously been reported by Braunschweiler and Ernst (4). As they and others (5-9) have pointed out, the key to net magnetization transfer between two coupled spins, A and X, is to remove the Zeeman contributions, HzA and Hzx, from the Hamiltonian, or to make them identical, i.e., HzA = Hz,. This can be accomplished by suitable rf irradiation schemes or by zero-field NMR (JO). In this communication we describe a new mixing scheme that is based on the MLEV-16 composite pulse decoupling cycle (II). We have modified this cycle to make it less sensitive to pulse imperfections and it will be shown that this type of MLEV mixing provides net magnetization transfer over a substantial bandwidth with only limited rf power. More importantly, the apparent decay constant of spin-locked magnetization can be prolonged by up to a factor of two (compared to TIP) by using this new type of mixing scheme. If the Zeeman part of the Hamiltonian is eliminated, the spin system will evolve solely under the influence of scalar coupling. Magnetization can then propagate through the molecule in a way that is very similar to spin diffusion among protons in a rigid solid, where dipolar couplings are usually much larger than differences in chemical shift. For molecules consisting of only two coupled homonuclear spins, A and X, Braunschweiler and Ernst have shown that in the isotropic coupling limit there is an oscillatory exchange of the A and X magnetization, with period 1 /JAx. Explicit results for the AX2 case have very recently been presented by Chandrakumar and Subramanian (12). For larger spin systems a computer simulation program appears to be the easiest way to predict the rate at which magnetization will propagate through the molecule. AS demonstrated earlier (2), the net magnetization transfer obtained in this type of experiment permits the recording of phase-sensitive spectra, and gives in many cases enhanced resolution and sensitivity compared to the widely used COSY experiment (13-16). For short mixing times (<0.1/J), only direct connectivities will be observed. For longer mixing times, magnetization that has been transferred from spin A to spin M during the first part of the mixing period can be relayed to spin X during the second",1985,27,3378,111,1,7,33,50,75,99,131,150,167,169
a7b6e4b5a830776775732b02f0ea5c657b0cb1ec,"In 1978 it was discovered, largely through the work of Fleischmann, Van Duyne, Creighton, and their coworkers that molecules adsorbed on specially prepared silver surfaces produce a Raman spectrum that is at times a millionfold more intense than expected. This effect was dubbed surface-enhanced Raman scattering (SERS). Since then the effect has been demonstrated with many molecules and with a number of metals, including Cu, Ag, Au, Li, Na, K, In, Pt, and Rh. In addition, related phenomena such as surface-enhanced second-harmonic generation, four-wave mixing, absorption, and fluorescence have been observed. Although not all fine points of the enhancement mechanism have been clarified, the majority view is that the largest contributor to the intensity amplification results from the electric field enhancement that occurs in the vicinity of small, interacting metal particles that are illuminated with light resonant or near resonant with the localized surface-plasmon frequency of the metal structure. Small in this context is gauged in relation to the wavelength of light. The special preparations required to produce the effect, which include among other techniques electrochemical oxidation-reduction cycling, deposition of metal on very cold substrates, and the generation of metal-island films and colloids, is now understood to be necessary as a means of producing surfaces with appropriate electromagnetic resonances that may couple to electromagnetic fields either by generating rough films (as in the case of the former two examples) or by placing small metal particles in close proximity to one another (as in the case of the latter two). For molecules chemisorbed on SERS-active surface there exists a ""chemical enhancement"" in addition to the electromagnetic effect. Although difficult to measure accurately, the magnitude of this effect rarely exceeds a factor of 10 and is best thought to arise from the modification of the Raman polarizability tensor of the adsorbate resulting from the formation of a complex between the adsorbate and the metal. Rather than an enhancement mechanism, the chemical effect is more logically to be regarded as a change in the nature and identity of the adsorbate.",1985,188,4063,29,1,10,30,32,39,37,35,18,29,22
372dc91b7f6c3ee06508b34b14a1bb06380d8fa8,"A new general technique for the investigation of exchange processes in molecular systems is proposed and demonstrated. Applications comprise the study of chemical exchange, of magnetization transfer by inter‐ and intramolecular relaxation in liquids, and of spin diffusion and cross‐relaxation processes in solids.",1979,9,3643,90,0,9,12,26,36,47,63,67,84,104
bf68d6a655a6024a485ca585bcc7a8418d02a881,"Vibrational and Rotational Spectra. IR Experimental Considerations. Molecular Symmetry. The Vibrational Origin of Group Frequencies. Methyl and Methylene Groups. Triple Bonds and Cumulated Double Bonds. Olefin Groups. Aromatic and Heteroaromatic Rings. Carbonyl Compounds. Ethers, Alcohols, and Phenols. Amines, C=N, and N=O Compounds. Compounds Conking Boron, Silicon, Phosphorus, Sulfur, or Halogen. Major Spectra-Structure Correlations by Spectral Regions. The Theoretical Analysis of Molecular Vibrations.",1965,0,4349,40,3,6,13,14,19,23,11,15,16,17
3f82a8924d5780793ef09267ed4169ef4653d9ef,"Series Preface.Preface.Acronyms, Abbreviations and Symbols.About the Author.1. Introduction.1.1 Electromagnetic Radiation.1.2 Infrared Absorptions.1.3 Normal Modes of Vibration.1.4 Complicating Factors.1.4.1 Overtone and Combination Bands.1.4.2 Fermi Resonance.1.4.3 Coupling.1.4.4 Vibration-Rotation Bands.References.2. Experimental Methods.2.1 Introduction.2.2 Dispersive Infrared Spectrometers.2.3 Fourier-Transform Infrared Spectrometers.2.3.1 Michelson Interferometers.2.3.2 Sources and Detectors.2.3.3 Fourier-Transformation.2.3.4 Moving Mirrors.2.3.5 Signal-Averaging.2.3.6 Advantages.2.3.7 Computers.2.3.8 Spectra.2.4 Transmission Methods.2.4.1 Liquids and Solutions.2.4.2 Solids.2.4.3 Gases.2.4.4 Pathlength Calibration.2.5 Reflectance Methods.2.5.1 Attenuated Total Reflectance Spectroscopy.2.5.2 Specular Reflectance Spectroscopy.2.5.3 Diffuse Reflectance Spectroscopy.2.5.4 Photoacoustic Spectroscopy.2.6 Microsampling Methods.2.7 Chromatography-Infrared Spectroscopy.2.8 Thermal Analysis-Infrared Spectroscopy.2.9 Other Techniques.References.3. Spectral Analysis.3.1 Introduction.3.2 Group Frequencies.3.2.1 Mid-Infrared Region.3.2.2 Near-Infrared Region.3.2.3 Far-Infrared Region.3.3 Identification.3.4 Hydrogen Bonding.3.5 Spectrum Manipulation.3.5.1 Baseline Correction.3.5.2 Smoothing.3.5.3 Difference Spectra.3.5.4 Derivatives.3.5.5 Deconvolution.3.5.6 Curve-Fitting.3.6 Concentration.3.7 Simple Quantitative Analysis.3.7.1 Analysis of Liquid Samples.3.7.2 Analysis of Solid Samples.3.8 Multi-Component Analysis.3.9 Calibration Methods.References.4. Organic Molecules.4.1 Introduction.4.2 Aliphatic Hydrocarbons.4.3 Aromatic Compounds.4.4 Oxygen-Containing Compounds.4.4.1 Alcohols and Phenols.4.4.2 Ethers.4.4.3 Aldehydes and Ketones.4.4.4 Esters.4.4.5 Carboxylic Acids and Anhydrides.4.5 Nitrogen-Containing Compounds.4.5.1 Amines.4.5.2 Amides.4.6 Halogen-Containing Compounds.4.7 Heterocyclic Compounds.4.8 Boron Compounds.4.9 Silicon Compounds.4.10 Phosphorus Compounds.4.11 Sulfur Compounds.4.12 Near-Infrared Spectra.4.13 Identification.References.5. Inorganic Molecules.5.1 Introduction.5.2 General Considerations.5.3 Normal Modes of Vibration.5.4 Coordination Compounds.5.5 Isomerism.5.6 Metal Carbonyls.5.7 Organometallic Compounds.5.8 Minerals.References.6. Polymers.6.1 Introduction.6.2 Identification.6.3 Polymerization.6.4 Structure.6.5 Surfaces.6.6 Degradation.References.7. Biological Applications.7.1 Introduction.7.2 Lipids.7.3 Proteins and Peptides.7.4 Nucleic Acids.7.5 Disease Diagnosis.7.6 Microbial Cells.7.7 Plants.7.8 Clinical Chemistry.References.8. Industrial and Environmental Applications.8.1 Introduction.8.2 Pharmaceutical Applications.8.3 Food Science.8.4 Agricultural Applications.8.5 Pulp and Paper Industries.8.6 Paint Industry.8.7 Environmental Applications.References.Responses to Self-Assessment Questions.Bibliography.Glossary of Terms.SI Units and Physical Constants.Periodic Table.Index.",2004,19,2415,151,2,3,12,25,46,67,101,123,138,159
8c1f7a4726fede07873f698abd8c47d873b3c935,"To address data management and data exchange problems in the nuclear magnetic resonance (NMR) community, the Collaborative Computing Project for the NMR community (CCPN) created a “Data Model” that describes all the different types of information needed in an NMR structural study, from molecular structure and NMR parameters to coordinates. This paper describes the development of a set of software applications that use the Data Model and its associated libraries, thus validating the approach. These applications are freely available and provide a pipeline for high‐throughput analysis of NMR data. Three programs work directly with the Data Model: CcpNmr Analysis, an entirely new analysis and interactive display program, the CcpNmr FormatConverter, which allows transfer of data from programs commonly used in NMR to and from the Data Model, and the CLOUDS software for automated structure calculation and assignment (Carnegie Mellon University), which was rewritten to interact directly with the Data Model. The ARIA 2.0 software for structure calculation (Institut Pasteur) and the QUEEN program for validation of restraints (University of Nijmegen) were extended to provide conversion of their data to the Data Model. During these developments the Data Model has been thoroughly tested and used, demonstrating that applications can successfully exchange data via the Data Model. The software architecture developed by CCPN is now ready for new developments, such as integration with additional software applications and extensions of the Data Model into other areas of research. Proteins 2005. © 2005 Wiley‐Liss, Inc.",2005,27,2521,132,7,8,20,38,65,72,98,148,163,195
ce7f14f54ccd3f3e199c091c800eb7b91fb251ee,"A. Schoenhals, F. Kremer: Theory of Dielectric Relaxation.- F. Kremer, A. Schoenhals: Broadband Dielectric Measurement Techniques.- A. Schoenhals, F. Kremer: Analysis of Dielectric Spectra.- F. Kremer, A. Schoenhals: The Scaling of the Dynamics of Glasses and Supercooled Liquids.- P. Lunkenheimer, A. Loidl:Glassy Dynamics Beyond the a-Relaxation.- F. Kremer, A. Huwe, A. Schoenhals, S. Rozanski: Molecular Dynamics in Confining Space.- A. Schoenhals: Molecular Dynamics in Polymer Model Systems.- G. Floudas: Effect of Pressure on the Dielectric Spectra of Polymeric Systems.- J. Mijovich: Dielectric Spectroscopy of Reactive Polymeric Systems.- F. Kremer, A. Schoenhals: Collective and Molecular Dynamics of (Polymeric) Liquid Crystals.- L. Hartmann, K. Fukao, F. Kremer: Molecular Dynamics in thin Polymer Layers.- F. Kremer, S. Rozanski: The Dielectric Poperties of Semiconducting Disordered Solids.- P.A.M. Steeman, J. v. Turnhout: The Dielectric Properties of Inhomogeneous Media.- R. Boehmer, G. Diezemann: Principles and Applications of Pulsed Dielectric Spectroscopy and Nonresonant Dielectric Hole Burning.- R. Richert: Local Dielectric Relaxation by Solvation Dynamics.- T. Pakula: Dielectric and Dynamic Mechanical Spectroscopy-A Comparison.- R. Boehmer, F. Kremer: Dielectric and (Multidimensional) NMR Spectroscopy-A Comparison.- A. Arbe, J. Colmenero, D. Richter: Polymer Dynamics by Dielectric Spectroscopy and Neutron Scattering-A Comparison",2003,0,2667,80,17,25,57,70,90,97,77,129,164,161
499083b0b8b26aac54091e220ef3817ac9ed3276,"The use of optical measurements to monitor electrochemical changes on the surface of nanosized metal particles is discussed within the Drude model. The absorption spectrum of a metal sol in water is shown to be strongly affected by cathodic or anodic polarization, chemisorption, metal adatom deposition, and alloying. Anion adsorption leads to strong damping of the free electron absorption. Cathodic polarization leads to anion desorption. Underpotential deposition (upd) of electropositive metal layers results in dramatic blue-shifts of the surface plasmon band of the substrate. The deposition of just 0.1 monolayer can be readily detected by eye. In some cases alloying occurs spontaneously during upd. Alloy formation can be ascertained from the optical absorption spectrum in the case of gold deposition onto silver sols. The underpotential deposition of silver adatoms onto palladium leads to the formation of a homogeneous silver shell, but the mean free path is less than predicted, due to lattice strain in t...",1996,0,2984,46,1,9,10,19,8,17,33,41,53,65
f3f7c208c7ef56040c878a0dcda895a25ba1cf47,"SummaryA novel approach to tailored selective excitation for the measurement of NMR spectra in non-deuterated aqueous solutions (WATERGATE, WATER suppression by GraAdient-Tailored Excitation) is described. The gradient echo sequence, which effectively combines one selective 180° radiofrequency pulse and two field gradient pulses, achieves highly selective and effective water suppression. This technique is ideally suited for the rapid collection of multi-dimensional data since a single-scan acquisition produces a pure phase NMR spectrum with a perfectly flat baseline, at the highest possible sensitivity. Application to the fast measurement of 2D NOE data of a 2.2. mM solution of a double-stranded DNA fragment in 90% H2O at 5 °C is presented.",1992,19,2794,74,0,1,9,31,40,93,90,139,126,138
dd6bba61338637997714c30479a2b6879df8424e,"A new technique, deep‐level transient spectroscopy (DLTS), is introduced. This is a high‐frequency capacitance transient thermal scanning method useful for observing a wide variety of traps in semiconductors. The technique is capable of displaying the spectrum of traps in a crystal as positive and negative peaks on a flat baseline as a function of temperature. It is sensitive, rapid, and easy to analyze. The sign of the peak indicates whether the trap is near the conduction or valence band, the height of the peak is proportional to the trap concentration, and the position, in temperature, of the peak is uniquely determined by the thermal emission properties of the trap. In addition, one can measure the activation energy, concentration profile, and electron‐ and hole‐capture cross sections for each trap. The technique is presented with a simple theoretical analysis for the case of exponential capacitance transients. Various traps in GaAs are used as examples to illustrate certain features of the DLTS technique. Finally, a critical comparison is made with other recent capacitance techniques.",1974,10,3050,74,1,5,20,21,35,46,62,57,78,78
9d5cdbab51eaf9ec3fe2a6b435d18a040dc7ea59,"Chapter 1. An Introduction to EELS 1.1. Interaction of Fast Electrons with a Solid 1.2. The Electron Energy-Loss Spectrum 1.3. The Development of Experimental Techniques 1.3.1. Energy-Selecting (Energy-Filtering) Electron Microscopes 1.3.2. Spectrometers as Attachments to Electron Microscopes 1.4. Alternative Analytical Methods 1.4.1. Ion-Beam Methods 1.4.2. Incident Photons 1.4.3. Electron-Beam Techniques 1.5. Comparison of EELS and EDX Spectroscopy 1.5.1. Detection Limits and Spatial Resolution 1.5.2. Specimen Requirements 1.5.3. Accuracy of Quantification 1.5.4. Ease of Use and Information Content 1.6. Further Reading Chapter 2. Energy-Loss Instrumentation 2.1. Energy-Analyzing and Energy-Selecting Systems 2.1.1. The Magnetic-Prism Spectrometer 2.1.2. Energy-Filtering Magnetic-Prism Systems 2.1.3. The Wien Filter 2.1.4. Electron Monochromators 2.2. Optics of a Magnetic-Prism Spectrometer 2.2.1. First-Order Properties 2.2.2. Higher-Order Focusing 2.2.3. Spectrometer Sesigns 2.2.4. Practical Considerations 2.2.5. Spectrometer Alignment 2.3. The Use of Prespectrometer Lenses 2.3.1. TEM Imaging and Diffraction Modes 2.3.2. Effect of Lens Aberrations on Spatial Resolution 2.3.3. Effect of Lens Aberrations on Collection Efficiency 2.3.4. Effect of TEM Lenses on Energy Resolution 2.3.5. STEM Optics 2.4. Recording the Energy-Loss Spectrum 2.4.1. Spectrum Shift and Scanning 2.4.2. Spectrometer Background 2.4.3. Coincidence Counting 2.4.4. Serial Recording of the Energy-Loss Spectrum 2.4.5. DQE of a Single-Channel System 2.4.6. Serial-Mode Signal Processing 2.5. Parallel Recording of Energy-Loss Data 2.5.1. Types of Self-Scanning Diode Array 2.5.2. Indirect Exposure Systems 2.5.3. Direct Exposure Systems 2.5.4. DQE of a Parallel-Recording System 2.5.5. Dealing with Diode Array Artifacts 2.6. Energy-Selected Imaging (ESI) 2.6.1. Post-Column Energy Filter 2.6.2. In-Column Filters 2.6.3. Energy Filtering in STEM Mode 2.6.4. Spectrum-Imaging 2.6.5. Elemental Mapping 2.6.6. Comparison of Energy-Filtered TEM and STEM 2.6.7. Z-Contrast and Z-Ratio Imaging Chapter 3. Physics of Electron Scattering 3.1. Elastic Scattering 3.1.1. General Formulas 3.1.2. Atomic Models 3.1.3. Diffraction Effects 3.1.4. Electron Channeling 3.1.5. Phonon Scattering 3.1.6. Energy Transfer in Elastic Scattering 3.2. Inelastic Scattering 3.2.1. Atomic Models 3.2.2. Bethe Theory 3.2.3. Dielectric Formulation 3.2.4. Solid-State Effects 3.3. Excitation of Outer-Shell Electrons 3.3.1. Volume Plasmons 3.3.2. Single-Electron Excitation 3.3.3. Excitons 3.3.4. Radiation Losses 3.3.5. Surface Plasmons 3.3.6. Surface-Reflection Spectra 3.3.7. Plasmon Modes in Small Particles 3.4. Single, Plural, and Multiple Scattering 3.4.1. Poisson's Law 3.4.2. Angular Distribution of Plural Inelastic Scattering 3.4.3. Influence of Elastic Scattering 3.4.4. Multiple Scattering 3.4.5. Coherent Double-Plasmon Excitation 3.5. The Spectral Background to Inner-Shell Edges 3.5.1. Valence-Electron Scattering 3.5.2. Tails of Core-Loss Edges 3.5.3. Bremsstrahlung Energy Losses 3.5.4. Plural Scattering Contributions to the Background 3.6. Atomic Theory of Inner-Shell Excitation 3.6.1. Generalized Oscillator Strength 3.6.2. Relativistic Kinematics of Scattering 3.6.3. Ionization Cross Sections 3.7. The Form of Inner-Shell Edges 3.7.1. Basic Edge Shapes 3.7.2. Dipole Selection Rule 3.7.3. Effect of Plural Scattering 3.7.4. Chemical Shifts in Threshold Energy 3.8. Near-Edge Fine Structure (ELNES) 3.8.1. Densities-of-States Interpretation 3.8.2. Multiple-Scattering Interpretation 3.8.3. Molecular-Orbital Theory 3.8.4. Multiplet and Crystal-Field Effects 3.9. Extended Energy-Loss Fine Structure (EXELFS) 3.10. Core Excitation in Anisotropic Materials 3.11. Delocalization of inelastic Scattering Chapter 4. Quantitative Analysis of Energy-Loss Data 4.1. Deconvolution of Low-Loss Spectra 4.1.1. Fourier-Log Method 4.1.2. Fourier-Ratio Method 4.1.3. Bayesian Deconvolution 4.1.4. Other Methods 4.2. Kramers-Kronig Analysis 4.3. Deconvolution of Core-Loss Data 4.3.1. Fourier-Log Method 4.3.2. Fourier-Ratio Method 4.3.3. Bayesian Deconvolution 4.3.4. Other Methods 4.4. Separation of Spectral Components 4.4.1. Least-Squares Fitting 4.4.2. Two-Area Fitting 4.4.3. Background-Fitting Errors 4.4.4. Multiple Least-Squares Fitting 4.4.5. Multivariate Statistical Analysis 4.4.6. Energy- and Spatial-Difference Techniques 4.5. Elemental Quantification 4.5.1. Integration Method 4.5.2. Calculation of Partial Cross Sections 4.5.3. Correction for Incident-Beam Convergence 4.5.4. Quantification from MLS Fitting 4.6. Analysis of Extended Energy-Loss Fine Structure 4.6.1. Fourier-Transform Method 4.6.2. Curve-Fitting Procedure 4.7. Simulation of Energy-Loss Near-Edge Structure (ELNES) 4.7.1. Multiple-Scattering Calculations 4.7.2. Band-Structure Calculations Chapter 5. TEM Applications of EELS 5.1. Measurement of Specimen Thickness 5.1.1. Log-Ratio Method 5.1.2. Absolute Thickness from the K-K Sum Rule 5.1.3. Mass-Thickness from the Bethe Sum Rule 5.2. Low-Loss Spectroscopy 5.2.1. Identification from Low-Loss Fine Structure 5.2.2. Measurement of Plasmon Energy and Alloy Composition 5.2.3. Characterization of Small Particles 5.3. Energy-Filtered Images and Diffraction Patterns 5.3.1. Zero-Loss Images 5.3.2. Zero-Loss Diffraction Patterns 5.3.3. Low-Loss Images 5.3.4. Z-Ratio Images 5.3.5. Contrast Tuning and MPL Imaging 5.3.6. Core-Loss Images and Elemental Mapping 5.4. Elemental Analysis from Core-Loss Spectroscopy 5.4.1. Measurement of Hydrogen and Helium 5.4.2. Measurement of Lithium, Beryllium, and Boron 5.4.3. Measurement of Carbon, Nitrogen, and Oxygen 5.4.4. Measurement of Fluorine and Heavier Elements 5.5. Spatial Resolution and Detection Limits 5.5.1. Electron-Optical Considerations 5.5.2. Loss of Resolution due to Elastic Scattering 5.5.3. Delocalization of Inelastic Scattering 5.5.4. Statistical Limitations and Radiation Damage 5.6. Structural Information from EELS 5.6.1. Orientation Dependence of Ionization Edges 5.6.2. Core-Loss Diffraction Patterns 5.6.3. ELNES Fingerprinting 5.6.4. Valency and Magnetic Measurements from White-Line Ratios 5.6.5. Use of Chemical Shifts 5.6.6. Use of Extended Fine Structure 5.6.7. Electron-Compton (ECOSS) Measurements 5.7. Application to Specific Materials 5.7.1. Semiconductors and Electronic Devices 5.7.2. Ceramics and High-Temperature Superconductors 5.7.3. Carbon-Based Materials 5.7.4. Polymers and Biological Specimens 5.7.5. Radiation Damage and Hole Drilling Appendix A. Bethe Theory forHigh Incident Energies and Anisotropic Materials Appendix B. Computer Programs B.1. First-Order Spectrometer Focusing B.2. Cross Sections for Atomic Displacement and High-Angle Elastic Scattering B.3. Lenz-Model Elastic and Inelastic Cross Sections B.4. Simulation of a Plural-Scattering Distribution B.5. Fourier-Log Deconvolution B.6. Maximum-Likelihood Deconvolution B.7. Drude Simulation of a Low-Loss Spectrum B.8. Kramers-Kronig Analysis B.9. Kroger Simulation of a Low-Loss Spectrum B.10. Core-Loss Simulation B.11. Fourier-Ratio Deconvolution B.12. Incident-Convergence Correction B.13. Hydrogenic K-shell Cross Sections B.14. Modified-Hydrogenic L-shell Cross Sections B.15. Parameterized K-, L-, N-, N- and O-shell Cross Sections B.16. Measurement of Absolute Specimen Thickness B.17. Total-Inelastic and Plasmon Mean Free Paths B.18. Constrained Power-Law Background Fitting Appendix C. Plasmon Energies and Inelastic Mean Free Paths Appendix D. Inner-Shell Energies and Edge Shapes Appendix E. Electron Wavelengths and Relativistic Factors Physical Constants Appendix F. Options for Energy-Loss Data Acquisition References Index",1995,0,2526,53,64,68,68,89,94,91,88,102,97,93
dbbe9374b0fec379043b2e5658ae760ea3675b60,"The possibilities for the extension of spectroscopy to two dimensions are discussed. Applications to nuclear magnetic resonance are described. The basic theory of two‐dimensional spectroscopy is developed. Numerous possible applications are mentioned and some of them treated in detail, including the elucidation of energy level diagrams, the observation of multiple quantum transitions, and the recording of high‐resolution spectra in inhomogenous magnetic fields. Experimental results are presented for some simple spin systems.",1976,51,2822,50,5,21,17,24,34,39,54,84,92,111
d2f3ea2dd93be7f987e498168becea92b86424b1,"We present Raman spectroscopy measurements on single- and few-layer graphene flakes. By using a scanning confocal approach, we collect spectral data with spatial resolution, which allows us to directly compare Raman images with scanning force micrographs. Single-layer graphene can be distinguished from double- and few-layer by the width of the D' line: the single peak for single-layer graphene splits into different peaks for the double-layer. These findings are explained using the double-resonant Raman model based on ab initio calculations of the electronic structure and of the phonon dispersion. We investigate the D line intensity and find no defects within the flake. A finite D line response originating from the edges can be attributed either to defects or to the breakdown of translational symmetry.",2006,21,2168,23,1,28,44,59,102,139,197,208,223,217
53b27fdc6b51f91b7542e0842a4231bb3522f077,"The ability to control the size, shape, and material of a surface has reinvigorated the field of surface-enhanced Raman spectroscopy (SERS). Because excitation of the localized surface plasmon resonance of a nanostructured surface or nanoparticle lies at the heart of SERS, the ability to reliably control the surface characteristics has taken SERS from an interesting surface phenomenon to a rapidly developing analytical tool. This article first explains many fundamental features of SERS and then describes the use of nanosphere lithography for the fabrication of highly reproducible and robust SERS substrates. In particular, we review metal film over nanosphere surfaces as excellent candidates for several experiments that were once impossible with more primitive SERS substrates (e.g., metal island films). The article also describes progress in applying SERS to the detection of chemical warfare agents and several biological molecules.",2008,215,2097,12,6,34,51,121,168,163,190,168,196,213
aae1555d2dda4b34ec5cd772b7de279f5d6ddbc7,"The depth-profiles of amorphous TbFeCo films sputtered onto polycarbonate substrate were studied by X-ray photoelectron spectroscopy. Oxidized metals, oxides and hydroxides for example, and adsorbed impurities were found to exist mainly in the vicinity of the film surface and film/ substrate interface.",1987,0,2503,51,0,0,0,1,0,0,0,0,0,0
f8f8b26c989b27a36cdd1f68fece4a8afaff81f7,,1983,19,2493,109,2,2,10,12,29,47,62,84,110,112
c37aa23da355a8bf6ca3b54abfc08bd9a86c91c0,,2001,32,2144,105,0,3,3,7,11,20,36,50,54,67
7c59e244f30a65d86de27dc4c920bf682ada859c,"The Raman spectra of a wide range of disordered and amorphous carbons have been measured under excitation from 785 to 229 nm. The dispersion of peak positions and intensities with excitation wavelength is used to understand the nature of resonant Raman scattering in carbon and how to derive the local bonding and disorder from the Raman spectra. The spectra show three basic features, the D and G around 1600 and 1350 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$ for visible excitation and an extra T peak, for UV excitation, at \ensuremath{\sim}1060 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$. The G peak, due to the stretching motion of ${\mathrm{sp}}^{2}$ pairs, is a good indicator of disorder. It shows dispersion only in amorphous networks, with a dispersion rate proportional to the degree of disorder. Its shift well above 1600 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$ under UV excitation indicates the presence of ${\mathrm{sp}}^{2}$ chains. The dispersion of the D peak is strongest in ordered carbons. It shows little dispersion in amorphous carbon, so that in UV excitation it becomes like a density-of-states feature of vibrations of ${\mathrm{sp}}^{2}$ ringlike structures. The intensity ratio $I(D)/I(G)$ falls with increasing UV excitation in all forms of carbon, with a faster decrease in more ordered carbons, so that it is generally small for UV excitation. The T peak, due to ${\mathrm{sp}}^{3}$ vibrations, only appears in UV Raman, lying around 1060 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$ for H-free carbons and around 980 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$ in hydrogenated carbons. In hydrogenated carbons, the ${\mathrm{sp}}^{3}{\mathrm{C}\ensuremath{-}\mathrm{H}}_{x}$ stretching modes around 2920 ${\mathrm{cm}}^{\mathrm{\ensuremath{-}}1}$ can be clearly detected for UV excitation. This assignment is confirmed by deuterium substitution.",2001,0,2197,30,1,12,33,28,36,40,49,69,73,69
72f16c2e77f670229abf917ea99e3242e8723d72,Acknowledgements 1. Introduction 2. Electromagnetic wave propagation 3. The absorption of light 4. Specular reflection 5. Single particle scattering: perfect spheres 6. Single particle scattering: irregular particles 7. Propagation in a nonuniform medium: the equation of radiative transfer 8. The bidirectional reflectance of a semi-infinite medium 9. The opposition effect 10. A miscellany of bidirectional reflectances and related quantities 11. Integrated reflectances and planetary photometry 12. Photometric effects of large scale roughness 13. Polarization 14. Reflectance spectroscopy 15. Thermal emission and emittance spectroscopy 16. Simultaneous transport of energy by radiation and conduction Appendix A. A brief review of vector calculus Appendix B. Functions of a complex variable Appendix C. The wave equation in spherical coordinates Appendix D. Fraunhoffer diffraction by a circular hole Appendix E. Table of symbols Bibliography Index.,1993,298,1732,209,2,2,10,23,18,37,33,37,33,49
0b8f4550e27daca8fe930448eebaca89a4a43f3a,Fundamentals of Impedance Spectroscopy Theory Measuring Techniques and Data Analysis Applications of Impedance,1987,0,2199,69,2,0,2,1,8,7,12,13,23,28
a12f654b8eb486fec50b334f42781fa744cf99a3,"An overview is given of near infrared (NIR) spectroscopy for use in measuring quality attributes of horticultural produce. Different spectrophotometer designs and measurement principles are compared, and novel techniques, such as time and spatially resolved spectroscopy for the estimation of light absorption and scattering properties of vegetable tissue, as well as NIR multi- and hyperspectral imaging techniques are reviewed. Special attention is paid to recent developments in portable systems. Chemometrics is an essential part of NIR spectroscopy, and the available preprocessing and regression techniques, including nonlinear ones, such as kernel-based methods, are discussed. Robustness issues due to orchard and species effects and fluctuating temperatures are addressed. The problem of calibration transfer from one spectrophotometer to another is introduced, as well as techniques for calibration transfer. Most applications of NIR spectroscopy have focussed on the nondestructive measurement of soluble solids content of fruit where typically a root mean square error of prediction of 1° Brix can be achieved, but also other applications involving texture, dry matter, acidity or disorders of fruit and vegetables have been reported. Areas where more research is required are identified.",2007,159,1528,100,0,16,42,41,56,79,104,121,121,150
03ff8aa86cfe0e4f891cb0b968f6ec2a7dcb62ce,"Single-molecule force spectroscopy has emerged as a powerful tool to investigate the forces and motions associated with biological molecules and enzymatic activity. The most common force spectroscopy techniques are optical tweezers, magnetic tweezers and atomic force microscopy. Here we describe these techniques and illustrate them with examples highlighting current capabilities and limitations.",2008,170,1838,43,12,57,75,98,138,137,148,148,176,193
c3953d23aef29a830eb205a01eaf341b897a5011,"Metabolic profiling, metabolomic and metabonomic studies mainly involve the multicomponent analysis of biological fluids, tissue and cell extracts using NMR spectroscopy and/or mass spectrometry (MS). We summarize the main NMR spectroscopic applications in modern metabolic research, and provide detailed protocols for biofluid (urine, serum/plasma) and tissue sample collection and preparation, including the extraction of polar and lipophilic metabolites from tissues. 1H NMR spectroscopic techniques such as standard 1D spectroscopy, relaxation-edited, diffusion-edited and 2D J-resolved pulse sequences are widely used at the analysis stage to monitor different groups of metabolites and are described here. They are often followed by more detailed statistical analysis or additional 2D NMR analysis for biomarker discovery. The standard acquisition time per sample is 4–5 min for a simple 1D spectrum, and both preparation and analysis can be automated to allow application to high-throughput screening for clinical diagnostic and toxicological studies, as well as molecular phenotyping and functional genomics.",2007,91,1596,61,0,24,37,50,83,101,113,139,136,156
66c78dab35d43b9524097c20c1c28da72cafd88d,"Two-dimensional correlated spectroscopy (COSY) is used for measurements of proton-proton spin-spin coupling constants in protein 1H NMR spectra. High digital resolution along the frequency axis ω2 is achieved by placing the carrier frequency in the center of the spectrum, using quadrature detection in both dimensions and presenting the spectrum in the phase sensitive mode. Compared to other techniques for studies of spin-spin coupling constants, COSY provides greatly improved spectral resolution. This is illustrated by experiments with H2O solutions of the small globular protein BUSI IIA (bull seminal inhibitor IIA).",1983,20,2224,6,2,8,21,28,39,89,97,126,129,167
2a625dd581a2d35dcfa824789412b137915a9c5d,"Publisher Summary The vibrational spectrum of a molecule is determined by its three-dimensional structure and its vibrational force field. An analysis of this (usually infrared (IR) and Raman) spectrum can therefore provide information on the structure and on intramolecular and intermolecular interactions. The more probing the analysis, the more detailed is the information that can be obtained. Detailed analyses of the vibrational spectra of macromolecules, however, have provided a deeper understanding of structure and interactions in these systems. An important advance in this direction for proteins came with the determination of the normal modes of vibration of the peptide group in N-methylacetamide, and the characterization of several specific amide vibrations in polypeptide systems. Extensive use has been made of spectra-structure correlations based on some of these amide modes, including attempts to determine secondary structure composition in proteins. Polypeptide molecules exhibit many more vibrational frequencies than the amide modes. Over the years, some normal-mode calculations have provided greater insight into the spectra of particular molecules. However, these have often been based on approximate structures or have employed limited force fields. These force fields can now serve as a basis for detailed analyses of spectral and structural questions in other polypeptide molecules. The aim of this chapter is to present these recent developments in the vibrational spectroscopy of peptides, polypeptides, and proteins.",1986,309,2269,43,0,1,16,23,22,18,36,41,36,59
f97be8005f46b0fea88be9bde94f4f627011464d,"The molecular exciton model has received its most extensive development and application in the field of molecular crystals1'2. More recently, numerous applications to non-crystalline molecular composite systems have been made, including van der Waals and hydrogen-bonded dimers, trimers, and higher order aggregates. Another type of composite system has also been investigated, namely the composite molecule consisting of covalently bonded molecular units, with intrinsic individual unsaturated electronic systems so isolated by single bonds that but little or insignificant electronic overlap between units may occur. It is now well established that in molecular aggregates and in composite molecules, exciton effects may be observed if sufficiently strong electronic transitions exist in the component sub-units. The result of exciton splitting of excited states in the composite molecule may be the appearance of strong spectral shifts or splittings (which may be of the order of 2000 cm—1) of the absorption bands for the component molecules. At the same time, as a consequence of the exciton splitting of the excited state manifold, an enhancement of triplet state excitation may result. The purpose of this paper is to present a summary of the various type cases for molecular dimers, trimers and double and triple molecules in the description of the molecular exciton strong-coupling model. Then it will be shown by new experimental examples that, even in those cases where no significant exciton effect is observable in the singlet—singlet absorption spectrum for the composite molecule (intermediate and weak coupling cases), the enhancement of lowest triplet state excitation may still be conspicuous and significant. The ideas which are summarized in this paper have a curious history. Long ago, Kautsky and Merkel3 demonstrated experimentally that aggregation of dyes facilitated their action as photophysical sensitizers in photochemical reactions, at the same time diminishing their fluorescence efficiency. Kautsky attributed these easily demonstrated effects to enhancement of metastable state excitation in the aggregate dye. There is no doubt today that the metastable state he described is the lowest triplet state of the molecules studied. However, he did not distinguish between intrinsic and enhanced metastable (triplet) state excitation, so his interpretations were largely overlooked. Forster in l946 used the quasi-classical vector model to",1965,1,2858,17,0,0,2,2,3,1,4,3,8,1
93c7cc6303f2aa0410bd7d6cfc1c07089f30773c,"This book focuses on topics at the forefront of electrochemical research. Splitting water by electrolysis; splitting water by visible light; the recent development of lithium batteries; theoretical approaches to intercalation; and fundamental concepts of electrode kinetics, particularly as applied to semiconductors are discussed. It is recommended for electrochemists, physical chemists, corrosion scientists, and those working in the fields of analytical chemistry, surface and colloid science, materials science, electrical engineering, and chemical engineering.",1974,42,5455,5,48,29,37,43,27,40,41,50,33,56
167d8a3763ad6c24ede2b72ace5b144bfe0cf0bc,3.6.1. Polishing and Cleaning 2663 3.6.2. Vacuum and Heat Treatments 2664 3.6.3. Carbon Electrode Activation 2665 3.7. Summary and Generalizations 2666 4. Advanced Carbon Electrode Materials 2666 4.1. Microfabricated Carbon Thin Films 2666 4.2. Boron-Doped Diamond for Electrochemistry 2668 4.3. Fibers and Nanotubes 2669 4.4. Carbon Composite Electrodes 2674 5. Carbon Surface Modification 2675 5.1. Diazonium Ion Reduction 2675 5.2. Thermal and Photochemical Modifications 2679 5.3. Amine and Carboxylate Oxidation 2680 5.4. Modification by “Click” Chemistry 2681 6. Synopsis and Outlook 2681 7. Acknowledgments 2682 8. References 2682,2008,5,1929,33,3,52,80,106,154,150,180,193,166,165
ba5122439748194f2fd01feff741fbd9358fc497,,1981,0,2147,15,4,10,27,28,41,42,40,40,41,38
95881d413439696453cf915a35d95912a68d4ebe,,1964,0,2505,23,5,1,10,9,3,5,27,17,30,41
1de7cdaa3a4ab99eb095d720ceb692d09aa6556b,"We first reported that polyvinylpyrrolidone-protected graphene was dispersed well in water and had good electrochemical reduction toward O(2) and H(2)O(2). With glucose oxidase (GOD) as an enzyme model, we constructed a novel polyvinylpyrrolidone-protected graphene/polyethylenimine-functionalized ionic liquid/GOD electrochemical biosensor, which achieved the direct electron transfer of GOD, maintained its bioactivity and showed potential application for the fabrication of novel glucose biosensors with linear glucose response up to 14 mM.",2009,0,1179,11,16,77,146,144,168,132,120,89,72,78
9c7e882122e39197e77bbc40f55a78665803623a,"Direct electrochemistry of a glucose oxidase (GOD)-graphene-chitosan nanocomposite was studied. The immobilized enzyme retains its bioactivity, exhibits a surface confined, reversible two-proton and two-electron transfer reaction, and has good stability, activity and a fast heterogeneous electron transfer rate with the rate constant (k(s)) of 2.83 s(-1). A much higher enzyme loading (1.12 x 10(-9)mol/cm(2)) is obtained as compared to the bare glass carbon surface. This GOD-graphene-chitosan nanocomposite film can be used for sensitive detection of glucose. The biosensor exhibits a wider linearity range from 0.08mM to 12mM glucose with a detection limit of 0.02mM and much higher sensitivity (37.93microAmM(-1)cm(-2)) as compared with other nanostructured supports. The excellent performance of the biosensor is attributed to large surface-to-volume ratio and high conductivity of graphene, and good biocompatibility of chitosan, which enhances the enzyme absorption and promotes direct electron transfer between redox enzymes and the surface of electrodes.",2009,52,1038,25,0,33,86,101,117,127,106,95,77,95
a7da090fdb85b79cd8a52aadc9ff4715814656e5,"One of the problems which has plagued thouse attempting to predict the behavior of capital marcets is the absence of a body of positive of microeconomic theory dealing with conditions of risk/ Althuogh many usefull insights can be obtaine from the traditional model of investment under conditions of certainty, the pervasive influense of risk in finansial transactions has forced those working in this area to adobt models of price behavior which are little more than assertions. A typical classroom explanation of the determinationof capital asset prices, for example, usually begins with a carefull and relatively rigorous description of the process through which individuals preferences and phisical relationship to determine an equilibrium pure interest rate. This is generally followed by the assertion that somehow a market risk-premium is also determined, with the prices of asset adjusting accordingly to account for differences of their risk.",1964,2,17475,1038,0,0,0,0,0,0,0,0,0,0
a95e6d3280e2db04563556fa576fa55465b7a317,"This paper tests the relationship between average return and risk for New York Stock Exchange common stocks. The theoretical basis of the tests is the ""two-parameter"" portfolio model and models of market equilibrium derived from the two-parameter portfolio model. We cannot reject the hypothesis of these models that the pricing of common stocks reflects the attempts of risk-averse investors to hold portfolios that are ""efficient"" in terms of expected value and dispersion of return. Moreover, the observed ""fair game"" properties of the coefficients and residuals of the risk-return regressions are consistent with an ""efficient capital market""--that is, a market where prices of securities",1973,38,13627,1396,0,0,0,0,0,0,0,0,0,0
b470ef87fcf834e0e90bf65b497732cef2376063,"Nose has modified Newtonian dynamics so as to reproduce both the canonical and the isothermal-isobaric probability densities in the phase space of an N-body system. He did this by scaling time (with s) and distance (with V/sup 1/D/ in D dimensions) through Lagrangian equations of motion. The dynamical equations describe the evolution of these two scaling variables and their two conjugate momenta p/sub s/ and p/sub v/. Here we develop a slightly different set of equations, free of time scaling. We find the dynamical steady-state probability density in an extended phase space with variables x, p/sub x/, V, epsilon-dot, and zeta, where the x are reduced distances and the two variables epsilon-dot and zeta act as thermodynamic friction coefficients. We find that these friction coefficients have Gaussian distributions. From the distributions the extent of small-system non-Newtonian behavior can be estimated. We illustrate the dynamical equations by considering their application to the simplest possible case, a one-dimensional classical harmonic oscillator.",1985,0,13405,297,0,0,0,0,0,0,0,0,0,0
cd7fb9e476e7002d5649309661a06c8688058f49,"This paper develops techniques for empirically analyzing demand and supply in differentiated product markets and then applies these techniques to the U.S. automobile industry. The authors' framework enables one to obtain estimates of demand and cost parameters for a class of oligopolistic differentiated products markets. These estimates can be obtained using only widely available product-level and aggregate consumer-level data, and they are consistent with a structural model of equilibrium in an oligopolistic industry. Applying these techniques, the authors obtain parameters for essentially all autos sold over a twenty-year period. Copyright 1995 by The Econometric Society.",1995,53,4984,681,5,9,23,23,26,49,62,83,102,116
0a577ebd728080640ba1f6da20e99cf6e9526c8e,Focuses on a study which examined perfect equilibrium in a bargaining model. Overview of the strategic approach adopted for the study; Details of the bargaining situation used; Discussion on perfect equilibrium. (From Ebsco),1982,22,5275,344,1,7,6,17,20,35,43,52,51,52
f2b954dab9db5e74d7000b6610378c2523119290,"If looking for a ebook by S. R. de Groot and P. Mazur Non-Equilibrium Thermodynamics in pdf form, in that case you come on to the correct site. We furnish full release of this book in ePub, DjVu, txt, doc, PDF formats. You may read by S. R. de Groot and P. Mazur online Non-Equilibrium Thermodynamics or download. In addition, on our website you may reading instructions and diverse artistic eBooks online, either load theirs. We like attract consideration what our site does not store the eBook itself, but we grant ref to website whereat you may load either reading online. So if have necessity to download pdf Non-Equilibrium Thermodynamics by S. R. de Groot and P. Mazur, then you have come on to faithful website. We have Non-Equilibrium Thermodynamics ePub, txt, PDF, DjVu, doc formats. We will be glad if you will be back to us anew.",1963,0,5809,332,6,8,24,31,33,40,46,45,42,50
f9a8f37fa587f4a50dceb4d6a4176ae424e8099f,"Abstract The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case.",1977,14,6023,619,1,4,3,9,2,3,5,10,7,9
f0d65b8633f39cbc40cd482c1a394726c8763fb3,"THE SPHERE of model financial economics encompasses finance, micro investment theory and much of the economics of uncertainty. As is evident from its influence on other branches of economics including public finance, industrial organization and monetary theory, the boundaries of this sphere are both permeable and flexible. The complex interactions of time and uncertainty guarantee intellectual challenge and intrinsic excitement to the study of financial economics. Indeed, the mathematics of the subject contain some of the most interesting applications of probability and optimization theory. But for all its mathematical refinement, the research has nevertheless had a direct and significant influence on practice. ’ It was not always thus. Thirty years ago, finance theory was little more than a collection of anecdotes, rules of thumb, and manipulations of accounting data with an almost exclusive focus on corporate financial management. There is no need in this meeting of the guild to recount the subsequent evolution from this conceptual potpourri to a rigorous economic theory subjected to systematic empirical examination? Nor is there a need on this occasion to document the wide-ranging impact of the research on finance practice.2 I simply note that the conjoining of intrinsic intellectual interest with extrinsic application is a prevailing theme of research in financial economics. The later stages of this successful evolution have however been marked by a substantial accumulation of empirical anomalies; discoveries of theoretical inconsistencies; and a well-founded concern about the statistical power of many of the test methodologies.3 Finance thus finds itself today in the seemingly-paradoxical position of having more questions and empirical puzzles than at the start of its",1987,46,5404,473,1,4,7,11,8,9,7,12,13,26
e3317b468c1b9a0d5d801ad11f12d3bffa6364af,Part 1 Unemployment in the model of balanced growth: the labour market long-run equilibrium and balanced growth adjustment dynamics. Part 2 further ananlysis of the labour market: search intensity and job advertising.,1990,145,3626,342,0,2,7,14,19,20,25,39,50,71
87e8816e5520a37849e368a0770219452715da18,"One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …",1950,3,6514,390,0,1,2,1,1,1,1,1,1,2
bb484ca72e7e04f643014d0f1be8070af2e0f031,"Involuntary unemployment appears to be a persistent feature of many modern labor markets. The presence of such unemployment raises the question of why wages do not fall to clear labor markets. In this paper we show how the information structure of employer-employee relationships, in particular the inability of employers to costlessly observe workers' on-the-job effort, can explain involuntary unemployment as an equilibrium phenomenon. Indeed, we show that imperfect monitoring necessitates unemployment in equilibrium. The intuition behind our result is simple. Under the conventional competitive paradigm, in which all workers receive the market wage and there is no unemployment, the worst that can happen to a worker who shirks on the job is that he is fired. Since he can immediately be rehired, however, he pays no penalty for his misdemeanor. With imperfect monitoring and full employment, therefore, workers will choose to shirk. To induce its workers not to shirk, the firm attempts to pay more than the “going wage”; then, if a worker is caught shirking and is fired, he will pay a penalty. If it pays one firm to raise its wage, however, it will pay all firms to raise their wages. When they all raise their wages, the incentive not to shirk again disappears. But as all firms raise their wages, their demand for labor decreases, and unemployment results. With unemployment, even if all firms pay the same wages, a worker has an incentive not to shirk.",1984,11,4900,215,3,17,23,33,40,45,54,66,51,56
e937fc6b51ab16bfdb3d7cde90a13c7e12e2c641,"A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game. Introduction L. Walras [ 24 ] first formulated the state of the economic system at any point of time as the solution of a system of simultaneous equations representing the demand for goods by consumers, the supply of goods by producers, and the equilibrium condition that supply equal demand on every market. It was assumed that each consumer acts so as to maximize his utility, each producer acts so as to maximize his profit, and perfect competition prevails, in the sense that each producer and consumer regards the prices paid and received as independent of his own choices. Walras did not, however, give any conclusive arguments to show that the equations, as given, have a solution.",1954,29,4256,282,0,0,2,0,3,5,2,2,8,0
5fbba8fcf94492a5902bad624b4f708e50e9c2f1,"A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q0 and frequency ω0 of the instability. Type Is systems (ω0=0, q0≠0) are stationary in time and periodic in space; type IIIo systems (ω0≠0, q0=0) are periodic in time and uniform in space; and type Io systems (ω0≠0, q0≠0) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via ""amplitude equations,"" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the ""phase equation"" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing ""real pattern effects"" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying ""temporal chaos,"" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Benard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future.",1993,918,5264,176,11,52,108,123,127,126,126,138,147,142
921471828f08f6bb7bcef8d2685811af0fb0dac7,"A modified Redlich-Kwong equation of state is proposed. Vapor pressures of pure com- pounds can be closely reproduced by assuming the parameter a in the original equation to be tempera- ture-dependent. With the introduction of the acentric factor as a third parameter, a generalized correla- tion for the modified parameter can be derived. It applies to all nonpolar compounds. With the application of the original generalized mixing rules, the proposed equation can be extended successfully to multicomponent-VLE calculations, for mixtures of nonpolar substances, with the exclusion of carbon dioxide. Less accurate results are obtained for hydrogen-containing mixtures.",1972,6,4598,258,0,1,1,1,7,9,14,8,13,11
12982242bcceff2de817f51fe1ccbcc0cbb926f8,,1969,0,4821,213,0,1,0,6,11,7,8,18,17,16
8692d38cdef40d3ec18e9e8a9b18743b768d039d,"This paper argues that the textbook search and matching model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies in response to shocks of a plausible magnitude. In the United States, the standard deviation of the vacancy-unemployment ratio is almost 20 times as large as the standard deviation of average labor productivity, while the search model predicts that the two variables should have nearly the same volatility. A shock that changes average labor productivity primarily alters the present value of wages, generating only a small movement along a downward-sloping Beveridge curve (unemployment-vacancy locus). A shock to the separation rate generates a counterfactually positive correlation between unemployment and vacancies. In both cases, the model exhibits virtually no propagation.",2005,85,2653,700,47,98,119,107,150,165,207,185,187,209
eda646f4a2d46adf404790e94276ca254285ad01,"This paper investigates the properties of a market for risky assets on the basis of a simple model of general equilibrium of exchange, where individual investors seek to maximize preference functions over expected yield and variance of yield on their port- folios. A theory of market risk premiums is outlined, and it is shown that general equilibrium implies the existence of a so-called ""market line,"" relating per dollar expected yield and standard deviation of yield. The concept of price of risk is discussed in terms of the slope of this line.",1966,9,4374,127,0,1,3,6,10,9,17,16,20,21
e9f31a23580f9b88ba34ca671c438839a068de7a,"A dynamic stochastic model for a competitive industry is developed in which entry, exit, and the growth of firms' output and employment is determined. The paper extends long-run industry equilibrium theory to account for entry, exit, and heterogeneity in the size and growth rate of firms. Conditions under which there is entry and exit in the long run are developed. Cross sectional implications and distributions of profits and value of firms are derived. Comparative statics on the equilibrium size distribution and turnover rates are analyzed. Copyright 1992 by The Econometric Society.",1992,1,2914,265,2,2,1,14,9,12,14,29,40,37
faa25a6c7c7b3ecd221e4fc7b7d6dee131e04e04,"Publisher Summary In recent years, the interest in the problem of brittle fracture and, in particular, in the theory of cracks has grown appreciably in connection with various technical applications. Numerous investigations have been carried out, enlarging in essential points the classical concepts of cracks and methods of analysis. The qualitative features of the problems of cracks, associated with their peculiar nonlinearity as revealed in these investigations, makes the theory of cracks stand out distinctly from the whole range of problems in terms of the theory of elasticity. The chapter presents a unified view of the way basic problems in the theory of equilibrium cracks are formulated and discusses the results obtained thereby. The object of the theory of equilibrium cracks is the study of the equilibrium of solids in the presence of cracks. However, there exists a fundamental distinction between these two problems, The form of a cavity undergoes only slight changes even under a considerable variation in the load acting on a body, while the cracks whose surface also constitutes a part of the body boundary can expand even with small increase of the load to which the body is subjected.",1962,80,4296,170,0,1,0,4,7,12,8,10,10,8
3f94a0fe29c97858ae8143e31ec621911865afda,,1972,3,3022,380,4,7,15,8,11,34,30,35,20,28
8979bdafefff9da97342fbc5cf3f511000306f60,Kinetics of Unireactant Enzymes. Simple Inhibition Systems. Rapid Equilibrium Partial and Mixed--Type Inhibition. Enzyme Activation. Rapid Equilibrium Bireactant and Terreactant Systems. Multisite and Allosteric Enzymes. Multiple Inhibition Analysis. Steady--State Kinetics of Multireactant Enzymes. Isotope Exchange. Effects of pH and Temperature. Appendix. Index.,1975,0,3084,215,0,2,10,10,12,13,13,17,20,19
4ef2492426f6eb90191d9c411b94f5d686f2e618,"Parts I and II deal with the theory of crystal growth, parts III and IV with the form (on the atomic scale) of a crystal surface in equilibrium with the vapour. In part I we calculate the rate of advance of monomolecular steps (i.e. the edges of incomplete monomolecular layers of the crystal) as a function of supersaturation in the vapour and the mean concentration of kinks in the steps. We show that in most cases of growth from the vapour the rate of advance of monomolecular steps will be independent of their crystallographic orientation, so that a growing closed step will be circular. We also find the rate of advance for parallel sequences of steps. In part II we find the resulting rate of growth and the steepness of the growth cones or growth pyramids when the persistence of steps is due to the presence of dislocations. The cases in which several or many dislocations are involved are analysed in some detail; it is shown that they will commonly differ little from the case of a single dislocation. The rate of growth of a surface containing dislocations is shown to be proportional to the square of the supersaturation for low values and to the first power for high values of the latter. Volmer & Schultze’s (1931) observations on the rate of growth of iodine crystals from the vapour can be explained in this way. The application of the same ideas to growth of crystals from solution is briefly discussed. Part III deals with the equilibrium structure of steps, especially the statistics of kinks in steps, as dependent on temperature, binding energy parameters, and crystallographic orientation. The shape and size of a two-dimensional nucleus (i.e. an ‘island* of new monolayer of crystal on a completed layer) in unstable equilibrium with a given supersaturation at a given temperature is obtained, whence a corrected activation energy for two-dimensional nucleation is evaluated. At moderately low supersaturations this is so large that a crystal would have no observable growth rate. For a crystal face containing two screw dislocations of opposite sense, joined by a step, the activation energy is still very large when their distance apart is less than the diameter of the corresponding critical nucleus; but for any greater separation it is zero. Part IV treats as a ‘co-operative phenomenon’ the temperature dependence of the structure of the surface of a perfect crystal, free from steps at absolute zero. It is shown that such a surface remains practically flat (save for single adsorbed molecules and vacant surface sites) until a transition temperature is reached, at which the roughness of the surface increases very rapidly (‘surface melting’). Assuming that the molecules in the surface are all in one or other of two levels, the results of Onsager (1944) for two-dimensional ferromagnets can be applied with little change. The transition temperature is of the order of, or higher than, the melting-point for crystal faces with nearest neighbour interactions in both directions (e.g. (100) faces of simple cubic or (111) or (100) faces of face-centred cubic crystals). When the interactions are of second nearest neighbour type in one direction (e.g. (110) faces of s.c. or f.c.c. crystals), the transition temperature is lower and corresponds to a surface melting of second nearest neighbour bonds. The error introduced by the assumed restriction to two available levels is investigated by a generalization of Bethe’s method (1935) to larger numbers of levels. This method gives an anomalous result for the two-level problem. The calculated transition temperature decreases substantially on going from two to three levels, but remains practically the same for larger numbers.",1951,0,4106,92,1,7,8,6,3,8,10,12,6,7
b56d311ae4f97b1248344ad8a891b8b59af08273,"We explore the determinants of liquidation values of assets, particularly focusing on the potential buyers of assets. When a firm in financial distress needs to sell assets, its industry peers are likely to be experiencing problems themselves, leading to asset sales at prices below value in best use. Such illiquidity makes assets cheap in bad times, and so ex ante is a significant private cost of leverage. We use this focus on asset buyers to explain variation in debt capacity across industries and over the business cycle, as well as the rise in U.S. corporate leverage in the 1980s.",1992,38,2859,173,1,4,19,19,16,22,30,33,49,50
9ead28b73dc3c2329ee2db668f288636b2870196,"It is shown that for allele frequency data a useful measure of the extent of gene flow between a pair of populations is M∘=(1/FST‐1)/4 , which is the estimated level of gene flow in an island model at equilibrium. For DNA sequence data, the same formula can be used if FST is replaced by NST. In a population with restricted dispersal, analytic theory shows that there is a simple relationship between M̂ and geographic distance in both equilibrium and non‐equilibrium populations and that this relationship is approximately independent of mutation rate when the mutation rate is small. Simulation results show that with reasonable sample sizes, isolation by distance can indeed be detected and that, at least in some cases, non‐equilibrium patterns can be distinguished. This approach to analyzing isolation by distance is used for two allozyme data sets, one from gulls and one from pocket gophers.",1993,31,2425,279,0,13,24,51,52,66,64,66,88,84
46fe07c5695229769631ca693658a82da6e9395d,"The concept of a perfect equilibrium point has been introduced in order to exclude the possibility that disequilibrium behavior is prescribed on unreached subgames. (Selten 1965 and 1973). Unfortunately this definition of perfectness does not remove all difficulties which may arise with respect to unreached parts of the game. It is necessary to reexamine the problem of defining a satisfactory non-cooperative equilibrium concept for games in extensive form. Therefore a new concept of a perfect equilibrium point will be introduced in this paper. In retrospect the earlier use of the word ""perfect"" was premature. Therefore a perfect equilibrium point in the old Sense will be called ""subgame perfect"". The new definition of perfectness has the property that a perfect equilibrium point is always subgame perfect but a subgame perfect equilibrium point may not be perfect. It will be shown that every finite extensive game with perfect recall has at least one perfect equilibrium point. Since subgame perfectness cannot be detected in the normal form, it is clear that for the purpose of the investigation of the problem of perfectness, the normal form is an inadequate representation of the extensive form. It will be convenient to introduce an ""agent normal form"" as a more adequate representation of games with perfect recall.",1975,5,3173,158,0,3,4,5,5,13,16,25,20,33
85d493e7b86d5f39e47926b8d52d895bd67a8ff1,"The different roles the attractive and repulsive forces play in forming the equilibrium structure of a Lennard‐Jones liquid are discussed. It is found that the effects of these forces are most easily separated by considering the structure factor (or equivalently, the Fourier transform of the pair‐correlation function) rather than the pair‐correlation function itself. At intermediate and large wave vectors, the repulsive forces dominate the quantitative behavior of the liquid structure factor. The attractions are manifested primarily in the small wave vector part of the structure factor; but this effect decreases as the density increases and is almost negligible at reduced densities higher than 0.65. These conclusions are established by considering the structure factor of a hypothetical reference system in which the intermolecular forces are entirely repulsive and identical to the repulsive forces in a Lennard‐Jones fluid. This reference system structure factor is calculated with the aid of a simple but accurate approximation described herein. The conclusions lead to a very simple prescription for calculating the radial distribution function of dense liquids which is more accurate than that obtained by any previously reported theory. The thermodynamic ramifications of the conclusions are presented in the form of calculations of the free energy, the internal energy (from the energy equation), and the pressure (from the virial equation). The implications of our conclusions to perturbation theories for liquids and to the interpretation of x‐ray scattering experiments are discussed.",1971,18,3574,40,4,11,15,16,20,24,39,17,18,25
f66608476a3cf3c6147823f55fc0ba235234175f,Computer program is described for numerical solution of chemical equilibria in complex systems by using nonlinear algebraic equations. Free-energy minimization technique is used.,1972,37,2940,194,4,5,8,6,10,10,12,19,23,26
879e8d7778c0ab1479339fe29d3cc4ded78fe4e5,"The probability of a configuration is given in classical theory by the Boltzmann formula exp [— V/hT] where V is the potential energy of this configuration. For high temperatures this of course also holds in quantum theory. For lower temperatures, however, a correction term has to be introduced, which can be developed into a power series of h. The formula is developed for this correction by means of a probability function and the result discussed.",1932,0,5250,45,0,0,0,0,1,0,1,1,0,0
9ba558e613372642d3c24357e21f76835859e709,,2007,0,2011,175,101,114,99,112,128,147,143,155,133,157
dca0630c63a5403a8383b4a2abe7f94a28a23eb7,Gibbs Measures.- General Thermodynamic Formalism.- Axiom a Diffeomorphisms.- Ergodic Theory of Axiom a Diffeomorphisms.,1975,43,2432,224,0,1,7,7,6,7,6,18,15,13
87a2272cc9a9abe4556509600073032fa6f01c33,The published experimental data of Hansson and of Mehrbach et al. have been critically compared after adjustment to a common pH scale based upon total hydrogen ion concentration. No significant systematic differences are found within the overall experimental error of the data. The results have been pooled to yield reliable equations that can be used to estimate pK1∗and pK2∗ for seawater media a salinities from 0 to 40 and at temperatures from 2 to 35°C.,1987,16,2628,123,0,1,1,4,4,4,17,5,12,10
2943f194efb591a1231ecea27e1af8a4b4c50827,,1994,0,2145,188,1,4,4,6,10,6,15,13,23,26
7bc19372cdca52dc43c2e8d5a2111161804ef865,"AbstractA number of experiments have been conducted in order to study the equilibria between olivine and basaltic liquids and to try and understand the conditions under which olivine will crystallize. These experiments were conducted with several basaltic compositions over a range of temperature (1150–1300° C) and oxygen fugacity (10−0.68–10−12 atm.) at one atmosphere total pressure. The phases in these experimental runs were analyzed with the electron microprobe and a number of empirical equations relating the composition of olivine and liquid were determined. The distribution coefficient 1
$$K_D = \frac{{(X_{{\text{FeO}}}^{{\text{Ol}}} )}}{{(X_{{\text{FeO}}}^{{\text{Liq}}} )}}\frac{{(X_{{\text{MgO}}}^{{\text{Liq}}} )}}{{(X_{{\text{MgO}}}^{{\text{Ol}}} )}}$$
 relating the partioning of iron and magnesium between olivine and liquid is equal to 0.30 and is independent of temperature. This means that the composition of olivine can be used to determine the magnesium to ferrous iron ratio of the liquid from which it crystallized and conversely to predict the olivine composition which would crystallize from a liquid having a particular magnesium to ferrous iron ratio.A model (saturation surface) is presented which can be used to estimate the effective solubility of olivine in basaltic melts as a function of temperature. This model is useful in predicting the temperature at which olivine and a liquid of a particular composition can coexist at equilibrium.",1970,30,2408,303,0,0,7,19,10,18,19,16,24,26
edf2bdb75e01b30d1f9d165738303081b6f630c3,"Following a recession, the aggregate labor market is slack-employment remains below normal and recruiting efforts of employers, as measured by help-wanted advertising and vacancies, are low. A model of matching friction explains the qualitative responses of the labor market to adverse shocks, but requires implausibly large shocks to account for the magnitude of observed fluctuations. The incorporation of wage stickiness vastly increases the sensitivity of the model to driving forces. I develop a new model of the way that wage stickiness affects unemployment. The stickiness arises in an economic equilibrium and satisfies the condition that no worker-employer pair has an unexploited opportunity for mutual improvement. Sticky wages neither interfere with the efficient formation of employment matches nor cause inefficient job loss. Thus the model provides an answer to the fundamental criticism previously directed at sticky-wage models of fluctuations.",2005,49,1454,273,36,56,88,85,104,102,121,103,92,90
310beac23e27b4d0689bd11338e459d03a1dc3c0,"Recently, a number of authors have argued that the standard search model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies, given shocks of a plausible magnitude. We use data on the cost of vacancy creation and cyclicality of wages to identify the two key parameters of the model - the value of non-market activity and the bargaining weights. Our calibration implies that the model is, in fact, consistent with the data.",2008,169,1197,287,94,61,85,115,85,81,78,91,82,69
f0f7e780651f1d6189b6c772e5ca0e2355e182d8,"A modified conjugate gradient algorithm for geometry optimization is outlined for use with ab initio MO methods. Since the computation time for analytical energy gradients is approximately the same as for the energy, the optimization algorithm evaluates and utilizes the gradients each time the energy is computed. The second derivative matrix, rather than its inverse, is updated employing the gradients. At each step, a one‐dimensional minimization using a quartic polynomial is carried out, followed by an n‐dimensional search using the second derivative matrix. By suitably controlling the number of negative eigenvalues of the second derivative matrix, the algorithm can also be used to locate transition structures. Representative timing data for optimizations of equilibrium geometries and transition structures are reported for ab initio SCF–MO calculations.",1982,92,2749,12,3,5,22,19,26,23,37,33,19,31
ac81ede712252992b364566fc2f44637e3b0eab3,An improved metal stamped and formed screw is disclosed. The subject screw is stamped and formed from continuous web of metal stock to form a plurality of screws joined by a carrier strip. The thus formed strip of screws can be machine applied to prebored holes and manually withdrawn therefrom and reapplied by conventional means.,1981,21,2816,35,0,2,4,3,2,5,5,4,10,13
98fe960b3f5354a987e33e62ab1de060a42ec9ec,"Economic theorists traditionally banish discussions of information to footnotes. Serious consideration of costs of communication, imperfect knowledge, and the like would, it is believed, complicate without informing. This paper, which analyzes competitive markets in which the characteristics of the commodities exchanged are not fully known to at least one of the parties to the transaction, suggests that this comforting myth is false. Some of the most important conclusions of economic theory are not robust to considerations of imperfect information.",1976,21,2530,91,1,7,8,11,7,15,23,24,26,22
c4db548b1437a0a564468889c4aaf5b0ae53aeb3,"A new suite of 10 programs concerned with equilibrium constants and solution equilibria is described. The suite includes data preparation programs, pretreatment programs, equilibrium constant refinement and post-run analysis. Data preparation is facilitated by a customized data editor. The pretreatment programs include manual trial and error data fitting, speciation diagrams, end-point determination, absorbance error determination, spectral baseline corrections, factor analysis and determination of molar absorbance spectra. Equilibrium constants can be determined from potentiometric data and/or spectrophotometric data. A new data structure is also described in which information on the model and on experimental measurements are kept in separate files.",1996,59,2322,32,0,2,15,23,18,30,28,41,55,54
52524271c3298b5541c4346b18f5e07ac6c4590e,"Research on how organizational systems develop and change is shaped, at every level of analysis, by traditional assumptions about how change works. New theories in several fields are challenging some of the most pervasive of these assumptions, by conceptualizing change as a punctuated equilibrium: an alternation between long periods when stable infrastructures permit only incremental adaptations, and brief periods of revolutionary upheaval. This article compares models from six domains—adult, group, and organizational development, history of science, biological evolution, and physical science—to explicate the punctuated equilibrium paradigm and show its broad applicability for organizational studies. Models are juxtaposed to generate new research questions about revolutionary change in organizational settings: how it is triggered, how systems function during such periods, and how it concludes. The article closes with implications for research and theory.",1991,59,2030,152,3,11,16,26,26,28,26,33,43,56
04228cd8a236c84355c75eeee0fc50203d15776b,"Prior to elections, governments (at all levels) frequently undertake a consumption binge. Taxes are cut, transfers are raised, and government spending is distorted towards highly visible items. The ""political business cycle"" (better be thought of as ""the political budget cycle"") has been intensively examined, at least for the case of national elections. A number of proposals have been advanced for mitigating electoral cycles in fiscal policy. The present paper is the first effort to provide a fully-specified equilibrium framework for analyzing such proposals. A political budget cycle arises here via a multidimensional signaling process, in which incumbent leaders try to convince voters that they have recently been doing an excellent job in administering the government. Efforts to mitigate the cycle can easily prove counterproductive, either by impeding the transmission of information or by inducing politicians to select more costly ways of signaling. The model also indicates new directions for empirical research.",1987,34,1976,128,1,2,6,6,8,13,13,9,17,18
7a58a1995f7fcd05d8820dabf25fd42c53cb422e,"Abstract A suite of divalent metal (Ca, Cd, Ba) carbonates was synthesized over the temperature range 10–40°C by the classical method of slowly bubbling N 2 through a bicarbonate solution. It was discovered that carbonates could be precipitated reproducibly in or out of isotopic equilibrium with the environmental solution by varying the concentrations of bicarbonate and cation. Precipitation rate had little or no influence on the isotopic composition of the product. Relatively high initial concentrations of up to 25 mM in both bicarbonate and cation were prepared by adding solid metal chlorides to solutions of NaHC0 3 . On the basis of results of equilibrium experiments and a new determination of the acid fractionation factor, a new expression is proposed for the oxygen isotope fractionation between calcite and water at low temperatures: 10001nα(Calcite-H 2 O) = 18.03(10 3 T −1 ) − 32.42 where α is the fractionation factor, and T is in kelvins. Combining new data for low-temperature precipitations and the high-temperature equilibrium fractionations published by O'Neil et al. (1969) results in a revised expression for the oxygen isotope fractionation between octavite (CdCO 3 ) and water from 0° to 500°C: 10001nα(CdC0P 3 H 2 O) = 2.7 6 (10 6 T −2 ) − 3.96 The ability to produce nonequilibrium carbonates allowed assessment to be made, for the first time, of the temperature dependence of nonequilibrium stable isotope fractionations in mineral systems. The temperature coefficients of a(carbonate-water) for nonequilibrium divalent metal carbonates are greater than those for equilibrium carbonates, a finding that may bear on the interpretation of analyses of biogenic carbonates forming out of isotopic equilibrium in nature. New determinations of acid fractionation factors (10001nα) at 25°C for calcite (10.44 − 0.10), aragonite (11.01 ± 0.01), and witherite (10.57 − 0.16) are mildly to strongly different from those published by Sharma and Clayton (1965) and point to a control on this fractionation by some physical property of the mineral. Reproducible values for octavite (CdC0 3 ) varied from 11.18 to 13.60 depending on the conditions of preparation of the carbonate. These new values need to be considered in determinations of absolute 18 80 16 60 ratios of international reference standards and in relating analyses of carbonates to those of waters, silicates, and oxides.",1997,25,1954,149,0,5,15,12,20,32,29,39,52,49
8a83305f83a86f7f1ca9fa5e15b22f42dade1a0a,"The authors study a rich class of noncooperative games that includes models of oligopoly competition, macroeconomic coordination failures, arms races, bank runs, technology adoption and diffusion, R&D competition, pretrial bargaining, coordination in teams, and many others. For all these games, the sets of pure strategy Nash equilibria, correlated equilibria, and rationalizable strategies have identical bounds. Also, for a class of models of dynamic adaptive choice behavior that encompasses both best-response dynamics and Bayesian learning, the players' choices lie eventually within the same bounds. These bounds are shown to vary monotonically with certain exogenous parameters. Copyright 1990 by The Econometric Society.",1990,55,1821,203,2,7,10,8,14,17,21,25,26,32
f3ca3e173ea3cfa0f8a74e4c68970e8a281d95eb,,1985,97,1930,151,0,3,7,4,7,5,9,7,12,13
3e3dcdc85b74c1b8c494292cef9eb9f893785718,"This paper develops a continuous time general equilibrium model of a simple but complete economy and uses it to examine the behavior of asset prices. In this model, asset prices and their stochastic properties are determined endogenously. One principal result is a partial differential equation which asset prices must satisfy. The solution of this equation gives the equilibrium price of any asset in terms of the underlying real variables in the economy. IN THIS PAPER, we develop a general equilibrium asset pricing model for use in applied research. An important feature of the model is its integration of real and financial markets. Among other things, the model endogenously determines the stochastic process followed by the equilibrium price of any financial asset and shows how this process depends on the underlying real variables. The model is fully consistent with rational expectations and maximizing behavior on the part of all agents. Our framework is general enough to include many of the fundamental forces affecting asset markets, yet it is tractable enough to be specialized easily to produce specific testable results. Furthermore, the model can be extended in a number of straightforward ways. Consequently, it is well suited to a wide variety of applications. For example, in a companion paper, Cox, Ingersoll, and Ross [7], we use the model to develop a theory of the term structure of interest rates. Many studies have been concerned with various aspects of asset pricing under uncertainty. The most relevant to our work are the important papers on intertemporal asset pricing by Merton [19] and Lucas [16]. Working in a continuous time framework, Merton derives a relationship among the equilibrium expected rates of return on assets. He shows that when investment opportunities are changing randomly over time this relationship will include effects which have no analogue in a static one period model. Lucas considers an economy with homogeneous individuals and a single consumption good which is produced by a number of processes. The random output of these processes is exogenously determined and perishable. Assets are defined as claims to all or a part of the output of a process, and the equilibrium determines the asset prices. Our theory draws on some elements of both of these papers. Like Merton, we formulate our model in continuous time and make full use of the analytical tractability that this affords. The economic structure of our model is somewhat similar to that of Lucas. However, we include both endogenous production and",1985,27,1985,102,4,12,21,24,35,35,37,32,37,29
828be060c75cd9e01f5b2c6aa48508b06c856239,"This paper develops and estimates a dynamic stochastic general equilibrium (DSGE) model with sticky prices and wages for the euro area. The model incorporates various other features such as habit formation, costs of adjustment in capital accumulation and variable capacity utilisation. It is estimated with Bayesian techniques using seven key macro-economic variables: GDP, consumption, investment, prices, real wages, employment and the nominal interest rate. The introduction of ten orthogonal structural shocks (including productivity, labour supply, investment, preference, cost-push and monetary policy shocks) allows for an empirical investigation of the effects of such shocks and of their contribution to business cycle fluctuations in the euro area. Using the estimated model, the paper also analyses the output (real interest rate) gap, defined as the difference between the actual and model-based potential output (real interest rate).",2002,92,1329,239,3,10,37,44,50,72,85,98,118,105
7ceae624bd1d65ccf66e8448a8f902c2415ce62c,"An analysis of the quantitative effects of agency costs in a real business cycle model, showing that these costs can explain why output growth displays positive autocorrelation at short horizons.",1998,32,1487,219,6,9,13,21,31,39,33,34,50,52
a63da98b44996c5c484612bc754205f975c96467,"This book provides a solid foundation and an extensive study for an important class of constrained optimization problems known as Mathematical Programs with Equilibrium Constraints (MPEC), which are extensions of bilevel optimization problems. The book begins with the description of many source problems arising from engineering and economics that are amenable to treatment by the MPEC methodology. Error bounds and parametric analysis are the main tools to establish a theory of exact penalisation, a set of MPEC constraint qualifications and the first-order and second-order optimality conditions. The book also describes several iterative algorithms such as a penalty-based interior point algorithm, an implicit programming algorithm and a piecewise sequential quadratic programming algorithm for MPECs. Results in the book are expected to have significant impacts in such disciplines as engineering design, economics and game equilibria, and transportation planning, within all of which MPEC has a central role to play in the modelling of many practical problems.",1996,0,1780,140,5,13,24,26,22,35,38,45,76,86
3a8322cc9ce071f1fce97d10687d8a64265d29e8,"The theory of inequality and intergenerational mobility presented in this essay assumes that each family maximizes a utility function spanning several generations. Utility depends on the consumption of parents and on the quantity and quality of their children. The income of children is raised when they receive more human and nonhuman capital from their parents. Their income is also raised by their ""endowment"" of genetically determined race, ability, and other characteristics, family reputation and ""connections,"" and knowledge, skills, and goals provided by their family environment. The fortunes of children are linked to their parents not only through investments but also through these endowments acquired from parents (and other family members). The equilibrium income of children is determined by their market and endowed luck, the own income and endowment of parents, and the two parameters, the degree of inheritability and the propensity to invest in children. If these parameters are both less than unity, the distribution of income between families approaches a stationary distribution. The stationary coefficient of variation is greater, the larger the degree of in-heritability and the smaller the propensity to invest in children. Intergenerational mobility measures the effect of a family on the well-being of its children. We show that the family is more important when the degree of inheritability and the propensity to invest are larger. If both these parameters are less than unity, an increase in family income in one generation has negligible effects on the incomes of much later descendants. However, the incomes of children, grandchildren, and other early descendants could significantly increase; indeed, if the sum of these parameters exceeds unity, the changes in income rise for several generations before falling, and the maximum increase in income could exceed the initial increase.",1979,37,1974,179,1,2,6,6,2,8,3,4,10,5
35cca93034be1f68c585d309bc0de963021b86ea,"Abstract Transport phenomena in spatially periodic systems far from thermal equilibrium are considered. The main emphasis is put on directed transport in so-called Brownian motors (ratchets), i.e. a dissipative dynamics in the presence of thermal noise and some prototypical perturbation that drives the system out of equilibrium without introducing a priori an obvious bias into one or the other direction of motion. Symmetry conditions for the appearance (or not) of directed current, its inversion upon variation of certain parameters, and quantitative theoretical predictions for specific models are reviewed as well as a wide variety of experimental realizations and biological applications, especially the modeling of molecular motors. Extensions include quantum mechanical and collective effects, Hamiltonian ratchets, the influence of spatial disorder, and diffusive transport.",2000,839,1751,51,0,7,31,74,82,122,110,111,98,112
0b787672d66ecd0ecd3c1ba4a0b7a740c842dfae,"ing and summarizing knowledge about range dynamics without distorting it. The amount of detail lost in a particular description would depend on how many states and transitions were recognized. We are proposing the state-and-transition formulation because it is a practicable way to organize information for management, not because it follows from theoretical models about dynamics. In consequence, we consider management rather than theoretical criteria should be used in deciding what states to recognize in a given situation. As a general rule, one would distinguish 2 states only if the difference between them represented an important change in the land from the point of view of management. For example, variation due to seasonal phenology of the plants would not normally be subdivided into states, while important changes in the underlying botanical composition would be recognized. It follows that a given rangeland could be described in terms of a greater or lesser number of states and transitions, depending on the nature and objectives of management and on the state of existing knowledge. There would not be a single correct description. Under the state-and-transition formulation, knowledge about a given rangeland should be organized and expressed in the follow-",1989,51,1875,101,0,6,15,19,21,32,39,40,36,46
0fc5d7098267bab662bdc7f455f6119699f24ee9,"A redundant internal coordinate system for optimizing molecular geometries is constructed from all bonds, all valence angles between bonded atoms, and all dihedral angles between bonded atoms. Redundancies are removed by using the generalized inverse of the G matrix; constraints can be added by using an appropriate projector. For minimizations, redundant internal coordinates provide substantial improvements in optimization efficiency over Cartesian and nonredundant internal coordinates, especially for flexible and polycyclic systems. Transition structure searches are also improved when redundant coordinates are used and when the initial steps are guided by the quadratic synchronous transit approach. © 1996 by John Wiley & Sons, Inc.",1996,24,1941,12,5,11,14,21,19,32,39,36,57,65
e8fcba63671d0c88bcc674b47d62dddea83e427b,This paper studies four classic fiscal-policy experiments within a quantitatively restricted neoclassical model. The authors' main findings are as follows: (1) permanent changes in government purchases can lead to short-run and long-run output multipliers that exceed one; (2) permanent changes in government purchases induce larger effects than temporary changes; (3) the financing decision is quantitatively more important than the resource cost of changes in government purchases; and (4) public investment has dramatic effects on private output and investment. These findings stem from important dynamic interactions of capital and labor absent in earlier equilibrium analyses of fiscal policy. Copyright 1993 by American Economic Association.,1990,0,1607,205,0,3,4,5,10,14,13,21,30,24
f7bb8b20c217c81cb446b0bd9c3f6b187de80b6c,"Deviations from Hardy-Weinberg equilibrium (HWE) can indicate inbreeding, population stratification, and even problems in genotyping. In samples of affected individuals, these deviations can also provide evidence for association. Tests of HWE are commonly performed using a simple chi2 goodness-of-fit test. We show that this chi2 test can have inflated type I error rates, even in relatively large samples (e.g., samples of 1,000 individuals that include approximately 100 copies of the minor allele). On the basis of previous work, we describe exact tests of HWE together with efficient computational methods for their implementation. Our methods adequately control type I error in large and small samples and are computationally efficient. They have been implemented in freely available code that will be useful for quality assessment of genotype data and for the detection of genetic association or population stratification in very large data sets.",2005,21,1322,121,7,22,62,82,113,115,104,105,111,91
06b2234a73c812eb01cc1fbc51b83de14107788d,,1964,0,2591,32,0,0,1,2,1,1,7,5,4,7
3e4e98e430a2ed0abd49f027d4d42bb4f422d299,"This paper considers the locational choice of firms in an upstream and a downstream industry. Both industries are imperfectly competitive, with firms subject to increasing returns. There are transport costs between the two locations. Depending on the level of these costs there may be a single equilibrium with production diversified between locations, or multiple equilibria, some of which involve agglomeration at a single location. Typically the forces for agglomeration are greatest at intermediate levels of transport costs. Reducing these costs from a high to an intermediate level will cause agglomeration and consequent divergence of economic structure and income levels; reducing them to a low level may cause the industries to operate in both locations, bringing convergence of structure and income.",1996,0,1476,113,9,13,37,32,40,48,49,72,95,78
577194f578a9abc404fe1d4e406714c2550d7c8e,"Bamboo, an abundant and inexpensive natural resource in Malaysia was used to prepare activated carbon by physiochemical activation with potassium hydroxide (KOH) and carbon dioxide (CO(2)) as the activating agents at 850 degrees C for 2h. The adsorption equilibrium and kinetics of methylene blue dye on such carbon were then examined at 30 degrees C. Adsorption isotherm of the methylene blue (MB) on the activated carbon was determined and correlated with common isotherm equations. The equilibrium data for methylene blue adsorption well fitted to the Langmuir equation, with maximum monolayer adsorption capacity of 454.2mg/g. Two simplified kinetic models including pseudo-first-order and pseudo-second-order equation were selected to follow the adsorption processes. The adsorption of methylene blue could be best described by the pseudo-second-order equation. The kinetic parameters of this best-fit model were calculated and discussed.",2007,24,1240,37,1,30,39,50,61,65,96,97,113,108
087d26e0fa16877c358764f714afac31de699397,"Equilibrium is analyzed for a simple barter model with identical risk-neutral agents where trade is coordinated by a stochastic matching process. It is shown that there are multiple steady-state rational expectations equilibria, with all non-corner solution equilibria inefficient. This implies that an economy with this type of trade friction does not have a unique natural rate of unemployment.",1982,7,1897,170,1,2,1,5,7,18,12,18,26,17
32916bb78af5e6f15f3a8b87a225c1ee59f7741b,"If it is common knowledge that the players in a game are Bayesian utility maximizers who treat uncertainty about other players' actions like any other uncertainty, then the outcome is necessarily a correlated equilibrium. Random strategies appear as an expression of each player's uncertainty about what the others will do, not as the result of willful randomization. Use is made of the common prior assumption, according to which differences in probability assessments by different individuals are due to the different information that they have (where ""information"" may be interpreted broadly, to include experience, upbringing, and genetic makeup). Copyright 1987 by The Econometric Society.",1987,23,1515,184,5,8,13,26,28,26,15,30,19,23
41590083788b7cd293953d930ebf4dff61772ba6,"The capacity of immunity to control and shape cancer, that is, cancer immunoediting, is the result of three processes that function either independently or in sequence: elimination (cancer immunosurveillance, in which immunity functions as an extrinsic tumour suppressor in naive hosts); equilibrium (expansion of transformed cells is held in check by immunity); and escape (tumour cell variants with dampened immunogenicity or the capacity to attenuate immune responses grow into clinically apparent cancers). Extensive experimental support now exists for the elimination and escape processes because immunodeficient mice develop more carcinogen-induced and spontaneous cancers than wild-type mice, and tumour cells from immunodeficient mice are more immunogenic than those from immunocompetent mice. In contrast, the equilibrium process was inferred largely from clinical observations, including reports of transplantation of undetected (occult) cancer from organ donor into immunosuppressed recipients. Herein we use a mouse model of primary chemical carcinogenesis and demonstrate that equilibrium occurs, is mechanistically distinguishable from elimination and escape, and that neoplastic cells in equilibrium are transformed but proliferate poorly in vivo. We also show that tumour cells in equilibrium are unedited but become edited when they spontaneously escape immune control and grow into clinically apparent tumours. These results reveal that, in addition to destroying tumour cells and sculpting tumour immunogenicity, the immune system of a naive mouse can also restrain cancer growth for extended time periods.",2007,35,1217,27,1,55,69,105,102,89,127,88,78,100
2b21e47e893a307d45f007c9f78ea7e2e6e322dd,"A global game is an incomplete information game where the actual payoff structure is determined by a random draw from a given class of games and where each player makes a noisy observation of the selected game. For 2 x 2 games, it is shown that, when the noise vanishes, iterated elimination of dominated strategies in the global game forces the players to conform to J. C. Harsanyi and R. Selten's risk dominance criterion. Copyright 1993 by The Econometric Society.",1993,8,1588,52,0,0,8,5,14,8,6,8,31,51
3bfc2313fa86a1649763898783937a21a3cf8a4c,"The first acidity scale to be established in a pure solvent other than water was the result of the pioneering work of Conant, Wheland, and McEwen in ether or ben~ene .~ During the past 20 years an ion-pair acidity scale covering an ""effective pKa rangefrom about 15 to 40 has been developed in cyclohexylamine (CHA),6 and similar studies in other low-dielectric-constant solvents including 1,2-dimethoxyethane (DME)7a and tetrahydrofuran (THF)7b*c have been carried out. A more limited ion-pair acidity scale has been developed in liquid NH,.7d Also, during this period, acidity scales have been established in the polar non-hydrogenbond-donor (NHBD) solvents dimethyl sulfoxide (Me$0)8 and N-methylpyrrolidin-2-one (NMP)? which have relatively high dielectric constants. The pK,'s measured in these solvents differ from ion-pair pK,'s in that they are absolute, in the sense that they are based on Me2S0 and NMP as the standard states, which allows direct comparisons to be made with H20 and gas-phase pK,'s. A truly absolute acidity scale has been established in the gas phase, which, for the first time, provides intrinsic measures of structural effects free of solvent effects.1° Our purpose in this Account is (a) to discuss briefly acidities in various solvent media, (b) to present a table of representative equilibrium acidity constants in M e 8 0 solution, and (c) to illustrate ways in which these pK, data can be used. In an accompanying Account we compare acidities in Me2S0 solution with intrinsic gas-phase acidities and discuss some of the insights into solvation effects provided thereby. Acidities in H 2 0 and Me2S0. It is important to recognize that pKa values are solvent dependent. The",1988,72,1780,4,0,5,12,13,15,16,24,23,27,22
187e2d614a0f342d7a6677528c7a4f3e3a0bb0ab,"In 1972 ~ and 19852 the Committee on Hearing and Equilibrium of the American Academy of Otolaryngolog3~-Head and Neck Surgery published recommended guidelines for reporting the results of treatment of Meniere's disease. These reports have proved very beneficial to efforts to understand this disorder and its treatment. With advancing knowledge it has become evident that the reporting guidelines could be refined further. In undertaking this review, the Committee established several guiding principles. We wished to establish a distinction between the recording of results and the analysis and interpretation of results. Insofar as is possible, we wanted to retain and integrate the methods recommended in the 1972 and 1985 reports. We wanted guidelines to be ""upwardly compatible"" in the sense of computer software, so that existing data could not only be conserved but analyzed in new ways. Reporting methods should be clearly stated, straightforward to apply, as simple as possible, and usable in a wide range of settings, from multicenter university studies to reviews of personal experience by individual private practitioners. Specialized test equipment should not be required. Methods should facilitate statistical evaluation and comparison of results among studies. The guidelines should encourage reports to reflect disease severity in a meaningful manner.",1995,9,1551,24,1,6,13,18,11,24,45,34,51,50
5a1e3136ac33b0cdcec827c245738f3e8ba0488c,"Because of its toxicity, arsenic is of considerable environmental concern. Its solubility in natural systems is strongly influenced by adsorption at iron oxide surfaces. The objective of this study was to compare the adsorption behavior of arsenite and arsenate on ferrihydrite, under carefully controlled conditions, with regard to adsorption kinetics, adsorption isotherms, and the influence of pH on adsorption. The adsorption reactions were relatively fast, with the reactions almost completed within the first few hours. At relatively high As concentrations, arsenite reacted faster than arsenate with the ferrihydrite, i.e., equilibrium was achieved sooner, but arsenate adsorption was faster at low As concentrations and low pH. Adsorp tion maxima of approximately 0.60 (0.58) and 0.25 (0.16) molAs molFe-1 were achieved for arsenite and arsenate, respectively, at pH 4.6 (pH 9.2 in parentheses). The high arsenite retention, which precludes its retention entirely as surface adsorbed species, indicates the likel...",1998,7,1320,90,1,6,22,25,31,40,33,52,54,54
0be2acd403746056d71f5a2c89c440100bd9127b,"Mendelian randomization (MR) permits causal inference between exposures and a disease. It can be compared with randomized controlled trials. Whereas in a randomized controlled trial the randomization occurs at entry into the trial, in MR the randomization occurs during gamete formation and conception. Several factors, including time since conception and sampling variation, are relevant to the interpretation of an MR test. Particularly important is consideration of the “missingness” of genotypes that can be originated by chance, genotyping errors, or clinical ascertainment. Testing for Hardy-Weinberg equilibrium (HWE) is a genetic approach that permits evaluation of missingness. In this paper, the authors demonstrate evidence of nonconformity with HWE in real data. They also perform simulations to characterize the sensitivity of HWE tests to missingness. Unresolved missingness could lead to a false rejection of causality in an MR investigation of trait-disease association. These results indicate that large-scale studies, very high quality genotyping data, and detailed knowledge of the life-course genetics of the alleles/genotypes studied will largely mitigate this risk. The authors also present a Web program (http://www.oege.org/software/hwe-mr-calc.shtml) for estimating possible missingness and an approach to evaluating missingness under different genetic models.",2009,19,925,27,6,23,43,70,86,112,104,104,87,80
bdd5f1ee849909127b9c91d299082c29071719f0,"Empirical research on cities starts with a spatial equilibrium condition: workers and firms are assumed to be indifferent across space. This condition implies that research on cities is different from research on countries, and that work on places within countries needs to consider population, income and housing prices simultaneously. Housing supply elasticity will determine whether urban success shows up in more people or higher incomes. Urban economists generally accept the existence of agglomeration economies, which exist when productivity rises with density, but estimating the magnitude of those economies is difficult. Some manufacturing firms cluster to reduce the costs of moving goods, but this force no longer appears to be important in driving urban success. Instead, modern cities are far more dependent on the role that density can play in speeding the flow of ideas. Finally, urban economics has some insights to offer related topics such as growth theory, national income accounts, public economics and housing prices.",2009,172,819,47,6,39,49,41,76,64,70,77,98,81
3273eccfed4825e1159a17fc4cb361ce670ad295,"An equilibrium theory of unemployment assumes that firms and workers maximize their payoffs under rational expectations and that wages are determined to exploit the private gains from trade. This book focuses on the modeling of the transitions in and out of unemployment, given the stochastic processes that break up jobs and lead to the formation of new jobs, and on the implications of this approach for macroeconomic equilibrium and for the efficiency of the labor market. This approach to labor market equilibrium and unemployment has been successful in explaining the determinants of the ""natural"" rate of unemployment and new data on job and worker flows, in modeling the labor market in equilibrium business cycle and growth models, and in analyzing welfare policy. The second edition contains two new chapters, one on endogenous job destruction and one on search on the job and job-to-job quitting. The rest of the book has been extensively rewritten and, in several cases, simplified.",2000,0,1167,159,6,16,32,36,44,53,52,76,64,73
4e8419da55942518cb52d59efb0af73cfe1a6e37,,1964,27,2342,13,7,28,43,61,87,104,137,159,146,135
bcb88e0d541ebde4a4d255a0bd67a3917d66b181,"As the area of sampling A increases in an ecologically uniform area, the number of plant and animal species s increases in an approximately logarithmic manner, or s = bAk, (1) where k < 1, as shown most recently in in the detailed analysis of Preston (1962). The same relationship holds for islands, where, as one of us has noted (Wilson, 1961), the parameters b and k vary among taxa. Thus, in the ponerine ants of Melanesia and the Moluccas, k (which might be called the faunal coefficient) is approximately 0.5 where area is measured in square miles; in the Carabidae and herpetofauna of the Greater Antilles and associated islands, 0.3; in the land and freshwater birds of Indonesia, 0.4; and in the islands of the Sahul Shelf (New Guinea and environs), 0.5.",1963,19,2080,82,0,1,6,7,7,6,13,2,3,4
