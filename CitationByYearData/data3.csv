paperId,abstract,year,referenceCount,citationCount,influentialCitationCount,year0_citation_count,year1_citation_count,year2_citation_count,year3_citation_count,year4_citation_count,year5_citation_count,year6_citation_count,year7_citation_count,year8_citation_count,year9_citation_count
4c0edcd685a2824cd26d3ccc2b6505a2f0229ec6,"This paper examines and rejects the commonplace view that the doctrines of classical contract law (offer and acceptance, consideration, damages) were logically linked to the political philosophy of laissez-faire. Many writers (Grant Gilmore, Patrick Atiyah, Lawrence Friedman) attribute much of the rigid and mechanical nature of nineteenth century contract law to its affinity with laissez-faire. In this paper I reject that connection. The key distinction is that between security of exchange and freedom of contract. Laissez-faire is strongly committed to both, but most of contract law only requires the former without the latter. Security of exchange ensures that the enforcement of legal contracts when, as commonly is the case, one party must perform before the other. Freedom of contract guarantees a broad sphere in which voluntary arrangements are permissible. Most contract law is devoted to the former, which can be supported even by those who favor extensive regulation of economic transactions. The strength or weakness of that law is largely determined by instrumental questions of whether they promote stable contracting over time.",1996,0,224,0,0,1,12,26,19,33,32,32,28,26
d3cdca6dcd3fdf4a19b6553f81665095de28cc8d,"The contract net protocol has been developed to specify problem-solving communication and control for nodes in a distributed problem solver. Task distribution is affected by a negotiation process, a discussion carried on between nodes with tasks to be executed and nodes that may be able to execute those tasks.",1980,13,4145,285,2,4,2,5,6,8,4,11,13,26
3db23b99e61a33113f29b1bd72b314f5b1de2a48,"I would like to thank Karl Aquino, Joel Brockner, Daniel Forbes, Matthew Kraatz, Judi McLean Parks, Alexandra Mithel, Lynn Shore, Jon Turner, Linn Van Dyne, Batia Wiesenfeld, Dale Zand, and four anonymous reviewers for their helpful assistance with this manuscript. This paper examines the theoretical and empirical relationships between employees' trust in their employers and their experiences of psychological contract breach by their employers, using data from a longitudinal field of 125 newly hired managers. Data were collected at three points in time over a two-and-a-half-year period: after the new hires negotiated and accepted an offer of employment; after 18 months on the job; and after 30 months on the job. Results show that the relationship between trust and psychological contract breach is strong and multifaceted. Initial trust in one's employer at time of hire was negatively related to psychological contract breach after 18 months on the job. Further, trust (along with unmet expectations) mediated the relationship between psychological contract breach and employees' subsequent contributions to the firm. Finally, initial trust in one's employer at the time of hire moderated the relationship between psychological contract breach and subsequent trust such that those with high initial trust experienced less decline in trust after a breach than did those with low initial trust.",1996,35,2769,306,1,3,22,24,27,40,38,71,67,91
54ce6dc5dca108231d2d70ec85c1420ee4eefaef,The psychological contract held by an employee consists of beliefs about the reciprocal obligations between that employee and his or her organization. Violation refers to the feelings of anger and betrayal that are often experienced when an employee believes that the organization has failed to fulfill one or more of those obligations. This article provides a model outlining the psychological sensemaking processes preceding an employee's experience of psychological contract violation. It also identifies factors that affect those processes with the aim of encouraging future empirical research.,1997,63,2422,269,2,19,23,32,38,36,51,63,56,71
d3df2627f9dc761fddbdef8c8192061619b11e82,"The occurrence and impact of psychological contract violations were studied among graduate management alumni (N = 128) who were surveyed twice, once at graduation (immediately following recruitment) and then two years later. Psychological contracts, reciprocal obligations in employment developed during and after recruitment, were reported by a majority of respondents (54.8 per cent) as having been violated by their employers. The impact of violations are examined using both quantitative and qualitative data. Occurrence of violations correlated positively with turnover and negatively with trust, satisfaction and intentions to remain.",1994,31,2183,172,2,6,9,18,22,17,32,31,38,47
144d05fc169b380401868cd60adef1acf2702ebd,"Methodological guidelines for object-oriented software construction that improve the reliability of the resulting software systems are presented. It is shown that the object-oriented techniques rely on the theory of design by contract, which underlies the design of the Eiffel analysis, design, and programming language and of the supporting libraries, from which a number of examples are drawn. The theory of contract design and the role of assertions in that theory are discussed.<<ETX>>",1992,30,2278,148,3,9,15,19,14,22,19,41,42,48
fdb84029310a2d220c78b301f2e8cc9d86c3e2bc,"This paper presents an economic institution which enabled eleventh-century traders to benefit from employing overseas agents despite the commitment problem inherent in these relations. Agency relations were governed by a coalition--an economic institution in which expectations, implicit contractual relations, and a specific information-transmission mechanism supported the operation of a reputation mechanism. Historical records and a simple game-theoretical model are used to examine this institution. The study highlights the interaction between social and economic institutions, the determinants of business practices, the nature of the merchants' law, and the interrelations between market and nonmarket institutions. Copyright 1993 by American Economic Association.",1993,46,2355,104,0,5,3,9,12,16,32,49,47,55
5a9a57fc2a6d67f389c82ca5263c3e634fc07736,"A meta-analysis was conducted to examine the influence of psychological contract breach on 8 work-related outcomes. Breach was related to all outcomes except actual turnover. Based on affective events theory, we developed a causal model integrating breach, affect (violation and mistrust), attitude (job satisfaction, organizational commitment, and turnover intentions), and individual effectiveness (actual turnover, organizational citizenship behavior, and in-role performance). Structural equation modeling was used to test the model. The results indicated that affect mediates the effect of breach on attitude and individual effectiveness. Two moderators were also examined including the type of breach measure (global vs. composite) and the content of the psychological contract breach (transactional vs. relational). Theoretical and practical implications of the results are discussed.",2007,124,1202,134,8,23,44,50,72,88,66,88,96,116
44946d0ccdc0c3f37605e8434456e2cc789ffa04,"In this remarkably original work of political philosophy, one of today's foremost feminist theorist challenges the way contemporary society functions by questioning the standard interpretation of an idea that is deeply embedded in American and British political thought: that our rights and freedoms derive from the social contract explicated by Locke, Hobbes, and Rousseau and interpreted in the United States by the Founding Fathers. The author shows how we are told only half the story of the original contract that establishes modern patriarchy. The sexual contract is ignored and thus men's patriarchal right over women is also glossed over. No attention is paid to the problems that arise when women are excluded from the original contract but incorporated into the new contractual order. One of the main targets of the book is those who try to turn contractarian theory to progressive use, and a major thesis of the book is that this is not possible. Thus those feminists who have looked to a more ""proper"" contract- one between genuinely equal partners, or one entered into without any coercion- are misleading themselves. In the author's words, ""In contract theory universal freedom is always a hypothesis, a story, a political fiction. Contract always generates political right in the forms of domination and subordination."" Thus the book is also aimed at mainstream political theorists, and socialist and other critics of contract theory. The author offers a sweeping challenge to conventional understandings- of both left and right- of actual contracts in everyday life: the marriage contract, the employment contract, the prostitution contract, and the new surrogate mother contract. By bringing a feminist perspective to bear on the contradictions and paradoxes surrounding women and contract, and the relation between the sexes, she is able to shed new light on fundamental political problems of freedom and subordination.",1988,0,1914,23,0,9,12,20,31,27,44,47,50,55
6027e64c7f70aa50fac551ffabc70e4e1015f932,"Nonstandard employment relations—such as part-time work, temporary help agency and contract company employment, short-term and contingent work, and independent contracting—have become increasingly prominent ways of organizing work in recent years. Our understanding of these nonstandard work arrangements has been hampered by inconsistent definitions, often inadequate measures, and the paucity of comparative research. This chapter reviews the emerging research on these nonstandard work arrangements. The review emphasizes the multidisciplinary nature of contributions to this field, including research by a variety of sociologists, economists, and psychologists. It also focuses on cross-national research, which is needed to investigate how macroeconomic, political, and institutional factors affect the nature of employment relations. Areas for future research are suggested.",2000,115,1243,108,3,5,11,21,9,39,36,39,67,61
4abaf1206384d432a8069b9324d1f680ac4fc987,"This study examines factors affecting employees' perceptions that their psychological contract has been breached by their organization, and factors affecting whether this perception will cause employees to experience feelings of contract violation. Data were obtained from 147 managers just prior to their beginning of new job (time 1) and 18 months later (time 2). It was found that perceived contract breach at time 2 was more likely when organizational performance and self-reported employee performance were low, the employee had not experienced a formal socialization process, the employee had little interaction with organizational agents prior to hire, the employee had a history of psychological contract breach with former employers, and the employee had many employment alternatives at the time of hire. Furthermore, perceived breach was associated with more intense feelings of violation when employees both attributed the breach to purposeful reneging by the employer and felt unfairly treated in the process. Theoretical and practical implications of these results are discussed. Copyright © 2000 John Wiley & Sons, Ltd.",2000,60,1180,92,1,5,7,15,22,31,33,41,45,46
6923e559a4ec75bed719333e121fef5b228b660b,"The propositions that organization matters and that it is susceptible to analysis were long greeted by skepticism by economists. One reason why this message took a long time to register is that it is much easier to say that organization matters than it is to show how and why. The prevalence of the science of choice approach to economics has also been an obstacle. As developed herein, the lessons of organization theory for economics are both different and more consequential when examined through the lens of contract. This paper examines economic organization from a science of contract perspective, with special emphasis on the theory of the firm.",2002,149,1179,60,1,17,17,29,37,40,43,58,58,93
80194d40718ffbe6071ba1d30b67f65b30ece2df,"In an exploratory longitudinal study of business school alumni, we investigated changes in employment obligations as perceived by employees. During the first two years of employment, employees came...",1994,26,1376,88,4,5,10,18,16,14,23,27,30,47
f4984b873486ffe9e221ed4406c295125617d3d1,"As the magnitude of human impacts on the ecological systems of the planet becomes apparent, there is increased realization of the intimate connections between these systems and human health, the economy, social justice, and national security. The concept of what constitutes “the environment” is changing rapidly. Urgent and unprecedented environmental and social changes challenge scientists to define a new social contract. This contract represents a commitment on the part of all scientists to devote their energies and talents to the most pressing problems of the day, in proportion to their importance, in exchange for public funding. The new and unmet needs of society include more comprehensive information, understanding, and technologies for society to move toward a more sustainable biosphere—one which is ecologically sound, economically feasible, and socially just. New fundamental research, faster and more effective transmission of new and existing knowledge to policy- and decision-makers, and better communication of this knowledge to the public will all be required to meet this challenge.",1998,110,1174,25,8,29,31,41,34,32,33,42,45,56
6a9e3477b02be2c682c0975295bf9f33287785be,"Understanding the dynamics of the psychological contract in employment is difficult without research into its formation. Unfortunately, far less research exists on the antecedents and formation of the psychological contract than on the consequences associated with it. Three concepts frequently studied in psychology are particularly important to advancing research on psychological contract formation: schemas, promises, and mutuality (i.e. objective and perceptual agreement). This article develops the implications these three concepts have for future research on psychological contract formation.",2001,0,1003,132,1,1,22,20,35,37,43,37,50,58
0529e0fd97ffa8560cb67da179b886a00247765d,"This paper develops a theory of inequality and the social contract aiming to explain how countries with similar economic and political ""fundamentals"" can sustain such different systems of social insurance, fiscal redistribution, and education finance as those, of the United States and Western Europe. With imperfect credit and insurance markets some redistributive policies can improve ex ante welfare, and this implies that their political support tends to decrease with inequality. Conversely, with credit constraints, lower redistribution translates into more persistent inequality; hence the potential for multiple steady states, with mutually reinforcing high inequality and low redistribution, or vice versa.",2000,84,1073,94,6,25,26,44,53,49,39,63,45,50
7a22b8cb941eb0dd02d09f908fc46efc39543c7f,,1980,0,1296,103,1,0,0,0,1,5,2,2,3,2
742a45998609b2b532f93cbb6ad361dc320ab43f,"""Contract farming is seen by proponents as a way to raise small-farm income by delivering technology and market information to small farmers, incorporating them into remunerative new markets. Critics, however, see it as a strategy for agribusiness firms to pass production risk to farmers, taking advantage of an unequal bargaining relationship. There is also concern that contract farming will worsen rural income inequality by favoring larger farmers. This study examines these issues in Shandong Province, China, using survey data collected from 162 apple and green onion farmers and interviews with four contracting firms in 2005. Using a probit model to estimate participation in a contract-farming scheme, we find little evidence that contracting firms prefer to work with larger farmers, though all farms in the area are quite small. Furthermore, using a Heckman selection-correction model to control for possible selection bias, we find that contract farmers earn significantly more than independent farmers after controlling for household labor availability, education, farm size, and other characteristics. Finally, we find that the way contracting contributes to farm income varies between commodities: contract apple growers benefit from higher yields (presumably due to technical assistance), while contract green onion growers receive higher prices (presumably due to better quality). These results suggest that contract farming can help small farmers raise their incomes and gain access to the growing urban and export markets. Questions remain regarding the number of farmers that are, or could be, brought into similar contract arrangements."" from Authors' Abstract",2009,59,431,33,8,16,12,27,29,42,45,45,43,31
0c45c9deab708fe52091685f5f22f5cc8fbd3822,"In summary, with respect to the proposed social contract: we are strongly in favour of increased public consultation. To be effective, the process must be 'customer friendly,' and therefore, we see the need for the local community panel to deal with all of the health care needs of that community. The hospital situated in that local community should take the lead role, on behalf of the total health care system, to ensure that the community's total health care needs are addressed. The District Health Council should play the key role of facilitator of the process, promoting collaboration between health care providers situated both within and outside of that local community.",1993,0,1156,40,3,8,6,6,2,12,13,16,16,14
69d04469005b73c804c47518e279110c4d1c39ea,"Summary The paper examines the revenue effects of certified organic contract farming for smallholders and of adoption of organic agricultural farming methods in a tropical African context. The comparison in both cases is with farming systems that are ""organic by default."" Survey data from a large organic coffee contract farming scheme in Uganda are reported and analyzed using a standard OLS regression and a full information maximum likelihood (FIML) estimate of the Heckman selection model. The analysis finds that, controlling for a range of factors, there are positive revenue effects both from participation in the scheme and, more modestly, from applying organic farming techniques.",2009,26,407,26,2,15,18,30,22,40,40,39,39,36
726a55f39b0abeccce4cbf2df69fa9adb3373a45,"In contractual relationships involving payments for environmental services, conservation buyers know less than landowners know about the costs of contractual compliance. Landowners in such circumstances use their private information as a source of market power to extract informational rents from conservation agents. Reducing informational rents is an important task for buyers of environmental services who wish to maximize the services obtained from their limited budgets. Reducing informational rents also mitigates concerns about the ""additionality"" of PES contracts because low-cost landowners are least likely to provide different levels of services in the absence of a contract. Paying low-cost landowners less thus makes resources available for contracts with higher opportunity cost landowners, who are more likely to provide substantially different levels of services in the absence of a contract. To reduce informational rents to landowners, conservation agents can take three approaches: (1) acquire information on observable landowner attributes that are correlated with compliance costs; (2) offer landowners a menu of screening contracts; and (3) allocate contracts through procurement auctions. Each approach differs in terms of its institutional, informational and technical complexity, as well as in its ability to reduce informational rents without distorting the level of environmental services provided. No single approach dominates in all environments. Current theory and empirical work provides practitioners with insights into the relative merits of each approach. However, more theoretical work and experimentation in the laboratory and the field are necessary before definitive conclusions about the superiority of one or more of these approaches can be drawn.",2008,49,542,45,24,27,43,49,45,54,44,36,43,43
079cb0234ea8f80ca6385c71ffc66dbfbc95d0e8,,1999,0,922,136,1,2,7,4,7,15,11,17,13,20
2b025bf6703f309cb1aada50243aee78441880a2,"Numerous experimental studies indicate that people tend to reciprocate favors and punish unfair behavior. It is hypothesized that these behavioral responses contribute to the enforcement of contracts and increase gains from trade. It turns out that, if only one side of the market has opportunities for reciprocal responses, the impact of reciprocity on contract enforcement depends on the details of the pecuniary incentive system. If both sides of the market have opportunities for reciprocal responses, robust and powerful reciprocity effects occur. In particular, reciprocal behavior causes a substantial increase in the set of enforceable actions and large efficiency gains. (This abstract was borrowed from another version of this item.)",1997,19,1143,35,2,12,17,38,26,41,32,48,81,75
a1c771e8dcaf6a905eace74a51ad3256c3e04018,"On the basis of a case study of two sequential alliances between the same firms, we develop a more integrative perspective on alliance governance, providing insights into the interactions between structural and relational aspects, both within and between transactions. In particular, we disentangle (1) how contracts with a similar degree but different nature of formalization (narrow versus broad) trigger different kinds of trust dynamics (negative versus positive) at both operational and managerial levels, (2) how trust dynamics and contract application (rigid versus flexible) coevolve over time, and (3) how relational dynamics in previous transactions influence the design of contracts in subsequent transactions.",2008,83,474,40,2,14,22,28,31,32,50,34,40,51
8eb5ea17fb6c04414f522ff3711f20eef50a0a7f,"A contract network extends the concept of a contract path to address the problem of loop flow and congestion in electric power transmission systems. A contract network option provides an internally consistent framework for assigning long-term capacity rights to a complicated electric transmission network. The contract network respects the special conditions induced by Kirchoff's Laws; accommodates thermal, voltage, and contingency constraints on transmission capacity; and can be adopted without disturbing existing methods for achieving an economic power dispatch subject to these constraints. By design, a contract network would maintain short-run efficiency through optimal spot-price determination of transmission prices. Through payment of congestion rentals, the contract network makes a long-term capacity-right holder indifferent between delivery of the power or receipt of payments in a settlement system. to]Everybody talks about the weather, but nobody does anything about it.",1992,22,997,80,2,5,8,9,21,34,21,28,30,37
062fd15c1bc7dee9995d51156f11f5a59cc16e16,We examined psychological contract breach and violation as they occur within social exchange relationships to account for employee outcomes. Results of a longitudinal study suggested that contract breach partially mediated the effects of perceived organizational support (POS) and leader-member exchange (LMX) (time 1 measures) on intentions to quit (time 2 measure). POS and LMX moderated the relationship between breach and violation (time 2 measure). Violation fully mediated the effects of breach on commitment and trust and partially mediated the effect of breach on turnover intentions. These findings highlight the interconnection of social exchange and psychological contract processes.,2008,66,465,29,0,7,16,43,36,30,32,45,44,60
b219dfa935479391c65389d1f6a5db4cf58d4e8e,"Organizational forms involving more detailed contracts than are found in traditional spot market exchanges appear to be increasingly prevalent. There has been relatively little analysis, however, of the extent to which firms learn how to use contracts to manage their interfirm relationships over time. In this paper, we conduct a detailed case study of a time series of 11 contracts concluded during 1989-1997 between the same two partners, both of whom participate in the personal computer industry, to explore whether and how firms learn to contract. We find many changes to the structure of the contracts that cannot be fully explained by changes in the assets at risk in the relationship, and evidence that these changes are largely the result of processes in which the firms were learning how to work together, including learning how to contract with each other. The nature of this learning appears to have been quite incremental and local, that is, not very far sighted. We suggest how and when contracts might serve as repositories for knowledge about how to govern collaborations, and suggest some boundary conditions for this phenomenon. Our findings also provide implications for the debate about whether contracts have a positive or negative effect on interorganizational trust. We conclude with suggestions for future research.",2004,94,819,79,4,15,24,43,57,66,51,60,48,53
548026bddbf566fb27c2b00f0e01fabf17811f40,,1997,0,951,66,0,4,5,5,8,14,19,14,25,23
c5567edf0d68effac453dea16db0a4a2bd8609ae,"The renewed interest in the concept of the psychological contract has come to the fore in attempts to describe, understand and predict the consequences of changes occurring in the employment relationship. Recognizing that the employment relationship includes two parties to the exchange process, we set out to examine the content and state of the psychological contract from both the employee and employer perspective. The two perspectives permit an examination of the mutuality of obligations, which has not received much empirical attention to date. The research methodology consists of two surveys conducted in a large local authority directly responsible and accountable for a range of public services including education, environmental health and social care to the local population. The key findings suggest that the majority of employees have experienced contract breach. This view is also supported by managers, as representatives of the employer, who further indicate that the organization, given its external pressures, is not fulfilling its obligations to employees to the extent that it could. Overall, the results indicate that employees are redressing the balance in the relationship through reducing their commitment and their willingness to engage in organizational citizenship behaviour when they perceive their employer as not having fulfilled its part in the exchange process.",2000,67,883,59,0,3,14,13,19,18,34,28,38,42
a4b8393bc2f3ab407f7c144ace58d6f8ef191c50,"Our aim is to unpack contract design capabilities for detailed commercial contracts, to draw out implications for the locus of such capabilities within the firm, and to examine implications for exploiting those capabilities as a potential source of competitive advantage. We argue that developing contract design capabilities involves learning how much and what kinds of detail to include in a contract. We further argue that knowledge about the management of these trade-offs resides differentially in managers, engineers, and lawyers regarding different types of contractual provisions.",2007,64,530,46,5,21,37,33,39,51,52,35,42,39
669c9b3dad159176c85997233dd88054d66565a9,"Consider a supply chain consisting of two independent agents, a supplier e.g., a manufacturer and its customer e.g., a retailer, the latter in turn serving an uncertain market demand. To reconcile manufacturing/procurement time lags with a need for timely response to the market, such supply chains often must commit resources to production quantities based on forecasted rather than realized demand. 
 
The customer typically provides a planning forecast of its intended purchase, which does not entail commitment. Benefiting from overproduction while not bearing the immediate costs, the customer has incentive to initially overforecast before eventually purchasing a lesser quantity. The supplier must in turn anticipate such behavior in its production quantity decision. This individually rational behavior results in an inefficient supply chain. 
 
This paper models the incentives of the two parties, identifying causes of inefficiency and suggesting remedies. Particular attention is given to the Quantity Flexibility QF contract, which couples the customer's commitment to purchase no less than a certain percentage below the forecast with the supplier's guarantee to deliver up to a certain percentage above. Under certain conditions, this method can allocate the costs of market demand uncertainty so as to lead the individually motivated supplier and customer to the systemwide optimal outcome. We characterize the implications of QF contracts for the behavior and performance of both parties, and the supply chain as a whole.",1999,27,877,37,6,4,10,20,16,20,44,37,46,61
2eb0b9eea9183356dba7c3fdda26d69c4ec66f12,"A psychological tax contract goes beyond the traditional deterrence model and explains tax morale as a complicated interaction between taxpayers and the government. As a contractual relationship implies duties and rights for each contract party, tax compliance is increased by sticking to the fiscal exchange paradigm between citizens and the state. Citizens are willing to honestly declare income even if they do not receive a full public good equivalent to tax payments as long as the political process is perceived to be fair and legitimate. Moreover, friendly treatment of taxpayers by the tax office in auditing processes increases tax compliance.",2007,232,525,26,14,11,15,21,20,38,38,37,45,45
db3a32b2303ea644ffc058677b0f22dfc6b66a8c,"This paper examines empirically the importance of relationship investments in determining the duration of coal contracts negotiated between coal suppliers and electric utilities, using data for 277 coal contracts. For each contract, measures of the duration of contractual commitments agreed to by the parties at the contract execution stage and measures of the importance of relationship specific investments are developed. The results provide strong support for the view that buyers and sellers make longer commitments to the terms of future trade at the contract execution stage, and rely less on repeated bargaining, when relationship-specific investments are more important. Copyright 1987 by American Economic Association.",1987,19,1079,42,4,6,8,7,5,4,11,12,12,19
928e0acf69114ed6a1fc007ec112752b75b10d9f,,2006,0,559,58,14,21,34,43,43,40,41,39,51,56
2e2d3200b6608b815cc065af364035da5cd0b045,"How do rational firms respond to consumer biases? In this paper we analyze the profit-maximizing contract design of firms if consumers have time-inconsistent preferences and are partially naive about it. We consider markets for two types of goods: goods with immediate costs and delayed benefits (investment goods) such as health club attendance, and goods with immediate benefits and delayed costs (leisure goods) such as credit card-financed consumption. We establish three features of the profit-maximizing contract design with partially naive time-inconsistent consumers. First, firms price investment goods below marginal cost. Second, firms price leisure goods above marginal cost. Third, for all types of goods firms introduce switching costs and charge back-loaded fees. The contractual design targets consumer misperception of future consumption and underestimation of the renewal probability. The predictions of the theory match the empirical contract design in the credit card, gambling, health club, life insurance, mail order, mobile phone, and vacation time-sharing industries. We also show that time inconsistency has adverse effects on consumer welfare only if consumers are naive.",2004,69,776,67,17,28,34,28,38,39,34,41,53,47
04223216d8ea38a7f5a007217e21d8c0cf8d51e0,"This study examines the relationships betweenviolations of employees' psychological contracts andtheir exit, voice, loyalty, and neglect behaviors. Usinga sample of over 800 managers, this research found that psychological contract violations resultin increased levels of exit, voice, and neglectbehaviors and decreased levels of loyalty to theorganization. In addition, this research examines themoderating effects that situational factors (such as theavailability of attractive employment alternatives) haveon the relationships between psychological contractviolations and managers' behaviors. The results suggest that these situational factors moderatethe relationship between psychological contractviolations and exit, but not the relationships betweenpsychological contract violations and voice, loyalty, or neglect. Finally, this research alsoexamines differences in the nature of psychologicalcontract violations experienced across three categoriesof workers: new managers entering the workforce,expatriates and managers in international business, andmanagers working in downsizing or restructuring firms.The results suggest that psychological contractviolations are both more frequent and more intense among managers working in downsizing or restructuringfirms, particularly in terms of job security,compensation, and opportunities foradvancement.",1999,41,808,68,1,3,12,21,38,31,30,37,38,51
3fb847f874f22c329019d68e650247a18eba3b9b,"1. Contracting In. 2. Patriarchal Confusions. 3. Contract, the Individual and Slavery. 4. Genesis, Fathers and the Political Liberty of Sons. 5. Wives, Slaves and Wage-Slaves. 6. Feminism and the Marriage Contract. 7. What's Wrong with Prostitution?",1989,0,966,45,0,1,5,6,3,6,9,4,9,5
9c6e6facfcdc1456776e57cb96158f224bfd4286,"The aim of this study was to examine the influence of age in the relation between psychological contract breach and the development of job attitudes. Based on affective events, social exchange, and lifespan theory, we hypothesized that (1) psychological contract breach would be related negatively to job attitudes, and (2) that age would moderate these relations. The hypotheses were tested by means of a meta-analysis of k = 60 studies, using Weighted Least Squares estimation. Our results supported both hypotheses for the outcomes trust and organizational commitment. However, for job satisfaction the moderating influence of age was in the unexpected direction. The relations between contract breach and trust and organizational commitment were indeed stronger for younger workers, whereas the relation between contract breach and job satisfaction was stronger for older workers. The implications are discussed, and a research agenda is presented.",2008,130,368,16,3,19,17,26,21,17,28,35,36,42
8d309c77797882d2192e3cf2df2a051a8d74cc93,"This study examines how contract, cooperation, and performance are associated with one another within international joint ventures (IJVs). We argue that contract and cooperation are not substitutes but complements in relation to IJV performance. An IJV contract provides an institutional framework guiding the course of cooperation, while cooperation overcomes the adaptive limits of contracts. Our analysis of 293 IJVs in a dynamic market demonstrates that previous cooperation bolsters contractual adaptability, which in turn nurtures current cooperation between the same partners. We find that contract completeness and cooperation drive IJV performance both independently and interactively. When contracts are more complete, cooperation contributes more to performance. Contract and cooperation differ in their quadratic effects such that the contribution of contract completeness to performance declines as completeness increases but the contribution of cooperation remains linear. Copyright © 2002 John Wiley & Sons, Ltd.",2002,37,712,100,0,5,13,17,26,34,29,40,37,47
82b4f79355ba73534b80131520d171fee2919115,"Abstract This paper demonstrates how contract farming functions as an economic institution and explores the causes of the observed variation in the scale of outgrower production in Latin America. We outline how market imperfections and transaction costs influence the decision of agroprocessing firms to contract-out, vertically integrate, or use spot markets to obtain raw product. The paper demonstrates how market conditions are likely to be associated with particular outgrower characteristics under contract farming. An analysis of the Mexican frozen vegetable industry illustrates determinants of successful and unsuccessful small-scale contracting and suggests alternative policies to promote contract farming with smallholders.",1999,46,776,65,2,9,5,13,10,11,26,31,29,35
9c1bffdea6e6fa33247f817a86d505742aa453a3,"We show experimentally that fairness concerns may have a decisive impact on the actual and optimal choice of contracts in a moral hazard context. Bonus contracts that offer a voluntary and unenforceable bonus for satisfactory performance provide powerful incentives and are superior to explicit incentive contracts when there are some fair-minded players, but trust contracts that pay a generous wage up front are less efficient than incentive contracts. The principals understand this and predominantly choose the bonus contracts. These results are consistent with recently developed theories of fairness, which offer important new insights into the interaction of contract choices, fairness, and incentives.",2007,48,458,32,21,29,28,41,31,52,41,36,28,37
772f92ff106a2e77828b2bba254b6d939068ba82,"This article contributes to the debate on the relation between trust and control in the management of inter-organizational relations. More specifically, we focus on the question how trust and formal contract are related. While there have been studies on whether trust and contract are substitutes or complements, they offer little insight into the dynamic interaction between the two. They fail to answer, first, whether contract precedes trust or follows it, in other words, what causal relationship exists between the concepts; second, how and why trust and contract can substitute or complement each other; and third, how the various combinations of trust and contract affect a relationship’s development and outcome. In search of answers, we conducted longitudinal case studies to reveal the relationship between trust, contract and relationship outcome in complex inter-firm relationships. We find trust and contract to be both complements and substitutes and find that a close study of a contract’s content offers alternative insight into the presence and use of contracts in inter-firm relationships.",2005,62,597,41,3,21,37,39,44,36,33,37,35,51
835461507aef99751ad708f9a197499b23b2e49b,"Prior integrations of the leader-member exchange (LMX) and psychological contract literatures have not clarified how within-group LMX differentiation influences employees' attitudes and behaviors in the employment relationship. Therefore, using a sample of 278 members and managers of 31 intact work groups at 4 manufacturing plants, the authors examined how LMX operating at the within-group level (relative LMX, or RLMX) and the group level influenced perceptions of psychological contract fulfillment and employee-level outcomes. Controlling for individual-level perceptions of LMX quality, results indicated a positive relationship between RLMX and fulfillment, which was strengthened as group-level variability in LMX quality increased. Perceptions of fulfillment mediated the relationship between RLMX and performance and sportsmanship behaviors. The importance of conceptualizing LMX as simultaneously operating at multiple levels is highlighted.",2008,55,310,25,0,5,19,11,20,28,28,25,34,27
3aa01863647ed7a59e0db0ff181b0227c97964f4,"Outsourcing of information technology (IT) services has received much attention in the information systems (IS) literature. However, considerably less attention has been paid to actual contract structures used in IT outsourcing (ITO). Examining contract structures yields important insights into how the contracting parties structure the governance provisions and the factors or transaction risks that influence them. Based on insights from prior literature, from practicing legal experts, and through in-depth content analysis of actual contracts, we develop a comprehensive coding scheme to capture contract provisions across four major dimensions: monitoring, dispute resolution, property rights protection, and contingency provisions. We then develop an empirical data set describing the contract structures across these distinct dimensions, using a sample of 112 ITO contracts from the Securities and Exchange Commission (SEC) database from 1993 to 2003. 
 
Drawing on transaction cost, agency, and relational exchange theories, we hypothesize the effects of transaction and relational characteristics on the specific contractual provisions, as well as on overall contract extensiveness. Furthermore, we examine how these associations vary under conditions of fixed price and time and materials pricing structures. The results provide good support for the main hypotheses of the study and yield interesting insights about contractual governance of ITO arrangements.",2009,84,166,19,2,8,18,14,20,17,18,19,13,8
c25bee8098dafe7bdc2456a2a5aabadf631190bd,"Under the prevailing contract between science and society, science has been expected to produce 'reliable' knowledge, provided merely that it communicates its discoveries to society. A new contract must now ensure that scientific knowledge is 'socially robust', and that its production is seen by society to be both transparent and participative.",1999,3,746,43,1,6,10,9,23,21,37,21,31,43
e35f650933a0e3dca8af504a3fdde33c9b4db6d5,"This research examines the relationships between psychological contract fulfillment and three types of employee behavior: in-role performance, organizational citizenship behavior directed at the organization, and organizational citizenship behavior directed at individuals within the organization. Using a sample of 134 supervisor-subordinate dyads, this study suggests that the extent of psychological contract fulfillment is positively related to the performance of all three types of employee behavior. In addition, the results indicate that psychological contract fulfillment is more strongly related to citizenship behavior directed at the organization than to citizenship behavior directed at one’s colleagues. Finally, this research investigates if employees’ attributions regarding the reasons that psychological contract breach occurred also impact their work performance. However, the data provide only limited support for the idea that employees are most likely to reduce their work effort when they perceive that the organization has intentionally failed to live up to its commitments.",2003,40,682,43,5,14,12,18,20,30,34,43,40,42
bb04ca7046522a531158e19567e0cef362b41ba6,"Uses the tools of Coase's (1937) theory of the firm, and an analysis of property rights, incentives and monitoring, to assess the nature of the franchise contract. Franchising is of interest to economists because it represents a situation in which there is not a sharp distinction between interfirm and intrafirm transactions. The institutional structure of the franchise is discussed, and a franchise agreement is defined as a contract between two legal firms--the franchisor, or parent company, and the franchisee, a firm set up in a specific location to market the product or service offered by the parent company. The franchisee pays a specific amount of money for the right to market this product or service. The standard explanation of franchising, in terms of capital markets, is then considered and rejected. The theory debated is that franchisors use franchising as a means to raise capital. However, by considering this argument in light of modern capital theory, it is concluded that the franchisor interested in raising capital would do better to create a portfolio of shares in many franchise outlets in order to diversify risks and maximize profit potential. An alternative explanation of franchisee motivations is given which suggests that both parts of the contract give property rights to the parties, i.e., the franchisor and the franchisee, for areas they can most efficiently control. Finally, the application of antitrust law to franchising is found to be false, since it confuses legal categories with economically meaningful ones. (SFL)",1978,0,980,76,0,1,2,3,0,3,2,5,7,3
09368ccba6d01b1aa2399c0c04a4cbc6bb5e8f02,"This paper investigates a revenue-sharing contract for coordinating a supply chain comprising one manufacturer and two competing retailers. The manufacturer, as a Stackelberg leader, offers a revenue-sharing contract to two competing retailers who face stochastic demand before the selling season. Under the offered contract terms, the competing retailers are to determine the quantities to be ordered from the manufacturer, prior to the season, and the retail price at which to sell the items during the season. The process of pricing and ordering is expected to result in an equilibrium as in the Bayesian Nash game. On the basis of anticipated responses and actions of the retailers, the manufacturer designs the revenue-sharing contract. Adopting the classic newsvendor problem model framework and using numerical methods, the study finds that the provision of revenue-sharing in the contract can obtain better performance than a price-only contract. However, the benefits earned under the revenue-sharing contract by different supply chain partners differ because of the impact of demand variability and price-sensitivity factors. The paper also analyses the impact of demand variability on decisions about optimal retail price, order quantity and profit sharing between the manufacturer and the retailers. Lastly, it investigates how the competition (between retailers) factor influences the decision-making of supply chain members in response to uncertain demand and profit variability.",2008,36,244,6,7,5,14,17,15,19,25,27,21,18
b1e5661a73b63b3df6d95b973805c94b15b9b3c2,"Summary The purpose of the current paper is to examine the ways in which age and work experience shape how individuals experience psychological contract breaches. We first introduce the concepts of contract malleability (the degree to which individuals can tolerate deviations from contract expectations) and contract replicability (the degree to which individuals believe that their psychological contracts can be replicated elsewhere). Next, we discuss the variety of reasons why contract malleability and replicability become greater with age and work experience and how contract malleability and replicability may temper negative reactions to psychological contract breaches. We also address the different ways contract malleability and replicability mediate the relationships between age and work experience, on one hand, and exit, voice, loyalty, and neglect behaviors on the other. We consider the moderating effects of age similarity and dissimilarity here as well. The paper concludes with a discussion of the implications for future research designs and for managing older and more experienced workers. Copyright # 2009 John Wiley & Sons, Ltd.",2009,121,125,10,0,4,11,5,8,12,13,15,8,11
3c69de53a307f8bc4bad0dcf3cb951c92a30b127,"This paper presents a framework to study the historical development of the relationship between science and society. We elaborate this relationship as a contract that specifies the mission of scientific research, the rationales for public support for science, and the conditions under which scientists work. These three structural elements will always be part of the contract, but their specific content can vary. The credibility cycle, as a model for scientific practice, helps to describe and understand the consequences of a changing contract for the work of individual scientists. A brief case study of chemistry in the Netherlands demonstrates the usefulness of the framework. We show how concepts of relevance have changed since 1975 and how this affects the practice of academic chemistry.",2009,5,132,7,0,8,9,10,12,14,8,14,11,19
590c9339a1849f363dde3e99bcfb67c24ae5896c,"Groundwater geochemistry is an interdisciplinary science concerned with the chemistry in the subsurface environment. The chemical composition of groundwater is the combined result of the quality of water that enters the groundwater reservoir and reactions with minerals and organic matter of the aquifer matrix may modify the water quality. Apart from natural processes as controlling factors on the groundwater quality, in recent years the effect of pollution, such as nitrate from fertilizers and acid rain, also influences the groundwater chemistry. Due to the long residence time of groundwater in the invisible subsurface environment, the effect of pollution may first become apparent tens to hundreds of years afterwards. A proper understanding of the processes occurring in aquifers is required in order to predict what the effect of present day human activities will be on that scale. This book presents a comprehensive and quantitative approach to the study of groundwater quality. Practical examples of application are presented throughout the text.",1993,0,4138,332,2,6,3,11,20,38,51,59,79,82
1cb1946049da4cff926028e4821d7cdb25d68f41,"Prentice Hall, 1997. Book Condition: New. Brand New, Unread Copy in Perfect Condition. A+ Customer Service! Summary: 1. Thermochemical Principles. 2. Chemical Kinetics. 3. Aqueous Complexes. 4. Activity Coefficients of Dissolved Species. 5. Acids and Bases. 6. Carbonate Chemistry. 7. Chemical Weathering. 8. General Controls on Natural Water Chemistry. 9. The Geochemistry of Clay Minerals. 10. Adsorption-Desorption Reactions. 11. Oxidation-Reduction Concepts. 12. Iron and Sulfur Geochemistry. 13. Actinides and Their Daughter and Fission Products. Geochemical Computer Models. References. Index.",1997,0,2359,343,1,10,35,52,47,58,79,113,103,141
60c71b175ad3a2861e2f5ed4b3eda85022f4171f,"This book is written as a reference on organic substances in natural waters and as a supplementary text for graduate students in water chemistry. The chapters address five topics: amount, origin, nature, geochemistry, and characterization of organic carbon. Of these topics, the main themes are the amount and nature of dissolved organic carbon in natural waters (mainly fresh water, although seawater is briefly discussed). It is hoped that the reader is familiar with organic chemistry, but it is not necessary. The first part of the book is a general overview of the amount and general nature of dissolved organic carbon. Over the past 10 years there has been an exponential increase in knowledge on organic substances in water, which is the result of money directed toward the research of organic compounds, of new methods of analysis (such as gas chromatography and mass spectrometry), and most importantly, the result of more people working in this field. Because of this exponential increase in knowledge, there is a need to pull together and summarize the data that has accumulated from many disciplines over the last decade.",1985,789,2700,281,1,6,32,35,45,51,49,58,40,51
569114559b424f536ef2133c2627740109ffc012,"Analytical data for Sr, Rb, Cs, Ba, Pb, rare earth elements, Y, Th, U, Zr, Hf, Sn, Nb, Mo, Ni, Co, V, Cr, Sc, Cu and major elements are reported for eocene volcanic rocks cropping out in the Kastamonu area, Pontic chain of Northern Turkey. SiO2% versus K2O% relationship shows that the analyzed samples belong to two major groups: the basaltic andesitic and the andesitic ones. High-K basaltic andesites and low-K andesites occur too. Although emplaced on continental type basement (the North Anatolian Crystalline Swell), the Pontic eocene volcanics show elemental abundances closely comparable with typical island arc calc-alkaline suites, e.g. low SiO2% range, low to moderate K2O% and large cations (Cs, Rb, Sr, Ba, Pb) contents and REE patterns with fractionated light and almost flat heavy REE patterns. ΣREE and highly charged cations (Th, U, Hf, Sn, Zr) are slightly higher than typical calc-alkaline values. Ferromagnesian elements show variable values. Within the basaltic andesite group the increase of K%, large cations, ΣREE, La/Yb ratio and high valency cations and the decrease of ferromagnesian element abundances with increasing SiO2% content indicate that the rock types making up this group developed by crystalliquid fractionation of olivine and clinopyroxene from a basic parent magma. Trace element concentration suggest that the andesite group was not derived by crystal-liquid fractionation processes from the basaltic andesites, but could represent a distinct group of rocks derived from a different parent magma.",1976,18,3798,98,0,0,3,7,9,9,9,7,20,8
416e13e8326f25de552aa6b49944252ca32d31e7,"Abstract We report analyses of the176Hf/177Hf ratio for 25 chondrites from different classes of meteorites (C, O, and E) and the176Lu/177Hf ratio for 23 of these as measured by plasma source mass spectrometry. We have obtained a new set of present-day mean values in chondrites of176Hf/177Hf= 0.282772 ± 29 and176Lu/177Hf= 0.0332 ± 2. The176Hf/177Hf ratio of the Solar System material 4.56 Ga ago was 0.279742 ± 29. Because the mantle array lies above the Bulk Silicate Earth in a143Nd/144Nd versus176Hf/177Hf plot, no terrestrial basalt seems to have formed from a primitive undifferentiated mantle, thereby casting doubt on the significance of high3He/4He ratios. Comparison of observedHf/Nd ratios with those inferred from isotopic plots indicates that, in addition to the two most prominent components at the surface of the Earth, the depleted mantle and the continental crust, at least one more reservoir, which is not a significant component in the source of oceanic basalts, is needed to account for the Bulk Silicate Earth Hf-Nd geochemistry. This unaccounted for component probably consists of subducted basalts, representing ancient oceanic crust and plateaus. The lower continental crust and subducted pelagic sediments are found to be unsuitable candidates. Although it would explain the Lu-Hf systematics of oceanic basalts, perovskite fractionation from an early magma ocean does not account for the associated Nd isotopic signature. Most basalts forming the mantle array tap a mantle source which corresponds to residues left by ancient melting events with garnet at the liquidus.",1997,62,2481,277,2,9,11,13,13,26,22,27,36,50
c0f22604d7137aabe1d6d0f03499dbbcd0635260,"The development of petroleum geochemistry and geology carbon and origin of life petroleum and its products how oil forms - natural hydrocarbons how oil forms - generated hydrocarbons modeling petroleum generation the origin of natural gas migration and accumulation abnormal pressures the source rock coals, shales, and other terrestrial source rocks petroleum in the reservoir seeps and surface prospecting a geochemical program for petroleum exploration crude oil correlation prospect evaluation.",1995,0,2405,208,41,18,35,45,48,39,41,53,56,52
055cda652a732def8f3fe95ec165bc8c17600c4a,"Abstract With the aim to link zircon composition with paragenesis and thus metamorphic conditions, zircons from eclogite- and granulite-facies rocks were analysed for trace elements using LA-ICP-MS and SHRIMP ion microprobe. Metamorphic zircons from these different settings display a large variation in trace element composition. In the granulites, zircon overgrowths formed in equilibrium with partial melt and are similar to magmatic zircon in terms of high Y, Hf and P content, steep heavy-enriched REE pattern, positive Ce anomaly and negative Eu anomaly. They are distinguishable from magmatic zircon because of their low Th/U ratio. Independently of whole rock composition, metamorphic zircon domains in eclogite-facies rocks have low Th/U ratio and reduced HREE enrichment and Eu anomaly. In a low grade metamorphic vein, zircon has low Th/U ratio but is extremely enriched in Y, Nb and HREE. Petrological and geochronological data demonstrate that metamorphic zircon overgrowths crystallised at granulite-facies conditions in equilibrium with unzoned garnet. It is thus possible for the first time to calculate trace element distribution coefficients between zircon and garnet. Hf is the elements that most strongly partition into zircon. Y, Nb and REE have distribution coefficients between 90 and 0.9 with minimum values for the MREE. In eclogite-facies rocks, the HREE depletion in metamorphic zircon domains is attributed to concurrent formation of garnet under sub-solidus conditions. In one sample, the zircon/garnet trace elements partitioning indicates that metamorphic zircon formed in equilibrium with the garnet rim, i.e. at the eclogitic peak. The reduced Eu anomaly in the metamorphic zircon is interpreted as indicating absence of feldspars and thus supports zircon formation in eclogite facies. In a metamorphic vein within the eclogite-facies rocks, zircons have larger Eu anomaly with respect to high-pressure zircon. Together with geochronological evidence, the Eu anomaly suggests that these zircons formed during prograde metamorphism, before the break down of feldspars at high pressure. The REE composition of zircon can therefore relate zircon formation to specific metamorphic stages such as eclogite, granulite or greenschist facies. This allows linking zircon U–Pb ages with pressure–temperature conditions, a fundamental step in constraining rates of metamorphic processes.",2002,46,1938,280,4,30,34,36,34,47,58,68,70,93
88e5fa11e77f8b9d37dedf455df04edb2d0ad25a,"Basaltic volcanism 'samples' the Earth's mantle to great depths, because solid-state convection transports deep material into the (shallow) melting region. The isotopic and trace-element chemistry of these basalts shows that the mantle contains several isotopically and chemically distinct components, which reflect its global evolution. This evolution is characterized by upper-mantle depletion of many trace elements, possible replenishment from the deeper, less depleted mantle, and the recycling of oceanic crust and lithosphere, but of only small amounts of continental material.",1997,109,2174,166,12,34,55,43,52,64,78,89,77,71
b912f87e69796129dc4d0723a516ad5ad87c1de7,"1. The Hydrologic Cycle. 2. Chemical Background. 3. Organic Compounds in Natural Waters. 4. The Carbonate System and pH Control. 5. Clay Minerals and Ion Exchange. 6. Stability Relationships and Silicate Equilibria. 7. Kinetics. 8. Weathering and Water Chemistry, I: Principles. 9. Weathering and Water Chemistry, II: Examples. 10. Acid Deposition and Surface Water Chemistry. 11. Evaporation and Saline Waters. 12. The Oceans. 13. Redox Equilibria. 14. Redox Conditions in Natural Waters. 15. Trace Elements. 16. Mathematical and Numerical Models. 17. Isotopes. Appendices.",1988,0,2390,124,20,24,45,33,52,51,52,59,47,69
ff80f7f75ee30403b2a9a23bde406b08c9214901,Theoretical and Experimental Principles.- Isotope Fractionation Processes of Selected Elements.- Variations of Stable Isotope Ratios in Nature.,1973,0,1930,161,0,0,0,2,0,0,2,0,4,2
f01c83c9df2f71b1471bc154ee0964a7d6050b3c,"Partial table of contents: Hydrothermal Mineral Deposits: What We Do and Don't Know (B. Skinner). Magmas and Hydrothermal Fluids (C. Burnham). Thermal Aspects of Ore Formation (L. Cathles). Oxygen and Hydrogen Isotope Relationships in Hydrothermal Mineral Deposits (H. Taylor). Hydrothermal Alteration and Its Relationship to Ore Fluid Composition (M. Reed). Sulfide Ore Mineral Stabilities, Morphologies, and Intergrowth Textures (D. Vaughan & J. Craig). Gangue Mineral Transport and Deposition (J. Rimstidt). Fluid Inclusion Studies of Hydrothermal Ore Deposits (E. Roedder & R. Bodnar). Geothermal Systems and Mercury Deposits (H. Barnes & T. Seward). Submarine Hydrothermal Systems and Deposits (S. Scott). Ore--Forming Brines in Active Continental Rifts (M. McKibben & L. Hardie). Appendix. Index.",1968,0,2362,22,1,4,3,7,2,6,10,6,3,8
965bedad12934e33dcb7164a3abe3f289d62a112,"The factors affecting the amounts and types of organic matter in lacustrine sediments are summarized in this review, and synthesis, of published studies. Biota hving in the lake and in its watershed are the sources of the organic compounds initially contributed to the lake system. Microbial reworking of these materials during sinking and early sedimentation markedly diminishes the total amount of organic matter while replacing many of the primary compounds with secondary ones. Much of the organic matter content of sediments is the product of this microbial reprocessing. Various organic matter components of lake sediments nonetheless retain source information and thereby contribute to the paleohmnological record. Carbon/nitrogen ratios of total organic matter reflect original proportions of algal and land- derived material. Carbon isotopic compositions indicate the history of lake productivity and carbon recycling. Biomarker compounds provide important information about contributions from different biota. Sterol compositions and chainlength distributions of n-alkanes, n-alkanoic acids, and n-alkanols help distinguish different algal and watershed sources and also record diagenetic alterations. Stabilization of functional-group-containing biomarkers by conversion into saturated or aromatic hydrocarbons or by incorporation into bound forms improves their preservation and hence record of source information. Lignin components provide important evidence of watershed plant cover, and pigments reflect algal assemblages. The interplay of the factors influencing the organic matter content of lake sediments is illustrated by overviews of sedimentary records of four lake systems--Lake Biwa (Japan), Lake Greifen (Switzerland), Lake Washington (Pacific Northwest), and the Great Lakes (American Midwest).",1993,172,1472,104,0,3,8,13,8,14,16,22,15,25
e1ca0329e5e1489a913646e10146d34ac1bbffc4,"Chemical equilibrium aqueous solutions solution-mineral equilibria silicates crystal chemistry surface chemistry surface chemistry - the solution-mineral interface chemical thermodynamics chemical thermodynamics - phase equilibria oxidation and reduction siotope geochemistry reaction rates and mass transfer the fluid envelopes weathering and soils sedimentation and diagenesis - inorganic geochemistry, organic geochemistry metamorphism formation and crystallization of magmas volatiles and magmas hydrothermal ore deposits distribution of the elements historical geochemistry.",1967,0,1833,75,1,4,4,9,9,17,18,10,23,12
7d46fb931847cfeb88768ffd63be4d0ecf23b911,1. The Hydrologic Cycle. 2. Chemical Background. 3. The Carbonate System and pH Control 4. Clay Minerals and Cation Exchange. 5. Adsorption. 6. Organic Compounds in Natural Waters. 7. Redox Equilibria. 8. Redox Conditions in Natural Waters. 9. Heavy Metals and Metalloids. 10. Stability Relationships and Silicate Equilibria. 11. Kinetics. 12. Weathering and Water Chemistry. 13. Acid Water. 14. Isotopes. 15. Evaporation and Saline Waters. 16. Transport and Reaction Modeling References. Glossary of Geologic Terms. Appendix I: Piper and Stiff Diagrams. Appendix II: Standard-State Thermodynamic Data for Some Common Species. Appendix III: Equilibrium Constants at 25 C and Enthalpies of Reaction for Selected Reactions. Answers to Problems. Author Index. Subject Index.,1997,0,1214,176,1,3,11,20,20,37,31,44,63,59
1095964c675aab8a15b163c4278b7bb93d48cb61,"The organic matter content of lake sediments contains information that helps to reconstruct past environmental conditions, evaluate histories of climate change, and assess impacts of humans on local ecosystems. The elemental, isotopic, and molecular compositions of organic matter buried in sediment provide evidence of the biota that have lived in a lake and its catchment area, and they serve as proxies of organic matter delivery and accumulation. Sedimentary records from the North American Great Lakes provide examples of applications of organic geochemistry to paleolimnological reconstructions. The records of these lakes date from retreat of the Laurentide ice sheet around 12 ka, include the mid-Holocene Hypsithermal, and show consequences of recent human changes. Low Corg/Ntotal ratios indicate that most of the sediment organic matter in the Great Lakes is from algal production, yet changes in biomarker molecule compositions also show that varying amounts of land-plant organic matter have been delivered to the lakes. Elevated algal productivity that accompanies nutrient enrichments of lake waters is recorded as excursions to less negative δ13C values in the organic matter of sediments that were deposited in the 1960s and 1970s. Increased organic carbon mass accumulation rates mirror the isotopic excursions in most parts of the Great Lakes. Accumulations of petroleum residues and pyrogenic polycyclic aromatic hydrocarbons in sediments identify fluvial and eolian delivery of organic matter components to different parts of the Great Lakes. Emerging applications of multiple and novel organic geochemical proxies to paleolimnological reconstructions are promising, yet some potentially important measurements remain underutilized.",2003,166,1124,146,1,11,15,25,25,36,43,46,60,52
38e78cce7ecb5e0ff2b004235c92ac61b68b316c,,1984,0,1535,35,0,3,15,10,12,9,4,13,19,16
c06fd0f58ae6ec7aadbfde7611d0a358817d2d02,,1958,0,1895,78,5,7,7,7,9,13,12,11,10,14
492d00b32087f7aeb59fc703113fff48f0cfc60f,"Abstract The distributions of certain minor and trace elements in marine sediments should potentially provide forensic tools for determining the redox conditions of the bottom waters at the time of deposition. The ability to identify such conditions in the geological past is important because (1) current models of the conditions of formation of organic-rich rocks require reexamination, (2) a method to determine whether the areal extent of anoxic waters expanded or retracted in response to palaeoceanographic changes is required, and (3) the effects of such environmental changes on the geochemical balance of these elements in the ocean need to be understood. Recent research has suggested that some minor and trace elements are precipitated where free dissolved sulphide is present (Cu, Cd, Ni, Zn) without undergoing a valency change, whereas others undergo a change in valency and are either more efficiently adsorbed onto solid surfaces under oxic (I) or anoxic (V) conditions or are precipitated under anoxic conditions (Cr, Mn, Mo, Re, U, V). Hence, the enrichment of these minor and trace elements relative to their crustal abundances indicates that the host sediments accumulated under anoxic conditions, although not necessarily under anoxic bottom waters. Examination of the chemical composition of the sediments of anoxic basins, continental margin sediments and oxidized deepsea sediments shows that I and Mn enrichments are reliable indicators of bottom water oxygenation, whereas enrichments of the remaining elements reflect either bottom water anoxia or element uptake by subsurface anoxic sediments below a relatively thin surficial oxic veneer. Hence, the absence of metal enrichment in these cases can be taken as firm evidence that the bottom waters of a basin of sedimentation were not anoxic. These behaviours may be used to propose, for example, that the Holocene sapropel in the Black Sea accumulated under oxic bottom waters, whereas the modern facies reflects its formation under the prevailing intensely anoxic conditions, and that the Panama Basin bottom waters were not anoxic during the Last Glacial Maximum when the rate of accumulation of organic carbon increased. Likewise, the enrichment of Mn as a mixed carbonate phase in some ancient black shales strongly suggests that they formed under oxic bottom waters rather than anoxic conditions as is commonly assumed.",1993,132,1234,78,0,1,6,16,15,10,21,22,26,20
62972351669150c84574853f49519bcf3c9c3e38,"Abstract Several hundred samples of carbon from various geologic sources have been analyzed in a new survey of the variation of the ratio C13/C12 in nature. Mass spectrometric determinations were made on the instruments developed by H. C. Urey and his co-workers utilizing two complete feed systems with magnetic switching to determine small differences in isotope ratios between samples and a standard gas. With this procedure variations of the ratio C13/C12 can be determined with an accuracy of ±0.01% of the ratio. The results confirm previous work with a few exceptions. The range of variation in the ratio is 4.5%. Terrestrial organic carbon and carbonate rocks constitute two well defined groups, the carbonates being richer in C13 by some 2%. Marine organic carbon lies in a range intermediate between these groups. Atmospheric CO2 is richer in C13 than was formerly believed. Fossil wood, coal and limestones show no correlation of C13/C12 ratio with age. If petroleum is of marine organic origin a considerable change in isotopic composition has probably occurred. Such a change seems to have occurred in carbon from black shales and carbonaceous schists. Samples of graphites, diamonds, igneous rocks and gases from Yellowstone Park have been analyzed. The origin of graphite cannot be determined from C13/C12 ratios. The terrestrial distribution of carbon isotopes between igneous rocks and sediments is discussed with reference to the available meteoritic determinations. Isotopic fractionation between iron carbide and graphite in meteorites may indicate the mechanism by which early fractionation between deep seated and surface terrestrial carbon may have occurred.",1953,51,1733,94,3,10,2,4,8,2,9,7,9,8
222e7fe0bbe103490d4f522f199f774b06bbfc89,"Suspended sediments from large and middle size Chinese estuaries, including the Yalujiang, Shuangtaizihe, Luanhe, Jiaojiang and Zhujiang, were analysed to understand trace metal transport in the coastal zone. The determinations of 13 major and trace elements plus organic carbon were made of total concentrations and were fully validated by certified reference materials (CRMs). The combination of the data sets with other Chinese estuaries, such as Changjiang and Huanghe, provides an overview of particulate trace metal geochemistry in this region. Trace metal levels in Chinese rivers are relatively low compared with those draining industrialized regions of Europe and North America. In the estuaries, most particulate elements illustrate stable distribution in the mixing zone until a salinity of 30, especially when absolute concentrations are normalized to aluminium, although the total suspended matter (TSM) is quite different in time and space. Using Al as a reference, it was estimated that 25–40% for Cu, and 5–20% for Pb could remain in labile part in the Jiaojiang, Shuangtaizihe and Zhujiang, whereas different features of labile elements were found in the Changjiang and Luanhe. The mean enrichment factor (EFm) increases with higher sewage to river runoff ratio (S/R) over the drainage basin and EFm for suspended matter is higher than that for bottom sediments. Finally, inputs of particulate trace metals to the coast are estimated based on the riverine sediment load and chemical compositions.",2002,37,839,240,0,0,1,1,10,13,12,10,21,44
c1445d3bb3728189cc10750efc64c4f53b1978fb,"Submission information at the series homepage and springer.com/authors Order online at springer.com ▶ or for the Americas call (toll free) 1-800-SPRINGER ▶ or email us at: customerservice@springer.com. ▶ For outside the Americas call +49 (0) 6221-345-4301 ▶ or email us at: customerservice@springer.com. Handbook of Geochemistry Editor-in-chief: K.H. Wedepohl Series Editors: C.W. Correns, D.M. Shaw, K.K. Turekian, J. Zemann",1969,0,1338,17,0,4,3,14,6,6,8,8,10,10
987014db9f8a7826065a53ac34009e43d3291b50,"C.A.J. APPELO and D. POSTMA (ed.) A.A. Balkema Publishers, Leiden, The Netherlands. 2005. 2nd ed. Hardcover, 649 pp. $43.95. ISBN 0-41-536428-0.

This book presents important fundamental concepts and current knowledge of groundwater geochemistry and the interaction of water, minerals, gases,",2006,0,681,88,13,22,18,15,21,30,39,32,39,25
f61ddae58a261329045df077a5b19b822e9cc1bc,Preface. The CO2-Carbonic Acid System and Solution Chemistry. Interactions Between Carbonate Minerals and Solutions. Coprecipitation Reactions and Solid Solutions of Carbonate Minerals. The Oceanic Carbonate System and Calcium Carbonate Accumulation in Deep Sea Sediments. Composition and Source of Shoal-Water Carbonate Sediments. Early Marine Diagenesis of Shoal-Water Carbonate Sediments. Early Non-Marine Diagenesis of Sedimentary Carbonates. Carbonates as Sedimentary Rocks in Subsurface Processes. Current Carbon Cycle and Human Impact. Sedimentary Carbonates in the Evolution of Earth's Surface Environment. References. Index.,1990,0,1086,84,0,3,10,29,24,23,24,32,25,32
66ac28f0807c399b0c309bbe6fc30065ea6c167e,,2006,0,658,101,2,10,23,27,30,46,42,66,52,51
5b71c8f5408fed955ab454a871f6ad8af7a0d0eb,"Abstract We analyzed the redox sensitive elements V, Mo, U, Re and Cd in surface sediments from the Northwest African margin, the U.S. Northwest margin and the Arabian Sea to determine their response under a range of redox conditions. Where oxygen penetrates 1 cm or less into the sediments, Mo and V diffuse to the overlying water as Mn is reduced and remobilized. Authigenic enrichments of U, Re and Cd are evident under these redox conditions. With the onset of sulfate reduction, all of the metals accumulate authigenically with Re being by far the most enriched. General trends in authigenic metal accumulation are described by calculating authigenic fluxes for the 3 main redox regimes: oxic, reducing where oxygen penetrates ≤1 cm, and anoxic conditions. Using a simple diagenesis model and global estimates of organic carbon rain rate and bottom water oxygen concentrations, we calculate the area of sediments below 1000 m water depth in which oxygen penetration is ≤1 cm to be 4% of the ocean floor. We conclude that sediments where oxygen penetrates ≤1 cm release Mn, V and Mo to seawater at rates of 140%–260%, 60%–150% and 5%–10% of their respective riverine fluxes, using the authigenic metal concentrations and accumulation rates from this work and other literature. These sediments are sinks for Re, Cd and U, with burial fluxes of 70%–140%, 30%–80% and 20%–40%, respectively, of their dissolved riverine inputs. We modeled the sensitivity of the response of seawater Re, Cd and V concentrations to changes in the area of reducing sediments where oxygen penetrates ≤1 cm. Our analysis suggests a negligible change in seawater Re concentration, whereas seawater concentrations of Cd and V could have decreased and increased, respectively, by 5%–10% over 20 kyr if the area of reducing sediments increased by a factor of 2 and by 10%–20% if the area increased by a factor of 3. The concentration variations for a factor of 2 increase in the area of reducing sediments are at about the level of uncertainty of Cd/Ca and V/Ca ratios observed in foraminifera shells over the last 40 kyr. This implies that the area of reducing sediments in the ocean deeper than 1000 m (4%) has not been greater than twice the present value in the recent past.",1999,84,906,66,0,7,11,19,18,24,26,27,20,34
504b24a796b2ae23203912cc6d37af7eaf5681cc,"A comprehensive review of the single and sequential extraction schemes for metal fractionation in environmental samples such as soil and industrially contaminated soils, sewage sludge and sludge amended soils, road dust and run off, waste and miscellaneous materials along with other approaches of sequential extraction methods are being presented. A discussion on the application of chemometric methods in sequential extraction analysis is also being given. The study of single and sequential extraction methods for various reference materials are also being looked into. The review covers several aspects of the single and sequential extraction methodologies. The use of each reagents involved in these schemes are also discussed briefly. Finally the present upto date information by different workers in various fields of environmental geochemistry along with the possible future developments are also being outlined.",2008,286,416,28,3,13,17,26,32,39,41,34,39,39
c61aa5ed1fd4c1ebef2798cb7efa828054844a1b,"Concentrations of naturally occurring arsenic in ground water vary regionally due to a combination of climate and geology. Although slightly less than half of 30,000 arsenic analyses of ground water in the United States were 1 μg/L, about 10% exceeded 10 μg/L. At a broad regional scale, arsenic concentrations exceeding 10 μg/L appear to be more frequently observed in the western United States than in the eastern half. Arsenic concentrations in ground water of the Appalachian Highlands and the Atlantic Plain generally are very low ( 1 μg/L). Concentrations are somewhat greater in the Interior Plains and the Rocky Mountain System. Investigations of ground water in New England, Michigan, Minnesota, South Dakota, Oklahoma, and Wisconsin within the last decade suggest that arsenic concentrations exceeding 10 μg/L are more widespread and common than previously recognized. 
 
Arsenic release from iron oxide appears to be the most common cause of widespread arsenic concentrations exceeding 10 μg/L in ground water. This can occur in response to different geochemical conditions, including release of arsenic to ground water through reaction of iron oxide with either natural or anthropogenic (i.e., petroleum products) organic carbon. Iron oxide also can release arsenic to alkaline ground water, such as that found in some felsic volcanic rocks and alkaline aquifers of the western United States. Sulfide minerals are both a source and sink for arsenic. Geothermal water and high evaporation rates also are associated with arsenic concentrations 10g/L in ground and surface water, particularly in the west.",2000,138,866,55,1,5,23,24,28,45,47,40,61,48
d999add354aceccbe8b815949a77dfbee53a5f9b,"Clumped isotope geochemistry is concerned with the state of ordering of rare isotopes in natural materials. That is, it examines the extent to which rare isotopes (D, ^(13)C, ^(15)N, ^(18)O, etc.) bond with or near each other rather than with the sea of light isotopes in which they swim. Abundances of isotopic ‘clumps’ in natural materials are influenced by a wide variety of factors. In most cases, their concentrations approach (within ca. 1%, relative) the amount expected for a random distribution of isotopes. Deviations from this stochastic distribution result from: enhanced thermodynamic stability of heavy-isotope ‘clumps’; slower kinetics of reactions requiring the breakage of bonds between heavy isotopes; the mass dependence of diffusive and thermo-gravitational fractionations; mixing between components that differ from one another in bulk isotopic composition; biochemical and photochemical fractionations that may reflect combinations of these simpler physical mechanisms; and, in some cases, other processes we do not yet understand. Although clumped isotope geochemistry is a young field, several seemingly promising applications have already emerged. Most importantly, it appears that proportions of ^(13)C–^(18)O bonds in carbonate minerals are sensitive to their growth temperatures, independent of bulk isotopic composition. Thus, ‘clumped isotope’ analysis of ancient carbonates can be used as a quantitative paleothermometer that requires no assumptions about the δ^(18)O of waters from which carbonates grew. This approach has been used to reconstruct marine temperatures across the Phanerozoic (reaching back to the Silurian), terrestrial ground temperatures across the Cenozoic, thermal histories of aqueously altered meteorites, among other applications. Clumped isotope geochemistry is also placing new constraints on the atmospheric budget and stratospheric photochemistry of CO_2, and should be capable of placing analogous new constraints on the budgets of other atmospheric gases. Finally, this field could be extended to encompass sulfates, volatile hydrocarbons, organic moieties and other materials.",2007,75,515,41,0,2,9,13,27,25,34,56,40,53
9a761b1044b318847d626beacaafdf6579715a9c,"▪ Abstract The Re-Os isotope sytem, based on the long-lived β− transition of 187Re to 187Os, has matured to wide use in cosmochemistry and high-temperature geochemistry. The siderophilic/chalcophilic behavior of Re and Os is different from that of the elements that comprise most other long-lived radiogenic isotope systems. Magmatic iron meteorites (IIIAB, IIAB, IVA, and IVB) have Re-Os isochrons that indicate asteroidal core crystallization within the first 10–40 million years of Solar System evolution. Rocks from Earth's convecting mantle show generally chondritic Re/Os evolution throughout Earth history that is explained by the addition of highly siderophile elements to the mantle after core formation via late accretion. Oceanic basalts have Os-isotope systematics that improve the detailed geological interpretation of extant mantle components. Some portions of ancient subcontinental lithospheric mantle are severely depleted in Re and have correspondingly subchondritic 187Os/188Os, indicating long-term i...",1998,214,838,86,3,14,22,22,37,29,40,29,17,37
f666190e624adbd530e3635624eb193c94639865,"This book concerns the use of concepts from statistical physics in the description of financial systems. The authors illustrate the scaling concepts used in probability theory, critical phenomena, and fully developed turbulent fluids. These concepts are then applied to financial time series. The authors also present a stochastic model that displays several of the statistical properties observed in empirical data. Statistical physics concepts such as stochastic dynamics, short- and long-range correlations, self-similarity and scaling permit an understanding of the global behaviour of economic systems without first having to work out a detailed microscopic description of the system. Physicists will find the application of statistical physics concepts to economic systems interesting. Economists and workers in the financial world will find useful the presentation of empirical analysis methods and well-formulated theoretical tools that might help describe systems composed of a huge number of interacting subsystems.",1999,234,2885,140,18,53,88,82,78,118,115,123,149,139
42b70d8c3c9a282a43665d4a0a01d5f1b2e62eb1,"BackgroundGene-expression analysis is increasingly important in biological research, with real-time reverse transcription PCR (RT-PCR) becoming the method of choice for high-throughput and accurate expression profiling of selected genes. Given the increased sensitivity, reproducibility and large dynamic range of this methodology, the requirements for a proper internal control gene for normalization have become increasingly stringent. Although housekeeping gene expression has been reported to vary considerably, no systematic survey has properly determined the errors related to the common practice of using only one control gene, nor presented an adequate way of working around this problem.ResultsWe outline a robust and innovative strategy to identify the most stably expressed control genes in a given set of tissues, and to determine the minimum number of genes required to calculate a reliable normalization factor. We have evaluated ten housekeeping genes from different abundance and functional classes in various human tissues, and demonstrated that the conventional use of a single gene for normalization leads to relatively large errors in a significant proportion of samples tested. The geometric mean of multiple carefully selected housekeeping genes was validated as an accurate normalization factor by analyzing publicly available microarray data.ConclusionsThe normalization strategy presented here is a prerequisite for accurate RT-PCR expression profiling, which, among other things, opens up the possibility of studying the biological relevance of small expression differences.",2002,26,16756,1576,0,0,0,0,0,0,0,0,0,0
3537fcd0ff99a3b3cb3d279012df826358420556,"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.",2000,58,11837,1076,0,0,0,0,2,1,1,8,448,657
19bb0dce99466077e9bc5a2ad4941607fc28b40c,We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.,2006,61,3532,451,8,30,89,158,161,175,204,255,286,257
6afead7b04547783aa0d47e4fe95890adadfcc3d,"Preliminaries.- Examples of nonlinear parabolic equations in physical, biological and engineering problems.- Existence, uniqueness and continuous dependence.- Dynamical systems and liapunov stability.- Neighborhood of an equilibrium point.- Invariant manifolds near an equilibrium point.- Linear nonautonomous equations.- Neighborhood of a periodic solution.- The neighborhood of an invariant manifold.- Two examples.",1989,0,5242,449,56,55,62,76,63,80,76,107,107,132
11473d353c7efd41559f8fda8cbe4f194ac9dc50,Introduction Chapter 1 Grassmann algebra 1.1 Tensor products 1.2 Graded algebras 1.3 Teh exterior algebra of a vectorspace 1.4 Alternating forms and duality 1.5 Interior multiplications 1.6 Simple m-vectors 1.8 Mass and comass 1.9 The symmetric algebra of a vectorspace 1.10 Symmetric forms and polynomial functions Chapter 2 General measure theory 2.1 Measures and measurable sets 2.2 Borrel and Suslin sets 2.3 Measurable functions 2.4 Lebesgue integrations 2.5 Linear functionals 2.6 Product measures 2.7 Invariant measures 2.8 Covering theorems 2.9 Derivates 2.10 Caratheodory's construction Chapter 3 Rectifiability 3.1 Differentials and tangents 3.2 Area and coarea of Lipschitzian maps 3.3 Structure theory 3.4 Some properties of highly differentiable functions Chapter 4 Homological integration theory 4.1 Differential forms and currents 4.2 Deformations and compactness 4.3 Slicing 4.4 Homology groups 4.5 Normal currents of dimension n in R(-63) superscript n Chapter 5 Applications to the calculus of variations 5.1 Integrands and minimizing currents 5.2 Regularity of solutions of certain differential equations 5.3 Excess and smoothness 5.4 Further results on area minimizing currents Bibliography Glossary of some standard notations List of basic notations defined in the text Index,1969,0,4970,361,2,6,6,15,20,25,25,31,21,28
22e709ddb37dcec0fbf5650d0c5de2c018b0677c,,1990,0,3867,282,30,59,74,99,100,123,139,151,132,147
d14d99312cd81321de0984cd8fd831b4b9336440,"We may not be able to make you love reading, but matrix geometric solutions in stochastic models an algorithmic approach will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full.",1982,11,3109,358,13,13,13,17,24,21,22,27,36,40
1dcc52126295f93be869635504adcd680de05f3a,"Numerical methods that preserve properties of Hamiltonian systems, reversible systems, differential equations on manifolds and problems with highly oscillatory solutions are the subject of this book. A complete self-contained theory of symplectic and symmetric methods, which include Runge-Kutta, composition, splitting, multistep and various specially designed integrators, is presented and their construction and practical merits are discussed. The long-time behaviour of the numerical solutions is studied using a backward error analysis (modified equations) combined with KAM theory. The book is illustrated by many figures, it treats applications from physics and astronomy and contains many numerical experiments and comparisons of different approaches. The second edition is substantially revised and enlarged, with many improvements in the presentation and additions concerning in particular non-canonical Hamiltonian systems, highly oscillatory mechanical systems, and the dynamics of multistep methods.",2004,19,3053,318,54,64,88,99,114,147,165,150,192,201
cc311723597b2e2a0c1ef23b55708f8d36644d0b,"The first edition of ""Geometric Morphometrics for Biologists"" has been the primary resource for teaching modern geometric methods of shape analysis to biologists who have a stronger background in biology than in multivariate statistics and matrix algebra. These geometric methods are appealing to biologists who approach the study of shape from a variety of perspectives, from clinical to evolutionary, because they incorporate the geometry of organisms throughout the data analysis. The second edition of this book retains the emphasis on accessible explanations, and the copious illustrations and examples of the first, updating the treatment of both theory and practice. The second edition represents the current state-of-the-art and adds new examples and summarizes recent literature, as well as provides an overview of new software and step-by-step guidance through details of carrying out the analyses. This title contains updated coverage of methods, especially for sampling complex curves and 3D forms and a new chapter on applications of geometric morphometrics to forensics. It offers a reorganization of chapters to streamline learning basic concepts. It presents detailed instructions for conducting analyses with freely available, easy to use software. It provides numerous illustrations, including graphical presentations of important theoretical concepts and demonstrations of alternative approaches to presenting results.",2004,19,2562,407,0,19,36,69,76,103,138,150,134,209
dc2025629efdb6e7dd1e55d93a9f60956dabe5e1,"Human and bovine capillary endothelial cells were switched from growth to apoptosis by using micropatterned substrates that contained extracellular matrix-coated adhesive islands of decreasing size to progressively restrict cell extension. Cell spreading also was varied while maintaining the total cell-matrix contact area constant by changing the spacing between multiple focal adhesion-sized islands. Cell shape was found to govern whether individual cells grow or die, regardless of the type of matrix protein or antibody to integrin used to mediate adhesion. Local geometric control of cell growth and viability may therefore represent a fundamental mechanism for developmental regulation within the tissue microenvironment.",1997,75,4339,82,14,76,127,92,83,137,113,137,155,188
5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",1991,286,4077,169,6,22,34,46,58,63,69,66,72,59
6962af2ce217d906d7e907d7dab7369820befc17,"We present a monotonic expression for the Ricci flow, valid in all dimensions and without curvature assumptions. It is interpreted as an entropy for a certain canonical ensemble. Several geometric applications are given. In particular, (1) Ricci flow, considered on the space of riemannian metrics modulo diffeomorphism and scaling, has no nontrivial periodic orbits (that is, other than fixed points); (2) In a region, where singularity is forming in finite time, the injectivity radius is controlled by the curvature; (3) Ricci flow can not quickly turn an almost euclidean region into a very curved one, no matter what happens far away. We also verify several assertions related to Richard Hamilton's program for the proof of Thurston geometrization conjecture for closed three-manifolds, and give a sketch of an eclectic proof of this conjecture, making use of earlier results on collapsing with local lower curvature bound.",2002,51,2481,196,8,27,49,53,93,153,170,149,147,163
ea2251357fa4d3f44c5623ab8b80c18dfe05e712,"0. Mathematical Preliminaries.- 0.1 Linear Algebra and Linear Programming.- Basic Notation.- Hulls, Independence, Dimension.- Eigenvalues, Positive Definite Matrices.- Vector Norms, Balls.- Matrix Norms.- Some Inequalities.- Polyhedra, Inequality Systems.- Linear (Diophantine) Equations and Inequalities.- Linear Programming and Duality.- 0.2 Graph Theory.- Graphs.- Digraphs.- Walks, Paths, Circuits, Trees.- 1. Complexity, Oracles, and Numerical Computation.- 1.1 Complexity Theory: P and NP.- Problems.- Algorithms and Turing Machines.- Encoding.- Time and Space Complexity.- Decision Problems: The Classes P and NP.- 1.2 Oracles.- The Running Time of Oracle Algorithms.- Transformation and Reduction.- NP-Completeness and Related Notion.- 1.3 Approximation and Computation of Numbers.- Encoding Length of Numbers.- Polynomial and Strongly Polynomial Computations.- Polynomial Time Approximation of Real Numbers.- 1.4 Pivoting and Related Procedures.- Gaussian Elimination.- Gram-Schmidt Orthogonalization.- The Simplex Method.- Computation of the Hermite Normal Form.- 2. Algorithmic Aspects of Convex Sets: Formulation of the Problems.- 2.1 Basic Algorithmic Problems for Convex Sets.- 2.2 Nondeterministic Decision Problems for Convex Sets.- 3. The Ellipsoid Method.- 3.1 Geometric Background and an Informal Description.- Properties of Ellipsoids.- Description of the Basic Ellipsoid Method.- Proofs of Some Lemmas.- Implementation Problems and Polynomiality.- Some Examples.- 3.2 The Central-Cut Ellipsoid Method.- 3.3 The Shallow-Cut Ellipsoid Method.- 4. Algorithms for Convex Bodies.- 4.1 Summary of Results.- 4.2 Optimization from Separation.- 4.3 Optimization from Membership.- 4.4 Equivalence of the Basic Problems.- 4.5 Some Negative Results.- 4.6 Further Algorithmic Problems for Convex Bodies.- 4.7 Operations on Convex Bodies.- The Sum.- The Convex Hull of the Union.- The Intersection.- Polars, Blockers, Antiblockers.- 5. Diophantine Approximation and Basis Reduction.- 5.1 Continued Fractions.- 5.2 Simultaneous Diophantine Approximation: Formulation of the Problems.- 5.3 Basis Reduction in Lattices.- 5.4 More on Lattice Algorithms.- 6. Rational Polyhedra.- 6.1 Optimization over Polyhedra: A Preview.- 6.2 Complexity of Rational Polyhedra.- 6.3 Weak and Strong Problems.- 6.4 Equivalence of Strong Optimization and Separation.- 6.5 Further Problems for Polyhedra.- 6.6 Strongly Polynomial Algorithms.- 6.7 Integer Programming in Bounded Dimension.- 7. Combinatorial Optimization: Some Basic Examples.- 7.1 Flows and Cuts.- 7.2 Arborescences.- 7.3 Matching.- 7.4 Edge Coloring.- 7.5 Matroids.- 7.6 Subset Sums.- 7.7 Concluding Remarks.- 8. Combinatorial Optimization: A Tour d'Horizon.- 8.1 Blocking Hypergraphs and Polyhedra.- 8.2 Problems on Bipartite Graphs.- 8.3 Flows, Paths, Chains, and Cuts.- 8.4 Trees, Branchings, and Rooted and Directed Cuts.- Arborescences and Rooted Cuts.- Trees and Cuts in Undirected Graphs.- Dicuts and Dijoins.- 8.5 Matchings, Odd Cuts, and Generalizations.- Matching.- b-Matching.- T-Joins and T-Cuts.- Chinese Postmen and Traveling Salesmen.- 8.6 Multicommodity Flows.- 9. Stable Sets in Graphs.- 9.1 Odd Circuit Constraints and t-Perfect Graphs.- 9.2 Clique Constraints and Perfect Graphs.- Antiblockers of Hypergraphs.- 9.3 Orthonormal Representations.- 9.4 Coloring Perfect Graphs.- 9.5 More Algorithmic Results on Stable Sets.- 10. Submodular Functions.- 10.1 Submodular Functions and Polymatroids.- 10.2 Algorithms for Polymatroids and Submodular Functions.- Packing Bases of a Matroid.- 10.3 Submodular Functions on Lattice, Intersecting, and Crossing Families.- 10.4 Odd Submodular Function Minimization and Extensions.- References.- Notation Index.- Author Index.",1981,0,2740,218,0,0,0,0,0,0,3,4,11,20
09a35fbc5d0a002102a00dad3cf16b67f7d6d694,"This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.",2008,18,1813,301,3,54,98,106,112,161,204,225,216,182
eca5b9447cae81ec01582b5834a19308d7b99def,Projective geometry modelling and calibrating cameras edge detection representing geometric primitives and their uncertainty stereo vision determining discrete motion from points and lines tracking tokens over time motion fields of curves interpolating and approximating three-dimensional data recognizing and locating objects and places answers to problems. Appendices: constrained optimization some results from algebraic geometry differential geometry.,1993,0,2561,145,4,23,47,58,91,120,125,153,159,165
9b86d8af6c5cce934044e482c89f786861392bd4,"A technique is presented for deforming solid geometric models in a free-form manner. The technique can be used with any solid modeling system, such as CSG or B-rep. It can deform surface primitives of any type or degree: planes, quadrics, parametric surface patches, or implicitly defined surfaces, for example. The deformation can be applied either globally or locally. Local deformations can be imposed with any desired degree of derivative continuity. It is also possible to deform a solid model in such a way that its volume is preserved.The scheme is based on trivariate Bernstein polynomials, and provides the designer with an intuitive appreciation for its effects.",1986,38,2813,168,3,7,6,10,16,21,30,24,32,43
af955cbdf752d427fd56a15f51f8e972ab409e3d,"The weighted geometric (WG) operator and the ordered weighted geometric (OWG) operator are two common aggregation operators in the field of information fusion. But these two aggregation operators are usually used in situations where the given arguments are expressed as crisp numbers or linguistic values. In this paper, we develop some new geometric aggregation operators, such as the intuitionistic fuzzy weighted geometric (IFWG) operator, the intuitionistic fuzzy ordered weighted geometric (IFOWG) operator, and the intuitionistic fuzzy hybrid geometric (IFHG) operator, which extend the WG and OWG operators to accommodate the environment in which the given arguments are intuitionistic fuzzy sets which are characterized by a membership function and a non-membership function. Some numerical examples are given to illustrate the developed operators. Finally, we give an application of the IFHG operator to multiple attribute decision making based on intuitionistic fuzzy sets.",2006,42,1676,143,0,16,16,26,35,54,71,85,130,105
a5c880ed3f3f2eafd87d44139f20003b813d43af,"The subject of this workshop was numerical methods that preserve geometric properties of the flow of an ordinary or partial differential 
equation. This was complemented by the question as to how structure preservation affects the long-time behaviour of numerical methods.",2006,83,1610,170,84,88,89,105,108,96,115,109,105,105
61adbe77d4c144eee1219c9271a976236130c241,Geometric Fourier analysis on spaces of constant curvature Integral geometry and Radon transforms Invariant differential operators Invariants and harmonic polynomials Spherical functions and spherical transforms Analysis on compact symmetric spaces Appendix Some details Bibliography Symbols frequently used Index Errata.,1984,0,2054,157,2,1,12,24,16,18,37,35,31,44
6b62d40cdeafb44cbdd730fbb7dde3e717b3f9b7,"Abstract The analysis of shape is a fundamental part of much biological research. As the field of statistics developed, so have the sophistication of the analysis of these types of data. This lead to multivariate morphometrics in which suites of measurements were analyzed together using canonical variates analysis, principal components analysis, and related methods. In the 1980s, a fundamental change began in the nature of the data gathered and analyzed. This change focused on the coordinates of landmarks and the geometric information about their relative positions. As a by‐product of such an approach, results of multivariate analyses could be visualized as configurations of landmarks back in the original space of the organism rather than only as statistical scatter plots. This new approach, called “geometric morphometrics”;, had benefits that lead Rohlf and Marcus (1993) to proclaim a “revolution”; in morphometrics. In this paper, we briefly update the discussion in that paper and summarize the advances in the ten years since the paper by Rohlf and Marcus. We also speculate on future directions in morphometric analysis.",2004,146,1801,113,12,23,45,69,60,98,95,133,106,135
8cffe6a48b2c971c33c2aa373e174e79349c784b,,1981,0,1988,252,2,2,4,16,16,19,9,19,36,30
61c78cec64f568b0b01ea1397540dc32f8f38112,"SummaryWe propose a new model for active contours based on a geometric partial differential equation. Our model is intrinsec, stable (satisfies the maximum principle) and permits a rigorous mathematical analysis. It enables us to extract smooth shapes (we cannot retrieve angles) and it can be adapted to find several contours simultaneously. Moreover, as a consequence of the stability, we can design robust algorithms which can be engineed with no parameters in applications. Numerical experiments are presented.",1993,31,2017,73,1,4,25,29,30,24,26,38,51,49
adbc278135d897ad2d64394b8930de84bd8d3319,"* Established textbook
 * Continues to lead its readers to some of the hottest topics of contemporary mathematical research

This established reference work continues to lead its readers to some of the hottest topics of contemporary mathematical research.This new edition introduces and explains the ideas of the parabolic methods that have recently found such a spectacular success in the work of Perelman at the examples of closed geodesics and harmonic forms. It also discusses further examples of geometric variational problems from quantum field theory, another source of profound new ideas and methods in geometry.


From the reviews:
""This book provides a very readable introduction to Riemannian geometry and geometric analysis. The author focuses on using analytic methods in the study of some fundamental theorems in Riemannian geometry, e.g., the Hodge theorem, the Rauch comparison theorem, the Lyusternik and Fet theorem and the existence of harmonic mappings. With the vast development of the mathematical subject of geometric analysis, the present textbook is most welcome. [..] The book is made more interesting by the perspectives in various sections."" Mathematical Reviews",1995,0,1707,117,0,4,1,4,5,12,16,25,33,36
b6e1d4f6bd21c664e15ae91276e5c875a062931e,"0 Mathematical Preliminaries.- 0.1 Notation.- 0.2 Linear Spaces.- 0.3 Subspaces.- 0.4 Maps and Matrices.- 0.5 Factor Spaces.- 0.6 Commutative Diagrams.- 0.7 Invariant Subspaces. Induced Maps.- 0.8 Characteristic Polynomial. Spectrum.- 0.9 Polynomial Rings.- 0.10 Rational Canonical Structure.- 0.11 Jordan Decomposition.- 0.12 Dual Spaces.- 0.13 Tensor Product. The Sylvester Map.- 0.14 Inner Product Spaces.- 0.15 Hermitian and Symmetric Maps.- 0.16 Well-Posedness and Genericity.- 0.17 Linear Systems.- 0.18 Transfer Matrices. Signal Flow Graphs.- 0.19 Rouche's Theorem.- 0.20 Exercises.- 0.21 Notes and References.- 1 Introduction to Controllability.- 1.1 Reachability.- 1.2 Controllability.- 1.3 Single-Input Systems.- 1.4 Multi-Input Systems.- 1.5 Controllability is Generic.- 1.6 Exercises.- 1.7 Notes and References.- 2 Controllability, Feedback and Pole Assignment.- 2.1 Controllability and Feedback.- 2.2 Pole Assignment.- 2.3 Incomplete Controllability and Pole Shifting.- 2.4 Stabilizability.- 2.5 Exercises.- 2.6 Notes and References.- 3 Observability and Dynamic Observers.- 3.1 Observability.- 3.2 Unobservable Subspace.- 3.3 Full Order Dynamic Observer.- 3.4 Minimal Order Dynamic Observer.- 3.5 Observers and Pole Shifting.- 3.6 Detectability.- 3.7 Detectors and Pole Shifting.- 3.8 Pole Shifting by Dynamic Compensation.- 3.9 Observer for a Single Linear Functional.- 3.10 Preservation of Observability and Detectability.- 3.11 Exercises.- 3.12 Notes and References.- 4 Disturbance Decoupling and Output Stabilization.- 4.1 Disturbance Decoupling Problem (DDP).- 4.2 (A, B)-Invariant Subspaces.- 4.3 Solution of DDP.- 4.4 Output Stabilization Problem (OSP).- 4.5 Exercises.- 4.6 Notes and References.- 5 Controllability Subspaces.- 5.1 Controllability Subspaces.- 5.2 Spectral Assignability.- 5.3 Controllability Subspace Algorithm.- 5.4 Supremal Controllability Subspace.- 5.5 Transmission Zeros.- 5.6 Disturbance Decoupling with Stability.- 5.7 Controllability Indices.- 5.8 Exercises.- 5.9 Notes and References.- 6 Tracking and Regulation I: Output Regulation.- 6.1 Restricted Regulator Problem (RRP).- 6.2 Solvability of RRP.- 6.3 Example 1 : Solution of RRP.- 6.4 Extended Regulator Problem (ERP).- 6.5 Example 2: Solution of ERP.- 6.6 Concluding Remarks.- 6.7 Exercises.- 6.8 Notes and References.- 7 Tracking and Regulation II: Output Regulation with Internal Stability.- 7.1 Solvability of RPIS: General Considerations.- 7.2 Constructive Solution of RPIS: N= 0.- 7.3 Constructive Solution of RPIS: N Arbitrary.- 7.4 Application: Regulation Against Step Disturbances.- 7.5 Application: Static Decoupling.- 7.6 Example 1 : RPIS Unsolvable.- 7.7 Example 2: Servo-Regulator.- 7.8 Exercises.- 7.9 Notes and References.- 8 Tracking and Regulation III: Structurally Stable Synthesis.- 8.1 Preliminaries.- 8.2 Example 1: Structural Stability.- 8.3 Well-Posedness and Genericity.- 8.4 Well-Posedness and Transmission Zeros.- 8.5 Example 2: RPIS Solvable but Ill-Posed.- 8.6 Structurally Stable Synthesis.- 8.7 Example 3: Well-Posed RPIS: Strong Synthesis.- 8.8 The Internal Model Principle.- 8.9 Exercises.- 8.10 Notes and References.- 9 Noninteraeting Control I: Basic Principles.- 9.1 Decoupling: Systems Formulation.- 9.2 Restricted Decoupling Problem (RDP).- 9.3 Solution of RDP: Outputs Complete.- 9.4 Extended Decoupling Problem (EDP).- 9.5 Solution of EDP.- 9.6 Naive Extension.- 9.7 Example.- 9.8 Partial Decoupling.- 9.9 Exercises.- 9.10 Notes and References.- 10 Noninteraeting Control II: Efficient Compensation.- 10.1 The Radical.- 10.2 Efficient Extension.- 10.3 Efficient Decoupling.- 10.4 Minimal Order Compensation: d(?) = 2.- 10.5 Minimal Order Compensation: d(?) = k.- 10.6 Exercises.- 10.7 Notes and References.- 11 Noninteraeting Control III: Generic Solvability.- 11.1 Generic Solvability of EDP.- 11.2 State Space Extension Bounds.- 11.3 Significance of Generic Solvability.- 11.4 Exercises.- 11.5 Notes and References.- 12 Quadratic Optimization I: Existence and Uniqueness.- 12.1 Quadratic Optimization.- 12.2 Dynamic Programming: Heuristics.- 12.3 Dynamic Programming: Formal Treatment.- 12.4 Matrix Quadratic Equation.- 12.5 Exercises.- 12.6 Notes and References.- 13 Quadratic Optimization II: Dynamic Response.- 13.1 Dynamic Response: Generalities.- 13.2 Example 1 : First-Order System.- 13.3 Example 2: Second-Order System.- 13.4 Hamiltoman Matrix.- 13.5 Asymptotic Root Locus: Single Input System.- 13.6 Asymptotic Root Locus: Multivariable System.- 13.7 Upper and Lower Bounds on P0.- 13.8 Stability Margin. Gain Margin.- 13.9 Return Difference Relations.- 13.10 Applicability of Quadratic Optimization.- 13.11 Exercises.- 13.12 Notes and References.- References.- Relational and Operational Symbols.- Letter Symbols.- Synthesis Problems.",1974,0,2227,124,0,2,5,3,8,18,21,26,36,30
a3269505b943d3549a82c4eef23d9e29cea3be11,"We describe a geometric-flow-based algorithm for computing a dense oversegmentation of an image, often referred to as superpixels. It produces segments that, on one hand, respect local image boundaries, while, on the other hand, limiting undersegmentation through a compactness constraint. It is very fast, with complexity that is approximately linear in image size, and can be applied to megapixel sized images with high superpixel densities in a matter of minutes. We show qualitative demonstrations of high-quality results on several complex images. The Berkeley database is used to quantitatively compare its performance to a number of oversegmentation algorithms, showing that it yields less undersegmentation than algorithms that lack a compactness constraint while offering a significant speedup over N-cuts, which does enforce compactness.",2009,31,1059,119,2,18,32,73,104,113,115,123,126,125
ea21791255647a9c758b9ec2ba8bf1f2269f8864,"1. Introduction 2. Probabilistic ingredients 3. Subgraph and component counts 4. Typical vertex degrees 5. Geometrical ingredients 6. Maximum degree, cliques and colourings 7. Minimum degree: laws of large numbers 8. Minimum degree: convergence in distribution 9. Percolative ingredients 10. Percolation and the largest component 11. The largest component for a binomial process 12. Ordering and partitioning problems 13. Connectivity and the number of components References Index",2003,11,1431,185,5,21,29,53,74,77,99,99,93,113
13f1db137bbce90efaf0561e184d87d28d98cb2c,"Geometric measure theory is an area of analysis concerned with solving geometric problems via measure theoretic techniques. The canonical motivating physical problem is probably that investigated experimentally by Plateau in the nineteenth century [3]: given a boundary wire, how does one find the (minimal) soap film which spans it? Slightly more mathematically, given a boundary curve, find the surface of minimal area spanning it. The many different approaches to solving this problem have found utility in most areas of modern mathematics and geometric measure theory is no exception: techniques and ideas from geometric measure theory have been found useful in the study of partial differential equations, the calculus of variations, harmonic analysis, and fractals. Successes in the field include: classifying the structure of singularities in soap films (see [18], together with the fine descriptive article [4]); showing that the standard ‘double bubble’ is the optimal shape for enclosing two prescribed volumes in space [13], and developing powerful computer software for modelling the evolution of surfaces under the action of physical forces [7]. The main reference text for the subject is still Federer’s comprehensive book [11]. It is very densely written and Morgan’s book [15] serves as a useful guide through it. Federer’s colloquium lectures [10] provide a comprehensive overview of the subject and contain a summary of the main results in his book [11]. More recent books include Simon [17], which contains an introduction to the theory of varifolds and Allard’s regularity theorem, and Mattila’s book [14] which includes information about tangent measures and their uses. Both of these books are also suitable as introductions to the area. For a slightly different slant, the book by Evans and Gariepy [9], discusses applications of some of the ideas of geometric measure theory in the theory of Sobolev Spaces and functions of bounded variation. Many variational problems are solved by enlarging the allowed class of solutions, showing that in this enlarged class a solution exists, and then showing that the solution possesses more regularity than an arbitrary element of the enlarged class. Much of the work in geometric measure theory has been directed towards placing this informal description on a formal footing appropriate for the study of surfaces. The key concept underlying the whole theory is that of rectifiability: this is a measure theoretic notion of smoothness. A set E in Euclidean n-space, R, is",2002,24,1513,89,50,53,60,58,73,82,74,79,64,61
845c046ab90030c54c7f1a9f6977d0ffd987a33b,,1979,17,1834,195,0,2,5,4,3,3,12,8,11,6
3ae65e1ea511643c6f6cec629ab56e73fd19ca31,"“Geometric Invariant Theory” by Mumford/Fogarty (the first edition was published in 1965, a second, enlarged edition appeared in 1982) is the standard reference on applications of invariant theory to the construction of moduli spaces. This third, revised edition has been long awaited for by the mathematical community. It is now appearing in a completely updated and enlarged version with an additional chapter on the moment map by Prof. Frances Kirwan (Oxford) and a fully updated bibliography of work in this area. The book deals firstly with actions of algebraic groups on algebraic varieties, separating orbits by invariants and construction quotient spaces; and secondly with applications of this theory to the construction of moduli spaces. It is a systematic exposition of the geometric aspects of the classical theory of polynomial invariants.",1965,0,1940,119,0,3,5,4,5,6,7,8,6,11
01b24de15cf337c55b9866c4b534596ca3d93abe,"We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis.",2005,26,1318,53,10,29,47,41,58,59,70,68,85,81
0d2f20815b1f03ad4d0295ee0ada04f2ea4075b0,"We analyze graphs in which each vertex is assigned random coordinates in a geometric space of arbitrary dimensionality and only edges between adjacent points are present. The critical connectivity is found numerically by examining the size of the largest cluster. We derive an analytical expression for the cluster coefficient, which shows that the graphs are distinctly different from standard random graphs, even for infinite dimensionality. Insights relevant for graph bipartitioning are included.",2002,62,1182,138,1,5,22,31,35,46,57,68,58,57
97cbd9b8937cd0471bf2173598454282b4e1384f,"Part I. Integral Geometry in the Plane: 1. Convex sets in the plane 2. Sets of points and Poisson processes in the plane 3. Sets of lines in the plane 4. Pairs of points and pairs of lines 5. Sets of strips in the plane 6. The group of motions in the plane: kinematic density 7. Fundamental formulas of Poincare and Blaschke 8. Lattices of figures Part II. General Integral Geometry: 9. Differential forms and Lie groups 10. Density and measure in homogenous spaces 11. The affine groups 12. The group of motions in En Part III. Integral Geometry in En: 13. Convex sets in En 14. Linear subspaces, convex sets and compact manifolds 15. The kinematic density in En 16. Geometric and statistical applications: stereology Part IV. Integral Geometry in Spaces of Constant Curvature: 17. Noneuclidean integral geometry 18. Crofton's formulas and the kinematic fundamental formula in noneuclidean spaces 19. Integral geometry and foliated spaces: trends in integral geometry.",1976,0,1627,119,0,0,7,9,10,13,11,11,16,16
8103ce437744cf415755b0f74f39b9e6c89debcd,"A classic reference and text, this book introduces the foundations used to create an accurate computer screen image using mathematical tools. This comprehensive guide is a handbook for students and practitioners and includes an extensive bibliography for further study.",1996,0,1386,91,33,47,56,60,57,65,71,67,86,83
c5923dcf52c651642fe9017153c37766164ad149,"Geometric morphometrics is the statistical analysis of form based on Cartesian landmark coordinates. After separating shape from overall size, position, and orientation of the landmark configurations, the resulting Procrustes shape coordinates can be used for statistical analysis. Kendall shape space, the mathematical space induced by the shape coordinates, is a metric space that can be approximated locally by a Euclidean tangent space. Thus, notions of distance (similarity) between shapes or of the length and direction of developmental and evolutionary trajectories can be meaningfully assessed in this space. Results of statistical techniques that preserve these convenient properties—such as principal component analysis, multivariate regression, or partial least squares analysis—can be visualized as actual shapes or shape deformations. The Procrustes distance between a shape and its relabeled reflection is a measure of bilateral asymmetry. Shape space can be extended to form space by augmenting the shape coordinates with the natural logarithm of Centroid Size, a measure of size in geometric morphometrics that is uncorrelated with shape for small isotropic landmark variation. The thin-plate spline interpolation function is the standard tool to compute deformation grids and 3D visualizations. It is also central to the estimation of missing landmarks and to the semilandmark algorithm, which permits to include outlines and surfaces in geometric morphometric analysis. The powerful visualization tools of geometric morphometrics and the typically large amount of shape variables give rise to a specific exploratory style of analysis, allowing the identification and quantification of previously unknown shape features.",2009,114,891,37,3,18,45,43,54,73,95,88,91,105
775e556645231e583c495920a82e237f20477f26,"Combinatorial and geometric computing is a core area of computer science (CS). In fact, most CS curricula contain a course in data structures and algorithms. The area deals with objects such as graphs, sequences, dictionaries, trees, shortest paths, flows, matchings, points, segments, lines, convex hulls, and Voronoi diagrams and forms the basis for application areas such as discrete optimization, scheduling, traffic control, CAD, and graphics. There is no standard library of the data structures and algorithms of combinatorial and geometric computing. This is in sharp contrast to many other areas of computing. There are, for example, packages in statistics (SPSS), numerical analysis (LINPACK, EISPACK), symbolic computation (MAPLE, MATHEMATICA), and linear programming (CPLEX).",1995,36,1366,108,12,33,53,52,66,96,99,76,100,94
dc08847b65953ef2ae3542e47b08b57a46b5ba34,"High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.",2009,34,697,94,2,37,45,50,76,92,73,97,62,59
1499f206b576914668ce4d01829f401ef57e4cf8,"Abstract
A geometric program (GP) is a type of mathematical optimization problem characterized by objective and constraint functions that have a special form. Recently developed solution methods can solve even large-scale GPs extremely efficiently and reliably; at the same time a number of practical problems, particularly in circuit design, have been found to be equivalent to (or well approximated by) GPs. Putting these two together, we get effective solutions for the practical problems. The basic approach in GP modeling is to attempt to express a practical problem, such as an engineering analysis or design problem, in GP format. In the best case, this formulation is exact; when this is not possible, we settle for an approximate formulation. This tutorial paper collects together in one place the basic background material needed to do GP modeling. We start with the basic definitions and facts, and some methods used to transform problems into GP format. We show how to recognize functions and problems compatible with GP, and how to approximate functions or data in a form compatible with GP (when this is possible). We give some simple and representative examples, and also describe some common extensions of GP, along with methods for solving (or approximately solving) them.
",2007,224,1040,144,62,40,54,49,71,69,64,74,79,79
38347685f4e4e351defe61cea2d424207843733e,Abstract We describe a method to explore geometrically feasible alignments of ligands and receptors of known structure. Algorithms are presented that examine many binding geometries and evaluate them in terms of steric overlap. The procedure uses specific molecular conformations. A method is included for finding putative binding sites on a macromolecular surface. Results are reported for two systems: the heme-myoglobin interaction and the binding of thyroid hormone analogs to prealbumin. In each case the program finds structures within 1 A of the X-ray results and also finds distinctly different geometries that provide good steric fits. The approach seems well-suited for generating starting conformations for energy refinement programs and interactive computer graphics routines.,1982,22,1854,68,1,2,2,1,6,8,0,6,15,11
ee0871e2669e3f27735159ce9293e06668d39275,,1984,0,1499,174,1,0,4,8,3,15,14,11,18,20
bc91c1d9d66eaa3d2e7a28979f72d863ca4c7757,"The spectrum of left ventricular geometric adaptation to hypertension was investigated in 165 patients with untreated essential hypertension and 125 age- and gender-matched normal adults studied by two-dimensional and M-mode echocardiography. Among hypertensive patients, left ventricular mass index and relative wall thickness were normal in 52%, whereas 13% had increased relative wall thickness with normal ventricular mass (""concentric remodeling""), 27% had increased mass with normal relative wall thickness (eccentric hypertrophy) and only 8% had ""typical"" hypertensive concentric hypertrophy (increase in both variables). Systemic hemodynamics paralleled ventricular geometry, with the highest peripheral resistance in the groups with concentric remodeling and hypertrophy, whereas cardiac index was super-normal in those with eccentric hypertrophy and low normal in patients with concentric remodeling. The left ventricular short-axis/long-axis ratio was positively related to stroke volume (r = 0.45, p less than 0.001), with cavity shape most elliptic in patients with concentric remodeling and most spheric in those with eccentric hypertrophy. Normality of left ventricular mass in concentric remodeling appeared to reflect offsetting by volume ""underload"" of the effects of pressure overload, whereas eccentric hypertrophy was associated with concomitant pressure and volume overload. Thus, arterial hypertension is associated with a spectrum of cardiac geometric adaptation matched to systemic hemodynamics and ventricular load. Concentric left ventricular remodeling and eccentric hypertrophy are more common than the typical pattern of concentric hypertrophy in untreated hypertensive patients.",1992,72,1465,63,2,3,15,21,28,31,34,43,40,64
3d3429f9a76c4941d7cf57382a61b58d51ef2623,"Geometrical methods have had a profound impact in the development of modern nonlinear control theory. Fundamental results such as the orbit theorem, feedback linearization, disturbance decoupling or the various controllability tests for nonlinear systems are all deeply rooted on a geometric view of control theory. It is perhaps surprising, and possibly debatable, that in order to understand and appreciate the “essence” of linear control systems one has to delve into the intricacies of Lie brackets and Lie algebras. This is because only the geometric perspective offers the tools to study the properties of control systems that are invariant under (nonlinear) changes of coordinates and can therefore be considered intrinsic. Consider, for example, an inverted pendulum or the ball and beam system. It should be apparent that reachability or optimality properties for these systems do not depend on the particular reference frame chosen to write their equations of motion. These are intrinsic properties of these physical systems and thus require geometric techniques for its study. “Control Theory from the Geometric Viewpoint” is a recent addition to the geometric control theory monograph/textbook literature having Jurdjevic (1997) as its closest neighbor and Nijmeijer and van der Schaft (1995), Isidori (1996) and Bloch (2003) as more distant ones. The book evolved from lecture notes for graduate courses taught by the first author at the International School for Advanced Studies in Trieste, Italy. The lecture notes style can be felt throughout the 24 chapters of the book treating a large number of topics ranging from controllability and reachability analysis to higher order conditions for optimality. This lecture notes style, patent on the relatively large number of treated topics in 400 pages, is the book’s main handicap and merit. If, on the one hand, most chapters can be independently read thus allowing the reader to immediately dive into the topic of choice and quickly reach the zenith result, on the other hand, there is a certain lack of fluidity when one tries to read the book chapters consecutively. In the remaining lines I will try to articulate my own opinion, naturally conditioned by my taste and background, on the choice of topics and presentation as I go through some of the individual chapters.",2004,10,1145,131,9,20,40,43,40,67,56,70,82,113
fb6129b9dcb5f250608e104d8495956c11d3ce87,"A method is described for the correction of geometric distortions occurring in echo planar images. The geometric distortions are caused in large part by static magnetic field inho‐mogeneities, leading to pixel shifts, particularly in the phase encode direction. By characterizing the field inhomogeneities from a field map, the image can be unwarped so that accurate alignment to conventionally collected images can be made. The algorithm to perform the unwarping is described, and results from echo planar images collected at 1.5 and 4 Tesla are shown.",1995,12,1342,93,0,4,15,18,21,22,19,27,29,32
266293b11d5b08c6febd991da7eddd879e5beefc,"We report the existence of an entorhinal cell type that fires when an animal is close to the borders of the proximal environment. The orientation-specific edge-apposing activity of these “border cells” is maintained when the environment is stretched and during testing in enclosures of different size and shape in different rooms. Border cells are relatively sparse, making up less than 10% of the local cell population, but can be found in all layers of the medial entorhinal cortex as well as the adjacent parasubiculum, often intermingled with head-direction cells and grid cells. Border cells may be instrumental in planning trajectories and anchoring grid fields and place fields to a geometric reference frame.",2008,67,886,75,5,17,29,46,62,55,89,84,74,84
27330398f082cb3e3828f722b3d9470cd6e93e27,"For the first time, we propose the Weibull-geometric (WG) distribution which generalizes the extended exponential-geometric (EG) distribution introduced by Adamidis et al. [K. Adamidis, T. Dimitrakopoulou, and S. Loukas, On a generalization of the exponential-geometric distribution, Statist. Probab. Lett. 73 (2005), pp. 259–269], the exponential-geometric distribution discussed by Adamidis and Loukas [K. Adamidis and S. Loukas, A lifetime distribution with decreasing failure rate, Statist. Probab. Lett. 39 (1998), pp. 35–42] and the Weibull distribution. We derive many of its standard properties. The hazard function of the EG distribution is monotone decreasing, but the hazard function of the WG distribution can take more general forms. Unlike the Weibull distribution, the new distribution is useful for modelling unimodal failure rates. We derive the cumulative distribution and hazard functions, moments, density of order statistics and their moments. We provide expressions for the Rényi and Shannon entropies. The maximum likelihood estimation procedure is discussed and an EM algorithm [A.P. Dempster, N.M. Laird, and D.B. Rubim, Maximum likelihood from incomplete data via the EM algorithm (with discussion), J. R. Stat. Soc. B 39 (1977), pp. 1–38; G.J. McLachlan and T. Krishnan, The EM Algorithm and Extension, Wiley, New York, 1997] is given for estimating the parameters. We obtain the observed information matrix and discuss inference issues. The flexibility and potentiality of the new distribution is illustrated by means of a real data set.",2008,18,187,10,0,1,2,6,20,20,18,21,25,24
d9a861caf1fb41cac710333618a87af687f7a724,"The geometric Langlands program can be described in a natural way by compactifying on a Riemann surface C a twisted version of N=4 super Yang-Mills theory in four dimensions. The key ingredients are electric-magnetic duality of gauge theory, mirror symmetry of sigma-models, branes, Wilson and 't Hooft operators, and topological field theory. Seemingly esoteric notions of the geometric Langlands program, such as Hecke eigensheaves and D-modules, arise naturally from the physics.",2006,170,881,115,34,44,53,60,64,45,50,57,51,67
9b5cfa32dc382c468fd7410a28783b4b85712844,,1995,0,1323,61,32,21,40,74,58,58,71,86,83,94
97421e92320d15c4e3f554d46784c028523861d9,"The application of the extended Kaman filter to the problem of mobile robot navigation in a known environment is presented. An algorithm for, model-based localization that relies on the concept of a geometric beacon, a naturally occurring environment feature that can be reliably observed in successive sensor measurements and can be accurately described in terms of a concise geometric parameterization, is developed. The algorithm is based on an extended Kalman filter that utilizes matches between observed geometric beacons and an a priori map of beacon locations. Two implementations of this navigation algorithm, both of which use sonar, are described. The first implementation uses a simple vehicle with point kinematics equipped with a single rotating sonar. The second implementation uses a 'Robuter' mobile robot and six static sonar transducers to provide localization information while the vehicle moves at typical speeds of 30 cm/s. >",1991,19,1411,58,5,14,15,29,28,30,32,39,46,45
8955aea3446060d2aee29fb8a2fdcaed872aa489,This paper reports on the current status of structure validation in chemical crystallography.,2009,47,11513,1679,1,442,1257,1140,859,900,1014,959,881,762
abcacba645152530b080db0fda434a5664936df6,"The solar chemical composition is an important ingredient in our understanding of the formation, structure, and evolution of both the Sun and our Solar System. Furthermore, it is an essential refer ...",2009,475,5297,1913,45,190,280,323,412,447,490,508,508,557
ff8ba4dd5e38358d23a42551ca8e7a7aba7411bf,"Summary Trace-element data for mid-ocean ridge basalts (MORBs) and ocean island basalts (OIB) are used to formulate chemical systematics for oceanic basalts. The data suggest that the order of trace-element incompatibility in oceanic basalts is Cs ≈ Rb ≈ (≈ Tl) ≈ Ba(≈ W) > Th > U ≈ Nb = Ta ≈ K > La > Ce ≈ Pb > Pr (≈ Mo) ≈ Sr > P ≈ Nd (> F) > Zr = Hf ≈ Sm > Eu ≈ Sn (≈ Sb) ≈ Ti > Dy ≈ (Li) > Ho = Y > Yb. This rule works in general and suggests that the overall fractionation processes operating during magma generation and evolution are relatively simple, involving no significant change in the environment of formation for MORBs and OIBs. In detail, minor differences in element ratios correlate with the isotopic characteristics of different types of OIB components (HIMU, EM, MORB). These systematics are interpreted in terms of partial-melting conditions, variations in residual mineralogy, involvement of subducted sediment, recycling of oceanic lithosphere and processes within the low velocity zone. Niobium data indicate that the mantle sources of MORB and OIB are not exact complementary reservoirs to the continental crust. Subduction of oceanic crust or separation of refractory eclogite material from the former oceanic crust into the lower mantle appears to be required. The negative europium anomalies observed in some EM-type OIBs and the systematics of their key element ratios suggest the addition of a small amount (⩽1% or less) of subducted sediment to their mantle sources. However, a general lack of a crustal signature in OIBs indicates that sediment recycling has not been an important process in the convecting mantle, at least not in more recent times (⩽2 Ga). Upward migration of silica-undersaturated melts from the low velocity zone can generate an enriched reservoir in the continental and oceanic lithospheric mantle. We propose that the HIMU type (eg St Helena) OIB component can be generated in this way. This enriched mantle can be re-introduced into the convective mantle by thermal erosion of the continental lithosphere and by the recycling of the enriched oceanic lithosphere back into the mantle.",1989,144,18256,1008,0,0,0,0,0,0,0,0,0,0
eb2beacef72033cfc3364de191e703e363b7721a,,1982,0,14021,888,0,0,0,0,0,0,0,0,1,0
64caa66b1c2ad226b18b6fee6ebbf83a77400703,"Publisher Summary This chapter discusses the sequencing end-labeled DNA with base-specific chemical cleavages. In the chemical DNA sequencing method, one end-labels the DNA, partially cleaves it at each of the four bases in four reactions, orders the products by size on a slab gel, and then reads the sequence from an autoradiogram by noting which base-specific agent cleaved at each successive nucleotide along the strand. This technique sequences the DNA made in and purified from cells. No enzymatic copying in vitro is required, and either single- or double-stranded DNA can be sequenced. Most chemical schemes that cleave at one or two of the four bases involve three consecutive steps: modification of a base, removal of the modified base from its sugar, and DNA strand scission at that sugar. Base-specific chemical cleavage is only one step in sequencing DNA. The chapter presents techniques for producing discrete DNA fragments, end-labeling DNA, segregating end-labeled fragments, extracting DNA from gels, and the protocols for partially cleaving it at specific bases using the chemical reactions. The chapter also discusses the electrophoresis of the chemical cleavage products on long-distance sequencing gels and a guide for troubleshooting problems in sequencing patterns.",1980,45,10412,289,1,6,232,555,766,847,916,855,875,760
74974454d1e1805de5e890bc38b36baed2b4ec19,"There are two formalisms for mathematically describing the time behavior of a spatially homogeneous chemical system: The deterministic approach regards the time evolution as a continuous, wholly predictable process which is governed by a set of coupled, ordinary differential equations (the “reaction-rate equations”); the stochastic approach regards the time evolution as a kind of random-walk process which is governed by a single differential-difference equation (the “master equation”). Fairly simple kinetic theory arguments show that the stochastic formulation of chemical kinetics has a firmer physical basis than the deterministic formulation, but unfortunately the stochastic master equation is often mathematically intractable. There is, however, a way to make exact numerical calculations within the framework of the stochastic formulation without having to deal with the master equation directly. It is a relatively simple digital computer algorithm which uses a rigorously derived Monte Carlo procedure to numerically simulate the time evolution of the given chemical system. Like the master equation, this “stochastic simulation algorithm” correctly accounts for the inherent fluctuations and correlations that are necessarily ignored in the deterministic formulation. In addition, unlike most procedures for numerically solving the deterministic reaction-rate equations, this algorithm never approximates infinitesimal time increments df by finite time steps At. The feasibility and utility of the simulation algorithm are demonstrated by applying it to several well-known model chemical systems, including the Lotka model, the Brusselator, and the Oregonator.",1977,30,9106,836,0,5,5,1,7,7,5,4,4,1
e0a7cdd0e0c9ade81d03ab6515767a2d2d9a90ff,,1939,0,13284,374,0,0,1,0,0,0,1,1,0,0
48152e8b704df77db10ca85c064e7a565bb52908,,1958,0,7569,1576,1,0,3,3,0,2,3,2,7,2
ec2f624e0a37acbf84a3c6b4d8b731de118b41eb,"Conversion Factors.Physical and Chemical Data.Mathematics.Thermodynamics.Heat and Mass Transfer.Fluid and Particle Mechanics.Reaction Kinetics.Process Control and Instrumentation.Process Economics.Transport and Storage of Fluids.Heat Transfer Operations and Equipment.Drying, Humidification and Evaporative Cooling.Distillation.Gas Absorption and Other Gas-Liquid Operations and Equipment.Solid-State Operations and Equipment.Size Reduction and Size Enlargement.Handling of Bulk Solids and Packaging of Solids and Liquids.Alternative Separation Processes.Chemical Reactors.Biochemcial Engineering.Waste Management.Safety and Handling of Hazardous Materials.Energy Sources, Conversion and Utilization.Materials of Construction.Process Machinery Drives.Analysis of Plant Performance.",2007,0,7927,285,421,434,427,494,554,558,653,611,601,554
05b9e01b36be8f688cdbd7131b89920fa46ac048,"Chemical Thermodynamics and Kinetics Acid-Base Dissolved Carbon Dioxide Atmosphere-Water Interactions Metal Ions in Aqueous Solution Aspects of Coordination Chemistry Precipitation and Dissolution Oxidation and Reduction Equilibria the Solid-Solution Interface Trace Metals: Cycling, Regulation and Biological Role Kinetics and Redox Processes Photochemical Processes Kinetics at the Solid-Water Interface Adsorption Dissolution of Minerals Nucleation and Crystal Growth Particle-Particle Interaction Colloids Coagulation and Filtration Regulation of the Chemical Composition of Natural Waters (Examples) Thermodynamic Data.",1970,0,5780,534,0,0,0,0,0,0,0,0,0,0
54832f5f4a629a115ad0069c62222973b3568eac,"Abstract An exact method is presented for numerically calculating, within the framework of the stochastic formulation of chemical kinetics, the time evolution of any spatially homogeneous mixture of molecular species which interreact through a specified set of coupled chemical reaction channels. The method is a compact, computer-oriented, Monte Carlo simulation procedure. It should be particularly useful for modeling the transient behavior of well-mixed gas-phase systems in which many molecular species participate in many highly coupled chemical reactions. For “ordinary” chemical systems in which fluctuations and correlations play no significant role, the method stands as an alternative to the traditional procedure of numerically solving the deterministic reaction rate equations. For nonlinear systems near chemical instabilities, where fluctuations and correlations may invalidate the deterministic equations, the method constitutes an efficient way of numerically examining the predictions of the stochastic master equation. Although fully equivalent to the spatially homogeneous master equation, the numerical simulation algorithm presented here is more directly based on a newly defined entity called “the reaction probability density function.” The purpose of this article is to describe the mechanics of the simulation algorithm, and to establish in a rigorous, a priori manner its physical and mathematical validity; numerical applications to specific chemical systems will be presented in subsequent publications.",1976,26,5063,428,0,1,2,3,3,5,4,4,6,2
d635e2843c6fb034e9126aa73ef9c2e4e2c4714d,"It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading. 1. A Model of the Embryo. Morphogens. In this section a mathematical model of the growing embryo will be described. This model will be a simplification and an idealization, and consequently a falsification. It is to be hoped that the features retained for discussion are those of greatest importance in the present state of knowledge.",1990,120,5100,467,10,7,11,20,23,14,27,29,35,44
0540f1ce83134444515d3a67f450e8dc3976452b,"The chemical composition of natural water is derived from many different sources of solutes, including gases and aerosols from the atmosphere, weathering and erosion of rocks and soil, solution or precipitation reactions occurring below the land surface, and cultural effects resulting from human activities. Broad interrelationships among these processes and their effects can be discerned by application of principles of chemical thermodynamics. Some of the processes of solution or precipitation of minerals can be closely evaluated by means of principles of chemical equilibrium, including the law of mass action and the Nernst equation. Other processes are irreversible and require consideration of reaction mechanisms and rates. The chemical composition of the crustal rocks of the Earth and the composition of the ocean and the atmosphere are significant in evaluating sources of solutes in natural freshwater. The ways in which solutes are taken up or precipitated and the amounts present in solution are influenced by many environmental factors, especially climate, structure and position of rock strata, and biochemical effects associated with life cycles of plants and animals, both microscopic and macroscopic. Taken together and in application with the further influence of the general circulation of all water in the hydrologic cycle, the chemical principles and environmental factors form a basis for the developing science of natural-water chemistry. Fundamental data used in the determination of water quality are obtained by the chemical analysis of water samples in the laboratory or onsite sensing of chemical properties in the field. Sampling is complicated by changes in the composition of moving water and by the effects of particulate suspended material. Some constituents are unstable and require onsite determination or sample preservation. Most of the constituents determined are reported in gravimetric units, usually milligrams per liter or milliequivalents",1989,722,5535,618,76,79,47,79,95,81,106,116,100,84
628389e5454a0b12398eafa375dbe111d51288b2,,1996,0,5684,402,1,1,13,21,39,46,87,104,129,141
e06bd6aaf990dd41c71447a154e8d8cb62965705,"Chemical equilibria in soils , Chemical equilibria in soils , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1979,0,3794,445,0,3,8,22,29,27,34,35,35,66
c54866766d02ede47649a5edf76c9dc7a2e1727b,Partial table of contents: Overview of Chemical Reaction Engineering. HOMOGENEOUS REACTIONS IN IDEAL REACTORS. Introduction to Reactor Design. Design for Single Reactions. Design for Parallel Reactions. Potpourri of Multiple Reactions. NON IDEAL FLOW. Compartment Models. The Dispersion Model. The Tank--in--Series Model. REACTIONS CATALYZED BY SOLIDS. Solid Catalyzed Reactions. The Packed Bed Catalytic Reactor. Deactivating Catalysts. HETEROGENEOUS REACTIONS. Fluid--Fluid Reactions: Kinetics. Fluid--Particle Reactions: Design. BIOCHEMICAL REACTIONS. Enzyme Fermentation. Substrate Limiting Microbial Fermentation. Product Limiting Microbial Fermentation. Appendix. Index.,1972,1,7453,319,24,18,18,23,32,30,25,35,46,51
7fa94fd8a528f3d4d622b0ffa11b3d0bcd0b5c30,"Preface. General notes on analytical techniques. Nutrients. Soluble organic material. Particulate material. Plant pigments. Photosynthesis. Bacteria. Gases in seawater. Counting, media and preservatives. Terms and equivalents.",1984,0,6542,496,2,2,15,12,23,49,46,47,61,75
f6f215bf741217b7acf66ffe301dd11b2a4f4d2b,,1984,0,5618,434,3,2,9,9,19,23,33,50,37,48
59eede676a521227f5c24850567b9b8dc654b4dd,"Interest in graphene centres on its excellent mechanical, electrical, thermal and optical properties, its very high specific surface area, and our ability to influence these properties through chemical functionalization. There are a number of methods for generating graphene and chemically modified graphene from graphite and derivatives of graphite, each with different advantages and disadvantages. Here we review the use of colloidal suspensions to produce new materials composed of graphene and chemically modified graphene. This approach is both versatile and scalable, and is adaptable to a wide variety of applications.",2009,67,5668,44,26,214,452,558,670,653,607,540,507,456
573563f780dd06764d24825c64d9ec426102cb34,"In the past few years several methods have been developed for the analysis of serum lipoproteins. Lindgren, Elliott, and Gofman (1) have utilized the relatively low density of the lipoproteins to separate them from the other serum proteins by ultracentrifugal flotation. Quantitation was subsequently performed by refractometric methods in the analytical ultracentrifuge. Separations of lipoproteins have also been made by Cohn fractionation in cold ethanol, and the quantities of lipoprotein have been estimated from the lipid. content of the fractions (2, 3). Widely used at the present time is the method of zone electrophoresis with quantitation either by staining (4) or by chemical analysis of eluates from the support",1955,26,8145,218,1,4,8,8,12,17,29,17,18,23
8be9c35feb862041e39082a35bee317e878cb4a7,"A universal method to detect and determine siderophores was developed by using their high affinity for iron(III). The ternary complex chrome azurol S/iron(III)/hexadecyltrimethylammonium bromide, with an extinction coefficient of approximately 100,000 M-1 cm-1 at 630 nm, serves as an indicator. When a strong chelator removes the iron from the dye, its color turns from blue to orange. Because of the high sensitivity, determination of siderophores in solution and their characterization by paper electrophoresis chromatography can be performed directly on supernatants of culture fluids. The method is also applicable to agar plates. Orange halos around the colonies on blue agar are indicative of siderophore excretion. It was demonstrated with Escherichia coli strains that biosynthetic, transport, and regulatory mutations in the enterobactin system are clearly distinguishable. The method was successfully used to screen mutants in the iron uptake system of two Rhizobium meliloti strains, DM5 and 1021.",1987,16,4922,258,3,14,23,25,55,43,36,45,43,54
85d3432db02840f07f7ce8603507cb6125ac1334,"Reduction of a colloidal suspension of exfoliated graphene oxide sheets in water with hydrazine hydrate results in their aggregation and subsequent formation of a high-surface-area carbon material which consists of thin graphene-based sheets. The reduced material was characterized by elemental analysis, thermo-gravimetric analysis, scanning electron microscopy, X-ray photoelectron spectroscopy, NMR spectroscopy, Raman spectroscopy, and by electrical conductivity measurements.",2007,60,11509,138,0,0,0,0,1,404,1033,1190,1265,1224
7d8ced0b17a12deb6bd64e4f51df4e7396270c45,,1976,0,3303,282,5,3,7,18,28,37,41,53,56,47
f106b9efd9f7e336e7968277a630858427ea20b7,,1960,16,3529,294,1,1,2,4,1,1,1,1,0,1
e5ba538944747a6b2ba2a59213268ca2f8df5456,"The increasing amount of genomic and molecular information is the basis for understanding higher-order biological systems, such as the cell and the organism, and their interactions with the environment, as well as for medical, industrial and other practical applications. The KEGG resource () provides a reference knowledge base for linking genomes to biological systems, categorized as building blocks in the genomic space (KEGG GENES) and the chemical space (KEGG LIGAND), and wiring diagrams of interaction networks and reaction networks (KEGG PATHWAY). A fourth component, KEGG BRITE, has been formally added to the KEGG suite of databases. This reflects our attempt to computerize functional interpretations as part of the pathway reconstruction process based on the hierarchically structured knowledge about the genomic, chemical and network spaces. In accordance with the new chemical genomics initiatives, the scope of KEGG LIGAND has been significantly expanded to cover both endogenous and exogenous molecules. Specifically, RPAIR contains curated chemical structure transformation patterns extracted from known enzymatic reactions, which would enable analysis of genome-environment interactions, such as the prediction of new reactions and new enzyme genes that would degrade new environmental compounds. Additionally, drug information is now stored separately and linked to new KEGG DRUG structure maps.",2005,15,2793,249,2,106,266,315,227,266,239,216,184,170
b69e927c5e0a673c8738fad96419a0e12e289f28,"Abstract A particle which is caught in a potential hole and which, through the shuttling action of Brownian motion, can escape over a potential barrier yields a suitable model for elucidating the applicability of the transition state method for calculating the rate of chemical reactions.",1940,8,6202,172,1,0,0,0,1,0,1,1,1,3
e274c1b6e17825feab52de205fd0bc4917d5be6c,"Chemical shifts of backbone atoms in proteins are exquisitely sensitive to local conformation, and homologous proteins show quite similar patterns of secondary chemical shifts. The inverse of this relation is used to search a database for triplets of adjacent residues with secondary chemical shifts and sequence similarity which provide the best match to the query triplet of interest. The database contains 13Cα, 13Cβ, 13C′, 1Hα and 15N chemical shifts for 20 proteins for which a high resolution X-ray structure is available. The computer program TALOS was developed to search this database for strings of residues with chemical shift and residue type homology. The relative importance of the weighting factors attached to the secondary chemical shifts of the five types of resonances relative to that of sequence similarity was optimized empirically. TALOS yields the 10 triplets which have the closest similarity in secondary chemical shift and amino acid sequence to those of the query sequence. If the central residues in these 10 triplets exhibit similar φ and Ψ backbone angles, their averages can reliably be used as angular restraints for the protein whose structure is being studied. Tests carried out for proteins of known structure indicate that the root-mean-square difference (rmsd) between the output of TALOS and the X-ray derived backbone angles is about 15°. Approximately 3% of the predictions made by TALOS are found to be in error.",1999,71,2852,232,9,36,73,86,120,186,199,213,224,264
d786d73c09a8580583ccd4a5e5c53bb418f06e91,"A system is presented whereby volcanic rocks may be classified chemically as follows:I. Subalkaline Rocks:A. Tholeiitic basalt series:Tholeiitic picrite-basalt; tholeiite; tholeiitic andesite.B. Calc-alkali series:High-alumina basalt; andesite; dacite; rhyolite.II. Alkaline Rocks:A. Alkali olivine basalt series:(1) Alkalic picrite–basalt; ankaramite; alkali basalt; hawaiite; mugearite; benmorite; trachyte.(2) Alkalic picrite–basalt; ankaramite; alkali basalt; trachybasalt; tristanite; trachyte.B. Nephelinic, leucitic, and analcitic rocks.III. Peralkaline Rocks:pantellerite, commendite, etc.",1971,23,5758,145,1,4,9,17,23,25,20,32,36,39
37fa4423d7ce39718d7b52cc56b2451340b59815,,1986,9,5075,144,0,9,13,24,34,54,57,42,46,56
3fd096fb208bd8d3c5d41fa6b800f0faccbd07a3,"The Origin of Porosity and Permeability. Ground-Water Movement. Main Equations of Flow, Boundary Conditions, and Flow Nets. Ground Water in the Basin Hydrologic Cycle. Hydraulic Testing: Models, Methods, and Applications. Ground Water as a Resource. Stress, Strain, and Pore Fluids. Heat Transport in Ground-Water Flow. Solute Transport. Principles of Aqueous Geochemistry. Chemical Reactions. Colloids and Microorganisms. The Equations of Mass Transport. Mass Transport in Natural Ground-Water Systems. Mass Transport in Ground-Water Flow: Geologic Systems. Introduction to Contaminant Hydrogeology. Modeling the Transport of Dissolved Contaminants. Multiphase Fluid Systems. Remediation: Overview and Removal Options. In Situ Destruction and Risk Assessment. Answers to Problems. Appendices. References. Index.",1990,0,2779,256,1,1,12,12,21,35,44,49,51,48
9b3508e82af76f3da1b233f57756ae9e6328e6f5,"1. Mole Balances. The Rate of Reaction The General Mole Balance Equation Batch Reactors Continuous-Flow Reactors Industrial Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 2. Conversion and Reactor Sizing. Definition of Conversion Batch Reactor Design Equations Design Equations for Flow Reactors Applications of the Design Equations for Continuous-Flow Reactors Reactors in Series Some Further Definitions Summary CD-ROM Materials Questions and Problems Supplementary Reading 3. Rate Laws and Stoichiometry. Part 1. Rate Laws Basic Definitions The Reaction Order and the Rate Law The Reaction Rate Constant Present Status of Our Approach to Reactor Sizing and Design Part 2. Stoichiometry Batch Systems Flow Systems Summary CD-ROM Material Questions and Problems Supplementary Reading 4. Isothermal Reactor Design. Part 1. Mole Balances in Terms of Conversion Design Structure for Isothermal Reactors Scale-Up of Liquid-Phase Batch Reactor Data to the Design of a CSTR Design of Continuous Stirred Tank Reactors (CSTRs) Tubular Reactors Pressure Drop in Reactors Synthesizing the Design of a Chemical Plant Part 2. Mole Balances Written in Terms of Concentration and Molar Flow Rate Mole Balances on CSTRs, PFRs, PBRs, and Batch Reactors Microreactors Membrane Reactors Unsteady-State Operation of Stirred Reactors The Practical Side Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Some Thoughts on Critiquing What You read Journal Critique Problems Supplementary Reading 5. Collection and Analysis of Rate Data. The Algorithm for Data Analysis Batch Reactor Data Method of Initial Rates Method of Half-Lives Differential Reactors Experimental Planning Evaluation of Laboratory Reactors Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 6. Multiple Reactions. Definitions Parallel Reactions Maximizing the Desired Product in Series Reactions Algorithm for Solution of Complex Reactions Multiple Reactions in a PFR/PBR Multiple Reactions in a CSTR Membrane Reactors to Improve Selectivity in Multiple Reactions Complex Reactions of Ammonia Oxidation Sorting It All Out The Fun Part Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 7. Reaction Mechanisms, Pathways, Bioreactions, and Bioreactors. Active Intermediates and Nonelementary Rate Laws Enzymatic Reaction Fundamentals Inhibition of Enzyme Reactions Bioreactors Physiologically Based Pharmacokinetic (PBPK) Models Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 8. Steady-State Nonisothermal Reactor Design. Rationale The Energy Balance Adiabatic Operation Steady-State Tubular Reactor with Heat Exchange Equilibrium Conversion CSTR with Heat Effects Multiple Steady States Nonisothermal Multiple Chemical Reactions Radial and Axial Variations in a Tubular Reactor The Practical Side Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 9. Unsteady-State Nonisothermal Reactor Design. The Unsteady-State Energy Balance Energy Balance on Batch Reactors Semibatch Reactors with a Heat Exchanger Unsteady Operation of a CSTR Nonisothermal Multiple Reactions Unsteady Operation of Plug-Flow Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 10. Catalysis and Catalytic Reactors. Catalysts Steps in a Catalytic Reaction Synthesizing a Rate Law, Mechanism, and Rate-Limiting Step Heterogeneous Data Analysis for Reactor Design Reaction Engineering in Microelectronic Fabrication Model Discrimination Catalyst Deactivation Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 11. External Diffusion Effects on Heterogeneous Reactions. Diffusion Fundamentals Binary Diffusion External Resistance to Mass Transfer What If ... ? (Parameter Sensitivity) The Shrinking Core Model Summary CD-ROM Material Questions and Problems Supplementary Reading 12. Diffusion and Reaction. Diffusion and Reaction in Spherical Catalyst Pellets Internal Effectiveness Factor Falsified Kinetics Overall Effectiveness Factor Estimation of Diffusion- and Reaction-Limited Regimes Mass Transfer and Reaction in a Packed Bed Determination of Limiting Situations from Reaction Data Multiphase Reactors Fluidized Bed Reactors Chemical Vapor Deposition (CVD) Summary CD-ROM Material Questions and Problems Journal Article Problems Journal Critique Problems Supplementary Reading 13. Distributions of Residence Times for Chemical Reactors. General Characteristics Part 1. Characteristics and Diagnostics Measurement of the RTD Characteristics of the RTD RTD in Ideal Reactors Diagnostics and Troubleshooting Part 2. Predicting Conversion and Exit Concentration Reactor Modeling Using the RTD Zero-Parameter Models Using Software Packages RTD and Multiple Reactions Summary CD-ROM Material Questions and Problems Supplementary Reading 14. Models for Nonideal Reactors. Some Guidelines Tanks-in-Series (T-I-S) Model Dispersion Model Flow, Reaction, and Dispersion Tanks-in-Series Model Versus Dispersion Model Numerical Solutions to Flows with Dispersion and Reaction Two-Parameter Models-Modeling Real Reactors with Combinations of Ideal Reactors Use of Software Packages to Determine the Model Parameters Other Models of Nonideal Reactors Using CSTRs and PFRs Applications to Pharmacokinetic Modeling Summary CD-ROM Material Questions and Problems Supplementary Reading Appendix A: Numerical Techniques. Appendix B: Ideal Gas Constant and Conversion Factors. Appendix C: Thermodynamic Relationships Involving the Equilibrium Constant. Appendix D: Measurement of Slopes on Semilog Paper. Appendix E: Software Packages. Appendix F: Nomenclature. Appendix G: Rate Law Data. Appendix H: Open-Ended Problems. Appendix I: How to Use the CD-ROM. Appendix J: Use of Computational Chemistry Software Packages. Index. About the CD-ROM.",1986,0,3428,177,0,1,1,1,3,4,2,7,12,24
5923de1dcbe5b4927500c57c62824194338e118c,"5.1. Detection Formats 475 5.2. Food Quality and Safety Analysis 477 5.2.1. Pathogens 477 5.2.2. Toxins 479 5.2.3. Veterinary Drugs 479 5.2.4. Vitamins 480 5.2.5. Hormones 480 5.2.6. Diagnostic Antibodies 480 5.2.7. Allergens 481 5.2.8. Proteins 481 5.2.9. Chemical Contaminants 481 5.3. Medical Diagnostics 481 5.3.1. Cancer Markers 481 5.3.2. Antibodies against Viral Pathogens 482 5.3.3. Drugs and Drug-Induced Antibodies 483 5.3.4. Hormones 483 5.3.5. Allergy Markers 483 5.3.6. Heart Attack Markers 484 5.3.7. Other Molecular Biomarkers 484 5.4. Environmental Monitoring 484 5.4.1. Pesticides 484 5.4.2. 2,4,6-Trinitrotoluene (TNT) 485 5.4.3. Aromatic Hydrocarbons 485 5.4.4. Heavy Metals 485 5.4.5. Phenols 485 5.4.6. Polychlorinated Biphenyls 487 5.4.7. Dioxins 487 5.5. Summary 488 6. Conclusions 489 7. Abbreviations 489 8. Acknowledgment 489 9. References 489",2008,0,3198,95,30,106,140,199,224,220,267,344,300,321
4b0b2db5ee057de28aee48403652cc5dac26d402,"Twelve zeolitic imidazolate frameworks (ZIFs; termed ZIF-1 to -12) have been synthesized as crystals by copolymerization of either Zn(II) (ZIF-1 to -4, -6 to -8, and -10 to -11) or Co(II) (ZIF-9 and -12) with imidazolate-type links. The ZIF crystal structures are based on the nets of seven distinct aluminosilicate zeolites: tetrahedral Si(Al) and the bridging O are replaced with transition metal ion and imidazolate link, respectively. In addition, one example of mixed-coordination imidazolate of Zn(II) and In(III) (ZIF-5) based on the garnet net is reported. Study of the gas adsorption and thermal and chemical stability of two prototypical members, ZIF-8 and -11, demonstrated their permanent porosity (Langmuir surface area = 1,810 m2/g), high thermal stability (up to 550°C), and remarkable chemical resistance to boiling alkaline water and organic solvents.",2006,30,4298,56,1,9,22,52,77,148,197,224,308,351
6eb64578b01ce2ae51b6ecbb025c6ee8d408ad5c,"Boron-doped silicon nanowires (SiNWs) were used to create highly sensitive, real-time electrically based sensors for biological and chemical species. Amine- and oxide-functionalized SiNWs exhibit pH-dependent conductance that was linear over a large dynamic range and could be understood in terms of the change in surface charge during protonation and deprotonation. Biotin-modified SiNWs were used to detect streptavidin down to at least a picomolar concentration range. In addition, antigen-functionalized SiNWs show reversible antibody binding and concentration-dependent detection in real time. Lastly, detection of the reversible binding of the metabolic indicator Ca2+ was demonstrated. The small size and capability of these semiconductor nanowires for sensitive, label-free, real-time detection of a wide range of chemical and biological species could be exploited in array-based screening and in vivo diagnostics.",2001,56,5080,66,2,32,64,119,191,228,252,304,351,340
bcacbbeec2ecb88e716e5dd66be4bd5b2699c168,"This handbook of chemical tests for diagnostic, agricultural, and environmental purposes promotes the use of consistent methods, procedures and terminologies in soil and land surveys undertaken throughout Australia. Soil and water chemical methods include sampling and sample preparation, and measuring electrical conductivity and pH. Soil analysis includes: chloride, carbon, nitrogen, phosphorus, sulfur, gypsum, Other CABI sites ",1992,0,2504,332,0,1,6,8,24,21,24,41,47,55
9dad457c31f7688b432689e48bf573a09b2d456f,"It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.",1952,0,5825,90,0,5,1,3,4,4,2,2,2,1
5392c8d762d3f6fe4a6ff28ad8265ecb5cb7db83,"Chemical sensors based on individual single-walled carbon nanotubes (SWNTs) are demonstrated. Upon exposure to gaseous molecules such as NO(2) or NH(3), the electrical resistance of a semiconducting SWNT is found to dramatically increase or decrease. This serves as the basis for nanotube molecular sensors. The nanotube sensors exhibit a fast response and a substantially higher sensitivity than that of existing solid-state sensors at room temperature. Sensor reversibility is achieved by slow recovery under ambient conditions or by heating to high temperatures. The interactions between molecular species and SWNTs and the mechanisms of molecular sensing with nanotube molecular wires are investigated.",2000,49,5158,61,10,43,79,135,181,225,307,337,355,350
9e4c2c2c7ecdc0a4fcc19c88b6c0e4bfb09a4864,,1991,0,3211,131,3,18,18,44,46,69,68,86,68,93
eaa7d6df21e3a097ab902e19ff2f2905aafb7ab3,"Subducted sediments play an important role in arc magmatism and crust-mantle recycling. Models of continental growth, continental composition, convergent margin magmatism and mantle heterogeneity all require a better understanding of the mass and chemical fluxes associated with subducting sediments. We have evaluated subducting sediments on a global basis in order to better define their chemical systematics and to determine both regional and global average compositions. We then use these compositions to assess the importance of sediments to arc volcanism and crust-mantle recycling, and to re-evaluate the chemical composition of the continental crust. The large variations in the chemical composition of marine . sediments are for the most part linked to the main lithological constituents. The alkali elements K, Rb and Cs and high . field strength elements Ti, Nb, Hf, Zr are closely linked to the detrital phase in marine sediments; Th is largely detrital but may be enriched in the hydrogenous Fe-Mn component of sediments; REE patterns are largely continental, but abundances are closely linked to fish debris phosphate; U is mostly detrital, but also dependent on the supply and burial rate of organic matter; Ba is linked to both biogenic barite and hydrothermal components; Sr is linked to carbonate phases. Thus, the important geochemical tracers follow the lithology of the sediments. Sediment lithologies are controlled in turn by a small . number of factors: proximity of detrital sources volcanic and continental ; biological productivity and preservation of carbonate and opal; and sedimentation rate. Because of the link with lithology and the wealth of lithological data routinely collected for ODP and DSDP drill cores, bulk geochemical averages can be calculated to better than 30% for most elements . from fewer than ten chemical analyses for a typical drill core 100-1000 m . Combining the geochemical systematics with convergence rate and other parameters permits calculation of regional compositional fluxes for subducting sediment. These regional fluxes can be compared to the compositions of arc volcanics to asses the importance of sediment subduction to arc volcanism. For the 70% of the trenches worldwide where estimates can be made, the regional fluxes also provide the basis . for a global subducting sediment GLOSS composition and flux. GLOSS is dominated by terrigenous material 76 wt% q. terrigenous, 7 wt% calcium carbonate, 10 wt% opal, 7 wt% mineral-bound H O , and therefore similar to upper 2 . continental crust UCC in composition. Exceptions include enrichment in Ba, Mn and the middle and heavy REE, and . depletions in detrital elements diluted by biogenic material alkalis, Th, Zr, Hf . Sr and Pb are identical in GLOSS and UCC as a result of a balance between dilution and enrichment by marine phases. GLOSS and the systematics of marine sediments provide an independent approach to the composition of the upper continental crust for detrital elements. Significant discrepancies of up to a factor of two exist between the marine sediment data and current upper crustal estimates for Cs, Nb, . . . . Ta and Ti. Suggested revisions to UCC include Cs 7.3 ppm , Nb 13.7 ppm , Ta 0.96 ppm and TiO 0.76 wt% . These 2",1998,178,2589,274,9,16,30,38,51,58,61,75,75,95
c7f679ffc767b5df24cee55860f46cdd1576214f,"Global energy consumption is projected to increase, even in the face of substantial declines in energy intensity, at least 2-fold by midcentury relative to the present because of population and economic growth. This demand could be met, in principle, from fossil energy resources, particularly coal. However, the cumulative nature of CO2 emissions in the atmosphere demands that holding atmospheric CO2 levels to even twice their preanthropogenic values by midcentury will require invention, development, and deployment of schemes for carbon-neutral energy production on a scale commensurate with, or larger than, the entire present-day energy supply from all sources combined. Among renewable energy resources, solar energy is by far the largest exploitable resource, providing more energy in 1 hour to the earth than all of the energy consumed by humans in an entire year. In view of the intermittency of insolation, if solar energy is to be a major primary energy source, it must be stored and dispatched on demand to the end user. An especially attractive approach is to store solar-converted energy in the form of chemical bonds, i.e., in a photosynthetic process at a year-round average efficiency significantly higher than current plants or algae, to reduce land-area requirements. Scientific challenges involved with this process include schemes to capture and convert solar energy and then store the energy in the form of chemical bonds, producing oxygen from water and a reduced fuel such as hydrogen, methane, methanol, or other hydrocarbon species.",2006,114,6114,53,1,16,51,90,128,222,313,408,547,614
1777b325b569ded8515323adf54d4f6e5b160070,"Indirect evidence is presented that free‐standing Si quantum wires can be fabricated without the use of epitaxial deposition or lithography. The novel approach uses electrochemical and chemical dissolution steps to define networks of isolated wires out of bulk wafers. Mesoporous Si layers of high porosity exhibit visible (red) photoluminescence at room temperature, observable with the naked eye under <1 mW unfocused (<0.1 W cm−2) green or blue laser line excitation. This is attributed to dramatic two‐dimensional quantum size effects which can produce emission far above the band gap of bulk crystalline Si.",1990,24,6950,37,1,63,170,279,257,244,308,266,268,246
8fc260f0a6070925e693e7048e95b55a0eed92b3,"An empirical many-body potential-energy expression is developed for hydrocarbons that can model intramolecular chemical bonding in a variety of small hydrocarbon molecules as well as graphite and diamond lattices. The potential function is based on Tersoff's covalent-bonding formalism with additional terms that correct for an inherent overbinding of radicals and that include nonlocal effects. Atomization energies for a wide range of hydrocarbon molecules predicted by the potential compare well to experimental values. The potential correctly predicts that the \ensuremath{\pi}-bonded chain reconstruction is the most stable reconstruction on the diamond {111} surface, and that hydrogen adsorption on a bulk-terminated surface is more stable than the reconstruction. Predicted energetics for the dimer reconstructed diamond {100} surface as well as hydrogen abstraction and chemisorption of small molecules on the diamond {111} surface are also given. The potential function is short ranged and quickly evaluated so it should be very useful for large-scale molecular-dynamics simulations of reacting hydrocarbon molecules.",1990,0,3270,107,0,8,17,14,32,31,38,38,45,34
7d7e9fa2b79dbaace65755a4e86d1ed3f1695cf4,"Examination of nature's favorite molecules reveals a striking preference for making carbon-heteroatom bonds over carbon-carbon bonds-surely no surprise given that carbon dioxide is nature's starting material and that most reactions are performed in water. Nucleic acids, proteins, and polysaccharides are condensation polymers of small subunits stitched together by carbon-heteroatom bonds. Even the 35 or so building blocks from which these crucial molecules are made each contain, at most, six contiguous C-C bonds, except for the three aromatic amino acids. Taking our cue from nature's approach, we address here the development of a set of powerful, highly reliable, and selective reactions for the rapid synthesis of useful new compounds and combinatorial libraries through heteroatom links (C-X-C), an approach we call ""click chemistry"". Click chemistry is at once defined, enabled, and constrained by a handful of nearly perfect ""spring-loaded"" reactions. The stringent criteria for a process to earn click chemistry status are described along with examples of the molecular frameworks that are easily made using this spartan, but powerful, synthetic strategy.",2001,17,5784,43,1,2,2,8,13,72,129,236,314,411
a252446c4054d5b000dcb80cf5ec0b50afa99580,"As part of a series of evaluated sets, rate constants and photochemical cross sections compiled by the NASA Panel for Data Evaluation are provided. The primary application of the data is in the modeling of stratospheric processes, with particular emphasis on the ozone layer and its possible perturbation by anthropogenic and natural phenomena. Copies of this evaluation are available from the Jet Propulsion Laboratory.",1985,12,3406,108,4,7,12,12,9,21,72,83,94,124
f66608476a3cf3c6147823f55fc0ba235234175f,Computer program is described for numerical solution of chemical equilibria in complex systems by using nonlinear algebraic equations. Free-energy minimization technique is used.,1972,37,2940,194,4,5,8,6,10,10,12,19,23,26
2c0f30d8e044c7443eb1d9a44f287dbce709a44c,3.2.3. Hydroformylation 2467 3.2.4. Dimerization 2468 3.2.5. Oxidative Cleavage and Ozonolysis 2469 3.2.6. Metathesis 2470 4. Terpenes 2472 4.1. Pinene 2472 4.1.1. Isomerization: R-Pinene 2472 4.1.2. Epoxidation of R-Pinene 2475 4.1.3. Isomerization of R-Pinene Oxide 2477 4.1.4. Hydration of R-Pinene: R-Terpineol 2478 4.1.5. Dehydroisomerization 2479 4.2. Limonene 2480 4.2.1. Isomerization 2480 4.2.2. Epoxidation: Limonene Oxide 2480 4.2.3. Isomerization of Limonene Oxide 2481 4.2.4. Dehydroisomerization of Limonene and Terpenes To Produce Cymene 2481,2007,17,4251,12,5,56,96,130,211,242,353,425,410,427
515609af801dc07869249a89c9496136ab556813,"The ability to sustain a diatropic ring current is the defining characteristic of aromatic species.1-7 Cyclic electron delocalization results in enhanced stability, bond length equalization, and special magnetic as well as chemical and physical properties.1 In contrast, antiaromatic compounds sustain paratropic ring currents3 despite their localized, destabilized structures.1-7 We have demonstrated the direct, quantitative relationships among energetic, geometrical, and magnetic criteria of aromaticity in a wide-ranging set of aromatic/antiaromatic fivemembered rings.5a While the diamagnetic susceptibility exaltation (Λ) is uniquely associated with aromaticity, it is highly dependent on the ring size (area2) and requires suitable calibration standards.6 Aromatic stabilization energies (ASEs) of strained and more complicated systems are difficult to evaluate. CC bond length variations in polybenzenoid hydrocarbons can be just as large as those in linear conjugated polyenes.2 The abnormal proton chemical shifts of aromatic molecules are the most commonly employed indicators of ring current effects.1 However, the ca. 2-4 ppm displacements of external protons to lower magnetic fields are relatively modest (e.g., δH ) 7.3 for benzene vs 5.6 for dC-H in cyclohexene). In contrast, the upfield chemical shifts of protons located inside aromatic rings are more unusual. The six inner hydrogens of [18]annulene, for example, resonate at -3.0 ppm vs δ ) 9.28 for the outer protons. This relationship is inverted dramatically in the antiaromatic [18]annulene dianion, C18H18, where δ ) 20.8 and 29.5 (in) vs. -1.1 (out).8 Similar demonstrations of paratropic ring currents in antiaromatic compounds are well documented.3,8,9 Chemical shifts of encapsulated 3He atoms are now employed as experimental and computed measures of aromaticity in fullerenes and fullerene derivatives.10 While the rings of most aromatic systems are too small to accommodate atoms internally, the chemical shifts of hydrogens in bridging positions have long been used as aromaticity and antiaromaticity probes.9 δLi+ can be employed similarly, with the advantage that Li+ complexes with individual rings in polycyclic systems can be computed.4,11 We now propose the use of absolute magnetic shieldings, computed at ring centers (nonweighted mean of the heavy atom coordinates) with available quantum mechanics programs,12 as a new aromaticity/antiaromaticity criterion. To correspond to the familiar NMR chemical shift convention, the signs of the computed values are reversed: Negative “nucleus-independent chemical shifts” (NICSs) denote aromaticity; positive NICSs, antiaromaticity (see Table 1 for selected results). Figure 1, a plot of NICSs vs the ASEs for our set of five-membered ring heterocycles,5a provides calibration. The equally good correlations with magnetic susceptibility exaltations and with structural variations establish NICS as an effective aromaticity criterion. Unlike Λ,6 NICS values for [n]annulenes (Table 1) show only a modest dependence on ring size. The 10 π electron systems give significantly higher values than those with 6 π electrons. The antiaromatic 4n π electron compounds, cyclobutadiene (27.6), pentalene (18.1), heptalene (22.7), and planar D4h cyclooctatetraene (30.1), all show highly positive NICSs. Like the Li+-complex probe,4 the NICS evaluates the aromaticity and antiaromaticity contributions of individual rings in polycyclic systems. Scheme 1 (HF/6-31+G*, data from Table 1) shows NICSs for selected examples. The benzenoid aromatic NICSs provide evidence both for localized and “perimeter” models. The naphthalene (1) NICS (-9.9) resembles that of benzene (-9.7), as do the NICSs for the outer rings of phenanthrene (2) (-10.2) and triphenylene (3); the aromaticity of the central rings of the latter two are reduced. The NICS of the central ring of anthracene (4) (-13.3) exceeds the benzene value in contrast to the outer ring NICS (-8.2). Remarkably, the NICS (-7.0) for the seven-membered ring of azulene (5) is very close to that of the tropylium ion (-7.6 ppm), whereas the azulene five-membered ring NICS (-19.7) is even larger in magnitude than that of the cyclopentadienyl anion (-14.3). The four-membered rings in benzocyclobutadiene (6) (NICS ) 22.5) and in biphenylene (7) (19.0) are antiaromatic, but less so than cyclobutadiene itself (27.6). The six-membered rings in these polycycles are still aromatic, but their NICSs (-2.5 (1) (a) Minkin, V. I.; Glukhovtsev, M. N.; Simkin, B. Y. Aromaticity and Antiaromaticity; Wiley: New York, 1994. (b) Garratt, P. J. Aromaticity; Wiley: New York, 1986. (c) Eluidge, J. A.; Jackman, L. M. J. Chem. Soc. 1961, 859. (2) Schleyer, P. v. R.; Jiao, H. Pure Appl. Chem. 1996, 28, 209. (3) Pople, J. A.; Untch, K. G. J. Am. Chem. Soc. 1966, 88, 4811. (4) Jiao, H; Schleyer, P. v. R. AIP Conference Proceedings 330, E.C.C.C.1, Computational Chemistry; Bernardi, F., Rivail, J.-L., Eds.; American Institute of Physics: Woodbury, New York, 1995; p 107. (5) (a) Schleyer, P. v. R.; Freeman, P.; Jiao, H.; Goldfuss, B. Angew. Chem., Int. Ed. Engl. 1995, 34, 337. (b) Jiao, H.; Schleyer, P. v. R. Unpublished IGLO results. (c) Kutzelnigg, W.; Fleischer, U.; Schindler, M. In NMR: Basic Princ. Prog.; Springer: Berlin, 1990; Vol. 23, p 165. (6) Dauben, H. J., Jr.; Wilson, J. D.; Laity, J. L. In Non-Benzenoid Aromatics; Synder, J., Ed.; Academic Press, 1971; Vol. 2, and references cited. The partitioning of ring current or ring current susceptabilitites among various rings in polycyclic syestems were considered earlier, e.g., by Aihara (Aihara, J. J. Am. Chem. Soc. 1985, 207, 298 and refs cited) and by Mallion (Haigh, C. W.; Mallion, J. Chem. Phys. 1982, 76, 1982). (7) Fleischer, U.; Kutzelnigg, W.; Lazzeretti, P.; Mühlenkamp, V. J. Am. Chem. Soc. 1994, 116, 5298. (8) Sondheimer, F. Acc. Chem. Res. 1972, 5, 81. (9) (a) Hunandi, R. J. J. Am. Chem. Soc. 1983, 105, 6889. (b) Pascal, R. A., Jr.; Winans, C. G.; Van Engen, D. J. Am. Chem. Soc. 1989, 111, 3007. (10) (a) Bühl, M.; Thiel, W.; Jiao, H.; Schleyer, P. v. R.; Saunders, M.; Anet, F. A. L. J. Am. Chem. Soc. 1994, 116, 7429 and references cited. (b) Bühl, M.; van Wüllen, C. Chem. Phys. Lett. 1995, 247, 63. The authors have shown that the negative absolute shielding in the center of C60 is nearly the same as δ3He, computed at the same level. (11) Paquette, L. A.; Bauer, W.; Sivik, M. R.; Bühl, M.; Feigel, M.; Schleyer, P. v. R. J. Am. Chem. Soc. 1990, 112, 8776. (12) Frisch, M. J.; Trucks, G. W.; Schlegel, H. B.; Gill, P. M. W.; Johnson, B. G.; Robb, M. A.; Cheeseman, J. R.; Keith, T. A.; Petersson, G. A.; Montgomery, J. A.; Raghavachari, K.; Al-Laham, M. A.; Zakrewski, V. G.; Ortiz, J. V.; Foresman, J. B.; Cioslowski, J.; Stefanov, B. B.; Nanayakkara, A.; Challacombe, M.; Peng, C. Y.; Ayala, P. Y.; Chen, W.; Wong, M. W.; Andres, J. L.; Replogle, E. S.; Gomperts, R.; Stewart, J. P.; Head-Gordon, M.; Gonzalez, C.; Pople, J. A. Gaussian 94, ReVision B.2; Gaussian Inc., Pittsburgh, PA, 1995. Figure 1. Plot of NICSs (ppm) vs the aromatic stabilization energies (ASEs, kcal/mol)5a for a set of five-membered ring heterocycles, C4H4X (X ) as shown) (cc ) 0.966). 6317 J. Am. Chem. Soc. 1996, 118, 6317",1996,16,3888,22,3,9,27,30,25,54,58,61,71,107
3f7983818b76a5f1b5daf9b605877ed401c8e73c,"(1) Klamer, A. D. “Some Results Concerning Polyominoes”. Fibonacci Q. 1965, 3(1), 9-20. (2) Golomb, S. W. Polyominoes·, Scribner, New York, 1965. (3) Harary, F.; Read, R. C. “The Enumeration of Tree-like Polyhexes”. Proc. Edinburgh Math. Soc. 1970, 17, 1-14. (4) Lunnon, W. F. “Counting Polyominoes” in Computers in Number Theory·, Academic: London, 1971; pp 347-372. (5) Lunnon, W. F. “Counting Hexagonal and Triangular Polyominoes”. Graph Theory Comput. 1972, 87-100. (6) Brunvoll, J.; Cyvin, S. J.; Cyvin, B. N. “Enumeration and Classification of Benzenoid Hydrocarbons”. J. Comput. Chem. 1987, 8, 189-197. (7) Balaban, A. T., et al. “Enumeration of Benzenoid and Coronoid Hydrocarbons”. Z. Naturforsch., A: Phys., Phys. Chem., Kosmophys. 1987, 42A, 863-870. (8) Gutman, I. “Topological Properties of Benzenoid Systems”. Bull. Soc. Chim., Beograd 1982, 47, 453-471. (9) Gutman, I.; Polansky, O. E. Mathematical Concepts in Organic Chemistry·, Springer: Berlin, 1986. (10) To3i6, R.; Doroslovacki, R.; Gutman, I. “Topological Properties of Benzenoid Systems—The Boundary Code”. MATCH 1986, No. 19, 219-228. (11) Doroslovacki, R.; ToSic, R. “A Characterization of Hexagonal Systems”. Rev. Res. Fac. Sci.-Univ. Novi Sad, Math. Ser. 1984,14(2) 201-209. (12) Knop, J. V.; Szymanski, K.; Trinajstic, N. “Computer Enumeration of Substituted Polyhexes”. Comput. Chem. 1984, 8(2), 107-115. (13) Stojmenovic, L; Tosió, R.; Doroslovaóki, R. “Generating and Counting Hexagonal Systems”. Proc. Yugosl. Semin. Graph Theory, 6th, Dubrovnik 1985; pp 189-198. (14) Doroslovaóki, R.; Stojmenovió, I.; Tosió, R. “Generating and Counting Triangular Systems”. BIT 1987, 27, 18-24. (15) Knop, J. V.; Miller, W. R.; Szymanski, K.; Trinajstic, N. Computer Generation of Certain Classes of Molecules·, Association of Chemists and Technologists of Croatia: Zagreb, 1985.",1988,15,3133,151,2,2,8,9,8,7,5,11,23,15
3537d34e0910792df21db1d6750a485396a3976e,"In contrast to a recently expressed, and widely cited, view that ""Ionic liquids are starting to leave academic labs and find their way into a wide variety of industrial applications"", we demonstrate in this critical review that there have been parallel and collaborative exchanges between academic research and industrial developments since the materials were first reported in 1914 (148 references).",2008,177,3782,10,30,85,151,228,287,308,338,388,403,349
7f7daa06e89a9ac23c3dfb928048a6a47efcc71f,"Encyclopedia of Chemical Technology The Third Edition of the Encyclopedia of Chemical Technology is built on the solid foundation of the previous editions. All of the articles have been rewritten and updated and many new subjects have been added to reflect changes in chemical technology through the 1970s. The new edition, however, will be familiar to users of the earlier editions comprehensive, authoritative, accessible, lucid. The Encyclopedia remains an indispensable information source for all producers and users of chemical products and materials. In the Third Edition, emphasis is given to major present-day topics of concern to all chemists, scientists, and engineers--energy, health, safety, toxicology, and new materials. New subjects have been added, especially those related to polymer and plastics technology, fuels and energy, inorganic and solid-state chemistry, composite materials, coating, fermentation and enzymes, pharmaceuticals, surfactant technology, fibers and textiles. New features include the use of SI units as well as English units, Chemical Abstracts Service's Registry Numbers, and complete indexing based on automated retrieval from a machine-readable composition system. Once again this classic serves as an unrivaled library of information for the chemical and allied industries. Some comments about Kirk-Othmer-- The First Edition ""No reference library worthy of the name will be without this series. It is simply a must for the chemist and chemical engineer..."" --Chemical and Engineering News The Second Edition ""A necessity for any technical library."" --Choice",1998,0,3287,25,90,102,96,113,137,122,123,172,171,186
1351954508140592e89d2d6fe6ccd76f4684ce56,"Abstract The effects of the ionic strength and pH of the hemolyzing solution on the hemoglobin content of human erythrocyte ghosts were studied in phosphate buffers and found to have a pronounced influence upon hemoglobin binding in the ghosts. Buffer concentrations between 10 and 20 ideal milliosmolar (imOsm), at pH values 5.8 – 8.0, resulted in maximum hemoglobin removal from ghosts. The pH optimum for hemoglobin binding to ghosts was between 5.8 and 5.9 in a 20 imOsm buffer. The influence of these variables suggest an electrophysical interaction of hemoglobin with membrane constituents. This study provides a basis for comparison of existing methods for ghost preparation, as well as a means for prediction of the conditions required for preparation of ghosts containing any desired amount of hemoglobin. Conditions were found that allowed the preparation of hemoglobin-free ghosts by single-stage hemolysis and washing. Hemoglobin-free ghosts were prepared in 20 imOsm phosphate buffer at pH 7.4. Essentially all the lipid was recovered in the ghosts, but non-hemoglobin nitrogen-containing substances were lost. The pyridine hemochromogen method for hemoglobin determination was adapted for the measurement of very small quantities of hemoglobin through use of the Soret band (418 mμ) for absorbancy measurements.",1963,25,4014,55,3,6,10,18,22,32,47,51,81,101
b6316cdad5c28ea26dc4a9d90ae464cd0ff21ab5,,1981,35,3820,23,0,3,4,3,4,5,1,8,7,7
b03c9cf546e6b18c9a4467ab555a53f995299795,"Glucosinolates (beta-thioglucoside-N-hydroxysulfates), the precursors of isothiocyanates, are present in sixteen families of dicotyledonous angiosperms including a large number of edible species. At least 120 different glucosinolates have been identified in these plants, although closely related taxonomic groups typically contain only a small number of such compounds. Glucosinolates and/or their breakdown products have long been known for their fungicidal, bacteriocidal, nematocidal and allelopathic properties and have recently attracted intense research interest because of their cancer chemoprotective attributes. Numerous reviews have addressed the occurrence of glucosinolates in vegetables, primarily the family Brassicaceae (syn. Cruciferae; including Brassica spp and Raphanus spp). The major focus of much previous research has been on the negative aspects of these compounds because of the prevalence of certain ""antinutritional"" or goitrogenic glucosinolates in the protein-rich defatted meal from widely grown oilseed crops and in some domesticated vegetable crops. There is, however, an opposite and positive side of this picture represented by the therapeutic and prophylactic properties of other ""nutritional"" or ""functional"" glucosinolates. This review addresses the complex array of these biologically active and chemically diverse compounds many of which have been identified during the past three decades in other families. In addition to the Brassica vegetables, these glucosinolates have been found in hundreds of species, many of which are edible or could provide substantial quantities of glucosinolates for isolation, for biological evaluation, and potential application as chemoprotective or other dietary or pharmacological agents.",2001,304,2471,167,12,45,43,51,73,88,89,117,103,124
24e70394a03ee6f25e582e7ebb8039f1207fae0f,"There is a general consensus that supports the need for standardized reporting of metadata or information describing large-scale metabolomics and other functional genomics data sets. Reporting of standard metadata provides a biological and empirical context for the data, facilitates experimental replication, and enables the re-interrogation and comparison of data by others. Accordingly, the Metabolomics Standards Initiative is building a general consensus concerning the minimum reporting standards for metabolomics experiments of which the Chemical Analysis Working Group (CAWG) is a member of this community effort. This article proposes the minimum reporting standards related to the chemical analysis aspects of metabolomics experiments including: sample preparation, experimental analysis, quality control, metabolite identification, and data pre-processing. These minimum standards currently focus mostly upon mass spectrometry and nuclear magnetic resonance spectroscopy due to the popularity of these techniques in metabolomics. However, additional input concerning other techniques is welcomed and can be provided via the CAWG on-line discussion forum at http://msi-workgroups.sourceforge.net/ or http://Msi-workgroups-feedback@lists.sourceforge.net. Further, community input related to this document can also be provided via this electronic forum.",2007,24,2495,69,5,19,29,36,38,51,103,122,186,164
8544cc89cbcedbf7455738c1ade4a9b62428d7f7,"To realize graphene-based electronics, various types of graphene are required; thus, modulation of its electrical properties is of great importance. Theoretic studies show that intentional doping is a promising route for this goal, and the doped graphene might promise fascinating properties and widespread applications. However, there is no experimental example and electrical testing of the substitutionally doped graphene up to date. Here, we synthesize the N-doped graphene by a chemical vapor deposition (CVD) method. We find that most of them are few-layer graphene, although single-layer graphene can be occasionally detected. As doping accompanies with the recombination of carbon atoms into graphene in the CVD process, N atoms can be substitutionally doped into the graphene lattice, which is hard to realize by other synthetic methods. Electrical measurements show that the N-doped graphene exhibits an n-type behavior, indicating substitutional doping can effectively modulate the electrical properties of graphene. Our finding provides a new experimental instance of graphene and would promote the research and applications of graphene.",2009,0,2453,19,10,51,100,172,192,263,308,243,253,251
55a63c2a72325a86de9a17814fb6243c132ac19a,"The problem of translating the theory of economic choice behavior into concrete models suitable for analyzing housing location is discussed. The analysis is based on the premise that the classical, economically rational consumer will choose a residential location by weighing the attributes of each available alternative and by selecting the alternative that maximizes utility. The assumption of independence in the commonly used multinomial logit model of choice is relaxed to permit a structure of perceived similarities among alternatives. In this analysis, choice is described by a multinomial logit model for aggregates of similar alternatives. Also discussed are methods for controlling the size of data collection and estimation tasks by sampling alternatives from the full set of alternatives. /Author/",1977,16,3010,247,0,1,4,6,9,11,14,15,28,19
096f17d9dc7437828ea3128c8463fdf41f19d577,This paper conceives of residential segregation as a multidimensional phenomenon varying along 5 distinct axes of measurement: evenness exposure concentration centralization and clustering. 20 indices of segregation are surveyed and related conceptually to 1 of the 5 dimensions. Using data from a large set of US metropolitan areas the indices are intercorrelated and factor analyzed. Orthogonal and oblique rotations produce pattern matrices consistent with the postulated dimensional structure. Based on the factor analyses and other information 1 index was chosen to represent each of the 5 dimensions and these selections were confirmed with a principal components analysis. The paper recommends adopting these indices as standard indicators in future studies of segregation. (authors),1988,59,2639,216,1,5,6,7,12,10,7,10,14,13
fbd9460a027b20e9ea1943d927eb6e9baf89282f,"Racial residential segregation is a fundamental cause of racial disparities in health. The physical separation of the races by enforced residence in certain areas is an institutional mechanism of racism that was designed to protect whites from social interaction with blacks. Despite the absence of supportive legal statutes, the degree of residential segregation remains extremely high for most African Americans in the United States. The authors review evidence that suggests that segregation is a primary cause of racial differences in socioeconomic status (SES) by determining access to education and employment opportunities. SES in turn remains a fundamental cause of racial differences in health. Segregation also creates conditions inimical to health in the social and physical environment. The authors conclude that effective efforts to eliminate racial disparities in health must seriously confront segregation and its pervasive consequences.",2001,121,1982,100,2,4,19,23,40,50,54,67,81,99
b26e0fa7b363f42e31bbde61356f5fcf24d509fd,"There is a growing interest in reducing energy consumption and the associated greenhouse gas emissions in every sector of the economy. The residential sector is a substantial consumer of energy in every country, and therefore a focus for energy consumption efforts. Since the energy consumption characteristics of the residential sector are complex and inter-related, comprehensive models are needed to assess the technoeconomic impacts of adopting energy efficiency and renewable energy technologies suitable for residential applications. The aim of this paper is to provide an up-to-date review of the various modeling techniques used for modeling residential sector energy consumption. Two distinct approaches are identified: top-down and bottom-up. The top-down approach treats the residential sector as an energy sink and is not concerned with individual end-uses. It utilizes historic aggregate energy values and regresses the energy consumption of the housing stock as a function of top-level variables such as macroeconomic indicators (e.g. gross domestic product, unemployment, and inflation), energy price, and general climate. The bottom-up approach extrapolates the estimated energy consumption of a representative set of individual houses to regional and national levels, and consists of two distinct methodologies: the statistical method and the engineering method. Each technique relies on different levels of input information, different calculation or simulation techniques, and provides results with different applicability. A critical review of each technique, focusing on the strengths, shortcomings and purposes, is provided along with a review of models reported in the literature.",2009,86,1570,84,5,26,53,88,90,115,155,167,200,192
51e640475b7811610c705378025330aa55944954,"The stress-threshold model (Wolpert, 1965; Brown and Moore, 1970) assumes that people do not consider moving unless they experience residential stress. This paper develops a similar model of residential mobility in which residential satisfaction acts as an intervening variable between individual and residence variables and mobility. The model is tested with data from a panel study of Rhode Island residents. The results indicate that residential satisfaction at the first interview is related to the wish to move and to mobility in the year following the interview. Individual and residence characteristics such as age of head duration of residence, home ownership, and room crowding are shown to affect mobility through their effect on residential satisfaction.",1974,31,391,24,0,0,1,2,1,1,0,0,0,0
93c7cc6303f2aa0410bd7d6cfc1c07089f30773c,"This book focuses on topics at the forefront of electrochemical research. Splitting water by electrolysis; splitting water by visible light; the recent development of lithium batteries; theoretical approaches to intercalation; and fundamental concepts of electrode kinetics, particularly as applied to semiconductors are discussed. It is recommended for electrochemists, physical chemists, corrosion scientists, and those working in the fields of analytical chemistry, surface and colloid science, materials science, electrical engineering, and chemical engineering.",1974,42,5455,5,48,29,37,43,27,40,41,50,33,56
167d8a3763ad6c24ede2b72ace5b144bfe0cf0bc,3.6.1. Polishing and Cleaning 2663 3.6.2. Vacuum and Heat Treatments 2664 3.6.3. Carbon Electrode Activation 2665 3.7. Summary and Generalizations 2666 4. Advanced Carbon Electrode Materials 2666 4.1. Microfabricated Carbon Thin Films 2666 4.2. Boron-Doped Diamond for Electrochemistry 2668 4.3. Fibers and Nanotubes 2669 4.4. Carbon Composite Electrodes 2674 5. Carbon Surface Modification 2675 5.1. Diazonium Ion Reduction 2675 5.2. Thermal and Photochemical Modifications 2679 5.3. Amine and Carboxylate Oxidation 2680 5.4. Modification by “Click” Chemistry 2681 6. Synopsis and Outlook 2681 7. Acknowledgments 2682 8. References 2682,2008,5,1929,33,3,52,80,106,154,150,180,193,166,165
2bd21c50a012deb63c0b1455288db9e3a6b4ed9b,,1987,0,2206,9,2,8,12,17,19,39,32,49,46,40
ba5122439748194f2fd01feff741fbd9358fc497,,1981,0,2147,15,4,10,27,28,41,42,40,40,41,38
95881d413439696453cf915a35d95912a68d4ebe,,1964,0,2505,23,5,1,10,9,3,5,27,17,30,41
1de7cdaa3a4ab99eb095d720ceb692d09aa6556b,"We first reported that polyvinylpyrrolidone-protected graphene was dispersed well in water and had good electrochemical reduction toward O(2) and H(2)O(2). With glucose oxidase (GOD) as an enzyme model, we constructed a novel polyvinylpyrrolidone-protected graphene/polyethylenimine-functionalized ionic liquid/GOD electrochemical biosensor, which achieved the direct electron transfer of GOD, maintained its bioactivity and showed potential application for the fabrication of novel glucose biosensors with linear glucose response up to 14 mM.",2009,0,1179,11,16,77,146,144,168,132,120,89,72,78
9c7e882122e39197e77bbc40f55a78665803623a,"Direct electrochemistry of a glucose oxidase (GOD)-graphene-chitosan nanocomposite was studied. The immobilized enzyme retains its bioactivity, exhibits a surface confined, reversible two-proton and two-electron transfer reaction, and has good stability, activity and a fast heterogeneous electron transfer rate with the rate constant (k(s)) of 2.83 s(-1). A much higher enzyme loading (1.12 x 10(-9)mol/cm(2)) is obtained as compared to the bare glass carbon surface. This GOD-graphene-chitosan nanocomposite film can be used for sensitive detection of glucose. The biosensor exhibits a wider linearity range from 0.08mM to 12mM glucose with a detection limit of 0.02mM and much higher sensitivity (37.93microAmM(-1)cm(-2)) as compared with other nanostructured supports. The excellent performance of the biosensor is attributed to large surface-to-volume ratio and high conductivity of graphene, and good biocompatibility of chitosan, which enhances the enzyme absorption and promotes direct electron transfer between redox enzymes and the surface of electrodes.",2009,52,1038,25,0,33,86,101,117,127,106,95,77,95
e1077944f6a4ca988f3635708cb23b92fb99ccd7,"We present two new hybrid meta exchange- correlation functionals, called M06 and M06-2X. The M06 functional is parametrized including both transition metals and nonmetals, whereas the M06-2X functional is a high-nonlocality functional with double the amount of nonlocal exchange (2X), and it is parametrized only for nonmetals.The functionals, along with the previously published M06-L local functional and the M06-HF full-Hartree–Fock functionals, constitute the M06 suite of complementary functionals. We assess these four functionals by comparing their performance to that of 12 other functionals and Hartree–Fock theory for 403 energetic data in 29 diverse databases, including ten databases for thermochemistry, four databases for kinetics, eight databases for noncovalent interactions, three databases for transition metal bonding, one database for metal atom excitation energies, and three databases for molecular excitation energies. We also illustrate the performance of these 17 methods for three databases containing 40 bond lengths and for databases containing 38 vibrational frequencies and 15 vibrational zero point energies. We recommend the M06-2X functional for applications involving main-group thermochemistry, kinetics, noncovalent interactions, and electronic excitation energies to valence and Rydberg states. We recommend the M06 functional for application in organometallic and inorganometallic chemistry and for noncovalent interactions.",2008,283,17654,201,0,0,0,0,0,0,0,2,140,1921
cffe5f6bcababfb9cd99079cf83e90a9bb516383,"The term apoptosis is proposed for a hitherto little recognized mechanism of controlled cell deletion, which appears to play a complementary but opposite role to mitosis in the regulation of animal cell populations. Its morphological features suggest that it is an active, inherently programmed phenomenon, and it has been shown that it can be initiated or inhibited by a variety of environmental stimuli, both physiological and pathological.The structural changes take place in two discrete stages. The first comprises nuclear and cytoplasmic condensation and breaking up of the cell into a number of membrane-bound, ultrastructurally well-preserved fragments. In the second stage these apoptotic bodies are shed from epithelial-lined surfaces or are taken up by other cells, where they undergo a series of changes resembling in vitro autolysis within phagosomes, and are rapidly degraded by lysosomal enzymes derived from the ingesting cells.Apoptosis seems to be involved in cell turnover in many healthy adult tissues and is responsible for focal elimination of cells during normal embryonic development. It occurs spontaneously in untreated malignant neoplasms, and participates in at least some types of therapeutically induced tumour regression. It is implicated in both physiological involution and atrophy of various tissues and organs. It can also be triggered by noxious agents, both in the embryo and adult animal.",1972,75,15076,358,0,0,0,0,0,1,0,3,1,2
583e1e985b915610f556cad4860619c4bac8cc4b,"Abstract A photo-induced cyclic peroxidation in isolated chloroplasts is described. In an osmotic buffered medium, chloroplasts upon illumination produce malondialdehyde (MDA)—a decomposition product of tri-unsaturated fatty acid hydroperoxides—bleach endogenous chlorophyll, and consume oxygen. These processes show ( a ) no reaction in the absence of illumination; ( b ) an initial lag phase upon illumination of 10–20 minutes duration; ( c ) a linear phase in which the rate is proportional to the square root of the light intensity; ( d ) cessation of reaction occurring within 3 minutes after illumination ceases; and ( e ) a termination phase after several hours of illumination. The kinetics of the above processes fit a cyclic peroxidation equation with velocity coefficients near those for chemical peroxidation. The stoichiometry of MDA/O 2 = 0.02, and O 2 Chl bleached = 6.9 correlates well with MDA production efficiency in other biological systems and with the molar ratio of unsaturated fatty acids to chlorophyll. The energies of activation for the lag and linear phases are 17 and 0 kcal/mole, respectively, the same as that for autoxidation. During the linear phase of oxygen uptake the dependence upon temperature and O 2 concentration indicates that during the reaction, oxygen tension at the site of peroxidation is 100-fold lower than in the aqueous phase. It is concluded that isolated chloroplasts upon illumination can undergo a cyclic peroxidation initiated by the light absorbed by chlorophyll. Photoperoxidation results in a destruction of the chlorophyll and tri-unsaturated fatty acids of the chloroplast membranes.",1968,21,7242,256,1,2,2,2,3,2,3,8,6,3
056ded69eecde9a755f8ad4c4abf7eed549388e8,"INTRODUCTION
Astronauts soaring through space modules with the grace of birds seems counterintuitive. How do they adapt to the weightless environment? Previous spaceflights have shown that astronauts in orbit adapt their motor strategies to each change in their gravitational environment. During adaptation, performance is degraded and can lead to mission-threatening injuries. If adaptation can occur before a mission, productivity during the mission might improve, minimizing risk. The goal is to combine kinetic and kinematic data to examine translational motions during microgravity adaptations.


METHODS
Experiments were performed during parabolic flights aboard NASA's C-9. Five subjects used their legs to push off from a sensor, landing on a target 3.96 m (13 ft) away. The sensor quantified the kinetics during contact, while four cameras recorded kinematics during push-off. Joint torques were calculated for a subset of traverses (N = 50) using the forces, moments, and joint angles.


RESULTS
During the 149 traverses, the average peak force exerted onto the sensor was 224.6 +/- 74.6 N, with peak values ranging between 65.8-461.9 N. Two types of force profiles were observed, some having single, strong peaks (N = 64) and others having multiple, weaker peaks (N = 86).


CONCLUSIONS
The force data were consistent with values recorded previously in sustained microgravity aboard Mir and the Space Shuttle. A training program for astronauts might be designed to encourage fine-control motions (i.e., multiple, weaker peaks) as these reduce the risk of injury and increase controllability. Additionally, a kinematic and kinetic sensor suite was successfully demonstrated in the weightless environment onboard the C-9 aircraft.",2009,0,5485,0,202,233,213,940,1159,1596,921,1,0,0
7d10232c90bf60ac69b2e99f3deb8fb177ddeed2,"Abstract An analysis is made of the process whereby diffusion effects can cause the precipitation of grains of a second phase in a supersaturated solid solution. The kinetics of this type of grain growth are examined in detail. Some grains grow, only to be later dissolved; others increase in size and incorporate further grains that they encounter in so doing. This latter phenomenon of coalescence is discussed in a new “kinetic” approximation. Formulae are given for the asymptotic grain size distribution, for the number of grains per unit volume and for the supersaturation as a function of time. The effects of anisotropy, strain, crystalline order and the finite size of the specimen are allowed for. It is pointed out that for a material that can be said to be “supersaturated with vacancies”, the discussion can be applied to the vacancies as solute “atoms” which cluster together to form internal cavities. The practical case of a real, finite crystal is here important, because the vacancies can in general also escape to the surface. A special analysis is made of this example, and the results are applied to the theory of sintering.",1961,1,6198,130,0,0,1,2,3,6,3,5,13,12
665c4ff4e6479348893fa8764e1c52a5959c114b,"Laboratory investigations show that rates of adsorption of persistent organic compounds on granular carbon are quite low. Intraparticle diffusion of solute appears to control the rate of uptake, thus the rate is partially a function of the pore size distribution of the adsorbent, of the molecular size and configuration of the solute, and of the relative electrokinetic properties of adsorbate and adsorbent. Systemic factors such as temperature and pH will influence the rates of adsorption; rates increase with increasing temperature and decrease with increasing pH. The effect of initial concentration of solute is of considerable significance, the rate of uptake being a linear function of the square-root of concentration within the range of experimentation. Relative reaction rates also vary reciprocally with the square of the diameter of individual carbon particle for a given weight of carbon. Based on the findings of the research, fluidized-bed operation is suggested as an efficient means of using adsorption for treatment of waters and waste waters.",1963,0,5563,214,0,0,2,0,1,1,0,1,1,1
03403d7469b3fa7fe672c861d3e53589d1b28791,"The theory of the kinetics of phase change is developed with the experimentally supported assumptions that the new phase is nucleated by germ nuclei which already exist in the old phase, and whose number can be altered by previous treatment. The density of germ nuclei diminishes through activation of some of them to become growth nuclei for grains of the new phase, and ingestion of others by these growing grains. The quantitative relations between the density of germ nuclei, growth nuclei, and transformed volume are derived and expressed in terms of a characteristic time scale for any given substance and process. The geometry and kinetics of a crystal aggregate are studied from this point of view, and it is shown that there is strong evidence of the existence, for any given substance, of an isokinetic range of temperatures and concentrations in which the characteristic kinetics of phase change remains the same. The determination of phase reaction kinetics is shown to depend upon the solution of a function...",1939,13,8550,65,0,1,1,0,1,0,0,0,0,0
14fd4ea9fae9173f518bc29aac107ee1aa26b49e,"We present a new local density functional, called M06-L, for main-group and transition element thermochemistry, thermochemical kinetics, and noncovalent interactions. The functional is designed to capture the main dependence of the exchange-correlation energy on local spin density, spin density gradient, and spin kinetic energy density, and it is parametrized to satisfy the uniform-electron-gas limit and to have good performance for both main-group chemistry and transition metal chemistry. The M06-L functional and 14 other functionals have been comparatively assessed against 22 energetic databases. Among the tested functionals, which include the popular B3LYP, BLYP, and BP86 functionals as well as our previous M05 functional, the M06-L functional gives the best overall performance for a combination of main-group thermochemistry, thermochemical kinetics, and organometallic, inorganometallic, biological, and noncovalent interactions. It also does very well for predicting geometries and vibrational frequencies. Because of the computational advantages of local functionals, the present functional should be very useful for many applications in chemistry, especially for simulations on moderate-sized and large systems and when long time scales must be addressed.",2006,313,3343,37,2,7,31,36,65,136,184,246,291,320
878b775577fe08d47260b781dddd74b16b748e08,"Following upon the general theory in Part I, a considerable simplification is here introduced in the treatment of the case where the grain centers of the new phase are randomly distributed. Also, the kinetics of the main types of crystalline growth, such as result in polyhedral, plate‐like and lineal grains, are studied. A relation between the actual transformed volume V and a related extended volume V1 ex is derived upon statistical considerations. A rough approximation to this relation is shown to lead, under the proper conditions, to the empirical formula of Austin and Rickett. The exact relation is used to reduce the entire problem to the determination of V1 ex, in terms of which all other quantities are expressed. The approximate treatment of the beginning of transformation in the isokinetic range is shown to lead to the empirical formula of Krainer and to account quantitatively for certain relations observed in recrystallization phenomena. It is shown that the predicted shapes for isothermal transfo...",1940,4,6655,55,0,1,0,0,0,0,0,0,0,1
8979bdafefff9da97342fbc5cf3f511000306f60,Kinetics of Unireactant Enzymes. Simple Inhibition Systems. Rapid Equilibrium Partial and Mixed--Type Inhibition. Enzyme Activation. Rapid Equilibrium Bireactant and Terreactant Systems. Multisite and Allosteric Enzymes. Multiple Inhibition Analysis. Steady--State Kinetics of Multireactant Enzymes. Isotope Exchange. Effects of pH and Temperature. Appendix. Index.,1975,0,3084,215,0,2,10,10,12,13,13,17,20,19
9e4c2c2c7ecdc0a4fcc19c88b6c0e4bfb09a4864,,1991,0,3211,131,3,18,18,44,46,69,68,86,68,93
97e84e177ca0946644eaa521399d8a616263abf3,,1957,13,8624,40,0,0,1,0,1,5,1,2,3,4
a252446c4054d5b000dcb80cf5ec0b50afa99580,"As part of a series of evaluated sets, rate constants and photochemical cross sections compiled by the NASA Panel for Data Evaluation are provided. The primary application of the data is in the modeling of stratospheric processes, with particular emphasis on the ozone layer and its possible perturbation by anthropogenic and natural phenomena. Copies of this evaluation are available from the Jet Propulsion Laboratory.",1985,12,3406,108,4,7,12,12,9,21,72,83,94,124
6c399c082c0085f97b000bffade60cad00bb9f60,"The theory of the preceding papers is generalized and the notation simplified. A cluster of molecules in a stable phase surrounded by an unstable phase is itself unstable until a critical size is reached, though for statistical reasons a distribution of such clusters may exist. Beyond the critical size, the cluster tends to grow steadily. The designation ``nuclei'' or ``grains'' is used according as the clusters are below or above the critical size. It is shown that a comprehensive description of the phenomena of phase change may be summarized in Phase Change, Grain Number and Microstructure Formulas or Diagrams, giving, respectively, the transformed volume, grain, and microstructure densities as a function of time, temperature, and other variables. To facilitate the deduction of formulas for these densities the related densities of the ``extended'' grain population are introduced. The extended population is that system of interpenetrating volumes that would obtain if the grains granulated and grew throug...",1941,3,5037,39,0,0,1,0,0,0,0,0,0,0
3d0933aa3de5854503f55770df771e4b7b7f873e,"We present a new hybrid meta exchange-correlation functional, called M05-2X, for thermochemistry, thermochemical kinetics, and noncovalent interactions. We also provide a full discussion of the new M05 functional, previously presented in a short communication. The M05 functional was parametrized including both metals and nonmetals, whereas M05-2X is a high-nonlocality functional with double the amount of nonlocal exchange (2X) that is parametrized only for nonmetals. In particular, M05 was parametrized against 35 data values, and M05-2X is parametrized against 34 data values. Both functionals, along with 28 other functionals, have been comparatively assessed against 234 data values:  the MGAE109/3 main-group atomization energy database, the IP13/3 ionization potential database, the EA13/3 electron affinity database, the HTBH38/4 database of barrier height for hydrogen-transfer reactions, five noncovalent databases, two databases involving metal-metal and metal-ligand bond energies, a dipole moment database, a database of four alkyl bond dissociation energies of alkanes and ethers, and three total energies of one-electron systems. We also tested the new functionals and 12 others for eight hydrogen-bonding and stacking interaction energies in nucleobase pairs, and we tested M05 and M05-2X and 19 other functionals for the geometry, dipole moment, and binding energy of HCN-BF3, which has recently been shown to be a very difficult case for density functional theory. We tested eight functionals for four more alkyl bond dissociation energies, and we tested 12 functionals for several additional bond energies with varying amounts of multireference character. On the basis of all the results for 256 data values in 18 databases in the present study, we recommend M05-2X, M05, PW6B95, PWB6K, and MPWB1K for general-purpose applications in thermochemistry, kinetics, and noncovalent interactions involving nonmetals and we recommend M05 for studies involving both metallic and nonmetallic elements. The M05 functional, essentially uniquely among the functionals with broad applicability to chemistry, also performs well not only for main-group thermochemistry and radical reaction barrier heights but also for transition-metal-transition-metal interactions. The M05-2X functional has the best performance for thermochemical kinetics, noncovalent interactions (especially weak interaction, hydrogen bonding, π···π stacking, and interactions energies of nucleobases), and alkyl bond dissociation energies and the best composite results for energetics, excluding metals.",2006,148,2650,28,11,27,65,130,140,214,263,244,247,220
e4dbd52d06832df3c6e812b02749fadd20fd3e39,Part 1 Equilibria: fundamentals of pure component adsorption equilibria practical approaches of pure component adsorption equilibria pure component adsorption in microporous solids multicomponent adsorption equilibria heterogeneous adsorption equilibria. Part 2 Kinetics: fundamentals of diffusion and adsorption in porous media diffusion and adsorption in porous media - Maxwell-Stefan approach analysis of adsorption kinetics in a single homogeneous particle analysis of adsorption kinetics in a zeolite particle analysis of adsorption kinetics in a heterogeneous particle. Part 3 Measurement techniques: time lag analysis in diffusion and adsorption in porous media analysis of steady state and transient diffusion cells adsorption and diffusion measurement by a chromatography method kinetics of a batch adsorber.,1998,0,2068,167,0,1,20,22,42,36,59,62,49,74
d64c71c6adc11b135c09d1fb6a29193d7d83ffe9,"A pseudo-second order rate equation describing the kinetics of sorption of divalent metal ions onto sphagnum moss peat at diAerent initial metal ion concentrations and peat doses has been developed. The kinetics of sorption were followed based on the amounts of metal sorbed at various time intervals. Results show that sorption (chemical bonding) might be rate-limiting in the sorption of divalent metal ions onto peat during agitated batch contact time experiments. The rate constant, the equilibrium sorption capacity and the initial sorption rate were calculated. From these parameters, an empirical model for predicting the sorption capacity of metal ions sorbed was derived. # 2000 Elsevier Science Ltd. All rights reserved",2000,11,2511,40,2,9,9,27,42,74,57,71,113,149
d64f64829ca2b04ec3fab50d58b7f5c83c95c1a3,"Stochastic chemical kinetics describes the time evolution of a well-stirred chemically reacting system in a way that takes into account the fact that molecules come in whole numbers and exhibit some degree of randomness in their dynamical behavior. Researchers are increasingly using this approach to chemical kinetics in the analysis of cellular systems in biology, where the small molecular populations of only a few reactant species can lead to deviations from the predictions of the deterministic differential equations of classical chemical kinetics. After reviewing the supporting theory of stochastic chemical kinetics, I discuss some recent advances in methods for using that theory to make numerical simulations. These include improvements to the exact stochastic simulation algorithm (SSA) and the approximate explicit tau-leaping procedure, as well as the development of two approximate strategies for simulating systems that are dynamically stiff: implicit tau-leaping and the slow-scale SSA.",2007,47,1689,125,15,54,72,116,130,131,137,138,115,146
e6c08e887f683af6f0bacd512764aa042d11595e,"The discovery of ammonia oxidation by mesophilic and thermophilic Crenarchaeota and the widespread distribution of these organisms in marine and terrestrial environments indicated an important role for them in the global nitrogen cycle. However, very little is known about their physiology or their contribution to nitrification. Here we report oligotrophic ammonia oxidation kinetics and cellular characteristics of the mesophilic crenarchaeon ‘Candidatus Nitrosopumilus maritimus’ strain SCM1. Unlike characterized ammonia-oxidizing bacteria, SCM1 is adapted to life under extreme nutrient limitation, sustaining high specific oxidation rates at ammonium concentrations found in open oceans. Its half-saturation constant (Km = 133 nM total ammonium) and substrate threshold (≤10 nM) closely resemble kinetics of in situ nitrification in marine systems and directly link ammonia-oxidizing Archaea to oligotrophic nitrification. The remarkably high specific affinity for reduced nitrogen (68,700 l per g cells per h) of SCM1 suggests that Nitrosopumilus-like ammonia-oxidizing Archaea could successfully compete with heterotrophic bacterioplankton and phytoplankton. Together these findings support the hypothesis that nitrification is more prevalent in the marine nitrogen cycle than accounted for in current biogeochemical models.",2009,67,1228,127,2,50,113,96,112,100,102,119,111,109
132bea6311f0ce960cd7de9219a7da0f6989d66b,"Fluorescence quenching rate constants, kq, ranging from 106 to 2 × 1010 M−1 sec−1, of more than 60 typical electron donor-acceptor systems have been measured in de-oxygenated acetonitrile and are shown to be correlated with the free enthalpy change, ΔG23, involved in the actual electron transfer process 
 
in the encounter complex and varying between + 5 and −60 kcal/mole. The correlation which is based on the mechanism of adiabatic outer-sphere electron transfer requires ΔG≠23, the activation free enthalpy of this process to be a monotonous function of ΔG23 and allows the calculation of rate constants of electron transfer quenching from spectroscopic and electrochemical data. 
 
A detailed study of some systems where the calculated quenching constants differ from the experimental ones by several orders of magnitude revealed that the quenching mechanism operative in these cases was hydrogen-atom rather than electron transfer. 
 
The conditions under which these different mechanisms apply and their consequences are discussed.",1970,25,2835,17,0,2,4,6,6,10,13,13,27,18
8af645645302948eb875ba9f34f848560d7ea3ed,to Kinetics and Many-Body Theory.- Boltzmann Equation.- Numerical Solutions of the Boltzmann Equation.- Equilibrium Green Function Theory.- Nonequilibrium Many-Body Theory.- Contour-Ordered Green Functions.- Basic Quantum Kinetic Equations.- Boltzmann Limit.- Gauge Invariance.- Quantum Distribution Functions.- Quantum Transport in Semiconductors.- Linear Transport.- Field-Dependent Green Functions.- Optical Absorption in Intense THz Fields.- Transport in Mesoscopic Semiconductor Structures.- Time-Dependent Phenomena.- Theory of Ultrafast Kinetics in Laser-Excited Semiconductors.- Optical Free-Carrier Interband Kinetics in Semiconductors.- Interband Quantum Kinetics with LO-Phonon Scattering.- Two-Pulse Spectroscopy.- Coulomb Quantum Kinetics in a Dense Electron-Hole Plasma.- The Buildup of Screening.- Femtosecond Four-Wave Mixing with Dense Plasmas.,2004,0,1619,118,70,88,93,86,121,111,102,104,96,102
ecc4155c6567b6c9ff61d7e699581385e2e3b466,"Fluorescence photobleaching recovery (FPR) denotes a method for measuring two-dimensional lateral mobility of fluorescent particles, for example, the motion of fluorescently labeled molecules in approximately 10 mum2 regions of a single cell surface. A small spot on the fluorescent surface is photobleached by a brief exposure to an intense focused laser beam, and the subsequent recovery of the fluorescence is monitored by the same, but attenuated, laser beam. Recovery occurs by replenishment of intact fluorophore in the bleached spot by lateral transport from the surrounding surface. We present the theoretical basis and some practical guidelines for simple, rigorous analysis of FPR experiments. Information obtainable from FPR experiments includes: (a) identification of transport process type, i.e. the admixture of random diffusion and uniform directed flow; (b) determination of the absolute mobility coefficient, i.e. the diffusion constant and/or flow velocity; and (c) the fraction of total fluorophore which is mobile. To illustrate the experimental method and to verify the theory for diffusion, we describe some model experiments on aqueous solutions of rhodamine 6G.",1976,17,2330,162,2,5,12,14,21,32,29,22,30,25
7667d74b712a1ad8567a83e519b2c0343f39e22a,"The oxidation of organic and inorganic compounds during ozonation can occur via ozone or OH radicals or a combination thereof. The oxidation pathway is determined by the ratio of ozone and OH radical concentrations and the corresponding kinetics. A huge database with several hundred rate constants for ozone and a few thousand rate constants for OH radicals is available. Ozone is an electrophile with a high selectivity. The second-order rate constants for oxidation by ozone vary over 10 orders of magnitude, between < 0.1 M(-1)s(-1) and about 7 x 10(9) M(-1)s(-1). The reactions of ozone with drinking-water relevant inorganic compounds are typically fast and occur by an oxygen atom transfer reaction. Organic micropollutants are oxidized with ozone selectively. Ozone reacts mainly with double bonds, activated aromatic systems and non-protonated amines. In general, electron-donating groups enhance the oxidation by ozone whereas electron-withdrawing groups reduce the reaction rates. Furthermore, the kinetics of direct ozone reactions depend strongly on the speciation (acid-base, metal complexation). The reaction of OH radicals with the majority of inorganic and organic compounds is nearly diffusion-controlled. The degree of oxidation by ozone and OH radicals is given by the corresponding kinetics. Product formation from the ozonation of organic micropollutants in aqueous systems has only been established for a few compounds. It is discussed for olefines, amines and aromatic compounds.",2003,194,1553,106,3,7,23,43,45,52,75,82,112,94
b1cb818493644eca95bbbeff3a73b30104646252,,1961,0,2937,24,0,3,11,8,3,14,12,22,27,29
8ab3cb75edb599cb90f122ddd50cfaf3424ef75c,"We describe the use of gel electrophoresis in studies of equilibrium binding, site distribution, and kinetics of protein-DNA interactions. The method, which we call protein distribution analysis, is simple, sensitive and yields thermodynamically rigorous results. It is particularly well suited to studies of simultaneous binding of several proteins to a single nucleic acid. In studies of the lac repressor-operator interaction, we found that binding to the so-called third operator site (03) is 15-18 fold weaker than operator binding, and that the binding reactions with the first and third operators are uncoupled, implying that there is no communication between the sites. Pseudo-first order dissociation kinetics of the repressor-203 bp operator complex were found to be temperature sensitive, with delta E of 80 kcal mol-1 above 29 degrees C and 26 kcal mol-1 below. The half life of the complex (5 min at 21 degrees C) is shorter than that reported for very high molecular weight operator-containing DNAs, but longer than values reported for much shorter fragments. The binding of lac repressor core to DNA could not be detected by this technique: the maximum binding constant consistent with this finding is 10(5) M-1.",1981,0,2075,40,0,2,9,12,15,36,97,108,155,152
0df70a70a760c4900df850ddef14028b16e596b6,"The kinetics and mechanism of methylene blue adsorption on commercial activated carbon (CAC) and indigenously prepared activated carbons from bamboo dust, coconut shell, groundnut shell, rice husk, and straw, have been studied. The effects of various experimental parameters have been investigated using a batch adsorption technique to obtain information on treating effluents from the dye industry. The extent of dye removal increased with decrease in the initial concentration of the dye and particle size of the adsorbent and also increased with increase in contact time, amount of adsorbent used and the initial pH of the solution. Adsorption data were modeled using the Freundlich and Langmuir adsorption isotherms and first order kinetic equations. The kinetics of adsorption were found to be first order with regard to intra-particle diffusion rate. The adsorption capacities of indigenous activated carbons have been compared with that of the commercial activated carbon. The results indicate that such carbons could be employed as low cost alternatives to commercial activated carbon in wastewater treatment for the removal of colour and dyes.",2001,17,1647,36,0,0,5,19,30,54,57,60,71,79
bd54c3596ae468c928bbc783aa43bc58c11ab974,"The theory of phase-ordering dynamics that is the growth of order through domain coarsening when a system is quenched from the homogeneous phase into a broken-symmetry phase, is reviewed, with the emphasis on recent developments. Interest will focus on the scaling regime that develops at long times after the quench.How can one determine the growth laws that describe the time dependence of characteristic length scales, and what can be said about the form of the associated scaling functions? Particular attention will be paid to systems described by more complicated order parameters than the simple scalars usually considered, for example vector and tensor ®elds. The latter are needed, for example, to describe phase ordering in nematic liquid crystals, on which there have been a number of recent experiments. The study of topological defects (domain walls, vortices, strings and monopoles) provides a unifying framework for discussing coarsening in these diA erent systems.",1994,301,1631,78,1,6,16,24,31,33,41,39,58,68
985f76317c3d17aa3ac7c346d29d972d19b55db1,"Intrinsic rate equations were derived for the steam reforming of methane, accompanied by water-gas shift on a Ni/MgAl2O4 catalyst. A large number of detailed reaction mechanisms were considered. Thermodynamic analysis helped in reducing the number of possible mechanisms. Twenty one sets of three rate equations were retained and subjected to model discrimination and parameter estimation. The parameter estimates in the best model are statistically significant and thermodynamically consistent.",1989,8,1766,83,0,4,4,4,5,4,4,3,8,5
a0405e8b39e24a9c37d3dd67f8f6bfe5aff0ac1f,,1965,24,2455,38,0,1,8,3,3,3,3,4,9,11
ab7dd811be154081aa8c7e778a0ecde7fbac17a8,"Protein levels have been shown to vary substantially between individual cells in clonal populations. In prokaryotes, the contribution to such fluctuations from the inherent randomness of gene expression has largely been attributed to having just a few transcripts of the corresponding mRNAs. By contrast, eukaryotic studies tend to emphasize chromatin remodeling and burst-like transcription. Here, we study single-cell transcription in Escherichia coli by measuring mRNA levels in individual living cells. The results directly demonstrate transcriptional bursting, similar to that indirectly inferred for eukaryotes. We also measure mRNA partitioning at cell division and correlate mRNA and protein levels in single cells. Partitioning is approximately binomial, and mRNA-protein correlations are weaker earlier in the cell cycle, where cell division has recently randomized the relative concentrations. Our methods further extend protein-based approaches by counting the integer-valued number of transcript with single-molecule resolution. This greatly facilitates kinetic interpretations in terms of the integer-valued random processes that produce the fluctuations.",2005,73,1367,83,1,32,56,55,80,78,101,88,108,88
9043b736c593390f4389409f8051c95b75e1de97,"Nicotine is of importance as the addictive chemical in tobacco, pharmacotherapy for smoking cessation, a potential medication for several diseases, and a useful probe drug for phenotyping cytochrome P450 2A6 (CYP2A6). We review current knowledge about the metabolism and disposition kinetics of nicotine, some other naturally occurring tobacco alkaloids, and nicotine analogs that are under development as potential therapeutic agents. The focus is on studies in humans, but animal data are mentioned when relevant to the interpretation of human data. The pathways of nicotine metabolism are described in detail. Absorption, distribution, metabolism, and excretion of nicotine and related compounds are reviewed. Enzymes involved in nicotine metabolism including cytochrome P450 enzymes, aldehyde oxidase, flavin-containing monooxygenase 3, amine N-methyltransferase, and UDP-glucuronosyltransferases are represented, as well as factors affecting metabolism, such as genetic variations in metabolic enzymes, effects of diet, age, gender, pregnancy, liver and kidney diseases, and racial and ethnic differences. Also effects of smoking and various inhibitors and inducers, including oral contraceptives, on nicotine metabolism are discussed. Due to the significance of the CYP2A6 enzyme in nicotine clearance, special emphasis is given to the effects and population distributions of CYP2A6 alleles and the regulation of CYP2A6 enzyme.",2005,549,1280,100,14,47,57,56,56,65,75,87,108,89
5a1e3136ac33b0cdcec827c245738f3e8ba0488c,"Because of its toxicity, arsenic is of considerable environmental concern. Its solubility in natural systems is strongly influenced by adsorption at iron oxide surfaces. The objective of this study was to compare the adsorption behavior of arsenite and arsenate on ferrihydrite, under carefully controlled conditions, with regard to adsorption kinetics, adsorption isotherms, and the influence of pH on adsorption. The adsorption reactions were relatively fast, with the reactions almost completed within the first few hours. At relatively high As concentrations, arsenite reacted faster than arsenate with the ferrihydrite, i.e., equilibrium was achieved sooner, but arsenate adsorption was faster at low As concentrations and low pH. Adsorp tion maxima of approximately 0.60 (0.58) and 0.25 (0.16) molAs molFe-1 were achieved for arsenite and arsenate, respectively, at pH 4.6 (pH 9.2 in parentheses). The high arsenite retention, which precludes its retention entirely as surface adsorbed species, indicates the likel...",1998,7,1320,90,1,6,22,25,31,40,33,52,54,54
9643b8e34043edd982ac64ee2ea7ec0b81d9852e,"Three major models (from Tofts, Larsson, and Brix) for collecting and analyzing dynamic MRI gadolinium‐diethylenetriamine penta‐acetic acid (Gd‐DTPA) data are examined. All models use compartments representing the blood plasma and the abnormal extravascular extracellular space (EES), and they are intercompatible. All measure combinations of three parameters: (1) kpsρ is the influx volume transfer constant (min−1), or permeability surface area product per unit volume of tissue, between plasma and EES; (2) ve is the volume of EES space per unit volume of tissue (0 < ve < 1); and (3) kep, the efflux rate constant (min−1), is the ratio of the first two parameters (kep = kpsρ/ve). The ratio kep is the simplest to measure, requiring only signal linearity with Gd tracer concentration or, alternatively, a measurement of T1 before injection of Gd (T10). To measure the physiologic parameters kpsρ and ve separately requires knowledge of T10 and of the tissue relaxivity R1 (≈︁ in vitro value).",1997,61,1431,88,3,10,15,24,8,30,34,35,54,56
7238deb8a30f94c0a5a2c64d9123095e78267aa6,"DNA is increasingly being used as the engineering material of choice for the construction of nanoscale circuits, structures, and motors. Many of these enzyme-free constructions function by DNA strand displacement reactions. The kinetics of strand displacement can be modulated by toeholds, short single-stranded segments of DNA that colocalize reactant DNA molecules. Recently, the toehold exchange process was introduced as a method for designing fast and reversible strand displacement reactions. Here, we characterize the kinetics of DNA toehold exchange and model it as a three-step process. This model is simple and quantitatively predicts the kinetics of 85 different strand displacement reactions from the DNA sequences. Furthermore, we use toehold exchange to construct a simple catalytic reaction. This work improves the understanding of the kinetics of nucleic acid reactions and will be useful in the rational design of dynamic DNA and RNA circuits and nanodevices.",2009,73,933,36,2,17,24,48,57,84,96,85,102,109
7cecac43cad599979f6d7fb75c85ddbe66c9e866,"Nicotine underlies tobacco addiction, influences tobacco use patterns, and is used as a pharmacological aid to smoking cessation. The absorption, distribution and disposition characteristics of nicotine from tobacco and medicinal products are reviewed. Nicotine is metabolized primarily by the liver enzymes CYP2A6, UDPglucuronosyltransferase (UGT), and flavin-containing monooxygenase (FMO). In addition to genetic factors, nicotine metabolism is influenced by diet and meals, age, sex, use of estrogen-containing hormone preparations, pregnancy and kidney disease, other medications, and smoking itself. Substantial racial/ethnic differences are observed in nicotine metabolism, which are likely influenced by both genetic and environmental factors. The most widely used biomarker of nicotine intake is cotinine, which may be measured in blood, urine, saliva, hair, or nails. The current optimal plasma cotinine cut-point to distinguish smokers from non-smokers in the general US population is 3 ng ml(-1). This cut-point is much lower than that established 20 years ago, reflecting less secondhand smoke exposure due to clear air policies and more light or occasional smoking.",2009,150,966,63,6,24,36,32,59,71,87,93,91,110
3a3d221aa1043a58f005b9ab6c581c5814634807,"Basic Principles of Chemical Kinetics Introduction to Enzyme Kinetics ""Alternative"" Enzymes Practical Aspects of Kinetics Deriving Steady-state Rate Equations Reversible Inhibition and Activation Tight-binding and Irreversible Inhibitors Reactions of More than One Substrate Use of Isotopes for Studying Enzyme Mechanisms Effect of pH on Enzyme Activity Temperature Effects on Enzyme Activity Regulation of Enzyme Activity Multienzyme Systems Fast Reactions Estimation of Kinetic Constants Standards for Reporting Enzymology Data Solutions and Notes to Problems Index",1979,0,1816,98,0,1,11,12,14,13,13,16,19,21
28713d928f1f9654917e819d737dd126cf32b50b,"Dissecting Amyloid Formation Amyloid fibrils are associated with clinical disorders ranging from Alzheimer's disease to type II diabetes. Their self-assembly can be described by a master equation that takes into account nucleation-dependent polymerization and fragmentation. Knowles et al. (p. 1533) now present an analytical solution to the master equation, which shows that amyloid growth kinetics is often limited by the fragmentation rate rather than by the rate of primary nucleation. In addition, the results reveal relationships between system properties (scaling laws) that provide mechanistic insight not only into amyloid growth, but also into related self-assembly processes. The growth kinetics of amyloid fibrils and related self-assembly phenomena are revealed by analytical theory. We present an analytical treatment of a set of coupled kinetic equations that governs the self-assembly of filamentous molecular structures. Application to the case of protein aggregation demonstrates that the kinetics of amyloid growth can often be dominated by secondary rather than by primary nucleation events. Our results further reveal a range of general features of the growth kinetics of fragmenting filamentous structures, including the existence of generic scaling laws that provide mechanistic information in contexts ranging from in vitro amyloid growth to the in vivo development of mammalian prion diseases.",2009,31,871,31,1,28,58,71,81,73,88,76,77,88
5b8d72c87266ac5cff1fa3ac9a5619354eb0ae7f,"Abstract The kinetics of glide at constant structure and the kinetics of structure evolution are correlated on the basis of various experimental observations in pure f.c.c. mono- and polycrystals. Two regimes of behavior are identified. In the initial regime, the Cottrell-Stokes law is satisfied, hardening is athermal, and a single structure parameter is adequate. With increasing importance of dynamic recovery, be it at large strains or at high temperatures, all of these simple assumptions break down. However, the proportionality between the flow stress and the square-root of the dislocation density holds, to a good approximation, over the entire regime; mild deviations arc primarily ascribed to differences between the various experimental techniques used. A phenomenological model is proposed, which incorporates the rate of dynamic recovery into the flow kinetics. It has been successful in matching many experimental data quantitatively.",1981,29,1799,43,0,2,6,2,5,15,11,5,7,7
bbb35e715579a067091eb0feb5dd912b2ee31dcc,Empirical Treatment of Reaction Rates. Experimental Methods and Treatment of Data. Elementary Processes: Molecular Collisions. Elementary Processes: Potential Energy Surfaces and Transition--State Theory. Simple Gas--Phase Reactions--Interplay of Theory and Experiment. Reactions in Solution. Complex Reactions. Homogeneous Catalysts. Chain Reactions. Photochemistry. Appendix.,1961,0,2047,40,7,9,10,13,17,12,23,19,22,25
87f942443f71277d7f1919e0824dcdb843f85fe1,"Abstract This paper describes a simple yet sensitive laboratory procedure that was developed to provide detailed information on the fermentation kinetics of ruminant feeds. In principle, the technique is similar to other in vitro digestibility procedures using ground particulate substrates, anaerobic media and a rumen fluid inoculum. It differs, however, in that incubations are conducted in gas-tight culture bottles, thus enabling gases to accumulate in the head-space as the fermentation proceeds. A pressure transducer connected to a digital readout voltmeter and gas-tight syringe assembly is then used to measure and release the accumulated gas pressures from the incubating culture bottles. By repeating this gas-measurement, gas-release procedure at regular intervals during the fermentation, it is possible to construct gas accumulation profiles for feeds by summation of regression-corrected gas volumes. These profiles are then described using a recently derived growth function developed to characterise gas production profiles. Results obtained establish the pressure transducer as a suitable tool for determining the fermentation kinetics of ruminant feeds and ranking them with respect to their in vitro fermentability.",1994,30,1378,76,0,12,21,17,29,27,20,17,20,19
dd4164b2dff46cb997266bb7ef0b4057b7a06755,"Abstract Chaotic dynamics can be considered as a physical phenomenon that bridges the regular evolution of systems with the random one. These two alternative states of physical processes are, typically, described by the corresponding alternative methods: quasiperiodic or other regular functions in the first case, and kinetic or other probabilistic equations in the second case. What kind of kinetics should be for chaotic dynamics that is intermediate between completely regular (integrable) and completely random (noisy) cases? What features of the dynamics and in what way should they be represented in the kinetics of chaos? These are the subjects of this paper, where the new concept of fractional kinetics is reviewed for systems with Hamiltonian chaos. Particularly, we show how the notions of dynamical quasi-traps, Poincare recurrences, Levy flights, exit time distributions, phase space topology prove to be important in the construction of kinetics. The concept of fractional kinetics enters a different area of applications, such as particle dynamics in different potentials, particle advection in fluids, plasma physics and fusion devices, quantum optics, and many others. New characteristics of the kinetics are involved to fractional kinetics and the most important are anomalous transport, superdiffusion, weak mixing, and others. The fractional kinetics does not look as the usual one since some moments of the distribution function are infinite and fluctuations from the equilibrium state do not have any finite time of relaxation. Different important physical phenomena: cooling of particles and signals, particle and wave traps, Maxwell's Demon, etc. represent some domains where fractional kinetics proves to be valuable.",2002,173,1229,32,3,26,46,89,72,82,78,59,70,62
1907fe052cd717a7ee3484c937025d0968cba2d3,,1969,0,1912,98,3,4,10,8,23,13,17,24,18,24
99ca73a0e103dca19d945f97a83fe318741ac9b4,"Abstract The intraparticle diffusion model (IPD) proposed by Weber and Morris has been widely applied for the analysis of adsorption kinetics. In this work, the characteristic curves based on this model were plotted with various initial adsorption factors (Ri). Four zones of the initial adsorption according to Ri value from 0 to 1 were classified; that is, approaching completely initial adsorption (zone 4), strongly initial adsorption (zone 3), intermediately initial adsorption (zone 2), and weakly initial adsorption (zone 1). Activated carbons with micropore volume fraction of 0.537 and 0.686 were prepared from oil-palm shells by steam activation. Based on the standard deviations, the kinetics of the adsorption of tannic acid (TA), methylene blue (MB), phenol, and 4-chlorophenol (4-CP) on activated carbons could be best described by intraparticle diffusion model. The initial adsorption of TA and MB belonged to zone 2, and that of phenol and 4-CP mostly belonged to zone 3. Nearly 80% of the 86 adsorption systems surveyed belonged to zones 2 and 3, indicating that the Ri value was smaller when the carbon with smaller particle and steam-activated carbon was used.",2009,62,771,12,1,15,17,22,33,39,39,68,70,85
993249be5ea76b470046b13eec5e6979d0fc8c0f,"28, 303 (1956). Heilbron, I. M., Kamm, E. D., Owens, W. M., J . Chem. SOC. 1926, 1631. Mer, O., Riiegg, R., Chopard-ditJean, L., Bernhard, K., Helv. Chim. Acta 39, 897 (195s). (7) Karrer, P., Helfenstein, A., Ibid., 14, 78 (1931). (8) Sax, K. J., Stross, F. H., J . Org. Chew 22, 1251 (1957). (9) Schmitt, J., Ann. 547, 115 (1941). (10) Trippett, S., Chem. & Ind. (London) 1956,80. (11) Tsujimoto, M., Ind. Ena. Chem. 8. . . 889 (191s). '",1957,2,1597,237,0,0,0,0,0,0,0,0,0,0
e97c8469ff95e4e873d4c9bb1bc32ffd79a5039c,Abstract Non-isothermal kinetics of the process of nucleation and its growth are derived by extending Avrami's equation. Kinetic analysis of the thermoanalytical data of the process is also described and applied to DSC curves of crystallization obtained by cooling poly(ethylene terephthalate) at constant rates. Crystallization is thought to proceed by nuclei being formed randomly and growing in three-dimensions.,1971,16,1841,31,0,1,1,0,0,0,4,1,4,1
58ea5f275157eb4fe2e67e1e062b9ec5848f14ed,"Though it incorporates much new material, this new edition preserves the general character of the book in providing a collection of solutions of the equations of diffusion and describing how these solutions may be obtained.",1956,22,18631,775,0,0,0,0,0,0,0,1,0,0
ab7b5917515c460b90451e67852171a531671ab8,"We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.",1993,30,4702,620,6,16,18,32,36,33,42,65,74,105
9d0c4389436d3ab8ed329dba8c58e8ac6737fd3b,"Many models for the spread of infectious diseases in populations have been analyzed mathematically and applied to specific diseases. Threshold theorems involving the basic reproduction number $R_{0}$, the contact number $\sigma$, and the replacement number $R$ are reviewed for the classic SIR epidemic and endemic models. Similar results with new expressions for $R_{0}$ are obtained for MSEIR and SEIR endemic models with either continuous age or age groups. Values of $R_{0}$ and $\sigma$ are estimated for various diseases including measles in Niger and pertussis in the United States. Previous models with age structure, heterogeneity, and spatial structure are surveyed.",2000,226,4875,259,0,10,20,43,55,80,121,124,129,154
9036f1e03c4da901805c56e169efa57869194885,,1988,1,4642,288,6,24,42,60,88,108,117,163,163,187
6ea24fcbbe780ce565251757b0a8cec5c753b90d,List of figures List of tables Preface 1. Introduction: psychology and anthropology I Part I. Theory in Practice: 2. Missionaries and cannibals (indoors) 3. Life after school 4. Psychology and anthropology II Part II. Practice in Theory: 5. Inside the supermarket (outdoors) and from the veranda 6. Out of trees of knowledge into fields for activity 7. Through the supermarket 8. Outdoors: a social anthropology of cognition in practice Notes References.,1988,0,3900,296,2,7,27,21,16,49,53,64,72,60
d162573ec41926a8bad701fab1eec6036a599030,"The Mathematics of Computerized Tomography covers the relevant mathematical theory of the Radon transform and related transforms and also studies more practical questions such as stability, sampling, resolution, and accuracy. Quite a bit of attention is given to the derivation, analysis, and practical examination of reconstruction algorithm, for both standard problems and problems with incomplete data.",1986,0,3327,278,3,1,14,17,24,29,38,38,24,39
7ba5078a4f81c74deb4c5e10fcb4aceb6eb635ef,"Graduate Texts in Mathematics bridge the gap between passive study and creative understanding, offering graduate-level introductions to advanced topics in mathematics. The volumes are carefully written as teaching aids and highlight characteristic features of the theory. Although these books are frequently used as textbooks in graduate courses, they are also suitable for individual study.    Series Editors:  Sheldon Axler, San Francisco State University  Kenneth Ribet, University of California, Berkeley  Advisory Board:  Alejandro Adem, University of British Columbia  David Eisenbud, University of California, Berkeley & MSRI  Irene M. Gamba, The University of Texas at Austin  J.F.Jardine, University of Western Ontario  Jeffrey C. Lagarias, University of Michigan  Ken Ono, Emory University  Jeremy Quastel, University of Toronto  Fadil Santosa, University of Minnesota  Barry Simon, California Institute of Technology",1982,0,8759,50,8,8,6,9,16,14,21,14,20,18
2331485d13ac3c7ad8b65c3d847ec4eefa293b59,,1957,0,3559,315,1,2,1,5,3,3,3,5,6,5
d3d720baa7080594c2fad06bb2ac90cc46666735,"The goals of this chapter are (1) to outline and substantiate a broad conceptualization of what it means to think mathematically, (2) to summarize the literature relevant to understanding mathematical thinking and problem solving, and (3) to point to new directions in research, development, and assessment consonant with an emerging understanding of mathematical thinking and the goals for instruction outlined here. The use of the phrase “learning to think mathematically” in this chapter’s title is deliberately broad. Although the original charter for this chapter was to review the literature on problem solving and metacognition, the literature itself is somewhat ill defined and poorly grounded. As the literature summary will make clear, problem solving has been used with multiple meanings that range from “working rote exercises” to “doing mathematics as a professional”; metacognition has multiple and almost disjoint meanings (from knowledge about one’s thought processes to self-regulation during problem solving) that make it difficult to use as a concept. This chapter outlines the various meanings that have been ascribed to these terms and discusses their role in mathematical thinking. The discussion will not have the character of a classic literature review, which is typically encyclopedic in its references and telegraphic in its discussions of individual papers or results. It will, instead, be selective and illustrative, with main points illustrated by extended discussions of pertinent examples. Problem solving has, as predicted in the 1980 Yearbook of the National Council of Teachers of Mathematics (Krulik, 1980, p. xiv), been the theme of the 1980s. The decade began with NCTM’s widely heralded statement, in its Agenda for Action, that “problem solving must be the focus of school mathematics” (NCTM, 1980, p. 1). It concluded with the publication of Everybody Counts (National Research Council, 1989) and the Curriculum and Evaluation Standards for School Mathematics (NCTM, 1989), both of which emphasize problem solving. One might infer, then, that there is general acceptance of the idea that the primary goal of mathematics instruction should be to have students become competent problem solvers. Yet, given the multiple interpretations of the term, the goal is hardly clear. Equally unclear is the role that problem solving, once adequately characterized, should play in the larger context of school mathematics. What are the goals for mathematics instruction, and how does problem solving fit within those goals? Such questions are complex. Goals for mathematics instruction depend on one’s conceptualization of what mathematics is, and what it means to understand mathematics. Such conceptualizations vary widely. At one end of the spectrum, mathematical knowledge is seen as a body of facts and procedures dealing with quantities, magnitudes, and forms, and the relationships among them; knowing mathematics is seen as having mastered these facts and procedures. At the other end of the spectrum, mathematics is conceptualized as the “science of patterns,” an (almost) empirical discipline closely akin to the sciences in its emphasis on pattern-seeking on the basis of empirical evidence. The author’s view is that the former perspective trivializes mathematics; that a curriculum based on mastering a corpus of mathematical facts and procedures is severely impoverished—in much the same way that an English curriculum would be considered impoverished if it focused largely, if not exclusively, on issues of grammar. The author characterizes the mathematical enterprise as follows:",2009,239,2890,266,123,144,163,155,189,196,226,163,163,178
6b7f08a8a3950a7df36c3a7cfe56a9a35d6daa4f,"Abstract : Even today, many complex and important skills, such as those required for language use and social interaction, are learned informally through apprenticeshiplike methods -- i.e., methods involving not didactic teaching, but observation, coaching, and successive approximation while carrying out a variety of tasks and activities. The differences between formal schooling and apprenticeship methods are many, but for our purposes, one is most important. Perhaps as a by-product of the specialization of learning in schools, skills and knowledge taught in schools have become abstracted from their uses in the world. In apprenticeship learning, on the other hand, target skills are not only continually in use by skilled practitioners, but are instrumental to the accomplishment of meaningful tasks. Said differently, apprenticeship embeds the learning of skills and knowledge in the social and functional context of their use. This difference is not academic, but has serious implications for the nature of the knowledge that students acquire. This paper attempts to elucidate some of those implications through a proposal for the retooling of apprenticeship methods for the teaching and learning of cognitive skills. Specifically, we propose the development of a new cognitive apprenticeship to teach students the thinking and problem-solving skills involved in school subjects such as reading, writing and mathematics.",1988,24,4583,154,10,23,28,29,66,81,73,87,103,95
f2e713dd0a1ee11892a90e0fb448dd5981e63550,,2000,7,7982,20,159,178,250,232,270,267,306,357,411,430
ebc5da28ca50a224960fb97a5e617c5fdcb4e92a,"This edited collection describes how the Autonomous Learning Behaviours (ALB) model, formulated by Fennema and Peterson, specifically relates to gender differences in mathematics education, learning and performance. The book provides a background to the debate on gender differences; considers the interactions between internal beliefs and external influences, as well as their effects on learning math; and provides a summary of the latest research relevant to the ALB model. Gender differences in learning mathematics is examined from a variety of perspectives, strengthened by longitudinal studies and a cross-cultural American and Australian perspective..",1990,0,211,3,0,0,4,4,7,14,29,8,8,4
7f3b042c8337fe9195ed6ca0cb017b76bbf1ff7c,"868 NOTICES OF THE AMS VOLUME 47, NUMBER 8 In April 2000 the National Council of Teachers of Mathematics (NCTM) released Principles and Standards for School Mathematics—the culmination of a multifaceted, three-year effort to update NCTM’s earlier standards documents and to set forth goals and recommendations for mathematics education in the prekindergarten-through-grade-twelve years. As the chair of the Writing Group, I had the privilege to interact with all aspects of the development and review of this document and with the committed groups of people, including the members of the Writing Group, who contributed immeasurably to this process. This article provides some background about NCTM and the standards, the process of development, efforts to gather input and feedback, and ways in which feedback from the mathematics community influenced the document. The article concludes with a section that provides some suggestions for mathematicians who are interested in using Principles and Standards.",2000,17,2165,296,10,25,47,49,66,78,79,118,115,118
a3b7e80260891dcd3844b1835df8dee3a1cd67c7,"L'A. propose quelques observations concernant l'ouvrage de Stanislas Dehaene The number sense. How the mind creates mathematics (1997) qui explore tous les aspects de la relation entre les hommes et les nombres : la numerosite chez les autres animaux, la numerosite et le calcul simple chez les bebes, l'histoire de l'expression du nombre dans le langage, l'histoire de la notation du nombre, le circuit neuronal necessaire pour faire de l'arithmetique et du calcul, la localisation dans le cerveau, l'ordre mathematique de l'univers, etc ... L'A. examine ici en particulier les questions portant sur la relation entre les nombres et le langage dans une perspective cognitive, puis explique ce que Dehaene entend par le sens du nombre en caracterisant les mathematiques comme une formalisation progressive de nos intuitions sur les ensembles, le nombre, l'espace, le temps et la logique",1998,0,2213,227,4,14,21,28,32,48,41,66,55,68
c9c852bc0e8774336734ff339c1592b2cf2d849f,,1982,0,3802,11,14,14,16,26,18,19,20,30,26,20
843632163541323571f14e92064bfb19a05f8f8d,,1992,0,3467,20,8,14,22,34,57,62,68,73,69,80
86b05bc7e953e683fa839ad01d6100a8f99558df,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",1991,0,2502,113,5,15,20,26,27,27,24,43,37,42
932ebbd6ae4b1c90dd85d3b38a59bfa610ea465b,"Foreword by Dennis Sparks Acknowledgments About the Authors Introduction What Has Happened Since the First and Second Editions The Enduring Challenges of Professional Development Carrying on Susan Loucks-Horsley's Work Purpose of the Book Changes in the Third Edition The Audience for This Book Organization of the Book How to Use This Book Values Shared by the Authors 1. A Framework for Designing Professional Development Inputs Into the Design Process The Design and Implementation Process 2. Knowledge and Beliefs Supporting Effective Professional Development Learners and Learning Teachers and Teaching The Nature of Science and Mathematics Adult Learning and Professional Development The Change Process 3. Context Factors Influencing Professional Development Students and Their Learning Needs Teachers and Their Learning Needs Curriculum, Instruction, Assessment Practices, and the Learning Environment Organizational Culture and Professional Learning Communities Leadership National, State, and Local Policies Available Resources Families and Communities Resources for Investigating Context 4. Critical Issues to Consider in Designing Professional Development Building Capacity for Sustainability Making Time for Professional Development Developing Leadership Ensuring Equity Building a Professional Learning Culture Garnering Public Support Scaling Up 5. Strategies for Professional Learning Selecting Strategies for a Professional Development Structures A Repertoire of Stratgies for Professional Learning 6. The Design Framework in Action Tapping the Knowledge Bases, Framing Beliefs: ""We Stood on the Shoulders of Giants"" Knowledge and Beliefs About the Nature of Learning and Teaching Mathematics and Science Equity Matters: ""All Humans Are Educable"" Knowledge and Beliefs About Teachers Knowledge of Effective Professional Development Knowledge of the Change Process Reflect and Revise: Experience as a Source of Knowledge Making Compromises Context The Professional Development Design Process Design Framework in Action: Cases References Index",1997,0,2108,142,0,8,14,32,41,48,61,64,91,107
73d73133142b945c362ff3d209bba7058af54622,Part 1 Hypergeometric Functions and Elliptic Integrals: Some Basic Topics In Analytical Dynamics The Problem of Two Bodies Two-Body Orbits and the Initial-Value Problem Solving Kepler's Equation Two-Body Orbital Boundary Value Problem Solving Lambert's Problem Appendices. Part 2 Non-Keplerian Motion: Patched-Conic Orbits and Perturbation Methods Variation of Parameters Two-Body Orbital Transfer Numerical Integration of Differential Equations The Celestial Position Fix Space Navigation Appendices.,1987,0,2020,203,3,4,11,12,17,13,13,18,18,13
6555e4f1ca5bcb835cc77b45b7cdb930fb4f5bf0,This book will be released simultaneously with Release 2.0 of Mathematica and will cover all the new features of Release 2.0. This new edition maintains the format of the original book and is the single most important user guide and reference for Mathematica--all users of Mathematica will need this edition. Includes 16 pages of full-color graphics.,1991,0,2566,65,53,108,158,177,205,226,232,186,145,114
a9386eb6808b41238381c708f2642bcb7dc34b29,"This book is about mathematical ideas, about what mathematics means-and why. Abstract ideas, for the most part, arise via conceptual metaphor-metaphorical ideas projecting from the way we function in the everyday physical world. Where Mathematics Comes From argues that conceptual metaphor plays a central role in mathematical ideas within the cognitive unconscious-from arithmetic and algebra to sets and logic to infinity in all of its forms.",2002,40,1919,107,19,40,48,55,72,74,102,91,120,102
fcd0ed6947c8aafe2ec5d971323176b2001a8af7,,1981,0,2409,83,1,5,14,42,71,84,104,127,127,116
fce6d3a85f28288f5d55760ef7575ffda7732e78,,1995,0,2069,85,2,4,4,5,8,6,7,10,18,21
a6891af0e04724965532095dec1b8198b943e31e,"PART A: ORDINARY DIFFERENTIAL EQUATIONS (ODE'S). Chapter 1. First-Order ODE's. Chapter 2. Second Order Linear ODE's. Chapter 3. Higher Order Linear ODE's. Chapter 4. Systems of ODE's Phase Plane, Qualitative Methods. Chapter 5. Series Solutions of ODE's Special Functions. Chapter 6. Laplace Transforms. PART B: LINEAR ALGEBRA, VECTOR CALCULUS. Chapter 7. Linear Algebra: Matrices, Vectors, Determinants: Linear Systems. Chapter 8. Linear Algebra: Matrix Eigenvalue Problems. Chapter 9. Vector Differential Calculus: Grad, Div, Curl. Chapter 10. Vector Integral Calculus: Integral Theorems. PART C: FOURIER ANALYSIS, PARTIAL DIFFERENTIAL EQUATIONS. Chapter 11. Fourier Series, Integrals, and Transforms. Chapter 12. Partial Differential Equations (PDE's). Chapter 13. Complex Numbers and Functions. Chapter 14. Complex Integration. Chapter 15. Power Series, Taylor Series. Chapter 16. Laurent Series: Residue Integration. Chapter 17. Conformal Mapping. Chapter 18. Complex Analysis and Potential Theory. PART E: NUMERICAL ANALYSIS SOFTWARE. Chapter 19. Numerics in General. Chapter 20. Numerical Linear Algebra. Chapter 21. Numerics for ODE's and PDE's. PART F: OPTIMIZATION, GRAPHS. Chapter 22. Unconstrained Optimization: Linear Programming. Chapter 23. Graphs, Combinatorial Optimization. PART G: PROBABILITY STATISTICS. Chapter 24. Data Analysis: Probability Theory. Chapter 25. Mathematical Statistics. Appendix 1: References. Appendix 2: Answers to Odd-Numbered Problems. Appendix 3: Auxiliary Material. Appendix 4: Additional Proofs. Appendix 5: Tables. Index.",1974,0,2416,145,5,1,3,3,2,6,2,3,4,4
eb6cfa0c51dce24ca6eb40d5072f4488f0931f2b,"This paper sets forth a way of interpreting mathematics classrooms that aims to account for how students develop mathematical beliefs and values and, consequently, how they become intellectually autonomous in mathematics. To do so, we advance the notion of sociomathematical norms, that is, normative aspects of mathematical discussions that are specific to students' mathematical activity. The explication of sociomathematical norms extends our previous work on general classroom social norms that sustain inquiry-based discussion and argumentation. Episodes from a second-grade classroom where mathematics instruction generally followed an inquiry tradition are used to clarify the processes by which sociomathematical norms are interactively constituted and to illustrate how these norms regulate mathematical argumentation and influence learning opportunities for both the students and the teacher. In doing so, we both clarify how students develop a mathematical disposition and account for students' development of increasing intellectual autonomy in mathematics. In the process, the teacher's role as a representative of the mathematical community is elaborated. For the past several years, we have been engaged in a research and development project at the elementary school level that has both pragmatic and theoretical goals. On one hand, we wish to support teachers as they establish classroom environments that facilitate students' mathematical conceptual development. On the other hand, we wish to investigate children's mathematical learning in the classroom. The latter involves developing perspectives that are useful for interpreting and attempting to make sense of the complexity of classroom life. The purpose of this paper is to set forth a way of interpreting classroom life that aims to account for how students develop specific mathematical beliefs and values and, consequently, how they become intellectually autonomous in mathematics, that is, how they come to develop a mathematical disposition (National Council of Teachers of Mathematics, 1991). To that end, we focus on classroom norms that we call sociomathematical norms. These norms are distinct from general classroom social norms in that they are specific to the mathematical aspects of students' activity. As a means of introducing and elaborating the theoretical discussion in this paper, we present episodes from a classroom that we have studied extensively. The episodes have been",1996,21,1617,112,1,8,7,12,22,28,46,33,50,44
2ed2cd231aad7ea8a2b69a60d6df7539d6ee107f,"Vol. 72: The Syntax and Semantics of lnfimtary Languages. Edited by J. Barwtse. IV, 268 pages. 1968. DM 18,I $ 5.00 Vol. 73: P. E. Conner, Lectures on the Action of a Finite Group. IV, 123 pages. 1968. DM 10,1 $ 2.80 Vol. 74:A Frohlich, Formal Groups. IV, 140pages. 1968. DM12, -I $3.30 Vol. 75: G. Lumer, Algebras de fonctions et espaces de Hardy. VI, 80 pages. 1968. DM 8,I $ 2. 20 Vol. 76: R. G. Swan, Algebraic K-Theory. IV, 262 pages. 1968. DM18,I$ 5.00",2001,497,1653,59,73,89,93,103,106,99,152,152,126,57
e0fa9cec8a16f31f481c9c02d869b2311e079fc0,"Results of 151 studies were integrated by meta-analysis to scrutinize the construct mathematics anxiety. Mathematics anxiety is related to poor performance on mathematics achievement tests. It relates inversely to positive attitudes toward mathematics and is bound directly to avoidance of the subject. Variables that exhibit differential mathematics anxiety levels include ability, school grade level, and undergraduate fields of study, with preservice arithmetic teachers especially prone to mathematics anxiety. Females display higher levels than males. However, mathematics anxiety appears more strongly linked with poor performance and avoidance of mathematics in precollege males than females. A variety of treatments are effective in reducing mathematics anxiety. Improved mathematics performance consistently accompanies valid treatment.",1990,23,1456,279,1,3,4,7,9,3,10,7,10,6
f5cf6daa6eeb53008e3004a7ce70e0f015017a63,1 Introduction.- 2 Limit Process Expansions Applied to Ordinary Differential Equations.- 3 Multiple-Variable Expansion Procedures.- 4 Applications to Partial Differential Equations.- 5 Examples from Fluid Mechanics.- Author Index.,1969,0,2302,84,9,25,40,38,45,28,56,38,42,30
bbbff45c27dccd114818ef334075db0a34b0e4fa,"Reviewers have consistently concluded that males perform better on mathematics tests than females do. To make a refined assessment of the magnitude of gender differences in mathematics performance, we performed a meta-analysis of 100 studies. They yielded 254 independent effect sizes, representing the testing of 3,175,188 Ss. Averaged over all effect sizes based on samples of the general population, d was -0.05, indicating that females outperformed males by only a negligible amount. For computation, d was -0.14 (the negative value indicating superior performance by females). For understanding of mathematical concepts, d was -0.03; for complex problem solving, d was 0.08. An examination of age trends indicated that girls showed a slight superiority in computation in elementary school and middle school. There were no gender differences in problem solving in elementary or middle school; differences favoring men emerged in high school (d = 0.29) and in college (d = 0.32). Gender differences were smallest and actually favored females in samples of the general population, grew larger with increasingly selective samples, and were largest for highly selected samples and samples of highly precocious persons. The magnitude of the gender difference has declined over the years; for studies published in 1973 or earlier d was 0.31, whereas it was 0.14 for studies published in 1974 or later. We conclude that gender differences in mathematics performance are small. Nonetheless, the lower performance of women in problem solving that is evident in high school requires attention.",1990,104,1662,79,5,10,17,27,32,49,75,42,37,22
f008393f240508c1686a468b2c67371a7cdcb354,,2009,0,948,91,8,35,43,56,81,101,105,96,74,112
da1b7c3ece9e017b83cb7ab213673b2b89fd1134,,1989,0,1478,223,3,15,21,54,68,85,103,107,127,87
2c888d9d033439a2d0a414efe4228e9177618930,,1969,0,2361,5,3,3,4,7,8,6,9,12,11,7
813c3c045849f82cac2db2f26cee0ca306349f28,"Children's mathematical skills were considered in relation to executive functions. Using multiple measures-including the Wisconsin Card Sorting Task (WCST), dual-task performance, Stroop task, and counting span-it was found that mathematical ability was significantly correlated with all measures of executive functioning, with the exception of dual-task performance. Furthermore, regression analyses revealed that each executive function measure predicted unique variance in mathematics ability. These results are discussed in terms of a central executive with diverse functions (Shallice & Burgess, 1996) and with recent evidence from Miyake, et al. (2000) showing the unity and diversity among executive functions. It is proposed that the particular difficulties for children of lower mathematical ability are lack of inhibition and poor working memory, which result in problems with switching and evaluation of new strategies for dealing with a particular task. The practical and theoretical implications of these results are discussed, along with suggestions for task changes and longitudinal studies that would clarify theoretical and developmental issues related to executive functioning.",2001,79,1318,73,0,2,5,18,38,25,30,34,54,60
ac84c7e453c6848f5b873492e56f74d11bde71aa,"Introduction to applied mathematics , Introduction to applied mathematics , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1988,0,1739,76,18,29,33,39,51,55,59,50,53,72
deac6a43706ea11bdd9fb07a73b54a08f0c114fb,"Teachers and teacher educators interested in synthesizing their current practice with new mathematics standards will welcome this highly useful volume. Presented are cases of mathematics instruction drawn from research of nearly 500 classroom lessons. Readers will gain insight about how to foster a challenging, cognitively rich, and exciting classroom climate that propels students toward a richer understanding of mathematics.",2009,0,713,82,39,42,46,57,62,50,74,59,57,52
f65040c3aa910788931c27c73006c5f3bb1e7e85,,2008,10,1101,87,44,48,62,72,80,103,88,77,98,55
34fb621cf73b7bad25d77ff1229467a34f736474,"Children's number competencies over 6 time points, from the beginning of kindergarten to the middle of 1st grade, were examined in relation to their mathematics achievement over 5 later time points, from the end of 1st grade to the end of 3rd grade. The relation between early number competence and mathematics achievement was strong and significant throughout the study period. A sequential process growth curve model showed that kindergarten number competence predicted rate of growth in mathematics achievement between 1st and 3rd grades as well as achievement level through 3rd grade. Further, rate of growth in early number competence predicted mathematics performance level in 3rd grade. Although low-income children performed more poorly than their middle-income counterparts in mathematics achievement and progressed at a slower rate, their performance and growth were mediated through relatively weak kindergarten number competence. Similarly, the better performance and faster growth of children who entered kindergarten at an older age were explained by kindergarten number competence. The findings show the importance of early number competence for setting children's learning trajectories in elementary school mathematics.",2009,70,765,38,3,16,21,32,54,72,66,77,73,94
69727a18c999db42c1e0062dd75870b4751ecc90,,2002,0,1397,4,56,59,58,63,81,92,111,128,120,121
b9b73e52ce88f609ffe0a66760c92b8cea21715b,"Constructivist theory has been prominent in recent research on mathematics learning and has provided a basis for recent mathematics education reform efforts. Although constructivism has the potential to inform changes in mathematics teaching, it offers no particular vision of how mathematics should be taught; models of teaching based on constructivism are needed. Data are presented from a whole-class, constructivist teaching experiment in which problems of teaching practice required the teacher/researcher to explore the pedagogical implications of his theoretical (constructivist) perspectives. The analysis of the data led to the development of a model of teacher decision making with respect to mathematical tasks. Central to this model is the creative tension between the teacher's goals with regard to student learning and his responsibility to be sensitive and responsive to the mathematical thinking of the students.",1995,74,1276,163,8,14,8,13,18,18,22,41,25,39
c0f83e632def1b85df9e4f316b673d9eac05b3f1,"ion is not inevitable. In fact, shopkeepers have never abstracted the structures of modules which regulate their financial exchanges because they haven’t the motivation to do so. Schematization and formulation The schematization and the formulation of the structure follows the process of identification and of updating. Diénès does not plan any general situations specific to this stage (would something which is well conceived of spell itself out clearly?) but the representation by a graph is often envisaged as a simplified but natural and direct expression of the thought of a child. Representing objects by points and operators by arrows is learned by the use of imitation, like a language. Symbolization Symbolization is the transcription in a new language of properties represented in the preceding stage.",1997,50,1182,186,1,2,5,9,11,15,20,30,53,48
c62e9c1114644b601296d860dd643ff86473071b,"Studies of teachers’ use of mathematics curriculum materials are particularly timely given the current availability of reform-inspired curriculum materials and the increasingly widespread practice of mandating the use of a single curriculum to regulate mathematics teaching. A review of the research on mathematics curriculum use over the last 25 years reveals significant variation in findings and in theoretical foundations. The aim of this review is to examine the ways that central constructs of this body of research—such as curriculum use, teaching, and curriculum materials—are conceptualized and to consider the impact of various conceptualizations on knowledge in the field. Drawing on the literature, the author offers a framework for characterizing and studying teachers’ interactions with curriculum materials.",2005,119,998,143,2,9,21,37,52,49,56,50,64,78
06bdbd4f011e9497fddfc47654116feab28b63bd,"Between 5% and 8% of school-age children have some form of memory or cognitive deficit that interferes with their ability to learn concepts or procedures in one or more mathematical domains. A review of the arithmetical competencies of these children is provided, along with discussion of underlying memory and cognitive deficits and potential neural correlates. The deficits are discussed in terms of three subtypes of mathematics learning disability and in terms of a more general framework for linking research in mathematical cognition to research in learning disabilities.",2004,97,1067,158,5,27,28,49,42,51,68,57,83,95
feeef8d91cec3a9ffe5cf135a36a7f8596426ae1,"Amid ongoing public speculation about the reasons for sex differences in careers in science and mathematics, we present a consensus statement that is based on the best available scientific evidence. Sex differences in science and math achievement and ability are smaller for the mid-range of the abilities distribution than they are for those with the highest levels of achievement and ability. Males are more variable on most measures of quantitative and visuospatial ability, which necessarily results in more males at both high- and low-ability extremes; the reasons why males are often more variable remain elusive. Successful careers in math and science require many types of cognitive abilities. Females tend to excel in verbal abilities, with large differences between females and males found when assessments include writing samples. High-level achievement in science and math requires the ability to communicate effectively and comprehend abstract ideas, so the female advantage in writing should be helpful in all academic domains. Males outperform females on most measures of visuospatial abilities, which have been implicated as contributing to sex differences on standardized exams in mathematics and science. An evolutionary account of sex differences in mathematics and science supports the conclusion that, although sex differences in math and science performance have not directly evolved, they could be indirectly related to differences in interests and specific brain and cognitive systems. We review the brain basis for sex differences in science and mathematics, describe consistent effects, and identify numerous possible correlates. Experience alters brain structures and functioning, so causal statements about brain differences and success in math and science are circular. A wide range of sociocultural forces contribute to sex differences in mathematics and science achievement and ability—including the effects of family, neighborhood, peer, and school influences; training and experience; and cultural practices. We conclude that early experience, biological factors, educational policy, and cultural context affect the number of women and men who pursue advanced study in science and math and that these effects add and interact in complex ways. There are no single or simple answers to the complex questions about sex differences in science and mathematics.",2007,486,905,89,4,19,34,48,46,75,75,61,68,69
d4d7370670ffa790900ff05f96c208c9eda7d58b,"Graph theory models the Internet mathematically, and a number of plausible mathematically intersecting network models for the Internet have been developed and studied. Simultaneously, Internet researchers have developed methodology to use real data to validate, or invalidate, proposed Internet models. The authors look at these parallel developments, particularly as they apply to scale-free network models of the preferential attachment type.",2009,73,228,11,8,19,28,28,24,18,26,16,16,9
e38ac5b98197284537fca03d8592adaae48841d4,"To understand the difficulties that many students have with comprehension of mathematics, we must determine the cognitive functioning underlying the diversity of mathematical processes. What are the cognitive systems that are required to give access to mathematical objects? Are these systems common to all processes of knowledge or, on the contrary, some of them are specific to mathematical activity? Starting from the paramount importance of semiotic representation for any mathematical activity, we put forward a classification of the various registers of semiotic representations that are mobilized in mathematical processes. Thus, we can reveal two types of transformation of semiotic representations: treatment and conversion. These two types correspond to quite different cognitive processes. They are two separate sources of incomprehension in the learning of mathematics. If treatment is the more important from a mathematical point of view, conversion is basically the deciding factor for learning. Supporting empirical data, at any level of curriculum and for any area of mathematics, can be widely and methodologically gathered: some empirical evidence is presented in this paper.",2006,28,900,111,10,12,20,25,59,41,61,55,62,91
952a90c12294b254aac85b35b0b15e6e34736aaa,"If looking for the ebook Implementing Mathematics with The Nuprl Proof Development System by R L Constable in pdf form, in that case you come on to the right site. We present full variant of this ebook in DjVu, PDF, ePub, doc, txt forms. You can reading Implementing Mathematics with The Nuprl Proof Development System online by R L Constable either downloading. Additionally to this ebook, on our website you may read guides and different art eBooks online, either download their. We wish to invite your note what our site does not store the book itself, but we grant url to website wherever you may load either reading online. So that if want to load pdf Implementing Mathematics with The Nuprl Proof Development System by R L Constable, then you have come on to correct website. We have Implementing Mathematics with The Nuprl Proof Development System txt, ePub, PDF, doc, DjVu forms. We will be pleased if you go back afresh.",1986,180,1451,79,10,13,34,33,42,47,56,66,69,62
786364fdd5a792fec5c5aaf23b87acbc4b57818b,"Abstract This study examines changes in teachers’ thinking as they participated in a video club designed to help them learn to notice and interpret students’ mathematical thinking. First, we investigate changes in teachers’ talk about classroom video segments before and after participation in the video club. Second, we identify three paths along which teachers learned to notice students’ mathematical thinking in this context: Direct, Cyclical, and Incremental. Finally, we explore ways the video club context influenced teacher learning. Understanding different forms of teacher learning provides insight for research on teacher cognition and may inform the design of video-based professional development.",2008,96,735,68,7,12,19,36,34,38,64,82,69,79
ab91f8b2372ec432d8f93c86b545cd9729446291,,1978,0,1478,112,0,0,1,3,3,4,5,8,6,6
7d25dbb45d8e5a12c2a12bab6ce7561752a045ca,"This survey provides a brief and selective overview of research in the philosophy of mathematics education. It asks what makes up the philosophy of mathematics education, what it means, what questions it asks and answers, and what is its overall importance and use? It provides overviews of critical mathematics education, and the most relevant modern movements in the philosophy of mathematics. A case study is provided of an emerging research tradition in one country. This is the Hermeneutic strand of research in the philosophy of mathematics education in Brazil. This illustrates one orientation towards research inquiry in the philosophy of mathematics education. It is part of a broader practice of ‘philosophical archaeology’: the uncovering of hidden assumptions and buried ideologies within the concepts and methods of research and practice in mathematics education. An extensive bibliography is also included.",1991,213,1180,142,3,10,14,18,26,27,41,34,22,28
4e034faaae5aa8821082af33da6da46925b12fb3,"Abstract The decomposition method can be an effective procedure for analytical solution of a wide class of dynamical systems without linearization or weak nonlinearity assumptions, closure approximations, perturbation theory, or restrictive assumptions on stochasticitiy.",1988,84,1326,97,0,1,0,1,0,1,1,2,1,2
4eee6b36dbe0e204d0fdbdae7cb36dcaf569d1d9,,1992,0,1195,140,1,5,7,7,5,16,11,24,25,19
e7a5ff509962afa8850e1e682ebc224018366054,"Although it is often assumed that abilities that reflect basic numerical understanding, such as numerical comparison, are related to children's mathematical abilities, this relationship has not been tested rigorously. In addition, the extent to which symbolic and nonsymbolic number processing play differential roles in this relationship is not yet understood. To address these questions, we collected mathematics achievement measures from 6- to 8-year-olds as well as reaction times from a numerical comparison task. Using the reaction times, we calculated the size of the numerical distance effect exhibited by each child. In a correlational analysis, we found that the individual differences in the distance effect were related to mathematics achievement but not to reading achievement. This relationship was found to be specific to symbolic numerical comparison. Implications for the role of basic numerical competency and the role of accessing numerical magnitude information from Arabic numerals for the development of mathematical skills and their impairment are discussed.",2009,42,587,57,20,21,40,52,53,63,70,75,63,43
cd3a45bdd2c0ebdcfe4f2353da16c8c11ae5cb7f,"Boston College is an equal opportunity, affi rmative action employer.",2004,0,1142,63,2,15,33,42,62,78,86,89,152,109
142dad2d2e8e5060eb9561b7744cbc5b4bb90990,"From the Publisher: 
This text is designed for the sophomore/junior level introduction to discrete mathematics taken by students preparing for future coursework in areas such as math,computer science and engineering. Rosen has become a bestseller largely due to how effectively it addresses the main portion of the discrete market,which is typically characterized as the mid to upper level in rigor. The strength of Rosen's approach has been the effective balance of theory with relevant applications,as well as the overall comprehensive nature of the topic coverage.",1984,2,1330,50,0,0,0,0,0,0,1,2,3,6
069ca4d3ce5fdcdad511ec6cb1a6bb5fccf1f298,"The aim of this paper is to obtain an effective rule (or algorithm) for distinguishing sentences from nonsentences, which works not only for the formal languages of interest to the mathematical logician, but also for natural languages such as English, or at least for fragments of such languages. An attempt to formulate such an algorithm is implicit in the work of Ajdukiewicz (1935). His method, later elaborated by Bar-Hillel (1953), depends on a kind of arithmetization of the so-called parts of speech, here called syntactic types.",1958,23,1499,136,0,1,2,0,2,1,1,1,2,2
1f80d103e387aeaa3513078745c41027b0f0f2a2,"Reprinted with permission from the Fall 2005 issue of American Educator, the quarterly journal of the American Federation of Teachers, AFL-CIO.",2005,17,898,78,3,9,28,37,43,65,50,68,88,91
fd3163724c815360c80e71b8e839b8cbfe567e2b,"We used structural modeling procedures to assess the influence of past math grades, math ability perceptions, performance expectancies, and value perceptions on the level of math anxiety reported in a sample of 7th- through 9th-grade students (N = 250). A second set of analyses examined the relative influence of these performance, self-perception, and affect variables on students' subsequent grades and course enrollment intentions in mathematics. The findings indicated that math anxiety was most directly related to students' math ability perceptions, performance expectancies, and value perceptions. Students' performance expectancies predicted subsequent math grades, whereas their value perceptions predicted course enrollment intentions. Math anxiety did not have significant direct effects on either grades or intentions. The findings also suggested that the pattern of relations are similar for boys and girls. The results are discussed in relation to expectancy-value and self-efficacy theories of academic achievement. A strong background in mathematics is critical for many career and job opportunities in today's increasingly technological society. However, many academically capable students prematurely restrict their educational and career options by discontinuing their mathematical training early in high school. Several recent surveys (National Assessment of Educational Progress [NAEP], 1988; National Center for Educational Statistics [NCES], 1984) indicate that only half of all high school graduates enroll in mathematics courses beyond the 10th grade. These reports also indicate that fewer women than men enroll in the more advanced courses in high school mathematics (NAEP, 1988; NCES, 1984), although the ""gender gap"" is beginning to narrow (Chipman & Thomas, 1985; Eccles, 1987). Furthermore, students of both sexes, but particularly women, do not attain a high level of mathematical competency, even if they have completed 4 years of high school math (NAEP, 1988).",1990,72,1231,41,1,1,2,5,13,12,41,16,8,14
0c24dd1033bc83cdb51c18345bc2ad8866cbca03,,1964,0,1306,195,0,0,1,1,0,0,2,0,1,0
fa96245255f18739dab9e70cc4ae5dda0765b509,"There is a story about two friends, who were classmates in high school, talking about their jobs. One of them became a statistician and was working on population trends. He showed a reprint to his former classmate. The reprint started, as usual, with the Gaussian distribution and the statistician explained to his former classmate the meaning of the symbols for the actual population, for the average population, and so on. His classmate was a bit incredulous and was not quite sure whether the statistician was pulling his leg. “How can you know that?” was his query. “And what is this symbol here?” “Oh,” said the statistician, “this is π.” “What is that?” “The ratio of the circumference of the circle to its diameter.” “Well, now you are pushing your joke too far,” said the classmate, “surely the population has nothing to do with the circumference of the circle.”",1960,11,1647,38,0,1,1,0,0,1,1,1,0,0
a16f7f1fe98951487b7b83097b47f43f9e83ac1c,"Early childhood mathematics is vitally important for young children's present and future educational success. Research demonstrates that virtually all young children have the capability to learn and become competent in mathematics. Furthermore, young children enjoy their early informal experiences with mathematics. Unfortunately, many children's potential in mathematics is not fully realized, especially those children who are economically disadvantaged. This is due, in part, to a lack of opportunities to learn mathematics in early childhood settings or through everyday experiences in the home and in their communities. Improvements in early childhood mathematics education can provide young children with the foundation for school success. Relying on a comprehensive review of the research, Mathematics Learning in Early Childhood lays out the critical areas that should be the focus of young children's early mathematics education, explores the extent to which they are currently being incorporated in early childhood settings, and identifies the changes needed to improve the quality of mathematics experiences for young children. This book serves as a call to action to improve the state of early childhood mathematics. It will be especially useful for policy makers and practitioners-those who work directly with children and their families in shaping the policies that affect the education of young children.",2009,0,439,33,4,10,17,26,46,38,56,52,34,53
e2ce8e9f362fb1caf22cf5b7ad038dc9753c1190,"In this article we discuss efforts to design and empirically test measures of teachers’ content knowledge for teaching elementary mathematics. We begin by reviewing the literature on teacher knowledge, noting how scholars have organized such knowledge. Next we describe survey items we wrote to represent knowledge for teaching mathematics and results from factor analysis and scaling work with these items. We found that teachers’ knowledge for teaching elementary mathematics was multidimensional and included knowledge of various mathematical topics (e.g., number and operations, algebra) and domains (e.g., knowledge of content, knowledge of students and content). The constructs indicated by factor analysis formed psychometrically acceptable scales.",2004,37,896,68,2,5,12,30,56,37,52,48,71,93
99eaa4a03d94547bfb294460ce78ff9c753e0c2b,"Using contemporary data from the U.S. and other nations, we address 3 questions: Do gender differences in mathematics performance exist in the general population? Do gender differences exist among the mathematically talented? Do females exist who possess profound mathematical talent? In regard to the first question, contemporary data indicate that girls in the U.S. have reached parity with boys in mathematics performance, a pattern that is found in some other nations as well. Focusing on the second question, studies find more males than females scoring above the 95th or 99th percentile, but this gender gap has significantly narrowed over time in the U.S. and is not found among some ethnic groups and in some nations. Furthermore, data from several studies indicate that greater male variability with respect to mathematics is not ubiquitous. Rather, its presence correlates with several measures of gender inequality. Thus, it is largely an artifact of changeable sociocultural factors, not immutable, innate biological differences between the sexes. Responding to the third question, we document the existence of females who possess profound mathematical talent. Finally, we review mounting evidence that both the magnitude of mean math gender differences and the frequency of identification of gifted and profoundly gifted females significantly correlate with sociocultural factors, including measures of gender equality across nations.",2009,56,434,21,8,16,22,30,40,36,43,24,52,60
3c6f487f79b23dcf2149b994d2eed3fc9f29795b,"This special issue of Mathematical Structures in Computer Science contains several contributions related to the modern field of Quantum Information and Quantum Computing. The first two papers deal with entanglement. The paper by R. Mosseri and P. Ribeiro presents a detailed description of the two- and three-qubit geometry in Hilbert space, dealing with the geometry of fibrations and discrete geometry. The paper by J.-G.Luque et al. is more algebraic and considers invariants of pure k-qubit states and their application to entanglement measurement.",2007,0,15206,2014,3,3,13,65,237,907,1102,1054,1100,1035
4663443d3db471873a251158a4a2ca36c9d99682,List of symbols 1. Atoms in chemistry 2. Atoms and the topology of the charge desnity 3. Molecular structure and its change 4. Mathematical models of structural change 5. The quantum atom 6. The mechanics of an atom in a molecule 7. Chemical models and the Laplacian of the charge density 8. The action principle for a quantunm subsystem Appendix - Tables of data Index,1990,0,10224,425,0,0,0,0,1,0,38,88,110,117
8cec28449f2f21e79c46b0e51fe9f8669c5c500e,"In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.",1935,3,10388,281,1,0,0,0,0,0,0,0,0,0
ea2c534b47a7e7203687c86c0c1d423dd79bacba,"Measurements of the quantum yields of chlorophyll fluorescence and CO2 assimilation for a number of plant species exposed to changing light intensity and atmospheric CO2 concentrations and during induction of photosynthesis are used to examine the relationship between fluorescence quenching parameters and the quantum yield of non-cyclic electron transport. Over a wide range of physiological conditions the quantum yield of non-cyclic electron transport was found to be directly proportional to the product of the photochemical fluorescence quenching (qQ) and the efficiency of excitation capture by open Photosystem II (PS II) reaction centres (Fv/Fm). A simple fluorescence parameter, ΔφF/φFm, which is defined by the difference in fluorescence yield at maximal φFm, and steady-state φFs, divided by φFm, can be used routinely to estimate changes in the quantum yield of non-cyclic electron transport. It is demonstrated that both the concentration of open PS II reaction centres and the efficiency of excitation capture by these centres will determine the quantum yield of non-cyclic electron transport in vivo and that deactivation of excitation within PS II complexes by non-photochemical processes must influence the quantum yield of non-cyclic electron transport.",1989,14,7275,662,3,34,23,42,58,71,94,102,95,138
2c83abe9ebac55cd2d6287636782ad81f8c1d444,"This book presents a comprehensive review of the subject of gravitational effects in quantum field theory. Although the treatment is general, special emphasis is given to the Hawking black hole evaporation effect, and to particle creation processes in the early universe. The last decade has witnessed a phenomenal growth in this subject. This is the first attempt to collect and unify the vast literature that has contributed to this development. All the major technical results are presented, and the theory is developed carefully from first principles. Here is everything that students or researchers will need to embark upon calculations involving quantum effects of gravity at the so-called one-loop approximation level.",1984,10,7315,594,27,29,34,54,64,92,69,58,94,131
f101fd5c5d711289e2904e37e6aebf7bcb893b60,Preface 1. Elements of probability theory 2. Random (or stochastic) processes 3. Some useful mathematical techniques 4. Second-order coherence theory of scalar wavefields 5. Radiation from sources of any state of coherence 6. Second-order coherence theory of vector electromagnetic fields 7. Some applications of second-order coherence theory 8. Higher-order correlations in optical fields 9. Semiclassical theory of photoelectric detection of light 10. Quantization of the free electromagnetic field 11. Coherent states of the electromagnetic field 12. Quantum correlations and photon statistics 13. Radiation from thermal equilibrium sources 14. Quantum theory of photoelectric detection of light 15. Interaction between light and a two-level atom 16. Collective atomic interactions 17. Some general techniques for treating interacting systems 18. The single-mode laser 19. The two-mode ring laser 20. The linnear light amplifier 21. Squeezed states of light 22. Some quantum effects in nonlinear optics References Author index Subject index.,1995,0,6759,506,7,33,62,114,138,159,207,241,286,304
b7cb7c0bbd8311919425187b1bb7d1221d8ac3ba,PREFACE ACKNOWLEDGEMENTS PART 1: PROBABILITY IN CLASSICAL AND QUANTUM MECHANICS 1. Classical probability theory and stochastic processes 2. Quantum Probability PART 2: DENSITY MATRIX THEORY 3. Quantum Master Equations 4. Decoherence PART 3: STOCHASTIC PROCESSES IN HILBERT SPACE 5. Probability distributions on Hilbert space 6. Stochastic dynamics in Hilbert space 7. The stochastic simulation method 8. Applications to quantum optical systems PART 4: NON-MARKOVIAN QUANTUM PROCESSES 9. Projection operator techniques 10. Non-Markovian dynamics in physical systems PART 5: RELATIVISTIC QUANTUM PROCESSES 11. Measurements in relativistic quantum mechanics 12. Open quantum electrodynamics,2002,0,4450,451,10,36,38,69,87,148,165,198,299,250
9e39731787bbf69a1a767b9bff08ae6b9fd660aa,"This book is a modern pedagogic introduction to the ideas and techniques of quantum field theory. After a brief overview of particle physics and a survey of relativistic wave equations and Lagrangian methods, the quantum theory of scalar and spinor fields, and then of gauge fields, is developed. The emphasis throughout is on functional methods, which have played a large part in modern field theory. The book concludes with a brief survey of 'topological' objects in field theory and, new to this edition, a chapter devoted to supersymmetry.",1956,0,7257,360,0,2,7,3,4,5,6,6,5,13
c5b1324b388e12d6c856460b955b3124d075e386,"QUANTUM ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling, based on density-functional theory, plane waves, and pseudopotentials (norm-conserving, ultrasoft, and projector-augmented wave). The acronym ESPRESSO stands for opEn Source Package for Research in Electronic Structure, Simulation, and Optimization. It is freely available to researchers around the world under the terms of the GNU General Public License. QUANTUM ESPRESSO builds upon newly-restructured electronic-structure codes that have been developed and tested by some of the original authors of novel electronic-structure algorithms and applied in the last twenty years by some of the leading materials modeling groups worldwide. Innovation and efficiency are still its main focus, with special attention paid to massively parallel architectures, and a great effort being devoted to user friendliness. QUANTUM ESPRESSO is evolving towards a distribution of independent and interoperable codes in the spirit of an open-source project, where researchers active in the field of electronic-structure calculations are encouraged to participate in the project by contributing their own codes or by implementing their own ideas into existing codes.",2009,483,9485,72,16,128,271,418,506,720,893,1025,1244,1045
04bcab2811cdb00ec1fe72a6061e50f23838d51d,Feynman Diagrams and Quantum Electrodynamics * Invitation: Pair Production in e+e- Annihilation * The Klein-Gordon Field * The Dirac Field * Interacting Fields and Feynman Diagrams * Elementary Processes of Quantum Electrodynamics * Radiative Corrections: Introduction * Radiative Corrections: Some Formal Developments * Final Project: Radiation of Gluon Jets Renormalization * Invitation: Ultraviolet Cutoffs and Critical Fluctuations * Functional Methods * Systematics of Renormalization * Renormalization and Symmetry * The Renormalization Group * Critical Exponents and Scalar Field Theory * Final Project: The Coleman-Weinberg Potential Non-Albelian Gauge Theory * Invitation: The Parton Model of Hadron Structure * Non-Albein Gauge Invariance * Quantization of Non-Abelian Gauge Theories * Quantum Chromodynamics * Operator Products and Effective Vertices * Perturbation Theory Anomalies * Gauge Theories with Spontaneous Symmetry Breaking * Quantization of Spontaneously Broken Gauge Theories * Final Project: Decays of the Higgs Boson * Epilogue: Field Theory at the Frontier,1995,0,4541,515,8,6,23,27,36,65,65,68,102,108
bc7505aea4e3bbad61df2837227d58c1af1762fd,"Quantum Mechanics for Organic Chemists.By Howard E. Zimmerman. Pp. x + 215. (Academic: New York and London, May 1975.) $16.50; £7.90.",1975,23,6608,575,4,6,1,4,3,4,6,3,3,9
d6431498449b1739f1d0397b6e79ddb7b31d5ffc,"A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time of at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.",1995,216,7075,325,0,8,22,72,92,108,118,144,147,155
6ebff330dc1907c3ff33c4532f3c0f007b2d6392,,1982,18,454,27,3,2,4,2,3,5,6,3,3,8
f2b196ffd931dfde6f021ab9ad67297147c2e0e3,The basic concepts of quantum mechanics Energy and momentum Schrodinger's equation Angular momentum Perturbation theory Spin The identity of particles The atom The theory of symmetry Polyatomic molecules Motion in a magnetic field Nuclear structure Elastic collisions Mathematical appendices.,1959,1,6522,364,1,2,6,6,1,2,11,9,5,7
f14f8f1470f0a95bf1157bb82da03f0efebcd774,,1965,0,7662,416,0,3,10,9,12,15,16,17,31,31
298d799da82395a64a3bda38ef9d2a4646828ccb,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",1996,26,5311,316,14,35,75,66,77,76,93,130,121,120
a5a22f4adc9ae7ca2e7a2025ec297d1f5aa6b250,"A new parametric quantum mechanical molecular model, AM1 (Austin Model l), based on the NDDO approximation, is described. In it the major weaknesses of MNDO, in particular failure to reproduce hydrogen bonds, have been overcome without any increase in computing time. Results for 167 molecules are reported. Parameters are currently available for C, H, 0, and N.",1985,1,11082,115,0,0,0,0,1,1,2,78,431,486
cf1925ab9276b43dfd6d2f7c2272eed263997d22,"It is shown that 2+1 dimensional quantum Yang-Mills theory, with an action consisting purely of the Chern-Simons term, is exactly soluble and gives a natural framework for understanding the Jones polynomial of knot theory in three dimensional terms. In this version, the Jones polynomial can be generalized fromS3 to arbitrary three manifolds, giving invariants of three manifolds that are computable from a surgery presentation. These results shed a surprising new light on conformal field theory in 1+1 dimensions.",1989,74,4032,284,44,129,143,126,129,106,107,89,75,76
60ea2eb503a73c2b084ffae580a726c092a1f821,"Fundamentals Survey of the Various Approaches Path Integral Description of Open Quantum Systems Imaginary-Time and Real-Time Approaches Influence Functional Method Phenomenological and Microscopic System-Plus-Reservoir Models Linear and Nonlinear Quantum Environments Ohmic, Super-Ohmic, and Sub-Ohmic Dissipation Quantum Decoherence and Relaxation Correlation Functions, Response Functions, and Fluctuation-Dissipation Theorem Damped Quantum Mechanical Harmonic Oscillator Quantum Brownian Motion Thermodynamic Variational Approach and Effective Potential Method Unified Approach to Quantum-Statistical Metastability: From Thermal Activation to Quantum Tunneling Electron Transfer and Incoherent Tunneling Macroscopic Quantum Effects in Josephson Systems Spin-Boson Model and Qubit Dissipative Two-State System: Thermodynamics and Dynamics Single-Charge and Cooper-Pair Tunneling Magnetic and Spin Tunneling Driven Quantum Tunneling Nonequilibrium Quantum Transport Full Counting Statistics Charge Transport in Quantum Impurity Systems Duality and Self-Duality.",1993,0,3360,380,3,12,29,34,46,48,57,60,86,116
a3e45ffd3886f1a26f849514de3791054eebcc42,"An unknown quantum state \ensuremath{\Vert}\ensuremath{\varphi}〉 can be disassembled into, then later reconstructed from, purely classical information and purely nonclassical Einstein-Podolsky-Rosen (EPR) correlations. To do so the sender, ``Alice,'' and the receiver, ``Bob,'' must prearrange the sharing of an EPR-correlated pair of particles. Alice makes a joint measurement on her EPR particle and the unknown quantum system, and sends Bob the classical result of this measurement. Knowing this, Bob can convert the state of his EPR particle into an exact replica of the unknown state \ensuremath{\Vert}\ensuremath{\varphi}〉 which Alice destroyed.",1993,9,9173,151,3,19,19,23,36,56,75,123,160,202
1cebe440b99bbcfab994eb9fce8f10e605a7d485,,1961,0,17019,70,0,0,0,0,0,0,0,0,0,1
bce3dbd69273ba6a9fa2024427bf7a849c7477c6,"When electrons are confined in two-dimensional materials, quantum-mechanically enhanced transport phenomena such as the quantum Hall effect can be observed. Graphene, consisting of an isolated single atomic layer of graphite, is an ideal realization of such a two-dimensional system. However, its behaviour is expected to differ markedly from the well-studied case of quantum wells in conventional semiconductor interfaces. This difference arises from the unique electronic properties of graphene, which exhibits electron–hole degeneracy and vanishing carrier mass near the point of charge neutrality. Indeed, a distinctive half-integer quantum Hall effect has been predicted theoretically, as has the existence of a non-zero Berry's phase (a geometric quantum phase) of the electron wavefunction—a consequence of the exceptional topology of the graphene band structure. Recent advances in micromechanical extraction and fabrication techniques for graphite structures now permit such exotic two-dimensional electron systems to be probed experimentally. Here we report an experimental investigation of magneto-transport in a high-mobility single layer of graphene. Adjusting the chemical potential with the use of the electric field effect, we observe an unusual half-integer quantum Hall effect for both electron and hole carriers in graphene. The relevance of Berry's phase to these experiments is confirmed by magneto-oscillations. In addition to their purely scientific interest, these unusual quantum transport phenomena may lead to new applications in carbon-based electronic and magneto-electronic devices.",2005,32,10144,71,1,21,257,379,416,630,838,892,843,845
2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea,"A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.<<ETX>>",1994,39,5368,192,7,33,58,57,77,65,89,93,117,115
f8dcc3047eef8da135bca13b926b1e6cf50e7f3a,Practical application of the generalized Bells theorem in the so-called key distribution process in cryptography is reported. The proposed scheme is based on the Bohms version of the Einstein-Podolsky-Rosen gedanken experiment and Bells theorem is used to test for eavesdropping. © 1991 The American Physical Society.,1991,0,7260,196,1,4,7,10,11,25,29,37,37,61
a13bd077717cb28cace6fd97c2fedeab8ba2449c,"Mathematical Foundations of Quantum Mechanics was a revolutionary book that caused a sea change in theoretical physics. Here, John von Neumann, one of the leading mathematicians of the twentieth century, shows that great insights in quantum physics can be obtained by exploring the mathematical structure of quantum mechanics. He begins by presenting the theory of Hermitean operators and Hilbert spaces. These provide the framework for transformation theory, which von Neumann regards as the definitive form of quantum mechanics. Using this theory, he attacks with mathematical rigor some of the general problems of quantum theory, such as quantum statistical mechanics as well as measurement processes. Regarded as a tour de force at the time of publication, this book is still indispensable for those interested in the fundamental issues of quantum mechanics.",1955,0,4318,294,0,1,0,2,2,5,3,9,3,11
d83672ce5828c207550e7c8732eb83a40bbe8255,,1971,6,4583,251,2,11,30,24,34,28,36,45,44,38
2ec88a1138f65a5e6f802547758c495ef69bbc79,1. Fundamental Concepts. 2. Quantum Dynamics. 3. Theory of Angular Momentum. 4. Symmetry in Quantum Mechanics. 5. Approximation Methods. 6. Identical Particles. 7. Scattering Theory. Appendices. Supplements. Bibliography. Index.,1986,0,3577,226,0,4,3,3,4,7,17,18,14,33
c61f19cafd1e2efd696756b5c5062369ca2c9bb5,,2009,0,6455,91,2,21,45,70,90,132,190,259,336,1058
3b94a093789cd4091a03b32117bf4aa11e1db1d1,"Nature abounds with phase transitions. The boiling and freezing of water are everyday examples of phase transitions, as are more exotic processes such as superconductivity and superfluidity. The universe itself is thought to have passed through several phase transitions as the high-temperature plasma formed by the big bang cooled to form the world as we know it today.",1999,0,3255,310,7,27,64,68,104,127,170,157,189,239
02d591c425864d9de9cfb31ef9c57e8d7bcac11f,"We study the effects of spin orbit interactions on the low energy electronic structure of a single plane of graphene. We find that in an experimentally accessible low temperature regime the symmetry allowed spin orbit potential converts graphene from an ideal two-dimensional semimetallic state to a quantum spin Hall insulator. This novel electronic state of matter is gapped in the bulk and supports the transport of spin and charge in gapless edge states that propagate at the sample boundaries. The edge states are nonchiral, but they are insensitive to disorder because their directionality is correlated with spin. The spin and charge conductances in these edge states are calculated and the effects of temperature, chemical potential, Rashba coupling, disorder, and symmetry breaking fields are discussed.",2004,0,4312,135,0,2,26,38,45,83,105,145,231,241
6b0f06617d9f5256a80ed62d9398acb92a55a6bd,"It is argued that underlying the Church–Turing hypothesis there is an implicit physical assertion. Here, this assertion is presented explicitly as a physical principle: ‘every finitely realizible physical system can be perfectly simulated by a universal model computing machine operating by finite means’. Classical physics and the universal Turing machine, because the former is continuous and the latter discrete, do not obey the principle, at least in the strong form above. A class of model computing machines that is the quantum generalization of the class of Turing machines is described, and it is shown that quantum theory and the 'universal quantum computer’ are compatible with the principle. Computing machines resembling the universal quantum computer could, in principle, be built and would have many remarkable properties not reproducible by any Turing machine. These do not include the computation of non-recursive functions, but they do include ‘quantum parallelism’, a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it. The intuitive explanation of these properties places an intolerable strain on all interpretations of quantum theory other than Everett’s. Some of the numerous connections between the quantum theory of computation and the rest of physics are explored. Quantum complexity theory allows a physically more reasonable definition of the ‘complexity’ or ‘knowledge’ in a physical system than does classical complexity theory.",1985,32,3805,175,3,6,7,7,7,42,5,15,19,28
16588533d90fd197572f249a51b78aedb2e46164,Current research into semiconductor clusters is focused on the properties of quantum dots-fragments of semiconductor consisting of hundreds to many thousands of atoms-with the bulk bonding geometry and with surface states eliminated by enclosure in a material that has a larger band gap. Quantum dots exhibit strongly size-dependent optical and electrical properties. The ability to join the dots into complex assemblies creates many opportunities for scientific discovery.,1996,27,9209,62,19,42,59,80,108,131,171,214,283,355
084f78140cb19f8e40d1cec105b7a5a198ba6879,6.2.2. Definition of Effective Properties 3064 6.3. Response Properties to Magnetic Fields 3066 6.3.1. Nuclear Shielding 3066 6.3.2. Indirect Spin−Spin Coupling 3067 6.3.3. EPR Parameters 3068 6.4. Properties of Chiral Systems 3069 6.4.1. Electronic Circular Dichroism (ECD) 3069 6.4.2. Optical Rotation (OR) 3069 6.4.3. VCD and VROA 3070 7. Continuum and Discrete Models 3071 7.1. Continuum Methods within MD and MC Simulations 3072,2005,24,10034,47,1,41,114,126,170,235,458,616,731,800
fb26f7e7e358d99079e670a0362c4b59d3e3652c,"Containing basic definitions and theorems as well as relations, tables of formulas and numerical tables which are essential for applications to many physical problems, the book is useful for specialists in nuclear and particle physics, atomic and molecular spectroscopy, plasma physics, collision and reaction theory, quantum chemistry, etc. The authors write many formulas in different coordinate systems. Each chapter opens with a list of its contents. New results relating to different aspects of the angular momentum theory are included. This book gathers together many useful formulas besides those related to angular momentum, and compares different notations used by previous authors.",1988,0,3281,184,19,17,24,29,36,25,38,58,60,82
89312d55997b8526db91a4506db41e754eb3a67f,The notion of a quantum dynamical semigroup is defined using the concept of a completely positive map. An explicit form of a bounded generator of such a semigroup onB(ℋ) is derived. This is a quantum analogue of the Lévy-Khinchin formula. As a result the general form of a large class of Markovian quantum-mechanical master equations is obtained.,1976,49,4537,150,8,13,14,14,14,9,12,9,16,17
c4af5a470de97629118fa19ef0ab6e086ae6c95a,Here is an introduction to the theory of quantum groups with emphasis on the spectacular connections with knot theory and Drinfeld's recent fundamental contributions. It presents the quantum groups attached to SL2 as well as the basic concepts of the theory of Hopf algebras. Coverage also focuses on Hopf algebras that produce solutions of the Yang-Baxter equation and provides an account of Drinfeld's elegant treatment of the monodromy of the Knizhnik-Zamolodchikov equations.,1994,0,4217,127,161,155,144,179,177,149,160,142,145,148
13e592e40746f0ccb1df7e0b88a751ed9e2a86f9,"This book offers a concise introduction to the angular momentum, one of the most fundamental quantities in all of quantum mechanics. Beginning with the quantization of angular momentum, spin angular momentum, and the orbital angular momentum, the author goes on to discuss the Clebsch-Gordan coefficients for a two-component system. After developing the necessary mathematics, specifically spherical tensors and tensor operators, the author then investigates the 3-""j,"" 6-""j,"" and 9-""j"" symbols. Throughout, the author provides practical applications to atomic, molecular, and nuclear physics. These include partial-wave expansions, the emission and absorption of particles, the proton and electron quadrupole moment, matrix element calculation in practice, and the properties of the symmetrical top molecule.",1957,0,5011,199,2,4,12,18,28,29,25,55,50,64
4b6e84debfa79cc5c4b960daafa9d8ba64ccf0ec,"Quantum field theory is the framework in which the regnant theories of the electroweak and strong interactions, which together form the standard model, are formulated. Quantum electrodynamics (QED), besides providing a complete foundation for atomic physics and chemistry, has supported calculations of physical quantities with unparalleled precision. The experimentally measured value of the magnetic dipole moment of the muon, 
 
$${\left({{g_\mu } - 2} \right)_{\exp }} = 233\,184\,600\,\left({1680} \right) \times {10^{ - 11}},$$ 
 
for example, should be compared with the theoretical prediction 
 
$${\left({{g_\mu } - 2} \right)_{{\rm{theor}}}} = 233\,183\,478\,\left( {308} \right) \times {10^{ - 11}}$$ 
 
(see the chapter by Hughes and Kinoshita on pp. 223-233 in this book).",1998,10,3107,273,94,88,109,100,104,98,100,87,104,140
7033363f0c6c2274cede210b6f86d86ec0f0eed8,,1979,0,3378,311,1,7,10,27,35,31,53,33,32,39
bccaa261b96906eb100d55c8bcbbbd9953e79971,"One of the simplest, and most completely treated, fields of application of quantum mechanics is the theory of atoms with one or two electrons. For hydrogen and the analcgous ions He+, Li++, etc., the calculations can be performed exactly, both in Schrodinger’s nonrelativistic wave mechanics and in Dirac’s relativistic theory of the electron. More specifically, the calculations are exact for a single electron in a fixed Coulomb potential. Hydrogen-like atoms thus furnish an excellent way of testing the validity of quantum mechanics. For such atoms the correction terms due to the motion and structure of atomic nuclei and due to quantum electrodynamic effects are small and can be calculated with high accuracy. Since the energy levels of hydrogen and similar atoms can be investigated experimentally to an astounding degree of accuracy, some accurate tests of the validity of quantum electrodynamics are also possible. Finally, the theory of such atoms in an external electric or magnetic field has also been developed in detail and compared with experiment.",1957,9,3980,190,0,1,4,3,12,19,21,32,22,19
850d1dc5db19e553458136b6cfe5e8e52b83c1bf,,1989,0,3071,346,2,3,5,12,20,14,47,30,31,56
c36eb90f86899a50decfc137f958d58c229627e9,"Research on fluorescent semiconductor nanocrystals (also known as quantum dots or qdots) has evolved over the past two decades from electronic materials science to biological applications. We review current approaches to the synthesis, solubilization, and functionalization of qdots and their applications to cell and animal biology. Recent examples of their experimental use include the observation of diffusion of individual glycine receptors in living neurons and the identification of lymph nodes in live animals by near-infrared emission during surgery. The new generations of qdots have far-reaching potential for the study of intracellular processes at the single-molecule level, high-resolution cellular imaging, long-term in vivo observation of cell trafficking, tumor targeting, and diagnostics.",2005,164,6836,65,100,242,263,375,461,470,560,583,616,608
054b680165a7325569ca6e63028ca9cee7f3ac9a,"Quantum computers promise to increase greatly the efficiency of solving problems such as factoring large integers, combinatorial optimization and quantum physics simulation. One of the greatest challenges now is to implement the basic quantum-computational elements in a physical system and to demonstrate that they can be reliably and scalably controlled. One of the earliest proposals for quantum computation is based on implementing a quantum bit with two optical modes containing one photon. The proposal is appealing because of the ease with which photon interference can be observed. Until now, it suffered from the requirement for non-linear couplings between optical modes containing few photons. Here we show that efficient quantum computation is possible using only beam splitters, phase shifters, single photon sources and photo-detectors. Our methods exploit feedback from photo-detectors and are robust against errors from photon loss and detector inefficiency. The basic elements are accessible to experimental investigation with current technology.",2001,56,4188,110,23,56,121,134,174,155,185,194,180,195
bd36e6625e075bf985bfdb76a7ab4813227a8758,"We describe the development of multifunctional nanoparticle probes based on semiconductor quantum dots (QDs) for cancer targeting and imaging in living animals. The structural design involves encapsulating luminescent QDs with an ABC triblock copolymer and linking this amphiphilic polymer to tumor-targeting ligands and drug-delivery functionalities. In vivo targeting studies of human prostate cancer growing in nude mice indicate that the QD probes accumulate at tumors both by the enhanced permeability and retention of tumor sites and by antibody binding to cancer-specific cell surface biomarkers. Using both subcutaneous injection of QD-tagged cancer cells and systemic injection of multifunctional QD probes, we have achieved sensitive and multicolor fluorescence imaging of cancer cells under in vivo conditions. We have also integrated a whole-body macro-illumination system with wavelength-resolved spectral imaging for efficient background removal and precise delineation of weak spectral signatures. These results raise new possibilities for ultrasensitive and multiplexed imaging of molecular targets in vivo.",2004,66,4196,81,18,93,158,208,261,312,288,363,348,360
0b7cd3a0975a9fb228dacf1c63615c98cf07591e,"Topological quantum computation has emerged as one of the most exciting approaches to constructing a fault-tolerant quantum computer. The proposal relies on the existence of topological states of matter whose quasiparticle excitations are neither bosons nor fermions, but are particles known as non-Abelian anyons, meaning that they obey non-Abelian braiding statistics. Quantum information is stored in states with multiple quasiparticles, which have a topological degeneracy. The unitary gate operations that are necessary for quantum computation are carried out by braiding quasiparticles and then measuring the multiquasiparticle states. The fault tolerance of a topological quantum computer arises from the nonlocal encoding of the quasiparticle states, which makes them immune to errors caused by local perturbations. To date, the only such topological states thought to have been found in nature are fractional quantum Hall states, most prominently the $\ensuremath{\nu}=5∕2$ state, although several other prospective candidates have been proposed in systems as disparate as ultracold atoms in optical lattices and thin-film superconductors. In this review article, current research in this field is described, focusing on the general theoretical concepts of non-Abelian statistics as it relates to topological quantum computation, on understanding non-Abelian quantum Hall states, on proposed experiments to detect non-Abelian anyons, and on proposed architectures for a topological quantum computer. Both the mathematical underpinnings of topological quantum computation and the physics of the subject are addressed, using the $\ensuremath{\nu}=5∕2$ fractional quantum Hall state as the archetype of a non-Abelian topological state enabling fault-tolerant quantum computation.",2007,577,3299,71,9,47,83,97,142,139,181,211,230,254
7541a704a719a32ae5f4d7149bb22fcba40629de,"Quantum networks provide opportunities and challenges across a range of intellectual and technical frontiers, including quantum computation, communication and metrology. The realization of quantum networks composed of many nodes and channels requires new scientific capabilities for generating and characterizing quantum coherence and entanglement. Fundamental to this endeavour are quantum interconnects, which convert quantum states from one physical system to those of another in a reversible manner. Such quantum connectivity in networks can be achieved by the optical interactions of single photons and atoms, allowing the distribution of entanglement across the network and the teleportation of quantum states between nodes.",2008,125,3318,76,9,64,89,130,141,186,203,227,298,341
c7bfcef5fea52ec8fa1988058db1332983908cb8,"In this paper, we shall show how the theory of measurements is to be understood from the point of view of a physical interpretation of the quantum theory in terms of hidden variables developed in a previous paper. We find that in principle, these ""hidden"" variables determine the precise results of each individual measurement process. In practice, however, in measurements that we now know how to carry out, the observing apparatus disturbs the observed system in an unpredictable and uncontrollable way, so that the uncertainty principle is obtained as a practical limitation on the possible precision of measurements. This limitation is not, however, inherent in the conceptual structure of our interpretation. We shall see, for example, that simultaneous measurements of position and momentum having unlimited precision would in principle be possible if, as suggested in the previous paper, the mathematical formulation of the quantum theory needs to be modined at very short distances in certain ways that are consistent with our interpretation but not with the usual interpretation. We give a simple explanation of the origin of quantum-mechanical correlations of distant objects in the hypothetical experiment of Einstein, Podolsky, and Rosen, which was suggested by these authors as a criticism of the usual interpretation. Finally, we show that von Neumann's proof that quantum theory is not consistent with hidden variables does not apply to our interpretation, because the hidden variables contemplated here depend both on the state of the measuring apparatus and the observed system and therefore go beyond certain of von 1umann's assumptions. In two appendixes, we treat the problem oi the electromagnetic field in our interpretation and answer certain additional objections which have arisen in the attempt to give a precise description for an individual system at the quantum level.",1952,0,4173,177,3,3,3,4,4,2,2,2,0,1
801895c9b1c9099e74d59daedba1ff0585f0927c,"Highly luminescent semiconductor quantum dots (zinc sulfide-capped cadmium selenide) have been covalently coupled to biomolecules for use in ultrasensitive biological detection. In comparison with organic dyes such as rhodamine, this class of luminescent labels is 20 times as bright, 100 times as stable against photobleaching, and one-third as wide in spectral linewidth. These nanometer-sized conjugates are water-soluble and biocompatible. Quantum dots that were labeled with the protein transferrin underwent receptor-mediated endocytosis in cultured HeLa cells, and those dots that were labeled with immunomolecules recognized specific antibodies or antigens.",1998,28,6315,77,1,15,42,52,79,107,177,282,344,361
a6e1b6ed82a5286baf684f538dab54c8235b05d4,We propose an implementation of a universal set of one- and two-quantum-bit gates for quantum computation using the spin states of coupled single-electron quantum dots. Desired operations are effected by the gating of the tunneling barrier between neighboring dots. Several measures of the gate quality are computed within a recently derived spin master equation incorporating decoherence caused by a prototypical magnetic environment. Dot-array experiments that would provide an initial demonstration of the desired nonequilibrium spin dynamics are proposed.,1997,33,4647,89,4,10,29,55,86,161,179,195,208,274
af393cb9f21adb29dfc6ff9a0e322e5cf87edc57,"Entanglement purification protocols (EPPs) and quantum error-correcting codes (QECCs) provide two ways of protecting quantum states from interaction with the environment. In an EPP, perfectly entangled pure states are extracted, with some yield D, from a mixed state M shared by two parties; with a QECC, an arbitrary quantum state |\ensuremath{\xi}〉 can be transmitted at some rate Q through a noisy channel \ensuremath{\chi} without degradation. We prove that an EPP involving one-way classical communication and acting on mixed state M^(\ensuremath{\chi}) (obtained by sharing halves of Einstein-Podolsky-Rosen pairs through a channel \ensuremath{\chi}) yields a QECC on \ensuremath{\chi} with rate Q=D, and vice versa. We compare the amount of entanglement E(M) required to prepare a mixed state M by local actions with the amounts ${\mathit{D}}_{1}$(M) and ${\mathit{D}}_{2}$(M) that can be locally distilled from it by EPPs using one- and two-way classical communication, respectively, and give an exact expression for E(M) when M is Bell diagonal. While EPPs require classical communication, QECCs do not, and we prove Q is not increased by adding one-way classical communication. However, both D and Q can be increased by adding two-way communication. We show that certain noisy quantum channels, for example a 50% depolarizing channel, can be used for reliable transmission of quantum states if two-way communication is available, but cannot be used if only one-way communication is available. We exhibit a family of codes based on universal hashing able to achieve an asymptotic Q (or D) of 1-S for simple noise models, where S is the error entropy. We also obtain a specific, simple 5-bit single-error-correcting quantum block code. We prove that iff a QECC results in high fidelity for the case of no error then the QECC can be recast into a form where the encoder is the matrix inverse of the decoder. \textcopyright{} 1996 The American Physical Society.",1996,72,3468,138,34,28,48,37,66,73,114,138,134,161
007c9375f8cde3bc49493f0fdfccc7eb4d759d8b,A polymer solar-cell based on a bulk hetereojunction design with an internal quantum efficiency of over 90% across the visible spectrum (425 nm to 575 nm) is reported. The device exhibits a power-conversion efficiency of 6% under standard air-mass 1.5 global illumination tests.,2009,43,3725,41,58,322,497,494,481,463,366,278,230,230
cd51ecfbc5ece7b78112cb3d622d5cfe4b4e0d80,"We show that the quantum spin Hall (QSH) effect, a state of matter with topological properties distinct from those of conventional insulators, can be realized in mercury telluride–cadmium telluride semiconductor quantum wells. When the thickness of the quantum well is varied, the electronic state changes from a normal to an “inverted” type at a critical thickness dc. We show that this transition is a topological quantum phase transition between a conventional insulating phase and a phase exhibiting the QSH effect with a single pair of helical edge states. We also discuss methods for experimental detection of the QSH effect.",2006,47,3792,57,3,10,29,46,101,142,196,230,254,314
f5075d1788bf0bf6488dda607b388378d86f5b21,"The field of nanotechnology holds great promise for the diagnosis and treatment of human disease. However, the size and charge of most nanoparticles preclude their efficient clearance from the body as intact nanoparticles. Without such clearance or their biodegradation into biologically benign components, toxicity is potentially amplified and radiological imaging is hindered. Using intravenously administered quantum dots in rodents as a model system, we have precisely defined the requirements for renal filtration and urinary excretion of inorganic, metal-containing nanoparticles. Zwitterionic or neutral organic coatings prevented adsorption of serum proteins, which otherwise increased hydrodynamic diameter by >15 nm and prevented renal excretion. A final hydrodynamic diameter <5.5 nm resulted in rapid and efficient urinary excretion and elimination of quantum dots from the body. This study provides a foundation for the design and development of biologically targeted nanoparticles for biomedical applications.",2007,26,3349,79,5,65,99,146,188,232,277,313,298,326
0426b933c3388ce2bea3f58b877edec5832af8ee,"One of the fastest moving and most exciting interfaces of nanotechnology is the use of quantum dots (QDs) in biology. The unique optical properties of QDs make them appealing as in vivo and in vitro fluorophores in a variety of biological investigations, in which traditional fluorescent labels based on organic molecules fall short of providing long-term stability and simultaneous detection of multiple signals. The ability to make QDs water soluble and target them to specific biomolecules has led to promising applications in cellular labelling, deep-tissue imaging, assay labelling and as efficient fluorescence resonance energy transfer donors. Despite recent progress, much work still needs to be done to achieve reproducible and robust surface functionalization and develop flexible bioconjugation techniques. In this review, we look at current methods for preparing QD bioconjugates as well as presenting an overview of applications. The potential of QDs in biology has just begun to be realized and new avenues will arise as our ability to manipulate these materials improves.",2005,115,5298,60,13,118,157,246,305,328,473,477,479,456
6687f6bcfd782d9dbb235cfef6fd22e60dbdc9e9,"This thesis consists of four papers. In the first paper we present methods and explicit formulas for describing simple weight modules over twisted generalized Weyl algebras. Under certain conditions we obtain a classification of a class of locally finite simple weight modules from simple modules over tensor products of noncommutative tori. As an application we describe simple weight modules over the quantized Weyl algebra of rank two. In the second paper we derive necessary and sufficient conditions for an ambiskew polynomial ring to have a Hopf algebra structure of a certain type, generalizing many known Hopf algebras, for example U(sl2), Uq(sl2) and the enveloping algebra of the 3-dimensional Heisenberg Lie algebra. In a torsion-free case we describe the finite-dimensional simple modules, and prove a generalized Clebsch-Gordan theorem. We construct a Casimir type operator and prove that any finite-dimensional weight module is semisimple. In the third paper we define a notion of unitarizability for weight modules over a generalized Weyl algebra (of rank one, with commutative coeffiecient ring R), which is assumed to carry an involution of the form X ∗ = Y , R ⊆ R. We prove that a weight module V is unitarizable iff it is isomorphic to its finitistic dual V . Using the classification of weight modules by Drozd, Guzner and Ovsienko, we prove necessary and sufficient conditions for an indecomposable weight module to be isomorphic to its finitistic dual, and thus to be unitarizable. Some examples are given, including Uq(sl2) for q a root of unity. In the fourth paper, using the language of h-Hopf algebroids, introduced by Etingof and Varchenko, we construct a dynamical quantum group, Fell(GL(n)), from Felder’s elliptic solution of the quantum dynamical Yang-Baxter equation with spectral parameter associated to the Lie algebra sln. We apply the generalized FRST construction and obtain a bialgebroidFell(M(n)) and study analogues of the exterior algebra and elliptic minors. We prove that the elliptic determinant it is grouplike and almost central. Localizing at this determinant and constructing an antipode we obtain the h-Hopf algebroid Fell(GL(n)).",1993,30,3001,237,58,83,96,104,107,89,82,84,83,81
0ea9c798317f23df8bda748e7bdb2608aa4967bd,"In this paper, we discuss some interesting properties of the electromagnetic potentials in the quantum domain. We shall show that, contrary to the conclusions of classical mechanics, there exist effects of potentials on charged particles, even in the region where all the fields (and therefore the forces on the particles) vanish. We shall then discuss possible experiments to test these conclusions; and, finally, we shall suggest further possible developments in the interpretation of the potentials.",1959,0,4613,127,0,1,4,3,1,2,2,3,11,6
b00ac81caf4ad7a151407477ce9f0db123615c71,Table of Contents: Special Considerations Cardiovascular Emergencies Pulmonary Emergencies Gastrointestinal Emergencies Genitourinary Emergencies Vascular Emergencies Neurologic Emergencies Trauma Metabolic and Endocrine Emergencies Dermatologic Emergencies Ophthalmologic Emergencies ENT Emergencies Hematologic and Oncologic Emergencies Musculoskeletal and Rheumatologic Emergencies Psychiatric Emergencies,2003,2,15,1,0,0,0,0,0,0,0,0,0,0
a898919a0c07eae8e6f309421b1aa5815952b191,"List of Symbols.Introduction.Atomic Structure and Interatomic Bonding.The Structure of Crystalline Solids.Imperfections in Solids.Diffusion.Mechanical Properties of Metals.Dislocations and Strengthening Mechanisms.Failure.Phase Diagrams.Phase Transformations in Metals: Development of Microstructure and Alteration of Mechanical Properties.Thermal Processing of Metal Alloys.Metals Alloys.Structures and Properties of Ceramics.Applications and Processing of Ceramics.Polymer Structures.Characteristics, Applications, and Processing of Polymers.Composites.Corrosion and Degradation of Materials.Electrical Properties.Thermal Properties.Magnetic Properties.Optical Properties.Materials Selection and Design Considerations.Economic, Environmental, and Societal Issues in Materials Science and Engineering.Appendix A: The International System of Units (SI).Appendix B: Properties of Selected Engineering Materials.Appendix C: Costs and Relative Costs for Selected Engineering Materials.Appendix D: Mer Structures for Common Polymers.Appendix E: Glass Transition and Melting Temperatues for Common Polymeric Materials.Glossary.Answers to Selected Problems.Index.",1985,0,6239,294,0,0,0,1,0,3,1,2,2,4
d2e70a35b896bc9a46f3d9625f27e361a700e2bb,"Metadynamics is a powerful algorithm that can be used both for reconstructing the free energy and for accelerating rare events in systems described by complex Hamiltonians, at the classical or at the quantum level. In the algorithm the normal evolution of the system is biased by a history-dependent potential constructed as a sum of Gaussians centered along the trajectory followed by a suitably chosen set of collective variables. The sum of Gaussians is exploited for reconstructing iteratively an estimator of the free energy and forcing the system to escape from local minima. This review is intended to provide a comprehensive description of the algorithm, with a focus on the practical aspects that need to be addressed when one attempts to apply metadynamics to a new system: (i) the choice of the appropriate set of collective variables; (ii) the optimal choice of the metadynamics parameters and (iii) how to control the error and ensure convergence of the algorithm.",2008,138,1129,15,0,16,35,51,73,81,89,104,131,132
a549d9228337cbabcfc6709a0110740c1963b782,This comprehensive book provides state-of-the-art scientific and technical information in a clear format and consistent structure making it suitable for formal course work or self-instruction. T ...,2001,0,1284,20,5,11,20,28,43,66,86,73,84,110
05df5cd581b1029d0e8cd5004fb4605bccd0f24f,"Abstract The paper addresses current needs in Natural Gas (NG) treating. Basic principles of Pressure Swing Adsorption (PSA) separation processes are described. A state of the art of microporous adsorbents in the frame of NG treating is given. It includes reference and advanced zeolites, carbon based materials and Metal-Organic Frameworks (MOFs). The pros and cons of each material category are discussed. Guidelines to develop on-purpose materials are given from thermodynamics and material state of the art. Finally, PSA applicability to inert (nitrogen and carbon dioxide) rejection from NG is discussed.",2009,96,334,6,0,3,16,21,22,34,27,35,33,29
a7599db290a008d910e973e9444b0c490e51b127,"The enzyme-modified electrode is the fundamental component of amperometric biosensors and biofuel cells. The selection of appropriate combinations of materials, such as: enzyme, electron transport mediator, binding and encapsulation materials, conductive support matrix and solid support, for construction of enzyme-modified electrodes governs the efficiency of the electrodes in terms of electron transfer kinetics, mass transport, stability, and reproducibility. This review investigates the varieties of materials that can be used for these purposes. Recent innovation in conductive electro-active polymers, functionalized polymers, biocompatible composite materials, composites of transition metal-based complexes and organometallic compounds, sol-gel and hydro-gel materials, nanomaterials, other nano-metal composites, and nano-metal oxides are reviewed and discussed here. In addition, the critical issues related to the construction of enzyme electrodes and their application for biosensor and biofuel cell applications are also highlighted in this article. Effort has been made to cover the recent literature on the advancement of materials sciences to develop enzyme electrodes and their potential applications for the construction of biosensors and biofuel cells.",2009,83,240,5,8,22,24,20,33,27,20,15,21,15
447a916ec26d769230d202e02424ba2a904db4fb,"The metal catalyzed azide/alkyne ‘click’ reaction (a variation of the Huisgen 1,3-dipolar cycloaddition reaction between terminal acetylenes and azides) has vastly increased in broadness and application in the field of polymer science. Thus, this reaction represents one of the few universal, highly efficient functionalization reactions, which combines both high efficiency with an enormously high tolerance of functional groups and solvents under highly moderate reaction temperatures (25–70 °C). The present review assembles an update of this reaction in the field of polymer science (linear polymers, surfaces) with a focus on the synthesis of functionalized polymeric architectures and surfaces.",2008,241,646,1,8,42,61,78,75,65,62,56,49,38
99af52a4b42684ba0507aa251f0692fb9ab162bf,"Answer FIVE questions, taking ANY TWO from Group A, ANY TWO from Group B and ALL from Group C. All parts of a question (a, b, etc) should be answered atone place, Answer should be brief and to-the-point and be supplemented with neat sketches. Unnecessary long answers may result in loss of marks. Any missing data ,or wrong data may be assumed suitably giving proper justification. Figures on the right-hand side margin indicate full marks. 1. (a) What is a Burger vector? Show it by drawing a Burger circuit? What is Frank-Read source? State its importance in plastic deformation. 2+2+2 (b) Distinguish between: (2x2)+ (2x2) (i) Slip and Cross slip (ii) Sessile dislocation and Glissile dislocation. (c) What is Critical Resolved Shear Stress? Derive its formulae. 2+2 (d) Calculate the degree of freedom of ice and water kept in a beaker at 1 atmosphere pressure. 2 2. (a) State Fick's laws of Diffusion. How can it help you m the problems of Case Carburising? Given an activation energy, Q of 142 kJ/mol, for the diffusion of carbon in FCC iron and an initial temperature of 1000 K, find the temperature that will increase the diffusion coefficient by a factor 10. [R =8.314 J/(mol.K)]: Will you use a very high temperature? 2+2+(3+1) (b) What is a Phase? What is the difference between α-iron and ferrite? Define an invariant reaction with an example. 2+2 (c) Differentiate between: (2x2)+ (2x2) (i) Phase Rule and Phase Diagram, (ii) Solvus Line and Solidus Line. 3. (a) Explain Lever Rule with a Tie Line. Find the weight percentage of pro-eutectoid ferrite just above, the eutectoid temperature of a 0 3%C-steel. 2+2 (b) Derive the relationship between True Strain and Engineering Strain. What is Resilience? Why is it important for spring material? 2+(1 +1) (c) Describe Yield Point Phenomenon. Draw the engineering stress-strain diagram of Glass. Why does necking occur during tension test of a ductile material 2+2+2 (d) Justify: 2x3 (i) Zinc is not as ductile as copper (ii) Cold working increases hardness of materials (iii) Steel is a brittle material at sub-zero atmosphere. 4. (a) Suggest one suitable material for each of the following purpose with justifications: 2x5 (i) File Cabinet (ii) WaterTap (iii) Manhole Cover (iv) Garden Chair (v) Glass Cutter.",2007,0,567,10,21,32,25,33,67,70,59,69,41,27
a8d88e305089129fb2e306d324d8cd482252abb7,"Recent examples on the functionalization of the ferrocene core by means of cross-coupling reactions are reported in this re- view. Several methods are discussed including Negishi, Suzuki and Stille couplings for ferrocene-aryl bond formation and Sonogashira reaction for ferrocene-alkyne coupling. The properties in material science and asymmetric catalysis of the prepared ferrocenyl com- pounds are briefly discussed.",2008,1,26,0,0,1,1,1,2,8,5,1,1,3
887d2eb4c920a11c83294e5d9565c3c70bb76bc0,"This book introduces the principles of electrochemistry with a special emphasis on materials science. This book is clearly organized around the main topic areas comprising electrolytes, electrodes, development of the potential differences in combining electrolytes with electrodes, the electrochemical double layer, mass transport, and charge transfer, making the subject matter more accessible. In the second part, several important areas for materials science are described in more detail. These chapters bridge the gap between the introductory textbooks and the more specialized literature. They feature the electrodeposition of metals and alloys, electrochemistry of oxides and semiconductors, intrinsically conducting polymers, and aspects of nanotechnology with an emphasis on the codeposition of nanoparticles.This book provides a good introduction into electrochemistry for the graduate student. For the research student as well as for the advanced reader there is sufficient information on the basic problems in special chapters. The book is suitable for students and researchers in chemistry, physics, engineering, as well as materials science. It includes: introduction into electrochemistry; metal and alloy electrodeposition; oxides and semiconductors, corrosion; intrinsically conducting polymers; and, codeposition of nanoparticles, multilayers.",2008,0,51,1,0,3,2,4,9,6,6,6,7,4
79c66d7379ee590a935fb554d78518106491d29e,"This review introduces the basic material science concepts and principles behind some common topics in the development of pharmaceutical solid formulations. The physiochemical properties of small organic pharmaceutical materials are summarized. Common phases, differences in phases, phase transitions, and their relation to pharmaceutical development are reviewed. The characteristics and physical nature of solid phases, including crystalline and amorphous solids, are presented in conjunction with some pharmaceutically relevant phenomena, such as polymorphism, phase transition kinetics, and relaxation. Mesophases, including liquid crystals and condis crystals, are introduced. The potential energy states of different phases are highlighted as the key connection between the physical nature of the materials and their pharmaceutical behavior, and energy landscape is employed to enhance the understanding of this relation.",2007,48,72,3,0,6,9,8,3,7,8,9,4,4
3a88e844e0e17651c2c03568b7b74e9ffd2a35e2,"We present the initial architecture and implementation of VLab, a Grid and Web‐Service‐based system for enabling distributed and collaborative computational chemistry and material science applications for the study of planetary materials. The requirements of VLab include job preparation and submission, job monitoring, data storage and analysis, and distributed collaboration. These components are divided into client entry (input file creation, visualization of data, task requests) and back‐end services (storage, analysis, computation). Clients and services communicate through NaradaBrokering, a publish/subscribe Grid middleware system that identifies specific hardware information with topics rather than IP addresses. We describe three aspects of VLab in this paper: (1) managing user interfaces and input data with JavaBeans and Java Server Faces; (2) integrating Java Server Faces with the Java CoG Kit; and (3) designing a middleware framework that supports collaboration. To prototype our collaboration and visualization infrastructure, we have developed a service that transforms a scalar data set into its wavelet representation. General adaptors are placed between the endpoints and NaradaBrokering, which serve to isolate the clients/services from the middleware. This permits client and service development independently of potential changes to the middleware. Copyright © 2007 John Wiley & Sons, Ltd.",2007,42,31,0,10,1,2,1,0,1,0,0,0,0
90aeafe24a1d7bd5d0c5db9c56718b1ea024b8dc,"In recent years, non-volatile solid state memories have in many applications replaced magnetic hard disk drives. While the most popular and successful non-volatile memory is the “FLASH” random access memory, several contenders have entered the stage that might be viable alternatives in the near future. One of the most promising storage concepts is based on phase change materials which are already successfully employed in rewritable optical data storage. In this paper we will review the present understanding of phase change materials as well as open questions.",2007,26,127,0,1,9,5,11,17,13,9,12,14,8
18bbd2ddc3d478ab6db6cc0a9eabb36a4173da84,"Nanoparticles are the subject of numerous papers and reports and are full of promises for electronic, optical, magnetic and biomedical applications. Although metallic nanoparticles have been functionalized with peptides, proteins and DNA during the last 20 years, carbohydrates have not been used with this purpose until 2001. Since the first synthesis of gold nanoparticles functionalized with carbohydrates (glyconanoparticles) was reported, the number of published articles has considerably increased. This article reviews progress in the development of nanoparticles functionalized with biological relevant oligosaccharides. The glyconanoparticles constitute a good bio-mimetic model of carbohydrate presentation at the cell surface, and maybe, excellent tools for Glycobiology, Biomedicine and Material Science investigations.",2006,132,241,3,0,10,21,14,35,21,25,17,24,22
b6ca80a2fba34dc39850712a37cb9d890ea62cf7,,2006,0,230,0,2,24,22,14,26,17,21,19,14,12
b6d1c6da4132a99a2d167f4c78e03b7aa4df1091,"Objectives There are several methods in the literature to quantify powder flow, such as Carr’s index, critical orifice diameter or powder rheometry. A disadvantage of these methods is that they require large sample sizes to perform a measurement. Novel, investigational and lab scale powder processing techniques, such as SEDS or freeze drying, may not produce sufficient sample sizes to allow the use of these techniques to quantify flow, an important consideration for pharmaceutical manufacturing processes. Methods In the Xcelodose system, powder is retained in a dispense head (hopper) with a known number, diameter and surface area of holes. Tapping the dispense head via a solenoid will break powder bridges, dislodging the powder, causing flow onto an eight place balance below. For a given powder and dispense head the amount of powder dispensed is proportional to the number and frequency of taps. A variety of heterogeneous powders, as well as nine spray dried lactose:lactose monohydrate blends, with different flow properties, were tested using the Xcelodose system. The powders were tested in three hoppers with the same hole surface area but different hole diameters and the amount of powder dispensed recorded. From this a ‘Flow Gradient’ was calculated. These powders were then tested using Carr’s index and Basic Flow Energy (BFE, Freeman Powder Rheometer) and these results compared. Results For the group of heterogeneous powders there is a clear correlation between Carr’s index and the Flow Gradient (R = 0.8376, P < 0.001), allowing flow categorisation, similar to Carr’s of the powders. With the lactose powders the BFE was also compared. A correlation between Flow Gradient and BFE of 0.8793 (P < 0.01) was observed for these 9 blends. This correlation improves to 0.9417 (P < 0.001) if only the 7 poorly flowing powders are included, suggesting the value of this technique for poorly flowing powders (Figure 1). Conclusions A novel method for the measurement of flow properties with very small quantities of material has been developed and validated against different powder types and different established powder characterisation methodologies. The flow gradient method produces an excellent correlation with larger scale measurement methods and would be suitable to categorise powder flow when the amount of material is limited. 41 Forming amorphous and nanocrystalline APIs through solid-state interactions with cross-linked polyvinylpyrrolidone",2007,0,3,0,0,0,0,0,0,1,0,0,1,1
b18edfa0ea92f539a929d9a44e356f20cb023b06,"This paper illustrates the merits of convergence in nanobiology of two seemingly disparate fields, material science and computational biology. Traditionally, material science has been a discipline involving design and fabrication of synthetic polymers consisting of repeating units. Collaboration with synthetic organic chemists allowed design of new polymers, with a range of altered conformations. Yet, naturally occurring proteins are also materials. Their varied sequences and structures should enrich material science providing more complex shapes, scaffolds and chemical properties. For material scientists, the enhanced coverage of chemical space obtained by integrating proteins and synthetic organic chemistry through the introduction of non-natural residues allows a range of new useful potential applications.",2006,64,20,0,0,2,7,2,0,5,1,1,0,1
388f82d66bcf1b16b9569a93667224af02f90a03,,2006,2,91,5,1,4,5,10,12,5,8,5,13,3
56280b5dc973d1a8ba220e7fd7cf90266883e60a,"Piezoelectric actuators are at an important stage of their development into a large component market. This market pull is for dynamically driven actuators for Diesel injector valves in automobiles. Cost, yield, and reliability are important concerns for the automobile industry. A number of these concerns relate back to basic material science issues in the manufacture of the piezoelectric actuators. This paper discusses material development of the piezoelectric ceramic and new opportunities for higher temperature materials. An important consideration in developing low-fire ceramics is the flux selection for a given system, and these must be selected to limit electrode-ceramic interface reactions in both Ag/Pd and copper-metallized electrode actuators.",2005,40,197,1,0,1,4,1,4,12,12,13,20,17
e4cbedde6ef293b7c112215731129fcd5b30753e,SYNTHESIS.Addition of Terminal Acetylides to C=O and C=N Electrophiles.Synthesis of Heterocycles and Carbocycles via Electrophilic Cyclization of Alkynes.Transition Metal Acetylides.ADVANCED MATERIALS-ORIENTED.Semiconducting Poly(arylene ethylene)s.Polyynes via Alkylidene Carbenes and Carbenoids.PROPERTIES AND THEORY.Theoretical Studies on Acetylenic Scaffolds.Macrocycles Based on Phenyl-Acetylene Scaffolding.Carbon-Rich Compounds: Acetylene-Based Carbon Allotropes.Chiral Acetylenic Macromolecules.Shape-Persistent Acetylenic Macrocycles for Ordered Systems.BIOLOGY-ORIENTED.Acetylenosaccharides.,2004,0,185,3,0,2,3,10,10,15,13,15,19,14
36eab53578807c9bf4ecaef321c003659c2076c9,"This chapter describes the development of actor-network theory and feminist material semiotics by exploring case studies within STS (science and technology studies). It notes that STS (and so material semiotics) develops its theoretical approaches through empirical case studies, and notes that unless this is understood it is difficult to understand the significance of 'actor network theory' or any other STS theory or approach.",2009,70,1358,132,10,26,66,65,94,129,142,168,165,150
9f5cf2bdf56fa5959bb86c4ae30b6b679e1b6f49,,2004,0,90,0,8,7,5,7,9,7,11,6,10,5
c313d1942e9b50cc836bf5da3e323c46b5cae0a1,"I. Fundamental Physics and Materials Technology of Ice.- 1, General Concepts.- 1. Introduction.- 2. Equations of Balance.- 3. Material Response.- (a) General constitutive relations, simple materials.- (b) The rule of material objectivity.- (c) Material symmetry.- (d) Constitutive response for isotropic bodies.- (e) Materials with bounded memory - some constitutive representations.- (f) Incompressibility.- (g) Some representations of isotropic functions.- 4. The Entropy Principle.- (a) The viscous heat-conducting compressible fluid.- (b) The viscous heat-conducting incompressible fluid.- (c) Pressure and extra stress as independent variables.- (d) Thermoelastic solid.- (e) Final remarks.- 5. Phase Changes.- (a) Phase changes for a viscous compressible heat-conducting fluid.- (b) Phase changes for a viscous incompressible heat-conducting fluid.- References.- 2. A Brief Summary of Constitutive Relations for Ice.- 1. Preliminary Remarks.- 2. The Mechanical Properties of Hexagonal Ice.- (a) The crystal structure of ordinary ice.- (b) The elastic behavior of hexagonal ice.- (c) The inelastic behavior of single-crystal ice.- 3. The Mechanical Properties of Polycrystalline Ice.- (a) The elastic behavior of polycrystalline ice.- (b) Linear viscoelastic properties of polycrystalline ice.- (?) General theory.- (?) Experimental results.- (c) Non-linear viscous deformation and creep.- (?) Results of creep tests.- (?) Generalization to a three-dimensional flow law.- (?) Other flow laws.- 4. The Mechanical Properties of Sea Ice.- (a) The phase diagram of standard sea ice and its brine content.- (b) Elastic properties.- (c) Other material properties.- References.- II. The Deformation of an Ice Mass Under Its Own Weight.- 3. A Mathematical Ice-flow Model and its Application to Parallel-sided Ice Slabs.- 1. Motivation and Physical Description.- 2. The Basic Model - Its Field Equations and Boundary Conditions.- (a) The field equations.- (?) Cold ice region.- (?) Temperate ice region.- (b) Boundary conditions.- (?) At the free surface.- (?) Along the ice-water interface.- (?) Along the bedrock surface.- (?) Along the melting surface.- 3. The Response of a Parallel-sided Ice Slab to Steady Conditions.- (a) Dimensionless forms of the field equations.- (b) Parallel-sided ice slab, a first approximation to glacier and ice-shelf flow dynamics.- (?) Velocity and temperature fields.7V-independent.- (?) Extending and compressing flow.- (?) Floating ice shelves.- 4. Concluding Remarks.- References.- 4. Thermo-mechanical Response of Nearly Parallel-sided Ice Slabs Sliding over their Bed.- 1. Motivation.- 2. The Basic Boundary-value Problem and its Reduction to Linear Form.- 3. The Solution of the Boundary-value Problems.- (a) Zeroth-order problem.- (b) First-order problem.- (?) Harmonic perturbation from uniform flow for a zero accumulation rate.- (?) Analytic solution for a Newtonian fluid.- (?) Numerical solution for non-linear rheology.- (?) Effect of a steady accumulation rate.- (?) A historical note on a previous approach.- (?) The first-order temperature problem.- (c) Numerical results for steady state.- (?) Transfer of bottom protuberances to the surface.- (?) Basal stresses.- (?) Surface velocities.- (?) Effect of a steady accumulation rate.- 4. Remarks on Response to a Time-dependent Accumulation Rate.- 5. Surface-wave Stability Analysis.- (a) The eigenvalue problem.- (b) Discussion of results.- 6. Final Remarks.- References.- 5, The Application of the Shallow-ice Approximation.- 1. Background and Previous Work.- 2. Derivation of the Basal Shear-stress Formula by Integrating the Momentum Equations over Ice Thickness.- (a) Derivation.- (b) The use of the basal shear-stress formula in applied glaciology.- 3. Solution of the Ice-flow Problem using the Shallow-ice Approximation.- (a) Governing equations.- (b) Shallow-ice approximation.- (c) Construction of the perturbation solution.- (d) Results.- (e) Temperature field.- 4. Theoretical Steady-state Profiles.- (a) Earlier theories and their limitations.- (b) Surface profiles determined by using the shallow-ice approximation.- 5. An Alternative Scaling - a Proper Analysis of Dynamics of Ice Sheets with Ice Divides.- (a) Finite-bed inclination.- (b) Small-bed inclination.- (c) Illustrations.- References.- 6. The Response of a Glacier or an Ice Sheet to Seasonal and Climatic Changes.- 1. Statement of the Problem.- 2. Development of the Kinematic Wave Theory.- (a) Full non-linear theory.- (b) Perturbation expansion - linear theory.- (c) An estimate for the coefficients C and D.- (d) Boundary and initial conditions.- 3. Theoretical Solutions for a Model Glacier.- (a) Solutions neglecting diffusion.- (b) Theoretical solutions for a diffusive model.- (?) Coefficient functions for the special model.- (?) Solution for a step function.- (?) General solution for uniform accumulation rate.- (?) The inverse problem - calculation of climate from variations of the snout.- 4. General Treatment for an Arbitrary Valley Glacier.- (a) Fourier analysis in time.- (?) Low-frequency response.- (?) High-frequency response.- (?) Use of the results.- (b) Direct integration methods.- 5. Derivation of the Surface-wave Equation from First Principles - Non-linear Theory.- (a) Surface waves in the shallow-ice approximation.- (?) Integration by the methods of characteristics.- (?) An illustrative example.- (?) A remark on linearization.- (?) Effects of diffusion.- (b) Remarks regarding time-dependent surface profiles in ice sheets.- (c) Long waves in an infinite ice slab - Is accounting for diffusion enough?.- (?) Basic equations.- (?) Construction of perturbation solutions.- (?) Numerical results.- 6. Concluding Remarks.- References.- 7. Three-dimensional and Local Flow Effects in Glaciers and Ice Sheets.- 1. Introduction.- 2. Effect of Valley Sides on the Motion of a Glacier.- (a) Solutions in special cases.- (?) Exact solutions for the limiting cases.- (?) Solution for a slightly off-circular channel.- (?) A note on very deep and wide channels.- (b) A useful result for symmetrical channels with no boundary slip.- (c) Numerical solution - discussion of results.- 3. Three-dimensional Flow Effects in Ice Sheets.- (a) Basic equations.- (b) Decoupling of the stress-velocity problem from the problem of surface profile.- (c) The equation describing the surface geometry.- (d) The margin conditions.- 4. Variational Principles.- (a) Fundamental variational theorem.- (b) Variational principle for velocities.- (c) Reciprocal variational theorem.- (d) Maximum and minimum principles.- (e) Adoption of the variational principles to ice problems.- 5. Discussion of Some Finite-element Solutions.- References.- Appendix. Detailed Calculations Pertaining to Higher-order Stresses in the Shallow-ice Approximation.- Author Index.",1983,0,581,70,0,0,4,4,2,3,5,2,1,0
889949974fd50e8111e93397663039188887d622,"The critical role of materials science and engineering in the development of fuel cell technology is surveyed. The inability to fabricate reliable triple-phase-boundary (tbp) structures involving electrolytes, electronic conductors, and gaseous reactants, severely restricted the progress of fuel cells until about four decades ago (∼1960). However at the start of the new millennium, commercialisation of four fuel cell types: polymeric electrolyte membrane (PEMFC), phosphoric acid (PAFC), molten carbonate (MCFC), and solid oxide (SOFC), is now being very energetically pursued. Materials selection for each type of fuel cell is briefly examined, and the predominant engineering issues related to the development of commercial products are summarised. The fabrication, reliability, and cost of the relevant materials is of paramount importance to ensure rapid market penetration. The choice of fuel and relevant infrastructure is also considered, and the crucial role of materials for energy storage (particularly hydrogen) and fuel processing, is emphasised.",2001,35,419,2,1,4,9,24,22,22,25,23,19,29
6dc1739bd1e7d8bda913799038a6acf43583b0b0,"An ultrasonic levitation device operable in both ordinary ground‐based as well as in potential space‐borne laboratories is described together with its various applications in the fields of fluid dynamics, material science, and light scattering. Some of the phenomena which can be studied by this instrument include surface waves on freely suspended liquids, the variations of the surface tension with temperature and contamination, the deep undercooling of materials with the temperature variations of their density and viscosity, and finally some of the optical diffraction properties of transparent substances.",1985,18,201,1,0,3,1,2,9,2,3,3,4,6
d0c0dd34cbf2de1aa266a13fc08d6bb463056799,"Abstract The material science beamline is a multi-purpose beamline intended for experiments in the photon energy range 2.4–20 keV. The beamline will be located at port I811 of the MAX-II ring. According to our planning the first experiments will be conducted in the spring 2001. In order to overcome the relatively low energy of the electron-beam in the MAX II ring (1.5 GeV) we will use a super-conducting multi-pole wiggler insertion device. The disadvantage with this solution is the high K value ( K =20) of the device, which leads to a divergent X-ray beam. The optical solutions for the beamline therefore emphasizes on solving the divergence problem of the photon beam and the high heat load on the optical elements.",2001,6,20,1,0,0,0,0,0,0,0,0,0,2
f67d87c2ba785779fde532970026b0e3ab7f062a,Abstract The development of the Material Science EXAFS line which covers the energy range of 4 keV to > 25 keV is described. The general design parameters of the line have been developed specifically for the application of X-ray absorption spectroscopy to problems in material science. The optics systems consisting of a vertically collimating SiC mirror and a unique four crystal monochromator are described along with the results of extensive ray tracing studies. The two backend configurations to be used on this line will also be discussed.,1983,4,29,0,0,1,0,2,1,1,2,0,1,1
90320d358c7d58996ada4a5f89a8279c24c068b2,"UNLABELLED
Reprocessing (repeated cleaning, disinfection, and sterilization) and reuse of single-use medical devices has been performed safely with some devices. The aim of our study was to analyze whether reprocessing of the Combitubes (Kendall-Sheridan, Argyll, NY) airway device, used for emergency endotracheal intubation and difficult airway management, is possible and can be performed appropriately and safely. Microbiological, microstructure, and material science examinations were performed with unused, as well as multiple reused and reprocessed Combitubes. The reprocessing procedure consisted of a cleaning, a disinfection, a final inspection, and a sterilization. Microbiological examinations of multiple reused and reprocessed Combitubes found no test organisms in quantitative cultures. A microbial reduction between four and five log levels compared with nonreprocessed tubes was found. Microstructure analysis for the examination of topographical alterations and changes in the chemical composition of the surface demonstrated nonsignificant alterations between new and reprocessed medical devices. In material science examinations, cuff burst pressures were not different between unused and multiple reprocessed Combitubes. The results of all examinations proved that the decontamination process is adequately effective, and that no significant superficial alterations are generated by the multiple reuse and reprocessing of the Combitubes. To assure uniformly good results, a quality management system must be established and only validated methods should be used.


IMPLICATIONS
Reprocessing of single-use medical devices offers the opportunity of significant savings and is already performed with some devices. Microbiological, microstructure, and material science examinations proved that reprocessing of multiple reused Combitubes (Kendall-Sheridan, Argyll, NY), mainly used for emergency airway management, is possible and safe.",2000,37,20,0,0,3,5,1,0,2,0,2,0,1
c0ee5787154e15cc1d3ed0cf38c186b88c7fe1ad,"Coordination metal complexes with one-dimensional polymeric structures have long been investigated as materials with unusual properties. Molecular-based ferromagnets, synthetic metallic conductors, non-linear optical materials, and ferroelectrics represent several applications of low-dimensional coordination polymers. It is possible to modify the bulk magnetic, electrical, and optical properties of such materials by tailoring the constituent molecules. This review presents recent examples of each of these applications and focuses on the correlation between the molecular structure and the bulk properties of the materials.",1993,179,425,2,0,1,5,7,11,3,12,8,19,22
a9d015e4c2ac2ebcf0bc170ad42eea73d2d48ceb,Partial table of contents: Enantioselective Addition of Dialkyzincs to Aldehydes Catalyzed by Chiral Ferrocenyl Aminoalcohols. Ferrocene Compounds Containing Heteroelements. Electrochemical and X-ray Structural Aspects of Transition Metal Complexes Containing Redox-Active Ferrocene Ligands. Ferrocene-Containing Thermotropic Liquid Crystals. Synthesis and Characterization of Ferrocene Containing Polymers.,1994,0,407,3,0,5,6,10,9,14,12,19,13,12
f5d8a14f2c5110e0f193009e531902a11f2473e9,,2000,0,273,10,0,0,4,6,2,16,14,17,16,12
5a70679ac5fe8725bebd57ca26bab3baacc0012c,"Abstract Many texture studies have been published on crispness because of the great interest of consumers towards crispy foods. This work reviews the existing literature on the topic, and especially the different approaches, instrumental and sensory, applied to study crispness. These studies result in a wide range of data but, because crispness is not a clearly defined sensory attribute, the conclusions that can be drawn from these studies should be carefully examined. The physical basis for crispness are discussed and the role of structure, hydration and ingredients on crispness and its stability are presented.",2002,100,229,10,0,2,4,8,9,11,14,17,22,16
e1eab0c1d033852ae041d86d31287d7ff7756bbf,Abstract The use of electrostatic forces in the design of a positioning system and acoustic forces in the implementation of a mixing system for material science experiments on Spacelab are described. The electrostatic positioning of samples is described with special reference to its advantages and disadvantages with regard to other positioning methods. The design of such a positioner is described including the considerations relating to the processing of both high and low vapour pressure materials in a positioner compatible with both the isothermal heating facility (IHF) and the mirror heating facility (MHF) of Spacelab under microgravity (10 −4 –10 −3 g) conditions. The application of acoustic and ultrasonic forces to the problem of sample mixing in material science experiments is explained. The design of a mixer compatible with existing furnace hardware for Spacelab and capable of effectively mixing samples at temperatures up to 1200°C is described. Tests of the mixer show that a 15 μm displacement adequate for good mixing can be achieved with a d.c. power input of 23 W and a conversion efficiency of 70%. Tests on alumina particles and carbon fibres in various alloy matrices show that complete wetting can be achieved.,1980,0,16,0,0,0,1,0,1,0,0,0,0,0
01c23f080f47379e915d31cd734448f1b6c7b9cc,"Abstract Since their first observation nearly a decade ago by Iijima (Iijima S. Helical microtubules of graphitic carbon Nature. 1991; 354:56–8), carbon nanotubes have been the focus of considerable research. Numerous investigators have since reported remarkable physical and mechanical properties for this new form of carbon. From unique electronic properties and a thermal conductivity higher than diamond to mechanical properties where the stiffness, strength and resilience exceeds any current material, carbon nanotubes offer tremendous opportunities for the development of fundamentally new material systems. In particular, the exceptional mechanical properties of carbon nanotubes, combined with their low density, offer scope for the development of nanotube-reinforced composite materials. The potential for nanocomposites reinforced with carbon tubes having extraordinary specific stiffness and strength represent tremendous opportunity for application in the 21st century. This paper provides a concise review of recent advances in carbon nanotubes and their composites. We examine the research work reported in the literature on the structure and processing of carbon nanotubes, as well as characterization and property modeling of carbon nanotubes and their composites.",2001,101,4407,84,0,18,75,115,128,152,210,188,240,219
c409669f26e30f2016273f9cf5fcb40ac3eaa973,,1984,0,191,1,1,4,8,8,7,13,9,11,17,6
818d63bcec8c028109c4d50d905061e6beeb5f9e,,1978,0,224,3,5,9,14,7,9,6,3,6,4,2
0d69ed8ecce3ab31950c26e98a42a0e67a71671f,"Tricalcium, tetracalcium phosphate and hydroxyapatite ceramics exhibit distinct differences in their chemical and structural composition. Only hydroxyapatite ceramic is identical with the original bone mineral. Different preparation methods lead to compact hydroxyapatite ceramic or to porous material with interconnecting macropores as structural equivalents of the spatial structure of cancellous bone. Concerning the behaviour in a biological environment, high crystallinity and large material density result in resistance to dissolution and long lasting stability. Amorphous ultrastructure and porous formation enhance interface activity and bone ingrowth, but also biological degradation of the ceramic implant material.",1980,5,193,0,0,2,1,1,0,0,0,2,2,2
c79245d20facedb5aa8ec27b43b72e9fd107847e,,1972,0,105,6,0,0,0,0,0,0,0,0,0,0
3c69bf381dd00605c74b01751dd62b271d1d0dfc,,1975,0,129,11,0,0,0,0,0,0,0,0,0,1
b3a5b7f4f14ce6112a1327b216839c6f9a3fd787,,1971,0,76,0,0,0,1,3,1,2,1,2,0,3
e9f7649b8da9fae259cfd34789121eb341a3ebdf,"Software for calculation of phase diagrams and thermodynamic properties have been developed since the 1970's. Software and computers have now developed to a level where such calculations can be used as tools for material and process development. In the present paper some of the latest software developments at Thermo-Calc Software are presented together with application examples. It is shown how advanced thermodynamic calculations have become more accessible since: - A more user-friendly windows version of Thermo-Calc, TCW, has been developed. - There is an increasing amount of thermodynamic databases for different materials available. - Thermo-Calc can be accessed from user-written software through several different programming interfaces are available which enables access to the thermodynamic software from a user-written software. Accurate data for thermodynamic properties and phase equilibria can then easily be incorporated into software written in e.g. C++, Matlab and FORTRAN. Thermo-Calc Software also produces DICTRA, a software for simulation of diffusion controlled phase transformations. Using DICTRA it is possible to simulate processes such as homogenization, carburising, microsegregation and coarsening in multicomponent alloys. The different models in the DICTRA software are briefly presented in the present paper together with some application examples.",2002,18,2557,53,1,13,40,45,59,56,69,58,71,65
bf5a13c1a5096e86eaa316b97ff43df0078a2042,"This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. Now results are discussed, along with the application of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book is not restricted to one particular discipline but rather will be of interest to scientists working in any area where inference from incomplete information is necessary.",2004,3,1631,202,38,48,50,87,91,94,105,113,123,123
20974e0e65b2831648cee5e589fbbdc37f1ef5b2,"Abstract The study of tin oxide is motivated by its applications as a solid state gas sensor material, oxidation catalyst, and transparent conductor. This review describes the physical and chemical properties that make tin oxide a suitable material for these purposes. The emphasis is on surface science studies of single crystal surfaces, but selected studies on powder and polycrystalline films are also incorporated in order to provide connecting points between surface science studies with the broader field of materials science of tin oxide. The key for understanding many aspects of SnO 2 surface properties is the dual valency of Sn. The dual valency facilitates a reversible transformation of the surface composition from stoichiometric surfaces with Sn 4+ surface cations into a reduced surface with Sn 2+ surface cations depending on the oxygen chemical potential of the system. Reduction of the surface modifies the surface electronic structure by formation of Sn 5s derived surface states that lie deep within the band gap and also cause a lowering of the work function. The gas sensing mechanism appears, however, only to be indirectly influenced by the surface composition of SnO 2 . Critical for triggering a gas response are not the lattice oxygen concentration but chemisorbed (or ionosorbed) oxygen and other molecules with a net electric charge. Band bending induced by charged molecules cause the increase or decrease in surface conductivity responsible for the gas response signal. In most applications tin oxide is modified by additives to either increase the charge carrier concentration by donor atoms, or to increase the gas sensitivity or the catalytic activity by metal additives. Some of the basic concepts by which additives modify the gas sensing and catalytic properties of SnO 2 are discussed and the few surface science studies of doped SnO 2 are reviewed. Epitaxial SnO 2 films may facilitate the surface science studies of doped films in the future. To this end film growth on titania, alumina, and Pt(1 1 1) is reviewed. Thin films on alumina also make promising test systems for probing gas sensing behavior. Molecular adsorption and reaction studies on SnO 2 surfaces have been hampered by the challenges of preparing well-characterized surfaces. Nevertheless some experimental and theoretical studies have been performed and are reviewed. Of particular interest in these studies was the influence of the surface composition on its chemical properties. Finally, the variety of recently synthesized tin oxide nanoscopic materials is summarized.",2005,455,2028,25,0,18,51,74,78,87,103,160,177,164
9f267904f7e36571f7001813eef4e5fffd5ef993,"The extended and generalized finite element methods are reviewed with an emphasis on their applications to problems in material science: (1) fracture, (2) dislocations, (3) grain boundaries and (4) phases interfaces. These methods facilitate the modeling of complicated geometries and the evolution of such geometries, particularly when combined with level set methods, as for example in the simulation growing cracks or moving phase interfaces. The state of the art for these problems is described along with the history of developments.",2009,183,674,16,2,15,47,50,46,50,69,74,68,69
d93ffe572b15a163e2ec1336a4e507b0b7a766f0,"Research in IT must address the design tasks faced by practitioners. Real problems must be properly conceptualized and represented, appropriate techniques for their solution must be constructed, and solutions must be implemented and evaluated using appropriate criteria. If significant progress is to be made, IT research must also develop an understanding of how and why IT systems work or do not work. Such an understanding must tie together natural laws governing IT systems with natural laws governing the environments in which they operate. This paper presents a two dimensional framework for research in information technology. The first dimension is based on broad types of design and natural science research activities: build, evaluate, theorize, and justify. The second dimension is based on broad types of outputs produced by design research: representational constructs, models, methods, and instantiations. We argue that both design science and natural science activities are needed to insure that IT research is both relevant and effective.",1995,90,3796,442,1,0,1,2,5,15,22,19,18,40
6fcaad2b337e0786a08486c3f71c9c9afddca930,"One: Descriptive.- One The Scientific Description of Personality.- Personality and Taxonomy: The Problem of Classification.- Type and Trait Theories: The Modern View.- Type-Trait Theories and Factor Analysis.- Situationism versus Type-Trait Theories.- Two The Development of a Paradigm.- Origins of Personality Theory.- The Beginnings of Modern Research.- Psychoticism as a Dimension of Personality.- Impulsiveness and Sensation Seeking: A Special Case.- The Question of Validity.- Three The Universality of P, E, and N.- Genetic Factors.- Personality in Animals.- Cross-cultural Studies.- Longitudinal Studies of Personality.- Four Alternative Systems of Personality Description.- R.B. Cattell and 16PF.- The Guilford-Zimmerman Factors.- The NEO Model of Personality.- The Minnesota Multiphasic Personality Inventor.- The California Psychological Inventory.- The Edwards Personal Preference Schedule and the Jackson Personality Research Form.- Other Systems.- Summary.- Five The Cognitive Dimension: Intelligence as a Component of Personality.- Galton versus Binet: IQ and Reaction Time.- The Psychophysiology of Intelligence.- The Theory of Intelligence.- Six Summary and Conclusions.- Two: Causal.- Seven Theories of Personality and Performance.- H. J. Eysenck (1957).- H. J. Eysenck (1967a).- Gray's Theory.- Brebner's Theory.- Eight The Psychophysiology of Personality.- Theoretical Background.- Extraversion.- Neuroticism.- Theoretical Implications.- Nine Extraversion, Arousal, and Performance.- Conditioning.- Sensitivity to Stimulation.- Vigilance.- Verbal Learning and Verbal Memory.- Psychomotor Performance.- Perceptual Phenomena.- Conclusions.- Ten Neuroticism, Anxiety, and Performance.- The State-Trait Approach.- Theories of Anxiety and Performance.- Worry and Performance.- Efficiency and Effectiveness.- Anxiety ? Task Interactions.- Attentional Mechanisms.- Learning and Memory.- Conclusions.- Eleven Social Behavior.- Social Interaction.- Sexual Behavior.- Educational Achievement.- Occupational Performance.- Antisocial Behavior and Crime.- Psychiatric Disorders.- Conclusions.- Three: Epilogue.- Twelve Is There a Paradigm in Personality Research?.- References and Bibliography.- Author Index.",1985,0,2054,111,1,24,46,14,25,25,41,24,53,36
e25083587debd035d2963d3ccdeb3eef477f083b,"In section 3.3 of [i]Philosophy of Natural Science[/i], Hempel argues that crucial tests are not sufficient enough to prove a given hypothesis or to disprove them. Hempel states what some may believe why a crucial test can prove or disprove a hypothesis. If there are two competing hypothesis which involve the same subject and no available evidence favors one or the other, then there exists a test, which will produce conflicting outcomes for the different hypotheses. This test is the so-called crucial test would then presumably refute one hypothesis while supporting the other. Hempel then presents his side of this argument using an example of past experiments involving the nature of light. He describes how Foucault performed an experiment involving the velocity of light through air and water. This experiment was meant to show whether light consists of waves or extremely small particles as presented by Newton. Foucault’s experiment was performed and the resulting outcome was used to refute Newton’s view of light as small particles traveling at a high velocity. Hempel believed that this test was not strong enough to completely support or refute either view of light. The experiment relied on the assumption that light as waves would travel faster in air than in water. However Hempel argues that the conception of light as streams of particles was too indefinite to assume that it would travel slower in air without additional assumptions about the motion of particles and their surrounding medium. So even though the results may seem to support and prove the wave hypothesis, it doesn’t necessarily disprove the particle theory. In fact Einstein later proposed a theory, which eliminated the classical wave theory, using support from an experiment by Lenard in 1903. But again as in the previous example one of the hypotheses was not definitely refuted, in this case being the wave theory. M.-M. V.",1966,0,1635,44,0,1,1,6,8,10,13,6,10,12
e22c08e168d74b55bd618a9026635fb60d5c7508,"Despite their successful use in many conscientious studies involving outdoor learning applications, mobile learning systems still have certain limitations. For instance, because students cannot obtain real-time, contextaware content in outdoor locations such as historical sites, endangered animal habitats, and geological landscapes, they are unable to search, collect, share, and edit information by using information technology. To address such concerns, this work proposes an environment of ubiquitous learning with educational resources (EULER) based on radio frequency identification (RFID), augmented reality (AR), the Internet, ubiquitous computing, embedded systems, and database technologies. EULER helps teachers deliver lessons on site and cultivate student competency in adopting information technology to improve learning. To evaluate its effectiveness, we used the proposed EULER for natural science learning at the Guandu Nature Park in Taiwan. The participants were elementary school teachers and students. The analytical results revealed that the proposed EULER improves student learning. Moreover, the largely positive feedback from a post-study survey confirms the effectiveness of EULER in supporting outdoor learning and its ability to attract the interest of students.",2009,40,180,9,2,3,10,16,17,19,17,26,11,21
a0e6a2a784c38a0052beec3c23419e9c11f22a60,"This study has three major purposes, including designing mobile natural-science learning activities that rest on the 5E Learning Cycle, examining the effects of these learning activities on students’ performances of learning aquatic plants, and exploring students’ perceptions toward these learning activities. A case-study method is utilized and the science club with 46 fourth-grade students is selected as the study case in the study. Besides, a set of quantitative and qualitative data were collected from the case to document the learning effects of and the students’ perceptions of the learning activities, and to discuss factors underlying these effects and students’ perceptions. The results indicate that the learning activities can enhance students’ scientific performances, including both knowledge and understanding levels. Students’ perceptions of these learning activities appear to be positive. The study identifies two factors that are prominent in the positive effects: students’ engaging in “mobile-technology supported” observation during their scientific inquiry; and students’ engaging in “mobiletechnology supported” manipulation during their scientific inquiry. Finally, the conclusions that our study has drawn could constitute a useful guide for educational practitioners concerned with the potentials of mobile computing in school settings.",2009,33,127,7,0,0,7,10,16,18,21,11,16,9
30a0ba6032b19c5d2ef61ec2281c814482f223bf,"The context and conduct of Arctic research are changing. In Nunavut, funding agencies, licensing bodies, and new regulatory agencies established under the Nunavut Land Claims Agreement require researchers to engage and consult with Inuit communities during all phases of research, to provide local training and other benefits, and to communicate project results effectively. Researchers are also increasingly expected to incorporate traditional knowledge into their work and to design studies that are relevant to local interests and needs. In this paper, we explore the challenges that researchers and communities experience in meeting these requirements by reviewing case studies of three natural science projects in Nunavut. Together, these projects exemplify both success and failure in negotiating research relationships. The case studies highlight three principal sources of researcher-community conflict: 1) debate surrounding acceptable impacts of research and the nature and extent of local benefits that research projects can and should provide; 2) uncertainty over who has the power and authority to dictate terms and conditions under which projects should be licensed; and 3) the appropriate research methodology and design to balance local expectations and research needs. The Nunavut research licensing process under the Scientists Act is an important opportunity for communities, scientists, and regulatory agencies to negotiate power relationships. However, the standards and procedures used to evaluate research impact remain unclear, as does the role of communities in the decision-making process for research licensing. The case studies also demonstrate the critical role of trust and rapport, forged through early and frequent communication, efforts to provide local training, and opportunities for community members to observe, participate in, and derive employment from project activities. Clarifying research policies in Nunavut is one step to improving relations between scientists and communities. In addition, steps need to be taken at both policy and project levels to train researchers, educate funding programs, mobilize institutions, and empower communities, thus strengthening the capacity of all stakeholders in northern research.",2009,15,91,6,5,5,6,8,8,5,6,6,9,12
15f688bdc381f2bc19a1411ec650a38ebec0c7a9,,1998,0,159,3,2,0,1,3,1,10,4,1,3,2
febde76b600bdcb5fa8005eda4da979e2311e965,,1892,0,57,2,0,0,0,0,0,0,0,0,0,0
0bdfc5a5fd0ef6e7bb4e993f7fa4d64228acefea,,1899,0,0,0,0,0,0,0,0,0,0,0,0,0
a022df6e1db5f2e1be32388ca33cfc0391bb490d,"EVERY observation is made and repeated for the purpose of defining a picture or image. Number work is a means of defining a picture or image through a determination of quantity. In the application of number, therefore, the same principles must be observed that are employed in defining the image by other means. I. There must be a clear conception of the image to be developed. Number work, not less than drawing, painting, modeling, and written composition, depends upon the primary",1902,0,0,0,0,0,0,0,0,0,0,0,0,0
61c80563df60e566d3cbfe6d26ea91d557860a9d,"IN order to meet as fully as possible the needs of those who desire work in nature study, the subject will be presented in three closely related courses. Course I deals with the subject in its general aspects, and Courses II and III deal with special topics in considerable detail. During the first three weeks, Course I will be open only to those who enter for the first half of the term. During the last three weeks it will be open (I) to those who enter at that time and (2) to those who are taking either Course II or Course III, and who may desire a somewhat broader survey of the field. The primaryand grammar-grade teachers who elect Course I will be assigned to separate sections, and the work will be adapted to each. The sections will be divided into groups for convenience, and each group will be assigned according to the choice of the individual to definite work upon which each student will be expected to make reports as often as necessary. The subjects given to the groups will be selected from the subjoined syllabus of topics. Each group will report to the entire class, so that the mutual relations of the different lines of study will appear. The topics for discussion will be assigned to different groups for presentation, through which the pedagogic aspects of the subject will be considered.",1901,1,0,0,0,0,0,0,0,0,0,0,0,0
30c9dbd8c68e009b1205da2c6ed7d3a57aebca7c,,1901,0,0,0,0,0,0,0,0,0,0,0,0,0
7fe1bc4a8e62195178294117a3ab64b97fe47d22,"Most behavior analysts emphasize that a science of behavior must be a natural science as opposed to a social science or any other such description. But what does this mean and why is it important? I explore these questions by attempting to characterize the designation “natural science” and briefly surveying what behavior analysis has in common with other putative natural sciences. Included in the discussion are problems of agency, background assumptions, mechanism, and some examples of shared problems and issues with other natural sciences. Special focus is placed on behavior analysis as a biological science and the implications of that status.",2009,34,19,0,0,1,2,0,3,0,4,2,2,1
f502a77d80062f2dd42ed0b065ded02a5868c2e9,"Following an initial discussion of the general nature of interpretation in contemporary psychology, and social and natural science, relevant views of Charles Taylor and Thomas Kuhn are considered in some detail. Although both Taylor and Kuhn agree that interpretation in the social or human sciences differs in some ways from interpretation in the natural sciences, they disagree about the nature and origins of such difference. Our own analysis follows, in which we consider differences in interpretation between the natural and social sciences (psychology in particular) in terms of Ian Hacking's use of Elizabeth Anscombe's conceptualization of actions as intentional acts under particular descriptions. We conclude that both Taylor and Kuhn are correct to point to differences in interpretation between the natural and social sciences. We also argue that in psychology, such interpretive differences, contra Kuhn and pro Taylor, are qualitative rather than quantitative. They arise from the nature of persons as self-interpretive, reactive beings who act under socioculturally sanctioned, linguistic descriptions. The actions of psychological persons may display qualitative differences over time and across contexts as these descriptions, including social scientific and psychological findings and interpretations, change. In contrast, even when descriptions in natural science change, such changes do not spawn changes in the self-interpretations and intentional actions of the focal phenomena of natural science. We also make the point that much current confusion surrounding interpretation in science arises from the unwarranted tendency of some commentators to treat interpretation as subjective, in ways that ignore the objective grounding of interpretation within regulated social practices, including scientific practices sanctioned by scientific communities.",2009,9,15,0,1,0,0,1,2,0,2,2,2,2
7f4f566b160c7abbf57da0fa51ae119640c28770,"The study aims at establishing whether Foundation Phase schooling provides a proper foundation for the promotion of scientific literacy. Natural Science in the Foundation Phase is understood as scientific knowledge, process skills, and values and attitudes, which together should foster scientific literacy. Influential perspectives on learning, and teaching methods appropriate to Natural Science education in the Foundation Phase, are reviewed, and the Natural Science Learning Area in the RNCS discussed in the context of global trends in curriculum development. Finally the findings of an empirical survey on the perceptions of Foundation Phase teachers with regard to Natural Science teaching and learning, are presented. Major findings include the following: (1) Scientific literacy is currently not a curriculum priority in the Foundation Phase, due mainly to meagre time allocation and lack of applicable Learning Outcomes. (2) Although teachers appear predominantly positive towards the Learning Area, significant shortcomings need to be addressed before Natural Science teaching in the Foundation Phase may claim to provide the required basis for promoting scientific literacy.",2009,129,12,1,0,0,0,1,4,1,2,1,0,1
a78d7e14769e88809543d71159c6b16f6b21b702,How was the hypothetical character of theories of experiencethought about throughout the history of science? The essays cover periods from the middle ages to the 19th and 20th centuries. It is fascinating to see how natural scientists and philosophers were increasingly forced to realize that a natural science without hypotheses is not possible.,2009,0,12,0,0,1,1,3,1,2,3,0,0,1
4b4a4b295a5e8221b0165e65a5a2aeaa58dbea85,"Analytical table of contents Preface Introduction: rationality Part I. Representing: 1. What is scientific realism? 2. Building and causing 3. Positivism 4. Pragmatism 5. Incommensurability 6. Reference 7. Internal realism 8. A surrogate for truth Part II. Intervening: 9. Experiment 10. Observation 11. Microscopes 12. Speculation, calculation, models, approximations 13. The creation of phenomena 14. Measurement 15. Baconian topics 16. Experimentation and scientific realism Further reading Index.",1983,0,1243,47,0,1,1,4,1,3,1,3,11,10
1610cf06f9281bfbda5368d15c286ad6a3191d80,"In this study, mixed research methods were adopted to examine the online game-based learning environment for ""Go Go Bugs"". The purpose of this study was to determine if an online game-based learning environment can engage and motivate students to learn more natural science by using game features than the control group, which had the same learning environment, but no game features. This experiment examined the engagement by participants' frequencies of returning to the learning environment after school hours, within a time period of two weeks. The results of the comparison of pre- and post- surveys showed significant improvement in increasing participants' interest in learning natural science in a game-based learning environment over that of the non-game-based learning environment. Both quantitative and qualitative results from this study indicated that this game-based learning environment successfully motivated participants in exploring natural science and engaging in the learning activities. However, there was no significant result showing that the game-based learning environment improves students' learning achievement more than with the non-game-based learning environment.",2007,28,50,3,0,1,0,2,7,2,7,3,1,8
e28fc8015226ba662b5a6468e3b2f33837358fd2,"Information processes and computation continue to be found abundantly in the deep structures of many fields. Computing is not---in fact, never was---a science only of the artificial.",2007,11,203,9,4,14,25,15,16,23,20,15,13,7
4d09ecd48768f8945dbec99323d03f7eb62b8316,Preface 1. Metaphysical foundations of phoronomy 2. Metaphysical foundations of dynamics 3. Metaphysical foundations of mechanics 4. Metaphysical foundations of phenomenology.,2007,0,209,8,10,10,9,10,9,8,16,19,16,14
cec073bd5d5418c7ff112e5807fef1089a59d453,"This study examines the relationship between citation frequency and the human capital of teams of authors. Analysis of a random sample of articles published in top natural science journals shows that articles co-authored by teams including frequently cited scholars and teams whose members have diverse disciplinary backgrounds have greater citation frequency. The institutional prestige, the percentage of team members at U. S. institutions and the variety of disciplines represented by team member backgrounds do not influence citation frequency. The study introduces a method for evaluating the extent of multidisciplinarity that accounts for the relatedness of disciplines or authors.",2009,46,22,1,1,1,0,2,4,2,1,4,2,0
050f600c03ea03d13839cd108acdbd1e0d59b96d,"The psychology classic a detailed study of scientific theories of human nature and the possible ways in which human behavior can be predicted and controlled from one of the most influential behaviorists of the twentieth century and the author of ""Walden Two."" This is an important book, exceptionally well written, and logically consistent with the basic premise of the unitary nature of science. Many students of society and culture would take violent issue with most of the things that Skinner has to say, but even those who disagree most will find this a stimulating book. Samuel M. Strong, ""The American Journal of Sociology"" This is a remarkable book remarkable in that it presents a strong, consistent, and all but exhaustive case for a natural science of human behavior It ought to be valuable for those whose preferences lie with, as well as those whose preferences stand against, a behavioristic approach to human activity. Harry Prosch, ""Ethics""""",1953,13,6841,265,0,5,2,10,12,16,11,10,17,15
413886ce56be596e068c8e069f1c3ad7a6e563b8,"I. METAPHYSICS, ONTOLOGY, AND LOGIC II. OBJECTS AND PROPERTIES III. METAPHYSICS AND NATURAL SCIENCE IV. TRUTH, TRUTHMAKING, AND METAPHYSICAL REALISM",2006,0,266,12,4,4,5,10,8,15,9,36,14,22
7fa07c973ff7d7a84c8658c25a6532fb1e018b1c,"Regional-scale restoration is a tool of growing importance in environmental management, and the number, scope, and complexity of restoration programs is increasing. Although the importance of natural science to the success of such projects generally is recognized, the actual use of natural science in these programs rarely has been evaluated. We used techniques of program evaluation to examine the use of natural science in six American and three Western European regional-scale restoration programs. Our results suggest that ensuring the technical rigor and directed application of the science is important to program development and delivery. However, the influence of science may be constrained if strategies for its integration into the broader program are lacking. Consequently, the influence of natural science in restoration programs is greatest when formal mechanisms exist for incorporating science into programs, for example, via a framework for integration of science and policy. Our evaluation proposes a model that can be used to enhance the influence of natural science in regional-scale restoration programs in the United States and elsewhere.",2006,39,23,5,0,1,3,4,5,1,0,3,1,2
8a2ce9609450a02cffe7d6c90ef98ed900880f2f,"In this paper, we studied the research areas of Chinese natural science basic research from a point view of complex network. Two research areas are considered to be connected if they appear in one fund proposal. The explicit network of such connections using data from 1999 to 2004 is constructed. The analysis of the real data shows that the degree distribution of the research areas network (RAN) may be better fitted by the exponential distribution. It displays small world effect in which randomly chosen pairs of research areas are typically separated by only a short path of intermediate research areas. The average distance of RAN decreases with time, while the average clustering coefficient increases with time, which indicates that the scientific study would like to be integrated together in terms of the studied areas. The relationship between the clustering coefficient C(k) and the degree k indicates that there is no hierarchical organization in RAN.",2005,112,26,0,0,1,2,4,4,5,1,1,1,1
63b37ee975c40e74bd26819ec6c3fe341656fe7f,"This paper reviews the recent literature on monetary policy rules. We exploit the monetary policy design problem within a simple baseline theoretical framework. We then consider the implications of adding various real world complications. Among other things, we show that the optimal policy implicitly incorporates inflation targeting. We also characterize the gains from making a credible commitment to fight inflation. In contrast to conventional wisdom, we show that gains from commitment may emerge even if the central bank is not trying to inadvisedly push output above its natural level. We also consider the implications of frictions such as imperfect information.",1999,186,5463,621,20,85,194,227,305,292,274,320,368,343
ec1d6bbda7331fc074cca94f0bc07127327dea8b,"Integrated studies of coupled human and natural systems reveal new and complex patterns and processes not evident when studied by social or natural scientists separately. Synthesis of six case studies from around the world shows that couplings between human and natural systems vary across space, time, and organizational units. They also exhibit nonlinear dynamics with thresholds, reciprocal feedback loops, time lags, resilience, heterogeneity, and surprises. Furthermore, past couplings have legacy effects on present conditions and future possibilities.",2007,75,2590,91,3,31,88,116,135,149,190,212,232,223
310beca34914ca073a88f04a2b3ae12b302dcd81,"Abstract The second phase of the Mpumalanga Secondary Science Initiative (MSSI) was launched in Mpumalanga Province, South Africa in 2003. The MSSI seeks to improve the teaching and learning of mathematics and science in all secondary schools in the province over a period of three years. To achieve this goal an in-service system has been developed. The long-term research of the MSSI is aimed at examining the effect of the intervention on the three science learning outcomes as defined by the new curriculum. Tests were developed to assess 10 learners in each of Grades 8 and 9 in 40 schools, representing a 6% sample. This paper reports on the methodology of the baseline assessments undertaken and the extent of learners' attainment levels in the three Natural Science outcomes. It then looks at the results in terms of learners' socio-economic backgrounds. Finally it discusses some of the dilemmas encountered in this research, such as the issue of validity in the light of possible misalignments between the instrument, practice and policy.",2005,26,22,3,1,1,1,1,6,3,3,0,1,2
c3d62bc0ab3e900f0fbf33949754ed9919139164,"This article discusses the results of a qualitative study, based on case studies, aimed at: (a) assessing a group of Portuguese secondary school natural science teachers regarding their conceptions of the nature, teaching and learning of science; (b) studying possible impacts of recent controversies surrounding scientific and technological issues on these conceptions and on teachers' classroom practices. Five teachers, with different backgrounds and teaching experience, were observed during classes and interviewed with the purpose of studying: (a) the relationship between their conceptions and classroom practices; and (b) the factors that impede or enhance this relationship. Subsequently, observation notes and interview transcriptions were systematically analysed. The socio-scientific controversies recently discussed in Portugal seem to have had an impact on teachers' (1) conceptions about the nature, teaching and learning of science; and (2) classroom practice. However, not all teachers were able to teach according to their conceptions. Some factors seem to mediate the relationship between teachers' conceptions and classroom practice: National Curriculum, national exams, teachers' previous experience as scientists, and personal educational priorities or aims. Based on the results obtained, some remarks and educational implications are discussed.",2004,23,49,1,1,1,2,3,5,5,0,1,5,4
2294cddf83c01629c138b307bffcecb19822e307,"Abstract The Chomskyan revolution in linguistics in the 1950s in essence turned linguistics into a branch of cognitive science (and ultimately biology) by both changing the linguistic landscape and forcing a radical change in cognitive science to accommodate linguistics as many of us conceive of it today. More recently Chomsky has advanced the boldest version of his naturalistic approach to language by proposing a Minimalist Program for linguistic theory. In this article, we wish to examine the foundations of the Minimalist Program and its antecedents and draw parallelisms with (meta-)methodological foundations in better-developed sciences such as physics. Once established, such parallelisms, we argue, help direct inquiry in linguistics and cognitive science/biology and unify both disciplines.",2005,150,54,1,1,0,2,4,6,5,2,1,5,2
2bfbe016c77a55119c84d93bfb1b5f3139e83771,"Interdisciplinary collaboration occurs when people with different educational and research backgrounds bring complementary skills to bear on a problem or task. The strength of interdisciplinary scientific research collaboration is its capacity to bring together diverse scientific knowledge to address complex problems and questions. However, interdisciplinary scientific research can be difficult to initiate and sustain. We do not yet fully understand factors that impact interdisciplinary scientific research collaboration. This study synthesizes empirical data from two empirical studies to provide a more comprehensive understanding of interdisciplinary scientific research collaboration within the natural sciences in ac ademia. Data analysis confirmed factors previously identified in various literatures and yielded new factors. A total of twenty factors were identified, and classified into four categories: personal, resources, motivation and common ground. These categorie s and their factors are described, and implications for academic policies and practices to facilitate and sustain interdisciplinary collaboration are discussed.",2005,17,50,3,0,1,4,2,0,0,4,5,2,3
9fff1ce502fd37eb5512b972177c4cff3b038c4b,"The global tendency is obvious: interest in science is on the decrease, the number of pupils choosing university science curriculums has been constantly declining, and scientific knowledge in society (especially among young people) is inadequate. In our opinion, humanity verges on social cataclysms owing to inadequate natural science education as well as on insufficient and often improper knowledge of nature and human. Natural sciences give us most fundamental knowledge about the world of nature. Encouragement of young people’s interest in science is the essential scientific problem. As educational paradigms are being altered we must search for new quality approaches to teaching chemistry and other science subjects. The research, which involved 350 senior pupils from Latvia and 762 from Lithuania, analyzes present-day situation in natural science education. We tried to analyze the factors that cause the interest in natural sciences to decline: inadequate content of teaching, issues related to teachers‘ competence, general attitude of society to natural sciences etc.",2004,13,17,0,2,1,1,1,0,2,0,0,2,1
9e86f2a2d795ac16288d030844b918e125658025,"From the viewpoint of biology, learning and education can be defined as the processes of forming neuronal connections in response to external environmental stimuli, and of controlling or adding appropriate stimuli, respectively. Learning and education can thus be studied as a new field of natural sciences with the entire human life span as its subject, thus including various problems such as fetal environment, childcare, language acquisition, general/special education, and rehabilitation. Non-invasive imaging of higher-order brain functions in humans will clarify the brain's developmental processes, and will provide various evidence for learning sciences. This new approach is called 'developing the brain' or 'brain science and education'. The origin of the concept and its present state are described and its future prospects are briefly analyzed.",2004,52,70,3,1,3,6,3,3,7,5,5,2,5
7b83b725196e087b2b96a6f185c93eff259c6182,,2004,0,67,0,0,0,1,0,3,1,1,0,3,3
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
78360a9b8edc5f4b2884024a39c318b484e3f3ce,"Given the growing multicultural composition of South African classrooms, educators of science and technology, like educators across the spectrum of all learning areas, are increasingly challenged to reflect how they and their learners conceive of and, as a result, construct knowledge. The reality is that in an expanding globalised world, learners can easily become alienated from what is taught in science and technology, as well as the way it is taught. Indigenous Knowledge Systems (IKS), as a broad framework of thinking about our local context, seeks to problematise the insufficient integration of the cultural-social and the canonical-academic dimensions of natural science and technology education. In this article I conceptualise and clarify IKS vis-a-vis knowledge production, particularly towards educational transformation in which educators may assume that all learners are the same in terms of identity and cultural dynamics. Natural science and technology, in particular, have assumed a definite culture of power, which has marginalized the majority of learners in the past. IKS strategically wishes to transform this view and therefore holds valuable implications for educators in the learning areas of natural science and technology.",2002,58,72,4,0,0,0,1,0,2,2,4,5,3
9b981fa3d1559ac7f17930a25abcb592d03419e3,"Marrying the biological and social sciences: culture, social constructions and natural science possible frameworks evolution and the theory of evolution alternative theories to NeoDarwinism how good a theory is evolutionary theory? suggested readings. The evolution of intelligence: why intelligence ever evolved at all the limits of reductionism intelligence unlimited? Fodor poses a problem human intelligence as adaptation or exaptation suggested readings. The emergence of culture: broadening the picture the trouble with ""levels"" a solution to the levels problem suggested readings. Naturalizing culture the process way: the puzzle of war universal Darwinism modelling co-evolution the ""new"" science of memetics suggested readings. Causal mechanisms: a general framework for understanding psychological mechanism what those mechanisms may be concepts, schemata and other higher-order knowledge structures imitation language theory of mind social force a single magical mechanism? suggested readings. Individuals, groups and culture: the behavioural ecology of group living the units and levels of selection vehicles, interactors and the revival of group selection niche construction suggested readings. The strangeness of culture: the construction of social reality a sociological turn social representations cultural psychology a tentative conclusion suggested readings.",2002,0,77,8,0,3,1,8,7,4,4,1,6,9
5d2af227cfafe9b468ac48d97658a8cc051ed08f,"Abstract We investigated the significance of risk assessment studies in the public discussion on CO 2 emissions. Politicians and representatives from the public where interviewed by using the social-science technique of qualitative in-depth interviews. Three different types of attitudes towards natural science were found among politicians. Depending on which attitude a politician holds, risk assessment studies can have an impact on his/her readiness to support environmental policy measures. Regarding lay people, key factors affecting the acceptance of environmental policy measures are knowledge of environmental problems, their impacts on ecosystems or human health as well as direct personal perception of those impacts. Since direct perception is not always possible in everyday life, natural science experiments might be a means for successfully mediating this lacking perception.",2003,36,23,0,0,0,0,1,2,2,2,0,0,3
16eaa16708e5c5a27c202ea300f80442b50d3939,"Research methods similar to those used in the natural sciences have long been the norm in organizational behavior and organization theory. However, several writers have recently questioned their appropriateness for the study of organizations and the groups and individuals who make them up. In this paper I examine five major objections to the use of such methods in organizational behavior and organization theory and conclude that, while they may indicate a need for more thoughtful application of the natural science approach, they do not rule it out as the primary research strategy for the study of organizations.",1980,27,84,2,0,0,2,2,1,2,2,2,0,2
c8831d7d318b8d59f9b958d250a58f253f08bd8a,"This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.",2009,90,5538,1042,9,66,155,236,340,507,531,647,665,702
e5136e9306bf1b0e3d4be0cea384ee9a969a44fa,"Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.",2006,110,75509,5315,0,0,0,0,0,0,1,0,0,0
67556c4f0cfdd1f09fff373768b03638f949be0d,"Chapter 3 deals with probability distributions, discrete and continuous densities, distribution functions, bivariate distributions, means, variances, covariance, correlation, and some random process material. Chapter 4 is a detailed study of the concept of utility including the psychological aspects, risk, attributes, rules for utilities, multidimensional utility, and normal form of analysis. Chapter 5 treats games and optimization, linear optimization, and mixed strategies. Entropy is the topic of Chapter 6 with sections devoted to entropy, disorder, information, Shannon’s theorem, demon’s roulette, Maxwell– Boltzmann distribution, Schrodinger’s nutshell, maximum entropy probability distributions, blackbodies, and Bose–Einstein distribution. Chapter 7 is standard statistical fare including transformations of random variables, characteristic functions, generating functions, and the classic limit theorems such as the central limit theorem and the laws of large numbers. Chapter 8 is about exchangeability and inference with sections on Bayesian techniques and classical inference. Partial exchangeability is also treated. Chapter 9 considers such things as order statistics, extreme value, intensity, hazard functions, and Poisson processes. Chapter 10 covers basic elements of risk and reliability, while Chapter 11 is devoted to curve fitting, regression, and Monte Carlo simulation. There is an ample number of exercises at the ends of the chapters with answers or comments on many of them in an appendix in the back of the book. Other appendices are on the common discrete and continuous distributions and mathematical aspects of integration.",2007,0,18868,4795,0,0,0,0,0,0,0,1,7,931
4d8a5338042da99819746ff835b6f299135e2023,Contents: Prefaces. The Concepts of Power Analysis. The t-Test for Means. The Significance of a Product Moment rs (subscript s). Differences Between Correlation Coefficients. The Test That a Proportion is .50 and the Sign Test. Differences Between Proportions. Chi-Square Tests for Goodness of Fit and Contingency Tables. The Analysis of Variance and Covariance. Multiple Regression and Correlation Analysis. Set Correlation and Multivariate Methods. Some Issues in Power Analysis. Computational Procedures.,1969,0,106858,18088,0,0,0,0,0,0,0,0,0,0
75c2f465d59739dbc06b70fd34dc3c1b2336103e,"Abstract The need for a simply applied quantitative assessment of handedness is discussed and some previous forms reviewed. An inventory of 20 items with a set of instructions and response- and computational-conventions is proposed and the results obtained from a young adult population numbering some 1100 individuals are reported. The separate items are examined from the point of view of sex, cultural and socio-economic factors which might appertain to them and also of their inter-relationship to each other and to the measure computed from them all. Criteria derived from these considerations are then applied to eliminate 10 of the original 20 items and the results recomputed to provide frequency-distribution and cumulative frequency functions and a revised item-analysis. The difference of incidence of handedness between the sexes is discussed.",1971,12,30938,2544,0,0,0,0,0,0,0,0,0,0
ec3d71a2fdd01968a6dc638ee261715a0f118c1e,"Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An overdispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of overdispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au",2009,10,21517,1161,0,0,0,0,0,0,1,1,7,65
a2893118e14c29a23472b02249b4641b9971786b,"Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.",2005,85,26854,2906,0,0,0,0,0,0,0,0,0,0
9529c25408dc86194d417aed73d49ae0e418f1be,"32.03 MB Free download Econometric Analysis of Cross Section and Panel Data book PDF, FB2, EPUB and MOBI. Read online Econometric Analysis of Cross Section and Panel Data which classified as Other that has 776 pages that contain constructive material with lovely reading experience. Reading online Econometric Analysis of Cross Section and Panel Data book will be provide using wonderful book reader and it's might gives you some access to identifying the book content before you download the book.",2002,0,21007,2367,0,0,0,0,0,0,0,3,0,4
d17669e4f3f4dd3a180bde84dd54a508d0dc22f4,"A comprehensive, but simple-to-use software package for executing a range of standard numerical analysis and operations used in quantitative paleontology has been developed. The program, called PAST (PAleontological STatistics), runs on standard Windows computers and is available free of charge. PAST integrates spreadsheet-type data entry with univariate and multivariate statistics, curve fitting, timeseries analysis, data plotting, and simple phylogenetic analysis. Many of the functions are specific to paleontology and ecology, and these functions are not found in standard, more extensive, statistical packages. PAST also includes fourteen case studies (data files and exercises) illustrating use of the program for paleontological problems, making it a complete educational package for courses in quantitative methods.",2001,17,17962,3587,0,0,0,0,0,0,0,0,0,0
1d6a71481bcb38a593480a9f8827079ce99835b1,"Matthew B. Miles, Qualitative Data Analysis A Methods Sourcebook, Third Edition. The Third Edition of Miles & Huberman's classic research methods text is updated and streamlined by Johnny Saldana, author of The Coding Manual for Qualitative Researchers. Several of the data display strategies from previous editions are now presented in re-envisioned and reorganized formats to enhance reader accessibility and comprehension. The Third Edition's presentation of the fundamentals of research design and data management is followed by five distinct methods of analysis: exploring, describing, ordering, explaining, and predicting. Miles and Huberman's original research studies are profiled and accompanied with new examples from Saldana's recent qualitative work. The book's most celebrated chapter, ""Drawing and Verifying Conclusions,"" is retained and revised, and the chapter on report writing has been greatly expanded, and is now called ""Writing About Qualitative Research."" Comprehensive and authoritative, Qualitative Data Analysis has been elegantly revised for a new generation of qualitative researchers. Johnny Saldana, The Coding Manual for Qualitative Researchers, Second Edition. The Second Edition of Johnny Saldana's international bestseller provides an in-depth guide to the multiple approaches available for coding qualitative data. Fully up-to-date, it includes new chapters, more coding techniques and an additional glossary. Clear, practical and authoritative, the book: describes how coding initiates qualitative data analysis; demonstrates the writing of analytic memos; discusses available analytic software; suggests how best to use the book for particular studies. In total, 32 coding methods are profiled that can be applied to a range of research genres from grounded theory to phenomenology to narrative inquiry. For each approach, Saldana discusses the method's origins, a description of the method, practical applications, and a clearly illustrated example with analytic follow-up. A unique and invaluable reference for students, teachers, and practitioners of qualitative inquiry, this book is essential reading across the social sciences. Stephanie D. H. Evergreen, Presenting Data Effectively Communicating Your Findings for Maximum Impact. This is a step-by-step guide to making the research results presented in reports, slideshows, posters, and data visualizations more interesting. Written in an easy, accessible manner, Presenting Data Effectively provides guiding principles for designing data presentations so that they are more likely to be heard, remembered, and used. The guidance in the book stems from the author's extensive study of research reporting, a solid review of the literature in graphic design and related fields, and the input of a panel of graphic design experts. Those concepts are then translated into language relevant to students, researchers, evaluators, and non-profit workers - anyone in a position to have to report on data to an outside audience. The book guides the reader through design choices related to four primary areas: graphics, type, color, and arrangement. As a result, readers can present data more effectively, with the clarity and professionalism that best represents their work.",1994,0,39168,2255,0,0,0,0,0,0,0,0,0,0
0ad5733eafb41274895bf1dce6b92ae8f3d68c60,,2004,0,18829,2817,1,3,6,2,10,714,1441,1290,1233,1117
7bd23e6ec32cb1507a385c21a21150e9c332682f,"genalex is a user-friendly cross-platform package that runs within Microsoft Excel, enabling population genetic analyses of codominant, haploid and binary data. Allele frequency-based analyses include heterozygosity, F statistics, Nei's genetic distance, population assignment, probabilities of identity and pairwise relatedness. Distance-based calculations include amova, principal coordinates analysis (PCA), Mantel tests, multivariate and 2D spatial autocorrelation and twogener. More than 20 different graphs summarize data and aid exploration. Sequence and genotype data can be imported from automated sequencers, and exported to other software. Initially designed as tool for teaching, genalex 6 now offers features for researchers as well. Documentation and the program are available at http://www.anu.edu.au/BoZo/GenAlEx/",2006,12,14542,3329,0,0,0,0,0,0,3,988,1332,1368
0d6cf3cd3794bc31a4e5a8ded4d4ba83bb20f9b0,"BOOK REVIEW: Constructing grounded theory. A practical guide through qualitative analysis Kathy Charmaz, 2006, 208 pp. London: Sage. ISBN 2005928035",2006,0,10706,2251,0,0,0,1,374,666,939,1134,1329,847
91dd073b9bfaf29b6c3d3a58418e2bdc765541ea,,1989,0,45850,9927,0,0,0,0,0,0,0,0,0,0
895860c6083736508d2541900cdf0960eb11592f,"The design, implementation, and capabilities of an extensible visualization system, UCSF Chimera, are discussed. Chimera is segmented into a core that provides basic services and visualization, and extensions that provide most higher level functionality. This architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features. Two unusual extensions are presented: Multiscale, which adds the ability to visualize large‐scale molecular assemblies such as viral coats, and Collaboratory, which allows researchers to share a Chimera session interactively despite being at separate locales. Other extensions include Multalign Viewer, for showing multiple sequence alignments and associated structures; ViewDock, for screening docked ligand orientations; Movie, for replaying molecular dynamics trajectories; and Volume Viewer, for display and analysis of volumetric data. A discussion of the usage of Chimera in real‐world situations is given, along with anticipated future directions. Chimera includes full user documentation, is free to academic and nonprofit users, and is available for Microsoft Windows, Linux, Apple Mac OS X, SGI IRIX, and HP Tru64 Unix from http://www.cgl.ucsf.edu/chimera/. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1605–1612, 2004",2004,70,28265,1711,0,0,0,0,1,0,0,0,0,0
e7c8aa2cb2223f17615c1b1ae3b33095466e95cc,"The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.",2001,21,115784,1655,0,0,0,0,0,0,0,0,1,0
c92d6fa1e30e12946c874e5a8b9aeee3c0155e29,Introduction The Logic of Hierarchical Linear Models Principles of Estimation and Hypothesis Testing for Hierarchical Linear Models An Illustration Applications in Organizational Research Applications in the Study of Individual Change Applications in Meta-Analysis and Other Cases Where Level-1 Variances are Known Three-Level Models Assessing the Adequacy of Hierarchical Models Technical Appendix,1992,2,24107,1999,0,0,0,0,0,0,0,0,0,0
05abdc87bcaf2963fd511672e64ab39d02239aaf,,2007,30,24477,2512,0,1,0,0,1,0,5,6,546,2032
2d6f573c36c5e2153b65859fb080523fc4d842d0,"We announce the release of the fourth version of MEGA software, which expands on the existing facilities for editing DNA sequence data from autosequencers, mining Web-databases, performing automatic and manual sequence alignment, analyzing sequence alignments to estimate evolutionary distances, inferring phylogenetic trees, and testing evolutionary hypotheses. Version 4 includes a unique facility to generate captions, written in figure legend format, in order to provide natural language descriptions of the models and methods used in the analyses. This facility aims to promote a better understanding of the underlying assumptions used in analyses, and of the results generated. Another new feature is the Maximum Composite Likelihood (MCL) method for estimating evolutionary distances between all pairs of sequences simultaneously, with and without incorporating rate variation among sites and substitution pattern heterogeneities among lineages. This MCL method also can be used to estimate transition/transversion bias and nucleotide substitution pattern without knowledge of the phylogenetic tree. This new version is a native 32-bit Windows application with multi-threading and multi-user supports, and it is also available to run in a Linux desktop environment (via the Wine compatibility layer) and on Intel-based Macintosh computers under the Parallels program. The current version of MEGA is available free of charge at (http://www.megasoftware.net).",2007,10,28416,6041,0,0,0,0,1,18,2040,2152,1667,1255
57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,"DAVID bioinformatics resources consists of an integrated biological knowledgebase and analytic tools aimed at systematically extracting biological meaning from large gene/protein lists. This protocol explains how to use DAVID, a high-throughput and integrated data-mining environment, to analyze gene lists derived from high-throughput genomic experiments. The procedure first requires uploading a gene list containing any number of common gene identifiers followed by analysis using one or more text and pathway-mining tools such as gene functional classification, functional annotation chart or clustering and functional annotation table. By following this protocol, investigators are able to gain an in-depth understanding of the biological themes in lists of genes that are enriched in genome-scale studies.",2008,16,27480,2131,0,0,0,0,0,0,0,0,7,39
e09bb0025f4939a4bd233a70937584b4d7bdd0a7,"BackgroundThe evolutionary analysis of molecular sequence variation is a statistical enterprise. This is reflected in the increased use of probabilistic models for phylogenetic inference, multiple sequence alignment, and molecular population genetics. Here we present BEAST: a fast, flexible software architecture for Bayesian analysis of molecular sequences related by an evolutionary tree. A large number of popular stochastic models of sequence evolution are provided and tree-based models suitable for both within- and between-species sequence data are implemented.ResultsBEAST version 1.4.6 consists of 81000 lines of Java source code, 779 classes and 81 packages. It provides models for DNA and protein sequence evolution, highly parametric coalescent analysis, relaxed clock phylogenetics, non-contemporaneous sequence data, statistical alignment and a wide range of options for prior distributions. BEAST source code is object-oriented, modular in design and freely available at http://beast-mcmc.googlecode.com/ under the GNU LGPL license.ConclusionBEAST is a powerful and flexible evolutionary analysis package for molecular sequence variation. It also provides a resource for the further development of new models and statistical methods of evolutionary analysis.",2007,45,11134,3502,0,0,1,253,973,1270,1219,1114,1018,940
cad327e1e3a0799202cb40da5da51d7b0616b64e,"Arlequin ver 3.0 is a software package integrating several basic and advanced methods for population genetics data analysis, like the computation of standard genetic diversity indices, the estimation of allele and haplotype frequencies, tests of departure from linkage equilibrium, departure from selective neutrality and demographic equilibrium, estimation or parameters from past population expansions, and thorough analyses of population subdivision under the AMOVA framework. Arlequin 3 introduces a completely new graphical interface written in C++, a more robust semantic analysis of input files, and two new methods: a Bayesian estimation of gametic phase from multi-locus genotypes, and an estimation of the parameters of an instantaneous spatial expansion from DNA sequence polymorphism. Arlequin can handle several data types like DNA sequences, microsatellite data, or standard multi-locus genotypes. A Windows version of the software is freely available on http://cmpg.unibe.ch/software/arlequin3.",2007,148,12555,4437,1,1,21,1208,1437,1392,1211,1052,891,751
80935b370bac09ce615a002caabc30fbb26f029b,"With its theoretical basis firmly established in molecular evolutionary and population genetics, the comparative DNA and protein sequence analysis plays a central role in reconstructing the evolutionary histories of species and multigene families, estimating rates of molecular evolution, and inferring the nature and extent of selective forces shaping the evolution of genes and genomes. The scope of these investigations has now expanded greatly owing to the development of high-throughput sequencing techniques and novel statistical and computational methods. These methods require easy-to-use computer programs. One such effort has been to produce Molecular Evolutionary Genetics Analysis (MEGA) software, with its focus on facilitating the exploration and analysis of the DNA and protein sequence variation from an evolutionary perspective. Currently in its third major release, MEGA3 contains facilities for automatic and manual sequence alignment, web-based mining of databases, inference of the phylogenetic trees, estimation of evolutionary distances and testing evolutionary hypotheses. This paper provides an overview of the statistical methods, computational tools, and visual exploration modules for data input and the results obtainable in MEGA.",2004,73,12037,3302,1,0,26,1588,2150,1625,1243,879,678,475
e2f4f64a17a05379e45f713d10d7c546bda7734a,"Contents: Preface. Introduction. Bivariate Correlation and Regression. Multiple Regression/Correlation With Two or More Independent Variables. Data Visualization, Exploration, and Assumption Checking: Diagnosing and Solving Regression Problems I. Data-Analytic Strategies Using Multiple Regression/Correlation. Quantitative Scales, Curvilinear Relationships, and Transformations. Interactions Among Continuous Variables. Categorical or Nominal Independent Variables. Interactions With Categorical Variables. Outliers and Multicollinearity: Diagnosing and Solving Regression Problems II. Missing Data. Multiple Regression/Correlation and Causal Models. Alternative Regression Models: Logistic, Poisson Regression, and the Generalized Linear Model. Random Coefficient Regression and Multilevel Models. Longitudinal Regression Methods. Multiple Dependent Variables: Set Correlation. Appendices: The Mathematical Basis for Multiple Regression/Correlation and Identification of the Inverse Matrix Elements. Determination of the Inverse Matrix and Applications Thereof.",1979,0,29550,1563,0,0,0,0,0,0,0,0,0,0
7c3b564bbdc8e7e3242257189ab7702d3e095115,"Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.",1985,0,29479,1819,0,0,0,0,0,0,0,0,0,0
4c97d9b9eb7c158e682844b394b42924af3c5b3f,"Interpretative phenomenological analysis (IPA) is an increasingly popular approach to qualitative inquiry. This handy text covers its theoretical foundations and provides a detailed guide to conducting IPA research. 
 
Extended worked examples from the authors' own studies in health, sexuality, psychological distress and identity illustrate the breadth and depth of IPA research. 
 
Each of the chapters also offers a guide to other good exemplars of IPA research in the designated area. The final section of the book considers how IPA connects with other contemporary qualitative approaches like discourse and narrative analysis and how it addresses issues to do with validity. The book is written in an accessible style and will be extremely useful to students and researchers in psychology and related disciplines in the health and social sciences.",2009,7,6189,1588,6,76,172,280,438,624,709,699,792,811
6126379e9ffbf088fc5cf838ba53a9b95526d0d5,"This journal frequently contains papers that report values of F-statistics estimated from genetic data collected from several populations. These parameters, FST, FIT, and FIS, were introduced by Wright (1951), and offer a convenient means of summarizing population structure. While there is some disagreement about the interpretation of the quantities, there is considerably more disagreement on the method of evaluating them. Different authors make different assumptions about sample sizes or numbers of populations and handle the difficulties of multiple alleles and unequal sample sizes in different ways. Wright himself, for example, did not consider the effects of finite sample size. The purpose of this discussion is to offer some unity to various estimation formulae and to point out that correlations of genes in structured populations, with which F-statistics are concerned, are expressed very conveniently with a set of parameters treated by Cockerham (1 969, 1973). We start with the parameters and construct appropriate estimators for them, rather than beginning the discussion with various data functions. The extension of Cockerham's work to multiple alleles and loci will be made explicit, and the use of jackknife procedures for estimating variances will be advocated. All of this may be regarded as an extension of a recent treatment of estimating the coancestry coefficient to serve as a mea-",1984,56,16883,2900,0,0,0,0,0,0,0,0,0,0
7ea9b1915072f2adff3762b59b7c7d79805fee8e,"If radiocarbon measurements are to be used at all for chronological purposes, we have to use statistical methods for calibration. The most widely used method of calibration can be seen as a simple application of Bayesian statistics, which uses both the information from the new measurement and information from the 14C calibration curve. In most dating applications, however, we have larger numbers of 14C measurements and we wish to relate those to events in the past. Bayesian statistics provides a coherent framework in which such analysis can be performed and is becoming a core element in many 14C dating projects. This article gives an overview of the main model components used in chronological analysis, their mathematical formulation, and examples of how such analyses can be performed using the latest version of the OxCal software (v4). Many such models can be put together, in a modular fashion, from simple elements, with defined constraints and groupings. In other cases, the commonly used ""uniform phase"" models might not be appropriate, and ramped, exponential, or normal distributions of events might be more useful. When considering analyses of these kinds, it is useful to be able run simulations on synthetic data. Methods for performing such tests are discussed here along with other methods of diagnosing possible problems with statistical models of this kind.",2009,69,4533,1445,11,103,201,275,402,450,446,453,577,449
25075e27b0df6f2be5a8c519171bdabd1c3ed817,History Conceptual Foundations Uses and Kinds of Inference The Logic of Content Analysis Designs Unitizing Sampling Recording Data Languages Constructs for Inference Analytical Techniques The Use of Computers Reliability Validity A Practical Guide,1980,0,23677,1926,0,0,0,0,0,0,0,0,0,0
2098391def9f8db6d400497b9ba397d526d227c3,This paper examines eight published reviews each reporting results from several related trials. Each review pools the results from the relevant trials in order to evaluate the efficacy of a certain treatment for a specified medical condition. These reviews lack consistent assessment of homogeneity of treatment effect before pooling. We discuss a random effects approach to combining evidence from a series of experiments comparing two treatments. This approach incorporates the heterogeneity of effects in the analysis of the overall treatment efficacy. The model can be extended to include relevant covariates which would reduce the heterogeneity and allow for more specific therapeutic recommendations. We suggest a simple noniterative procedure for characterizing the distribution of treatment effects in a series of studies.,1986,29,30518,936,0,0,0,0,0,0,0,0,0,0
b9544a1bf4b02c6648dbd12702bc10c00e20e197,"Content analysis is a widely used qualitative research technique. Rather than being a single method, current applications of content analysis show three distinct approaches: conventional, directed, or summative. All three approaches are used to interpret meaning from the content of text data and, hence, adhere to the naturalistic paradigm. The major differences among the approaches are coding schemes, origins of codes, and threats to trustworthiness. In conventional content analysis, coding categories are derived directly from the text data. With a directed approach, analysis starts with a theory or relevant research findings as guidance for initial codes. A summative content analysis involves counting and comparisons, usually of keywords or content, followed by the interpretation of the underlying context. The authors delineate analytic procedures specific to each approach and techniques addressing trustworthiness with hypothetical examples drawn from the area of end-of-life care.",2005,57,24869,943,0,0,0,0,0,0,0,0,0,0
2fcf90089d9f95025e8953812a43f0db2de3af7c,,2009,0,10327,2656,0,0,279,430,655,922,1128,1232,1332,1349
d76bde423b71f1cb900b988311bd2d71b700d506,"The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.",2002,35,21356,493,0,0,0,0,1,0,1,0,1,1
0cd64c55c98cdc4a7ef041a843ff796a995952a4,"Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure",1997,87,34023,852,0,0,0,0,0,0,0,0,0,0
fc448a7db5a2fac242705bd8e37ae1fc4a858643,"The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence.",2001,431,17450,430,5,1,4,2,1,2,8,513,764,774
9301eab07d6c64ee86651bc15ffab9663a6995b6,"Social Network Analysis Methods And Social network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them. Examples of social structures commonly visualized through social network ...",1996,0,13803,1523,0,0,0,0,0,2,1,0,0,1
1a77e19441f3a0e030998fb2d11d9dd774582403,"The techniques available for the interrogation and analysis of neuroimaging data have a large influence in determining the flexibility, sensitivity, and scope of neuroimaging experiments. The development of such methodologies has allowed investigators to address scientific questions that could not previously be answered and, as such, has become an important research area in its own right. In this paper, we present a review of the research carried out by the Analysis Group at the Oxford Centre for Functional MRI of the Brain (FMRIB). This research has focussed on the development of new methodologies for the analysis of both structural and functional magnetic resonance imaging data. The majority of the research laid out in this paper has been implemented as freely available software tools within FMRIB's Software Library (FSL).",2004,52,10627,1539,0,0,0,0,0,96,419,577,673,764
76ad159a2887b008e5a7335d124c148c13e65465,"We have developed a toolbox and graphic user interface, EEGLAB, running under the crossplatform MATLAB environment (The Mathworks, Inc.) for processing collections of single-trial and/or averaged EEG data of any number of channels. Available functions include EEG data, channel and event information importing, data visualization (scrolling, scalp map and dipole model plotting, plus multi-trial ERP-image plots), preprocessing (including artifact rejection, filtering, epoch selection, and averaging), independent component analysis (ICA) and time/frequency decompositions including channel and component cross-coherence supported by bootstrap statistical methods based on data resampling. EEGLAB functions are organized into three layers. Top-layer functions allow users to interact with the data through the graphic interface without needing to use MATLAB syntax. Menu options allow users to tune the behavior of EEGLAB to available memory. Middle-layer functions allow users to customize data processing using command history and interactive 'pop' functions. Experienced MATLAB users can use EEGLAB data structures and stand-alone signal processing functions to write custom and/or batch analysis scripts. Extensive function help and tutorial information are included. A 'plug-in' facility allows easy incorporation of new EEG modules into the main menu. EEGLAB is freely available (http://www.sccn.ucsd.edu/eeglab/) under the GNU public license for noncommercial use and open source development, together with sample data, user tutorial and extensive documentation.",2004,85,14433,1913,0,0,0,0,0,0,0,0,0,0
e250c4b0cc0180af9f47b97f3b9ff1727a2767aa,"New software, OLEX2, has been developed for the determination, visualization and analysis of molecular crystal structures. The software has a portable mouse-driven workflow-oriented and fully comprehensive graphical user interface for structure solution, refinement and report generation, as well as novel tools for structure analysis. OLEX2 seamlessly links all aspects of the structure solution, refinement and publication process and presents them in a single workflow-driven package, with the ultimate goal of producing an application which will be useful to both chemists and crystallographers.",2009,10,13128,493,0,0,0,0,0,0,3,686,1472,1660
42abddd227d653a0375d7d037ddb885f6c07f66f,"We present Model-based Analysis of ChIP-Seq data, MACS, which analyzes data generated by short read sequencers such as Solexa's Genome Analyzer. MACS empirically models the shift size of ChIP-Seq tags, and uses it to improve the spatial resolution of predicted binding sites. MACS also uses a dynamic Poisson distribution to effectively capture local biases in the genome, allowing for more robust predictions. MACS compares favorably to existing ChIP-Seq peak-finding algorithms, and is freely available.",2008,17,9854,1177,6,43,122,233,339,455,628,662,729,980
7a6142cfa79cc01ceced5e144bd0e01a0f241a74,,2009,43,7888,2013,377,456,500,626,706,655,723,661,657,644
94559c249d204110296c39ed4af2042cc4468e68,"The Karhunen-Lo eve basis functions, more frequently referred to as principal components or empirical orthogonal functions (EOFs), of the noise response of the climate system are an important tool for geophysical studies. Many researchers have used this tool to examine the geophysical and climatological phenomena. Perhaps more frequent use of EOFs in recent studies is in conjunction with the development of the signal detection and estimation methods of the background uctuations of a detection variable serve as an orthogonal basis set and are used to design optimal techniques for detecting and estimating signals. A detection and prediction approach is to design a lter or optimal weights for the signal to be detected. It has been reported that weighted averaging of data over the surface of the Earth improved the detectability of climatic changes (Hasselmann 1979; Stefanick 1981; Bell 1982). Since the signal-to-noise ratio (SNR) varies geographically, there exists an optimal geographical weighting of the signal which maximizes the SNR. The design of an optimal weighting function may require detailed knowledge on the natural uctuation of the climate system. A conceptually similar approach is to employ a particular pattern (or patterns) of climatic change for detection and prediction (e.g., Barnett and Hasselmann 1979; Hasselmann 1979). The patterns of interest (also called the predictors) may include the principal components (empirical orthogonal functions) (e. von Storch 1990) among others. This approach also requires complete knowledge of the natural variability of the climate system. To test and improve the detection and prediction techniques addressed above, a complete cross-spectral covariance matrix, or similarly, a complete set of the principal components of natural uctuations of the climate system for each frequency band of the spectrum is necessary. In reality, a reliable spectrum of observational covariance matrix is not available because observations are not suuciently long and sampling errors contaminate the observational records (Preisendorfer and Barnett 1977; North et al. 1982). Further, inadequate spatial coverage of observations may introduce bias. Therefore, the covariance matrix of the noise response is often estimated from a simple stochastic model. Kim and North (1991, 1992) examined the covariance matrix in terms of various second-moment statistics earlier. Examined here are the principal components of the covariance matrix of the surface temperature uctuations in a simple coupled climate model in comparison with observations. The principal components not only are an",2009,52,14084,1663,1,0,3,13,206,905,1082,1144,1134,1079
20aeb2357e9e215787c7e0d0acfe7a6b598c9103,"This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkisons Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, its easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot. This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and youll learn everything you need in the book. After reading this book youll be able to produce graphics customized precisely for your problems,and youll find it easy to get graphics out of your head and on to the screen or page.",2009,0,20836,1801,0,0,0,0,0,1,2,0,3,9
da7ab2f1b6278472f3671a7430c9a72bad07781f,"PAML, currently in version 4, is a package of programs for phylogenetic analyses of DNA and protein sequences using maximum likelihood (ML). The programs may be used to compare and test phylogenetic trees, but their main strengths lie in the rich repertoire of evolutionary models implemented, which can be used to estimate parameters in models of sequence evolution and to test interesting biological hypotheses. Uses of the programs include estimation of synonymous and nonsynonymous rates (d(N) and d(S)) between two protein-coding DNA sequences, inference of positive Darwinian selection through phylogenetic comparison of protein-coding genes, reconstruction of ancestral genes and proteins for molecular restoration studies of extinct life forms, combined analysis of heterogeneous data sets from multiple gene loci, and estimation of species divergence times incorporating uncertainties in fossil calibrations. This note discusses some of the major applications of the package, which includes example data sets to demonstrate their use. The package is written in ANSI C, and runs under Windows, Mac OSX, and UNIX systems. It is available at -- (http://abacus.gene.ucl.ac.uk/software/paml.html).",2007,203,9008,1776,10,131,298,412,471,583,614,666,638,749
ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed,"The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. That is, if p is the probability mass function for the population, S = {S1, S2, * *, Sk} is a partition of EN, and ui, i = 1, 2, * , k, is the conditional mean of p over the set Si, then W2(S) = ff=ISi f z u42 dp(z) tends to be low for the partitions S generated by the method. We say 'tends to be low,' primarily because of intuitive considerations, corroborated to some extent by mathematical analysis and practical computational experience. Also, the k-means procedure is easily programmed and is computationally economical, so that it is feasible to process very large samples on a digital computer. Possible applications include methods for similarity grouping, nonlinear prediction, approximating multivariate distributions, and nonparametric tests for independence among several variables. In addition to suggesting practical classification methods, the study of k-means has proved to be theoretically interesting. The k-means concept represents a generalization of the ordinary sample mean, and one is naturally led to study the pertinent asymptotic behavior, the object being to establish some sort of law of large numbers for the k-means. This problem is sufficiently interesting, in fact, for us to devote a good portion of this paper to it. The k-means are defined in section 2.1, and the main results which have been obtained on the asymptotic behavior are given there. The rest of section 2 is devoted to the proofs of these results. Section 3 describes several specific possible applications, and reports some preliminary results from computer experiments conducted to explore the possibilities inherent in the k-means idea. The extension to general metric spaces is indicated briefly in section 4. The original point of departure for the work described here was a series of problems in optimal classification (MacQueen [9]) which represented special",1967,17,23395,1186,0,0,0,0,0,0,0,0,0,0
d40ee5dd758c525dfb9932d726bb4e844b7b8478,"Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.",2006,45,14288,1233,0,0,0,0,0,0,1,5,100,1122
a193120975be25b4ba2764e6d7bf9dc01588aafb,"ACADEMICIANS SEEM to be moving toward the elimination of ratio analysis as an analytical technique in assessing the performance of the business enterprise. Theorists downgrade arbitrary rules of thumb, such as company ratio comparisons, widely used by practitioners. Since attacks on the relevance of ratio analysis emanate from many esteemed members of the scholarly world, does this mean that ratio analysis is limited to the world of ""nuts and bolts""? Or, has the significance of such an approach been unattractively garbed and therefore unfairly handicapped? Can we bridge the gap, rather than sever the link, between traditional ratio ""analysis"" and the more rigorous statistical techniques which have become popular among academicians in recent years? The purpose of this paper is to attempt an assessment of this issue-the quality of ratio analysis as an analytical technique. The prediction of corporate bankruptcy is used as an illustrative case.' Specifically, a set of financial and economic ratios will be investigated in a bankruptcy prediction context wherein a multiple discriminant statistical methodology is employed. The data used in the study are limited to manufacturing corporations. A brief review of the development of traditional ratio analysis as a technique for investigating corporate performance is presented in section I. In section II the shortcomings of this approach are discussed and multiple discriminant analysis is introduced with the emphasis centering on its compatibility with ratio analysis in a bankruptcy prediction context. The discriminant model is developed in section III, where an initial sample of sixty-six firms is utilized to establish a function which best discriminates between companies in two mutually exclusive groups: bankrupt and non-bankrupt firms. Section IV reviews empirical results obtained from the initial sample and several secondary samples, the latter being selected to examine the reliability of the discriminant",1968,0,11358,2162,0,0,0,0,0,0,0,0,0,0
215579acece24b34f6ab91fab62bac8ba7ccfe03,,2002,0,16246,1013,1,1,5,5,9,12,519,1115,1268,1194
39dae53515afb42664369c291ec6d1ce34d778bd,"BackgroundCorrelation networks are increasingly being used in bioinformatics applications. For example, weighted gene co-expression network analysis is a systems biology method for describing the correlation patterns among genes across microarray samples. Weighted correlation network analysis (WGCNA) can be used for finding clusters (modules) of highly correlated genes, for summarizing such clusters using the module eigengene or an intramodular hub gene, for relating modules to one another and to external sample traits (using eigengene network methodology), and for calculating module membership measures. Correlation networks facilitate network based gene screening methods that can be used to identify candidate biomarkers or therapeutic targets. These methods have been successfully applied in various biological contexts, e.g. cancer, mouse genetics, yeast genetics, and analysis of brain imaging data. While parts of the correlation network methodology have been described in separate publications, there is a need to provide a user-friendly, comprehensive, and consistent software implementation and an accompanying tutorial.ResultsThe WGCNA R software package is a comprehensive collection of R functions for performing various aspects of weighted correlation network analysis. The package includes functions for network construction, module detection, gene selection, calculations of topological properties, data simulation, visualization, and interfacing with external software. Along with the R package we also present R software tutorials. While the methods development was motivated by gene expression data, the underlying data mining approach can be applied to a variety of different settings.ConclusionThe WGCNA package provides R functions for weighted correlation network analysis, e.g. co-expression network analysis of gene expression data. The R package along with its source code and additional material are freely available at http://www.genetics.ucla.edu/labs/horvath/CoexpressionNetwork/Rpackages/WGCNA.",2008,51,9006,1051,2,9,29,60,112,163,263,361,592,783
b6fbb3a44a8e946b4a231118a737a396517c83de,"AIM
This paper is a description of inductive and deductive content analysis.


BACKGROUND
Content analysis is a method that may be used with either qualitative or quantitative data and in an inductive or deductive way. Qualitative content analysis is commonly used in nursing studies but little has been published on the analysis process and many research books generally only provide a short description of this method.


DISCUSSION
When using content analysis, the aim was to build a model to describe the phenomenon in a conceptual form. Both inductive and deductive analysis processes are represented as three main phases: preparation, organizing and reporting. The preparation phase is similar in both approaches. The concepts are derived from the data in inductive content analysis. Deductive content analysis is used when the structure of analysis is operationalized on the basis of previous knowledge.


CONCLUSION
Inductive content analysis is used in cases where there are no previous studies dealing with the phenomenon or when it is fragmented. A deductive approach is useful if the general aim was to test a previous theory in a different situation or to compare categories at different time periods.",2008,65,11808,608,0,0,0,0,0,2,410,1016,1090,1289
c98386ddf2fe4973da42187a9e3c9167095acf4e,"During the past 30 years, meta-analysis has been an indispensable tool for revealing the hidden meaning of our research literatures. The four articles in this special section on meta-analysis illustrate some of the complexities entailed in meta-analysis methods. Although meta-analysis is a powerful tool for advancing cumulative knowledge, researchers can be confused by the complicated issues involved in the methodology. Each of these four articles contributes both to advancing this methodology and to the increasing complexities that can befuddle researchers. In these comments, the author attempts to clarify both of these aspects and provide a perspective on the methodological issues examined in these articles.",2008,114,15143,703,0,0,0,0,0,0,0,0,0,4
0e2532c31c992ac0930998561932f198b467572d,"This article examines the function of documents as a data source in qualitative research and discusses document analysis procedure in the context of actual research experiences. Targeted to research novices, the article takes a nuts‐and‐bolts approach to document analysis. It describes the nature and forms of documents, outlines the advantages and limitations of document analysis, and offers specific examples of the use of documents in the research process. The application of document analysis to a grounded theory study is illustrated.",2009,34,4563,534,0,12,16,59,127,198,275,417,534,725
dd1b3a3793619cec8994cc7cca10e6dee656fb7b,"UNLABELLED
Research over the last few years has revealed significant haplotype structure in the human genome. The characterization of these patterns, particularly in the context of medical genetic association studies, is becoming a routine research activity. Haploview is a software package that provides computation of linkage disequilibrium statistics and population haplotype patterns from primary genotype data in a visually appealing and interactive interface.


AVAILABILITY
http://www.broad.mit.edu/mpg/haploview/


CONTACT
jcbarret@broad.mit.edu",2005,12,13385,1397,0,0,0,1,78,1112,1159,1137,1018,973
6fb968167f3c9c76d00d085a57b265cb912ad012,"Data Mining Methods and Models is the second volume of a three-book series on data mining authored by Larose. The following review was performed independently of LaRose’s other two books. Paraphrasing from the Preface, the goal of this book is to “explore the process of data mining from the point of view of model building.” Nevertheless, the reader will soon be aware that this book is not intended to provide a systematic or comprehensive coverage of various data mining algorithms. Instead, it considers supervised learning or predictive modeling only, and it walks the reader through the data mining process merely with a few selected modeling methods such as (generalized) linear modeling and the Bayesian approach. The book has seven chapters. Chapter 1 introduces dimension reduction, with a focus on principal components analysis (PCA) types of techniques. Chapters 2, 3, and 4 provide a detailed coverage of simple linear regression, multiple linear regression, and logistic regression, respectively. Chapter 5 introduces naive Bayes estimation and Bayesian networks. In Chapter 6, the basic idea of genetic algorithms is discussed. Finally, Chapter 7 presents a case study example of modeling response to direct mail marketing within the CRISP (crossindustry standard process) framework. This book is very easy to read, and this is absolutely the strength which many readers, especially those nonstatistically oriented ones, will greatly appreciate. Predictive modeling is perhaps the most technical part in a data mining process. The author has done an excellent job in making this difficult topic accessible to a broad audience. For example, I like the way in which Bayesian networks are introduced in Chapter 5. After the reader goes through a churn example on naive Bayes estimation in a step-by-step manner, Bayesian belief networks become easily understood as natural extensions. The overall style of the book is clear and patient. The main limitation of the book is its limited coverage. An inspired reader would expect to see a much more extended list of topics. Hastie, Tibishirani, and Friedman (2001) gave a full and more technical account of various data mining algorithms. The inclusion of genetic algorithms in Chapter 6 seems novel when compared to Hastie, Tibishirani, and Friedman (2001), but at the same time, a little unexpected as a separate chapter, since a genetic algorithm involves a stochastics search scheme, which is somewhat involved given the elementary nature of this text. Another noteworthy issue is that the author does not make an attempt to distinguish between conventional statistical analysis and data mining. I found a few errors. On Page 25, for example, it should be ai = 1, instead of ai = 1/4. Also, in the frame on the top of Page 211, it might have been “Posterior Odds,” instead of “Posterior Odds Ratio.” The book uses three different software packages to implement the ideas including SPSS with Clementine, Minitab, and WEKA, which might not be appealing. On the other hand, it is justifiable as it allows one to perform data mining with affordable costs. In summary, I recommend this fairly readable book for adoption in a graduate-level introductory course on data mining, especially when the students come from varied backgrounds.",2008,5,6076,1830,57,100,190,265,350,401,473,572,669,712
85dfac3a261fbdea3b4e1a6f3264c6384bbc4485,"Qualitative content analysis as described in published literature shows conflicting opinions and unsolved issues regarding meaning and use of concepts, procedures and interpretation. This paper provides an overview of important concepts (manifest and latent content, unit of analysis, meaning unit, condensation, abstraction, content area, code, category and theme) related to qualitative content analysis; illustrates the use of concepts related to the research procedure; and proposes measures to achieve trustworthiness (credibility, dependability and transferability) throughout the steps of the research procedure. Interpretation in qualitative content analysis is discussed in light of Watzlawick et al.'s [Pragmatics of Human Communication. A Study of Interactional Patterns, Pathologies and Paradoxes. W.W. Norton & Company, New York, London] theory of communication.",2004,58,14398,1104,0,0,0,0,0,0,0,0,0,5
38825d4f600ceb71825eca070543f7ebb3cfc7eb,"Analysis of decision making under risk has been dominated by expected utility theory, which generally accounts for people's actions. Presents a critique of expected utility theory as a descriptive model of decision making under risk, and argues that common forms of utility theory are not adequate, and proposes an alternative theory of choice under risk called prospect theory. In expected utility theory, utilities of outcomes are weighted by their probabilities. Considers results of responses to various hypothetical decision situations under risk and shows results that violate the tenets of expected utility theory. People overweight outcomes considered certain, relative to outcomes that are merely probable, a situation called the ""certainty effect."" This effect contributes to risk aversion in choices involving sure gains, and to risk seeking in choices involving sure losses. In choices where gains are replaced by losses, the pattern is called the ""reflection effect."" People discard components shared by all prospects under consideration, a tendency called the ""isolation effect."" Also shows that in choice situations, preferences may be altered by different representations of probabilities. Develops an alternative theory of individual decision making under risk, called prospect theory, developed for simple prospects with monetary outcomes and stated probabilities, in which value is given to gains and losses (i.e., changes in wealth or welfare) rather than to final assets, and probabilities are replaced by decision weights. The theory has two phases. The editing phase organizes and reformulates the options to simplify later evaluation and choice. The edited prospects are evaluated and the highest value prospect chosen. Discusses and models this theory, and offers directions for extending prospect theory are offered. (TNM)",1979,0,20074,607,0,0,0,0,0,0,0,0,0,0
b07ce649d6f6eb636872527104b0209d3edc8188,"Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.",1974,0,16987,737,0,0,1,3,0,1,1,3,3,1
d96faee1898de7a052115ecfe533a5b2fd1151c0,"This paper develops a new approach to the problem of testing the existence of a level relationship between a dependent variable and a set of regressors, when it is not known with certainty whether the underlying regressors are trend- or first-difference stationary. The proposed tests are based on standard F- and t-statistics used to test the significance of the lagged levels of the variables in a univariate equilibrium correction mechanism. The asymptotic distributions of these statistics are non-standard under the null hypothesis that there exists no level relationship, irrespective of whether the regressors are I(0) or I(1). Two sets of asymptotic critical values are provided: one when all regressors are purely I(1) and the other if they are all purely I(0). These two sets of critical values provide a band covering all possible classifications of the regressors into purely I(0), purely I(1) or mutually cointegrated. Accordingly, various bounds testing procedures are proposed. It is shown that the proposed tests are consistent, and their asymptotic distribution under the null and suitably defined local alternatives are derived. The empirical relevance of the bounds procedures is demonstrated by a re-examination of the earnings equation included in the UK Treasury macroeconometric model. Copyright © 2001 John Wiley & Sons, Ltd.",2001,46,11367,2341,0,0,0,0,0,0,0,0,0,4
a82d7e0dc7b2d3a1b159e0902bb9f3017788a786,"A comprehensive, but simple-to-use software package for executing a range of standard numerical analysis and operations used in quantitative paleontology has been developed. The program, called PAST (PAleontological STatistics), runs on standard Windows computers and is available free of charge. PAST integrates spreadsheet-type data entry with univariate and multivariate statistics, curve fitting, timeseries analysis, data plotting, and simple phylogenetic analysis. Many of the functions are specific to paleontology and ecology, and these functions are not found in standard, more extensive, statistical packages. PAST also includes fourteen case studies (data files and exercises) illustrating use of the program for paleontological problems, making it a complete educational package for courses in quantitative methods.",2001,31,10370,1794,0,0,0,0,0,0,0,0,0,9
39dfeda235027e1bed00ab33bcb2ad16831677f8,"Hypothesis-testing methods for multivariate data are needed to make rigorous probability statements about the effects of factors and their interactions in experiments. Analysis of variance is particularly powerful for the analysis of univariate data. The traditional multivariate analogues, however, are too stringent in their assumptions for most ecological multivariate data sets. Non-parametric methods, based on permutation tests, are preferable. This paper describes a new non-parametric method for multivariate analysis of variance, after McArdle and Anderson (in press). It is given here, with several applications in ecology, to provide an alternative and perhaps more intuitive formulation for ANOVA (based on sums of squared distances) to complement the description pro- vided by McArdle and Anderson (in press) for the analysis of any linear model. It is an improvement on previous non-parametric methods because it allows a direct additive partitioning of variation for complex models. It does this while maintaining the flexibility and lack of formal assumptions of other non-parametric methods. The test- statistic is a multivariate analogue to Fisher's F-ratio and is calculated directly from any symmetric distance or dissimilarity matrix. P-values are then obtained using permutations. Some examples of the method are given for tests involving several factors, including factorial and hierarchical (nested) designs and tests of interactions.",2001,127,10952,1442,0,0,0,0,0,0,0,0,0,171
21c41fcec6ac8a7f55c539ac199247f200633753,"Microarrays can measure the expression of thousands of genes to identify changes in expression between different biological states. Methods are needed to determine the significance of these changes while accounting for the enormous number of genes. We describe a method, Significance Analysis of Microarrays (SAM), that assigns a score to each gene on the basis of change in gene expression relative to the standard deviation of repeated measurements. For genes with scores greater than an adjustable threshold, SAM uses permutations of the repeated measurements to estimate the percentage of genes identified by chance, the false discovery rate (FDR). When the transcriptional response of human cells to ionizing radiation was measured by microarrays, SAM identified 34 genes that changed at least 1.5-fold with an estimated FDR of 12%, compared with FDRs of 60 and 84% by using conventional methods of analysis. Of the 34 genes, 19 were involved in cell cycle regulation and 3 in apoptosis. Surprisingly, four nucleotide excision repair genes were induced, suggesting that this repair pathway for UV-damaged DNA might play a previously unrecognized role in repairing DNA damaged by ionizing radiation.",2001,54,11565,1626,0,0,1,3,250,864,864,894,921,881
31e243a7fb9a4dcd411896c262c94ca48a7c6ece,"We provide a concise overview of time series analysis in the time and frequency domains, with lots of references for further reading.",1986,132,11227,1310,0,1,1,0,0,0,1,0,0,2
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
fc40d567e4434e96fc1c1437d16f4855d3280abb,"The foreign body reaction composed of macrophages and foreign body giant cells is the end-stage response of the inflammatory and wound healing responses following implantation of a medical device, prosthesis, or biomaterial. A brief, focused overview of events leading to the foreign body reaction is presented. The major focus of this review is on factors that modulate the interaction of macrophages and foreign body giant cells on synthetic surfaces where the chemical, physical, and morphological characteristics of the synthetic surface are considered to play a role in modulating cellular events. These events in the foreign body reaction include protein adsorption, monocyte/macrophage adhesion, macrophage fusion to form foreign body giant cells, consequences of the foreign body response on biomaterials, and cross-talk between macrophages/foreign body giant cells and inflammatory/wound healing cells. Biomaterial surface properties play an important role in modulating the foreign body reaction in the first two to four weeks following implantation of a medical device, even though the foreign body reaction at the tissue/material interface is present for the in vivo lifetime of the medical device. An understanding of the foreign body reaction is important as the foreign body reaction may impact the biocompatibility (safety) of the medical device, prosthesis, or implanted biomaterial and may significantly impact short- and long-term tissue responses with tissue-engineered constructs containing proteins, cells, and other biological components for use in tissue engineering and regenerative medicine. Our perspective has been on the inflammatory and wound healing response to implanted materials, devices, and tissue-engineered constructs. The incorporation of biological components of allogeneic or xenogeneic origin as well as stem cells into tissue-engineered or regenerative approaches opens up a myriad of other challenges. An in depth understanding of how the immune system interacts with these cells and how biomaterials or tissue-engineered constructs influence these interactions may prove pivotal to the safety, biocompatibility, and function of the device or system under consideration.",2008,161,3439,153,17,70,126,163,160,186,277,318,316,382
39d931673c5f50a98b2165e8ff25ebcf4fbd3f8f,"The field of biomaterials has become a vital area, as these materials can enhance the quality and longevity of human life and the science and technology associated with this field has now led to multi-million dollar business. The paper focuses its attention mainly on titanium-based alloys, even though there exists biomaterials made up of ceramics, polymers and composite materials. The paper discusses the biomechanical compatibility of many metallic materials and it brings out the overall superiority of Ti based alloys, even though it is costlier. As it is well known that a good biomaterial should possess the fundamental properties such as better mechanical and biological compatibility and enhanced wear and corrosion resistance in biological environment, the paper discusses the influence of alloy chemistry, thermomechanical processing and surface condition on these properties. In addition, this paper also discusses in detail the various surface modification techniques to achieve superior biocompatibility, higher wear and corrosion resistance. Overall, an attempt has been made to bring out the current scenario of Ti based materials for biomedical applications.",2009,156,3417,69,13,59,135,138,228,269,308,302,351,366
257305088dd5c98284436ce991dfcd96df446428,"Biomass represents an abundant carbon-neutral renewable resource for the production of bioenergy and biomaterials, and its enhanced use would address several societal needs. Advances in genetics, biotechnology, process chemistry, and engineering are leading to a new manufacturing concept for converting renewable biomass to valuable fuels and products, generally referred to as the biorefinery. The integration of agroenergy crops and biorefinery manufacturing technologies offers the potential for the development of sustainable biopower and biomaterials that will lead to a new manufacturing paradigm.",2006,59,4830,76,37,117,177,181,260,363,350,414,423,460
b21b5a36acad5be6dd47f3332dd0a05333287001,"New generations of synthetic biomaterials are being developed at a rapid pace for use as three-dimensional extracellular microenvironments to mimic the regulatory characteristics of natural extracellular matrices (ECMs) and ECM-bound growth factors, both for therapeutic applications and basic biological studies. Recent advances include nanofibrillar networks formed by self-assembly of small building blocks, artificial ECM networks from protein polymers or peptide-conjugated synthetic polymers that present bioactive ligands and respond to cell-secreted signals to enable proteolytic remodeling. These materials have already found application in differentiating stem cells into neurons, repairing bone and inducing angiogenesis. Although modern synthetic biomaterials represent oversimplified mimics of natural ECMs lacking the essential natural temporal and spatial complexity, a growing symbiosis of materials engineering and cell biology may ultimately result in synthetic materials that contain the necessary signals to recapitulate developmental processes in tissue- and organ-specific differentiation and morphogenesis.",2005,141,3965,71,23,90,147,144,200,254,293,320,314,368
fbd2a5392926f69ef17fb1093c40874b055e8c18,"As a lightweight metal with mechanical properties similar to natural bone, a natural ionic presence with significant functional roles in biological systems, and in vivo degradation via corrosion in the electrolytic environment of the body, magnesium-based implants have the potential to serve as biocompatible, osteoconductive, degradable implants for load-bearing applications. This review explores the properties, biological performance, challenges and future directions of magnesium-based biomaterials.",2006,55,3257,69,6,23,40,70,115,152,175,236,282,333
e3c98b087826c8bf032d1934fe96130ae7f39c03,"Abstract During the past two decades significant advances have been made in the development of biodegradable polymeric materials for biomedical applications. Degradable polymeric biomaterials are preferred candidates for developing therapeutic devices such as temporary prostheses, three-dimensional porous structures as scaffolds for tissue engineering and as controlled/sustained release drug delivery vehicles. Each of these applications demands materials with specific physical, chemical, biological, biomechanical and degradation properties to provide efficient therapy. Consequently, a wide range of natural or synthetic polymers capable of undergoing degradation by hydrolytic or enzymatic route are being investigated for biomedical applications. This review summarizes the main advances published over the last 15 years, outlining the synthesis, biodegradability and biomedical applications of biodegradable synthetic and natural polymers.",2007,222,3340,77,3,30,70,121,196,229,253,288,359,314
0416e7ec53a62833045bd3e25845b69c947089ff,,2008,0,3412,96,194,225,247,300,267,373,439,398,277,131
e6d528559f803caa1577247c6be8dd15c9580d1c,"Two complementary strategies can be used in the fabrication of molecular biomaterials. In the 'top-down' approach, biomaterials are generated by stripping down a complex entity into its component parts (for example, paring a virus particle down to its capsid to form a viral cage). This contrasts with the 'bottom-up' approach, in which materials are assembled molecule by molecule (and in some cases even atom by atom) to produce novel supramolecular architectures. The latter approach is likely to become an integral part of nanomaterials manufacture and requires a deep understanding of individual molecular building blocks and their structures, assembly properties and dynamic behaviors. Two key elements in molecular fabrication are chemical complementarity and structural compatibility, both of which confer the weak and noncovalent interactions that bind building blocks together during self-assembly. Using natural processes as a guide, substantial advances have been achieved at the interface of nanomaterials and biology, including the fabrication of nanofiber materials for three-dimensional cell culture and tissue engineering, the assembly of peptide or protein nanotubes and helical ribbons, the creation of living microlenses, the synthesis of metal nanowires on DNA templates, the fabrication of peptide, protein and lipid scaffolds, the assembly of electronic materials by bacterial phage selection, and the use of radiofrequency to regulate molecular behaviors.",2003,105,2753,29,0,27,60,97,115,135,130,142,215,181
62343a260e8d476cb49491ee4b1e9c66e8b8c773,"Silk from the silkworm, Bombyx mori, has been used as biomedical suture material for centuries. The unique mechanical properties of these fibers provided important clinical repair options for many applications. During the past 20 years, some biocompatibility problems have been reported for silkworm silk; however, contamination from residual sericin (glue-like proteins) was the likely cause. More recent studies with well-defined silkworm silk fibers and films suggest that the core silk fibroin fibers exhibit comparable biocompatibility in vitro and in vivo with other commonly used biomaterials such as polylactic acid and collagen. Furthermore, the unique mechanical properties of the silk fibers, the diversity of side chain chemistries for 'decoration' with growth and adhesion factors, and the ability to genetically tailor the protein provide additional rationale for the exploration of this family of fibrous proteins for biomaterial applications. For example, in designing scaffolds for tissue engineering these properties are particularly relevant and recent results with bone and ligament formation in vitro support the potential role for this biomaterial in future applications. To date, studies with silks to address biomaterial and matrix scaffold needs have focused on silkworm silk. With the diversity of silk-like fibrous proteins from spiders and insects, a range of native or bioengineered variants can be expected for application to a diverse set of clinical needs.",2003,117,2645,36,8,28,38,49,93,110,138,143,182,169
2ba2bf0bf72e415f4124d3e485019f659b2708ee,"The development of tissue engineering in the field of orthopaedic surgery is now booming. Two fields of research in particular are emerging: the association of osteo-inductive factors with implantable materials; and the association of osteogenic stem cells with these materials (hybrid materials). In both cases, an understanding of the phenomena of cell adhesion and, in particular, understanding of the proteins involved in osteoblast adhesion on contact with the materials is of crucial importance. The proteins involved in osteoblast adhesion are described in this review (extracellular matrix proteins, cytoskeletal proteins, integrins, cadherins, etc.). During osteoblast/material interactions, their expression is modified according to the surface characteristics of materials. Their involvement in osteoblastic response to mechanical stimulation highlights the significance of taking them into consideration during development of future biomaterials. Finally, an understanding of the proteins involved in osteoblast adhesion opens up new possibilities for the grafting of these proteins (or synthesized peptide) onto vector materials, to increase their in vivo bioactivity or to promote cell integration within the vector material during the development of hybrid materials.",2000,158,2238,64,2,19,44,47,54,83,73,105,114,122
a194bb9c7ea9b48fc9ed1cc2602e7152829f0f7f,"Nutritional Physiology: Introduction, Definition, and Classification of Mineral Nutrients. Ion Uptake Mechanisms of Individual Cells and Roots: Short Distance Transport. Long-Distance Transport in the Xylem and Phloem and its Regulation. Uptake and Release of Mineral Elements by Leaves and Other Aerial Plant Parts. Yield and the Source-Sink Relationships. Mineral Nutrition and Yield Response. Nitrogen Fixation. Functions of Mineral Nutrients: Macronutrients. Function of Mineral Nutrients: Micronutrients. Beneficial Mineral Elements. Relationship between Mineral Nutrition and Plant Diseases and Pests. Diagnosis of Deficiency and Toxicity of Mineral Nutrients. Plant-Soil Relationships: Nutrient Availability in Soils. Effect of Internal and External Factors on Root Growth and Development. The Soil-Root Interface (Rhizosphere) in Relation to Mineral Nutrition. Adaptation of Plants to Adverse Chemical Soil Conditions. References. Subject Index.",1988,0,19001,1911,0,0,0,0,0,0,0,0,0,0
15f824dd4b569f6ec10f6cbed9dc9131649f7a2a,"We have used the Escherichia coli beta‐glucuronidase gene (GUS) as a gene fusion marker for analysis of gene expression in transformed plants. Higher plants tested lack intrinsic beta‐glucuronidase activity, thus enhancing the sensitivity with which measurements can be made. We have constructed gene fusions using the cauliflower mosaic virus (CaMV) 35S promoter or the promoter from a gene encoding the small subunit of ribulose bisphosphate carboxylase (rbcS) to direct the expression of beta‐glucuronidase in transformed plants. Expression of GUS can be measured accurately using fluorometric assays of very small amounts of transformed plant tissue. Plants expressing GUS are normal, healthy and fertile. GUS is very stable, and tissue extracts continue to show high levels of GUS activity after prolonged storage. Histochemical analysis has been used to demonstrate the localization of gene activity in cells and tissues of transformed plants.",1987,4,9184,727,1,17,64,119,128,148,181,176,213,182
77d574bda2cc7edd4129da81a5f1aeff0c45572a,Chapter 1 The Biosphere Chapter 2 The Anthroposphere Introduction Air Pollution Water Pollution Soil Plants Chapter 3 Soils and Soil Processes Introduction Weathering Processes Pedogenic Processes Chapter 4 Soil Constituents Introduction Trace Elements Minerals Organic Matter Organisms in Soils Chapter 5 Trace Elements in Plants Introduction Absorption Translocation Availability Essentiality and Deficiency Toxicity and Tolerance Speciation Interaction Chapter 6 Elements of Group 1 (Previously Group Ia) Introduction Lithium Rubidium Cesium Chapter 7 Elements of Group 2 (Previously Group IIa) Beryllium Strontium Barium Radium Chapter 8 Elements of Group 3 (Previously Group IIIb) Scandium Yttrium Lanthanides Actinides Chapter 9 Elements of Group 4 (Previously Group IVb) Titanium Zirconium Hafnium Chapter 10 Elements of Group 5 (Previously Group Vb) Vanadium Niobium Tantalum Chapter 11 Elements of Group 6 (Previously Group VIb) Chromium Molybdenum Tungsten Chapter 12 Elements of Group 7 (Previously Group VIIb) Manganese Technetium Rhenium Chapter 13 Elements of Group 8 (Previously Part of Group VIII) Iron Ruthenium Osmium Chapter 14 Elements of Group 9 (Previously Part of Group VIII) Cobalt Rhodium Iridium Chapter 15 Elements of Group 10 (Previously Part of Group VIII) Nickel Palladium Platinum Chapter 16 Elements of Group 11 (Previously Group Ib) Copper Silver Gold Chapter 17 Trace Elements of Group 12 (Previously of Group IIb) Zinc Cadmium Mercury Chapter 18 Elements of Group 13 (Previously Group IIIa) Boron Aluminum Gallium Indium Thallium Chapter 19 Elements of Group I4 (Previously Group IVa) Silicon Germanium Tin Lead Chapter 20 Elements of Group 15 (Previously Group Va) Arsenic Antimony Bismuth Chapter 21 Elements of Group 16 (Previously Group VIa) Selenium Tellurium Polonium Chapter 22 Elements of Group 17 (Previously Group VIIa) Fluorine Chlorine Bromine Iodine,1984,813,7166,397,0,2,10,14,13,11,19,19,21,21
544a324945198b486c58cb2ffe800fc56db07975,"Reactive oxygen species (ROS) control many different processes in plants. However, being toxic molecules, they are also capable of injuring cells. How this conflict is resolved in plants is largely unknown. Nonetheless, it is clear that the steady-state level of ROS in cells needs to be tightly regulated. In Arabidopsis, a network of at least 152 genes is involved in managing the level of ROS. This network is highly dynamic and redundant, and encodes ROS-scavenging and ROS-producing proteins. Although recent studies have unraveled some of the key players in the network, many questions related to its mode of regulation, its protective roles and its modulation of signaling networks that control growth, development and stress response remain unanswered.",2004,81,4393,303,1,32,113,118,150,190,197,225,331,311
4e98194e61ce6de2f033947ec65370ce0cf1d97d,,1979,0,7584,269,42,77,106,130,149,145,188,146,186,190
abd930550b7c9301ea366382c3628494864b5f27,"Shoots, roots, and seeds of corn (Zea mays L., cv. Michigan 500), oats (Avena sativa L., cv. Au Sable), and peas (Pisum sativum L., cv. Wando) were analyzed for their superoxide dismutase content using a photochemical assay system consisting of methionine, riboflavin, and p-nitro blue tetrazolium. The enzyme is present in the shoots, roots, and seeds of the three species. On a dry weight basis, shoots contain more enzyme than roots. In seeds, the enzyme is present in both the embryo and the storage tissue. Electrophoresis indicated a total of 10 distinct forms of the enzyme. Corn contained seven of these forms and oats three. Peas contained one of the corn and two of the oat enzymes. Nine of the enzyme activities were eliminated with cyanide treatment suggesting that they may be cupro-zinc enzymes, whereas one was cyanide-resistant and may be a manganese enzyme. Some of the leaf superoxide dismutases were found primarily in mitochondria or chloroplasts. Peroxidases at high concentrations interfere with the assay. In test tube assays of crude extracts from seedlings, the interference was negligible. On gels, however, peroxidases may account for two of the 10 superoxide dismutase forms.",1977,21,4092,488,1,3,0,2,3,8,5,2,3,7
9e61a2515807d1c7cc6a81327072f4520b47978b,"Salt and drought stress signal transduction consists of ionic and osmotic homeostasis signaling pathways, detoxification (i.e., damage control and repair) response pathways, and pathways for growth regulation. The ionic aspect of salt stress is signaled via the SOS pathway where a calcium-responsive SOS3-SOS2 protein kinase complex controls the expression and activity of ion transporters such as SOS1. Osmotic stress activates several protein kinases including mitogen-activated kinases, which may mediate osmotic homeostasis and/or detoxification responses. A number of phospholipid systems are activated by osmotic stress, generating a diverse array of messenger molecules, some of which may function upstream of the osmotic stress-activated protein kinases. Abscisic acid biosynthesis is regulated by osmotic stress at multiple steps. Both ABA-dependent and -independent osmotic stress signaling first modify constitutively expressed transcription factors, leading to the expression of early response transcriptional activators, which then activate downstream stress tolerance effector genes.",2002,129,4874,231,4,53,86,120,150,149,186,210,206,294
3bca850920c736324a97fbcb8ec57a895ceb31b4,"Fifteen or more elements present in rocks and soils normally in very small amounts are essential for plant and/or animal nutrition. By the nature of their low abundance in natural uncontaminated earth materials or plants, they are known as trace elements, minor elements or micro-nutrients. Boron, copper, iron, manganese, molybdenum, silicon, vanadium and zinc are required by plants; copper, cobalt, iodine, iron, manganese, molybdenum, selenium and zinc by animals. In addition essential roles of arsenic, fluorine, nickel, silicon, tin and vanadium have in recent years been established in animal nutrition.",1980,80,3261,595,0,1,1,0,0,0,1,2,1,2
876f3e75121b2de085d850ca8d4816f17e9c9ab7,"Medicinal plants and traditional medicine in Africa , Medicinal plants and traditional medicine in Africa , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,5116,201,0,0,0,2,4,3,1,7,7,8
e69e904ec6339303464dd2ef11a3d16f160eb73e,"Responses of plants to environmental stresses , Responses of plants to environmental stresses , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1973,0,4773,187,13,16,27,30,36,29,44,57,45,46
5855944e55f5599ac352d7da356877c467a3aa92,"Physiological and ecological constraints play key roles in the evolution of plant growth patterns, especially in relation to defenses against herbivores. Phenotypic and life history theories are unified within the growth-differentiation balance (GDB) framework, forming an integrated system of theories explaining and predicting patterns of plant defense and competitive interactions in ecological and evolutionary time. Plant activity at the cellular level can be classified as growth (cell division and enlargement) of differentiation (chemical and morphological changes leading to cell maturation and specialization). The GDB hypothesis of plant defense is premised upon a physiological trade-off between growth and differentiation processes. The trade-off between growth and defense exists because secondary metabolism and structural reinforcement are physiologically constrained in dividing and enlarging cells, and because they divert resources from the production of new leaf area. Hence the dilemma of plants: They must grow fast enough to complete, yet maintain the defenses necessary to survive in the presence of pathogens and hervivores. The physiological trade-off between growth and differentiation processes interacts with herbivory and plant-plant competition to manifest itself as a genetic trade-off between growth and defense in the evolution of plant life history strategies. Evolutionary theories of plant defense are reviewed. We also extend a standard growth rate model by separating its ecological and evolutionary components,and formalizing the role of competition in the evolution of plant defense. We conclude with a conceptual model of the evolution of plant defense in which plant physioligical trade-offs interact with the abiotic environment, competition and herbivory.",1992,661,3613,211,1,10,30,19,52,51,55,71,93,84
9739f7c963191891f04d56d1138b93a6c815d141,"Over the past 100 years, the global average temperature has increased by approximately 0.6 °C and is projected to continue to rise at a rapid rate. Although species have responded to climatic changes throughout their evolutionary history, a primary concern for wild species and their ecosystems is this rapid rate of change. We gathered information on species and global warming from 143 studies for our meta-analyses. These analyses reveal a consistent temperature-related shift, or ‘fingerprint’, in species ranging from molluscs to mammals and from grasses to trees. Indeed, more than 80% of the species that show changes are shifting in the direction expected on the basis of known physiological constraints of species. Consequently, the balance of evidence from these studies strongly suggests that a significant impact of global warming is already discernible in animal and plant populations. The synergism of rapid temperature rise and other stresses, in particular habitat destruction, could easily disrupt the connectedness among species and lead to a reformulation of species communities, reflecting differential changes in species, and to numerous extirpations and possibly extinctions.",2003,46,4262,204,56,124,172,185,236,231,234,273,269,303
1a2c56c4af02697f2b21a6b9353dd367e2d236fa,"Abstract.  Much confusion exists in the English-language literature on plant invasions concerning the terms ‘naturalized’ and ‘invasive’ and their associated concepts. Several authors have used these terms in proposing schemes for conceptualizing the sequence of events from introduction to invasion, but often imprecisely, erroneously or in contradictory ways. This greatly complicates the formulation of robust generalizations in invasion ecology. 
 
Based on an extensive and critical survey of the literature we defined a minimum set of key terms related to a graphic scheme which conceptualizes the naturalization/invasion process. Introduction means that the plant (or its propagule) has been transported by humans across a major geographical barrier. Naturalization starts when abiotic and biotic barriers to survival are surmounted and when various barriers to regular reproduction are overcome. Invasion further requires that introduced plants produce reproductive offspring in areas distant from sites of introduction (approximate scales: > 100 m over  6 m/3 years for taxa spreading by roots, rhizomes, stolons or creeping stems). Taxa that can cope with the abiotic environment and biota in the general area may invade disturbed, seminatural communities. Invasion of successionally mature, undisturbed communities usually requires that the alien taxon overcomes a different category of barriers. 
 
We propose that the term ‘invasive’ should be used without any inference to environmental or economic impact. Terms like ‘pests’ and ‘weeds’ are suitable labels for the 50–80% of invaders that have harmful effects. About 10% of invasive plants that change the character, condition, form, or nature of ecosystems over substantial areas may be termed ‘transformers’.",2000,120,3244,201,3,11,17,59,95,81,111,139,137,148
de31d3468dc6086993403b00d906fd036bc01bfb,"Plants exposed to salt stress undergo changes in their environment. The ability of plants to tolerate salt is determined by multiple biochemical pathways that facilitate retention and/or acquisition of water, protect chloroplast functions, and maintain ion homeostasis. Essential pathways include those that lead to synthesis of osmotically active metabolites, specific proteins, and certain free radical scavenging enzymes that control ion and water flux and support scavenging of oxygen radicals or chaperones. The ability of plants to detoxify radicals under conditions of salt stress is probably the most critical requirement. Many salt-tolerant species accumulate methylated metabolites, which play crucial dual roles as osmoprotectants and as radical scavengers. Their synthesis is correlated with stress-induced enhancement of photorespiration. In this paper, plant responses to salinity stress are reviewed with emphasis on physiological, biochemical, and molecular mechanisms of salt tolerance. This review may help in interdisciplinary studies to assess the ecological significance of salt stress.",2005,235,3279,137,8,24,43,70,95,113,166,171,263,270
54d1abdf4487202215024c07c355b580caf676b2,"Our understanding of plant mineral nutrition comes largely from studies of herbaceous crops that evolved from ruderal species characteristic of nutri­ ent-rich disturbed sites (52). With the development of agriculture, these ancestral species were bred for greater productivity and reproductive output at high nutrient levels where there was little selective advantage in efficient nutrient use. This paper briefly reviews the nature of crop responses to nutrient stress and compares these responses to those of species that have evolved under more natural conditions, particularly in low-nutrient envi­ ronments. I draw primarily upon nutritional studies of nitrogen and phos­ phorus because these elements most commonly limit plant growth and because their role in controlling plant growth and metabolism is most clearly understood (51). Other more specific aspects of nutritional plant ecology not discussed here include ammonium/nitrate nutrition (79), cal­ cicole/calcifuge nutrition (51,88), heavy metal tolerance (4), and serpentine ecology (133).",1980,79,4080,180,0,4,15,34,27,43,57,41,65,77
5f06f0f5202c32631c81fdda2a419b0fc113bceb,"Lignocellulosic biomass has long been recognized as a potential sustainable source of mixed sugars for fermentation to biofuels and other biomaterials. Several technologies have been developed during the past 80 years that allow this conversion process to occur, and the clear objective now is to make this process cost-competitive in today's markets. Here, we consider the natural resistance of plant cell walls to microbial and enzymatic deconstruction, collectively known as “biomass recalcitrance.” It is this property of plants that is largely responsible for the high cost of lignocellulose conversion. To achieve sustainable energy production, it will be necessary to overcome the chemical and structural properties that have evolved in biomass to prevent its disassembly.",2007,34,3694,104,18,83,102,180,249,258,339,373,324,367
2ef2ae40f89de3d996c2ba407bb39204ca3dcc55,"The volatile oils of black pepper [Piper nigrum L. (Piperaceae)], clove [Syzygium aromaticum (L.) Merr. & Perry (Myrtaceae)], geranium [Pelargonium graveolens L'Herit (Geraniaceae)], nutmeg [Myristica fragrans Houtt. (Myristicaceae), oregano [Origanum vulgare ssp. hirtum (Link) Letsw. (Lamiaceae)] and thyme [Thymus vulgaris L. (Lamiaceae)] were assessed for antibacterial activity against 25 different genera of bacteria. These included animal and plant pathogens, food poisoning and spoilage bacteria. The volatile oils exhibited considerable inhibitory effects against all the organisms under test while their major components demonstrated various degrees of growth inhibition.",2000,90,3808,123,2,10,19,30,50,54,89,118,122,158
26ca41ef0ccf8700a54f1e59c69d08874ca958f9,"Indian medicinal plants / , Indian medicinal plants / , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",2005,0,4369,54,117,131,149,150,246,350,421,429,401,390
a8d4e645455a0944323882ee1ad75899546bee35,"Severe droughts have been associated with regional-scale forest mortality worldwide. Climate change is expected to exacerbate regional mortality events; however, prediction remains difficult because the physiological mechanisms underlying drought survival and mortality are poorly understood. We developed a hydraulically based theory considering carbon balance and insect resistance that allowed development and examination of hypotheses regarding survival and mortality. Multiple mechanisms may cause mortality during drought. A common mechanism for plants with isohydric regulation of water status results from avoidance of drought-induced hydraulic failure via stomatal closure, resulting in carbon starvation and a cascade of downstream effects such as reduced resistance to biotic agents. Mortality by hydraulic failure per se may occur for isohydric seedlings or trees near their maximum height. Although anisohydric plants are relatively drought-tolerant, they are predisposed to hydraulic failure because they operate with narrower hydraulic safety margins during drought. Elevated temperatures should exacerbate carbon starvation and hydraulic failure. Biotic agents may amplify and be amplified by drought-induced plant stress. Wet multidecadal climate oscillations may increase plant susceptibility to drought-induced mortality by stimulating shifts in hydraulic architecture, effectively predisposing plants to water stress. Climate warming and increased frequency of extreme events will probably cause increased regional mortality episodes. Isohydric and anisohydric water potential regulation may partition species between survival and mortality, and, as such, incorporating this hydraulic framework may be effective for modeling plant survival and mortality under future climate conditions.",2008,295,2789,175,13,48,91,130,139,222,198,242,249,258
363468e953a9d3c16cf71c3d3fa17023840ab1eb,"Transformed petunia, tobacco, and tomato plants have been produced by means of a novel leaf disk transformation-regeneration method. Surface-sterilized leaf disks were inoculated with an Agrobacterium tumefaciens strain containing a modified tumor-inducing plasmid (in which the phytohormone biosynthetic genes from transferred DNA had been deleted and replaced with a chimeric gene for kanamycin resistance) and cultured for 2 days. The leaf disks were then transferred to selective medium containing kanamycin. Shoot regeneration occurred within 2 to 4 weeks, and transformants were confirmed by their ability to form roots in medium containing kanamycin. This method for producing transformed plants combines gene transfer, plant regeneration, and effective selection for transformants into a single process and should be applicable to plant species that can be infected by Agrobacterium and regenerated from leaf explants.",1985,0,4262,83,9,45,81,87,111,118,88,104,116,92
10d1c4b96dc8af23fa1cd89984d14c9a36efc8f2,"Heat stress due to increased temperature is an agricultural problem in many areas in the world. Transitory or constantly high temperatures cause an array of morpho-anatomical, physiological and biochemical changes in plants, which affect plant growth and development and may lead to a drastic reduction in economic yield. The adverse effects of heat stress can be mitigated by developing crop plants with improved thermotolerance using various genetic approaches. For this purpose, however, a thorough understanding of physiological responses of plants to high temperature, mechanisms of heat tolerance and possible strategies for improving crop thermotolerance is imperative. Heat stress affects plant growth throughout its ontogeny, though heat-threshold level varies considerably at different developmental stages. For instance, during seed germination, high temperature may slow down or totally inhibit germination, depending on plant species and the intensity of the stress. At later stages, high temperature may adversely affect photosynthesis, respiration, water relations and membrane stability, and also modulate levels of hormones and primary and secondary metabolites. Furthermore, throughout plant ontogeny, enhanced expression of a variety of heat shock proteins, other stress-related proteins, and production of reactive oxygen species (ROS) constitute major plant responses to heat stress. In order to cope with heat stress, plants implement various mechanisms, including maintenance of membrane stability, scavenging of ROS, production of antioxidants, accumulation and adjustment of compatible solutes, induction of mitogen-activated protein kinase (MAPK) and calcium-dependent protein kinase (CDPK) cascades, and, most importantly, chaperone signaling and transcriptional activation. All these mechanisms, which are regulated at the molecular level, enable plants to thrive under heat stress. Based on a complete understanding of such mechanisms, potential genetic strategies to improve plant heat-stress tolerance include traditional and contemporary molecular breeding protocols and transgenic approaches. While there are a few examples of plants with improved heat tolerance through the use of traditional breeding protocols, the success of genetic transformation approach has been thus far limited. The latter is due to limited knowledge and availability of genes with known effects on plant heat-stress tolerance, though these may not be insurmountable in future. In addition to genetic approaches, crop heat tolerance can be enhanced by preconditioning of plants under different environmental stresses or exogenous application of osmoprotectants such as glycinebetaine and proline. Acquiring thermotolerance is an active process by which considerable amounts of plant resources are diverted to structural and functional maintenance to escape damages caused by heat stress. Although biochemical and molecular aspects of thermotolerance in plants are relatively well understood, further studies focused on phenotypic flexibility and assimilate partitioning under heat stress and factors modulating crop heat tolerance are imperative. Such studies combined with genetic approaches to identify and map genes (or QTLs) conferring thermotolerance will not only facilitate marker-assisted breeding for heat tolerance but also pave the way for cloning and characterization of underlying genetic factors which could be useful for engineering plants with improved heat tolerance.",2007,313,2568,218,0,16,31,67,101,132,179,202,213,242
2a1f60cefc8d8981a45f1cc27bec2e9e2cf7418b,,1956,0,5897,167,0,0,1,1,0,2,2,5,5,6
257c08b0a31deee145a7ec85890f11ec96eefbcc,1 Graphical modeling using L-systems.- 1.1 Rewriting systems.- 1.2 DOL-systems.- 1.3 Turtle interpretation of strings.- 1.4 Synthesis of DOL-systems.- 1.4.1 Edge rewriting.- 1.4.2 Node rewriting.- 1.4.3 Relationship between edge and node rewriting.- 1.5 Modeling in three dimensions.- 1.6 Branching structures.- 1.6.1 Axial trees.- 1.6.2 Tree OL-systems.- 1.6.3 Bracketed OL-systems.- 1.7 Stochastic L-systems.- 1.8 Context-sensitive L-systems.- 1.9 Growth functions.- 1.10 Parametric L-systems.- 1.10.1 Parametric OL-systems.- 1.10.2 Parametric 2L-systems.- 1.10.3 Turtle interpretation of parametric words.- 2 Modeling of trees.- 3 Developmental models of herbaceous plants.- 3.1 Levels of model specification.- 3.1.1 Partial L-systems.- 3.1.2 Control mechanisms in plants.- 3.1.3 Complete models.- 3.2 Branching patterns.- 3.3 Models of inflorescences.- 3.3.1 Monopodial inflorescences.- 3.3.2 Sympodial inflorescences.- 3.3.3 Polypodial inflorescences.- 3.3.4 Modified racemes.- 4 Phyllotaxis.- 4.1 The planar model.- 4.2 The cylindrical model.- 5 Models of plant organs.- 5.1 Predefined surfaces.- 5.2 Developmental surface models.- 5.3 Models of compound leaves.- 6 Animation of plant development.- 6.1 Timed DOL-systems.- 6.2 Selection of growth functions.- 6.2.1 Development of nonbranching filaments.- 6.2.2 Development of branching structures.- 7 Modeling of cellular layers.- 7.1 Map L-systems.- 7.2 Graphical interpretation of maps.- 7.3 Microsorium linguaeforme.- 7.4 Dryopteris thelypteris.- 7.5 Modeling spherical cell layers.- 7.6 Modeling 3D cellular structures.- 8 Fractal properties of plants.- 8.1 Symmetry and self-similarity.- 8.2 Plant models and iterated function systems.- Epilogue.- Appendix A Software environment for plant modeling.- A.1 A virtual laboratory in botany.- A.2 List of laboratory programs.- Appendix B About the figures.- Turtle interpretation of symbols.,1990,207,2983,229,0,15,31,28,33,41,50,41,53,47
36e2c49a23cb62bc5f47c82603373970e975b87d,"The rhizosphere encompasses the millimeters of soil surrounding a plant root where complex biological and ecological processes occur. This review describes recent advances in elucidating the role of root exudates in interactions between plant roots and other plants, microbes, and nematodes present in the rhizosphere. Evidence indicating that root exudates may take part in the signaling events that initiate the execution of these interactions is also presented. Various positive and negative plant-plant and plant-microbe interactions are highlighted and described from the molecular to the ecosystem scale. Furthermore, methodologies to address these interactions under laboratory conditions are presented.",2006,209,3163,137,9,38,63,86,108,122,169,242,244,261
90a2296556990639faec40787fe27661f1f51fbd,"One of the least understood aspects of population biology is community evolution-the evolutionary interactions found among different kinds or organisms where exchange of genetic information among the kinds is assumed to be minimal or absent. Studies of community evolution have, in general, tended to be narrow in scope and to ignore the reciprocal aspects of these interactions. Indeed, one group of organisms is all too often viewed'as a kind of physical constant. In an extreme example a parasitologist might not consider the evolutionary history and responses of hosts, while a specialist in vertebrates might assume species of vertebrate parasites to be invariate entities. This viewpoint is one factor in the general lack of progress toward the understanding of organic diversification. One approach to what we would like to call coevolution is the examination of patterns of interaction between two major groups of organisms with a close and evident ecological relationship, such as plants and herbivores. The considerable amount of information available about butterflies and their food plants make them particularly suitable for these investigations. Further, recent detailed investigations have provided a relatively firm basis for statements about the phenetic relationships of the various higher groups of Papilionoidea (Ehrlich, 1958, and unpubl.). It should, however, be remembered that we are considering the butterflies as a model. They are only one of the many groups of herbivorous organisms coevolving with plants. In this paper, we shall investigate the relationship between butterflies and their food",1964,125,3775,116,0,1,3,5,4,3,7,7,14,11
115214635784abe693708c5746526b9f61900f90,,1987,0,3887,98,8,14,13,7,10,9,23,21,21,35
a0155c2d2d69db4e2bf5870fc9be9b0adecdd9ee,"A method is described which permits measurement of sap pressure in the xylem of vascular plants. As long predicted, sap pressures during transpiration are normally negative, ranging from -4 or -5 atmospheres in a damp forest to -80 atmospheres in the desert. Mangroves and other halophytes maintain at all times a sap pressure of -35 to -60 atmospheres. Mistletoes have greater suction than their hosts, usually by 10 to 20 atmospheres. Diurnal cycles of 10 to 20 atmospheres are common. In tall conifers there is a hydrostatic pressure gradient that closely corresponds to the height and seems surprisingly little influenced by the intensity of transpiration. Sap extruded from the xylem by gas pressure on the leaves is practically pure water. At zero turgor this procedure gives a linear relation between the intracellular concentration and the tension of the xylem.",1965,14,3830,79,6,7,10,11,8,13,18,20,25,32
f5df7a0982dffb91a033e211fc5b66d6b4fc1193,,1938,0,3866,251,0,0,1,0,0,0,0,0,2,0
bf8f9de501397315843fbb8894544c48d1aa67cf,PATHWAY OF ETHYLENE BIOSyNTHESIS 156 Methionine as an Intermediate ....... 157 S-Adenosylmethionine as an Intermediate 158 I-Aminocyclopropanecarboxylic Acid as an Intermediate . ......... 158 Methionine Cycle ........ 161 Conversion of l-Aminocyclopropanecarboxylic Acid to Ethylene 164 REGULATION OF ETHYLENE BIOSYNTHESIS 167 Regulation in Ripening Fruits and Senescing Flowers ...... 167 Auxin-Induced Ethylene Production 169 Regulation of Ethylene Biosynthesis by Ethylene 169 Stress-Induced Ethylene Production . . . . . . . . 171 Regulation by Light and Carbon Dioxide 173 Inhibitors of Ethylene Biosynthesis : 174 Conjugation of l-Aminocyclopropanecarboxylic Acid to l-(Malonylamino) cyclopropanecarboxylic Acid 178 CONCLUDING REMARKS 180,1984,56,3095,140,2,29,39,45,48,88,69,72,64,88
afca469ed6f40dfa69540f139a3b06764928dab6,"Tolerance to high soil [Na(+)] involves processes in many different parts of the plant, and is manifested in a wide range of specializations at disparate levels of organization, such as gross morphology, membrane transport, biochemistry and gene transcription. Multiple adaptations to high [Na(+)] operate concurrently within a particular plant, and mechanisms of tolerance show large taxonomic variation. These mechanisms can occur in all cells within the plant, or can occur in specific cell types, reflecting adaptations at two major levels of organization: those that confer tolerance to individual cells, and those that contribute to tolerance not of cells per se, but of the whole plant. Salt-tolerant cells can contribute to salt tolerance of plants; but we suggest that equally important in a wide range of conditions are processes involving the management of Na(+) movements within the plant. These require specific cell types in specific locations within the plant catalysing transport in a coordinated manner. For further understanding of whole plant tolerance, we require more knowledge of cell-specific transport processes and the consequences of manipulation of transporters and signalling elements in specific cell types.",2003,251,2850,167,4,31,59,72,97,108,148,165,193,197
6d9c4b121f781c46b6c9227f6889524db86f5b8c,"Inducible defense-related proteins have been described in many plant species upon infection with oomycetes, fungi, bacteria, or viruses, or insect attack. Several types of proteins are common and have been classified into 17 families of pathogenesis-related proteins (PRs). Others have so far been found to occur more specifically in some plant species. Most PRs and related proteins are induced through the action of the signaling compounds salicylic acid, jasmonic acid, or ethylene, and possess antimicrobial activities in vitro through hydrolytic activities on cell walls, contact toxicity, and perhaps an involvement in defense signaling. However, when expressed in transgenic plants, they reduce only a limited number of diseases, depending on the nature of the protein, plant species, and pathogen involved. As exemplified by the PR-1 proteins in Arabidopsis and rice, many homologous proteins belonging to the same family are regulated developmentally and may serve different functions in specific organs or tissues. Several defense-related proteins are induced during senescence, wounding or cold stress, and some possess antifreeze activity. Many defense-related proteins are present constitutively in floral tissues and a substantial number of PR-like proteins in pollen, fruits, and vegetables can provoke allergy in humans. The evolutionary conservation of similar defense-related proteins in monocots and dicots, but also their divergent occurrence in other conditions, suggest that these proteins serve essential functions in plant life, whether in defense or not.",2006,205,2579,139,9,66,110,135,146,163,200,229,220,185
030989e2e5056d3eca03e13c1006670f5d034dc9,,1962,0,3109,187,3,1,3,2,4,12,7,5,5,4
a72c494aba62e75c696566d924f7b70da9e4c3fe,"Since its publication in 2000, Biochemistry & Molecular Biology of Plants, has been hailed as a major contribution to the plant sciences literature and critical acclaim has been matched by global sales success. Maintaining the scope and focus of the first edition, the second will provide a major update, include much new material and reorganise some chapters to further improve the presentation. This book is meticulously organised and richly illustrated, having over 1,000 full-colour illustrations and 500 photographs. It is divided into five parts covering: Compartments: Cell Reproduction: Energy Flow; Metabolic and Developmental Integration; and Plant Environment and Agriculture. Specific changes to this edition include: Completely revised with over half of the chapters having a major rewrite. Includes two new chapters on signal transduction and responses to pathogens. Restructuring of section on cell reproduction for improved presentation. Dedicated website to include all illustrative material. Biochemistry & Molecular Biology of Plants holds a unique place in the plant sciences literature as it provides the only comprehensive, authoritative, integrated single volume book in this essential field of study.",2002,0,3146,64,96,112,127,150,196,227,225,200,232,261
b03c9cf546e6b18c9a4467ab555a53f995299795,"Glucosinolates (beta-thioglucoside-N-hydroxysulfates), the precursors of isothiocyanates, are present in sixteen families of dicotyledonous angiosperms including a large number of edible species. At least 120 different glucosinolates have been identified in these plants, although closely related taxonomic groups typically contain only a small number of such compounds. Glucosinolates and/or their breakdown products have long been known for their fungicidal, bacteriocidal, nematocidal and allelopathic properties and have recently attracted intense research interest because of their cancer chemoprotective attributes. Numerous reviews have addressed the occurrence of glucosinolates in vegetables, primarily the family Brassicaceae (syn. Cruciferae; including Brassica spp and Raphanus spp). The major focus of much previous research has been on the negative aspects of these compounds because of the prevalence of certain ""antinutritional"" or goitrogenic glucosinolates in the protein-rich defatted meal from widely grown oilseed crops and in some domesticated vegetable crops. There is, however, an opposite and positive side of this picture represented by the therapeutic and prophylactic properties of other ""nutritional"" or ""functional"" glucosinolates. This review addresses the complex array of these biologically active and chemically diverse compounds many of which have been identified during the past three decades in other families. In addition to the Brassica vegetables, these glucosinolates have been found in hundreds of species, many of which are edible or could provide substantial quantities of glucosinolates for isolation, for biological evaluation, and potential application as chemoprotective or other dietary or pharmacological agents.",2001,304,2471,167,12,45,43,51,73,88,89,117,103,124
70a9222687cbd9c0c24e60a56e914abe0cf8977e,"MicroRNAs (miRNAs) are small, endogenous RNAs that regulate gene expression in plants and animals. In plants, these approximately 21-nucleotide RNAs are processed from stem-loop regions of long primary transcripts by a Dicer-like enzyme and are loaded into silencing complexes, where they generally direct cleavage of complementary mRNAs. Although plant miRNAs have some conserved functions extending beyond development, the importance of miRNA-directed gene regulation during plant development is now particularly clear. Identified in plants less than four years ago, miRNAs are already known to play numerous crucial roles at each major stage of development-typically at the cores of gene regulatory networks, targeting genes that are themselves regulators, such as those encoding transcription factors and F-box proteins.",2006,195,2379,167,32,99,121,118,151,138,204,199,185,197
50d072f18f0566700dba5e1f7084d140d905215f,"SummaryThe photosynthetic capacity of leaves is related to the nitrogen content primarily bacause the proteins of the Calvin cycle and thylakoids represent the majority of leaf nitrogen. To a first approximation, thylakoid nitrogen is proportional to the chlorophyll content (50 mol thylakoid N mol-1 Chl). Within species there are strong linear relationships between nitrogen and both RuBP carboxylase and chlorophyll. With increasing nitrogen per unit leaf area, the proportion of total leaf nitrogen in the thylakoids remains the same while the proportion in soluble protein increases. In many species, growth under lower irradiance greatly increases the partitioning of nitrogen into chlorophyll and the thylakoids, while the electron transport capacity per unit of chlorophyll declines. If growth irradiance influences the relationship between photosynthetic capacity and nitrogen content, predicting nitrogen distribution between leaves in a canopy becomes more complicated. When both photosynthetic capacity and leaf nitrogen content are expressed on the basis of leaf area, considerable variation in the photosynthetic capacity for a given leaf nitrogen content is found between species. The variation reflects different strategies of nitrogen partitioning, the electron transport capacity per unit of chlorophyll and the specific activity of RuBP carboxylase. Survival in certain environments clearly does not require maximising photosynthetic capacity for a given leaf nitrogen content. Species that flourish in the shade partition relatively more nitrogen into the thylakoids, although this is associated with lower photosynthetic capacity per unit of nitrogen.",2004,88,2380,183,93,66,64,69,83,95,80,109,108,133
27c764dde11259d2fdc0ab41514eff2903913c74,"INTRODUCTION 492 ECOLOGICAL ASPECTS OF PHOTOSYNTHETIC TEMPERATURE ADAPTATION 493 Photosynthetic Temperature Dependence in Thermally Contrasting Climates ........ 493 Photosynthetic Temperature Acclimation 497 Seasonal acclimation in natural habitats ...... ....... 497 Studies in controlled environments 499 THE MECHANISTIC BASIS FOR PHOTOSYNTHETIC RESPONSE AND ADAPTATION TO TEMPERATURE 504 Reversible Temperature Respon.ses 505 Stomata! effec� o� the . temJH!.rature response 0/ photo.rynthesis ......... ....... 505 Interacttons with /lght mtenslty 507 C, photo.rynthesis (lM photorespiration 507 C, photo.rynthesis 515 Comparison 0/ plants from contrasting thermol regimes ...... ...... 517 [""eversible Temperature Respon.ses 519 Low temperature sensitMty ....... 519 High temperature sensitivity 524 Adoptive responses in the heat stability 0/ the photosynthetic apparatus 530 CONCLUDING REMARKS 532",1980,67,2662,155,0,7,18,21,34,26,27,27,27,27
d729e27861197182ff1bb89517cfa93b60786076,,1988,0,2341,224,1,4,8,11,13,16,13,12,13,21
12163e44c1388a62c833b74566e5a7e6daded4f8,"A-Z presentation of Indian medicinal plants including taxonomy, traditional and international synonyms, plant parts, applications and pharmacokinetic action.",2007,30,2149,133,0,4,22,71,117,164,193,243,242,214
a8e79ad55b64d09a09798aeaf1f77679d2d80682,"Publisher Summary In this chapter, the advances that have been made in understanding the ecology of the mineral nutrition of wild plants from terrestrial ecosystems have been reviewed. This chapter is organized along three lines. First, the issues of nutrient-limited plant growth and nutrient uptake, with special emphasis on the importance of the uptake of nutrients in organic form—both by mycorrhizal and by non-mycorrhizal plants—and the importance of symbiotic nitrogen fixation is treated. In addition, the influence of allocation patterns on mineral nutrient uptake is described. Next, a few of the nutritional aspects of leaf functioning and how nutrients are used for biomass production by the plant are explored. That is done by studying the nutrient use efficiency (NUE) of plants and the various components of NUE. Finally, the feedback of plant species to soil nutrient availability by reviewing patterns in litter decomposition and nutrient mineralization is investigated. The chapter concludes with a synthesis of the various aspects of the mineral nutrition of wild plants. The chapter ends with a conceptual description of plant strategies with respect to mineral nutrition.",1999,241,2332,124,3,6,22,41,70,77,118,71,90,98
e5cf8b5bad58d9cfe9d950d98c59c2fb37489264,"There are at least three RNA silencing pathways for silencing specific genes in plants. In these pathways, silencing signals can be amplified and transmitted between cells, and may even be self-regulated by feedback mechanisms. Diverse biological roles of these pathways have been established, including defence against viruses, regulation of gene expression and the condensation of chromatin into heterochromatin. We are now in a good position to investigate the full extent of this functional diversity in genetic and epigenetic mechanisms of genome control.",2004,94,2205,117,10,141,136,145,160,128,130,120,135,142
edb564c5673f3dcc37dd8fb1d52b50061fea733f,"DNA barcoding involves sequencing a standard region of DNA as a tool for species identification. However, there has been no agreement on which region(s) should be used for barcoding land plants. To provide a community recommendation on a standard plant barcode, we have compared the performance of 7 leading candidate plastid DNA regions (atpF–atpH spacer, matK gene, rbcL gene, rpoB gene, rpoC1 gene, psbK–psbI spacer, and trnH–psbA spacer). Based on assessments of recoverability, sequence quality, and levels of species discrimination, we recommend the 2-locus combination of rbcL+matK as the plant barcode. This core 2-locus barcode will provide a universal framework for the routine use of DNA sequence data to identify specimens and contribute toward the discovery of overlooked species of land plants.",2009,32,2012,77,9,90,130,151,148,145,189,198,179,199
16a64ffec2bddf4e48251660240b93fd3a7c7000,"Alkaloids, tannins, saponins, steroid, terpenoid, flavonoids, phlobatannin and cardic glycoside distribution in ten medicinal plants belonging to different families were assessed and compared. The medicinal plants investigated were Cleome nutidosperma, Emilia coccinea, Euphorbia heterophylla, Physalis angulata, Richardia bransitensis, Scopania dulcis, Sida acuta, Spigelia anthelmia, Stachytarpheta cayennensis and Tridax procumbens. All the plants were found to contain alkaloids, tannins and flavonoids except for the absence of tannins in S. acuta and flavonoids in S. cayennsis respectively. The significance of the plants in traditional medicine and the importance of the distribution of these chemical constituents were discussed with respect to the role of these plants in ethnomedicine in Nigeria.",2005,22,2511,90,0,5,13,20,58,85,135,188,235,282
561d6920bfe33e087fd9d411d873e83a5e71872b,,1959,0,3656,9,1,3,5,3,3,4,4,8,5,5
526efd0a489d35f3184263110a7041a201ca8601,"Despite widespread concern about declines in pollination services, little is known about the patterns of change in most pollinator assemblages. By studying bee and hoverfly assemblages in Britain and the Netherlands, we found evidence of declines (pre-versus post-1980) in local bee diversity in both countries; however, divergent trends were observed in hoverflies. Depending on the assemblage and location, pollinator declines were most frequent in habitat and flower specialists, in univoltine species, and/or in nonmigrants. In conjunction with this evidence, outcrossing plant species that are reliant on the declining pollinators have themselves declined relative to other plant species. Taken together, these findings strongly suggest a causal connection between local extinctions of functionally linked plant and pollinator species.",2006,46,2314,102,7,42,70,85,123,117,138,170,203,182
5ab7efc9d71304c10f0c1cdf0021aa293b3961ea,"Phosphorus (P) is limiting for crop yield on > 30% of the world's arable land and, by some estimates, world resources of inexpensive P may be depleted by 2050. Improvement of P acquisition and use by plants is critical for economic, humanitarian and environmental reasons. Plants have evolved a diverse array of strategies to obtain adequate P under limiting conditions, including modifications to root architecture, carbon metabolism and membrane structure, exudation of low molecular weight organic acids, protons and enzymes, and enhanced expression of the numerous genes involved in low-P adaptation. These adaptations may be less pronounced in mycorrhizal-associated plants. The formation of cluster roots under P-stress by the nonmycorrhizal species white lupin (Lupinus albus), and the accompanying biochemical changes exemplify many of the plant adaptations that enhance P acquisition and use. Physiological, biochemical, and molecular studies of white lupin and other species response to P-deficiency have identified targets that may be useful for plant improvement. Genomic approaches involving identification of expressed sequence tags (ESTs) found under low-P stress may also yield target sites for plant improvement. Interdisciplinary studies uniting plant breeding, biochemistry, soil science, and genetics under the large umbrella of genomics are prerequisite for rapid progress in improving nutrient acquisition and use in plants. Contents I. Introduction 424 II. The phosphorus conundrum 424 III. Adaptations to low P 424 IV. Uptake of P 424 V. P deficiency alters root development and function 426 VI. P deficiency modifies carbon metabolism 431 VII. Acid phosphatase 436 VIII. Genetic regulation of P responsive genes 437 IX. Improving P acquisition 439 X. Synopsis 440.",2003,292,2144,147,0,26,47,50,50,71,85,90,128,132
17ded1f9e7cd14d7632aaf936053453e9698646d,"Agricultural productivity worldwide is subject to increasing environmental constraints, particularly to drought and salinity due to their high magnitude of impact and wide distribution. Traditional breeding programs trying to improve abiotic stress tolerance have had some success, but are limited by the multigenic nature of the trait. Tolerant plants such as Craterostigma plantagenium, Mesembryanthemum crystallinum, Thellungiella halophila and other hardy plants could be valuable tools to dissect the extreme tolerance nature. In the last decade, Arabidopsis thaliana, a genetic model plant, has been extensively used for unravelling the molecular basis of stress tolerance. Arabidopsis also proved to be extremely important for assessing functions for individual stress-associated genes due to the availability of knock-out mutants and its amenability for genetic transformation. In this review, the responses of plants to salt and water stress are described, the regulatory circuits which allow plants to cope with stress are presented, and how the present knowledge can be applied to obtain tolerant plants is discussed.",2005,457,2203,93,12,28,39,80,85,109,162,155,181,169
20ffe80b1746fb63c442674ba0ba798072267291,"This paper empirically investigates the effects of trade liberalization on plant productivity in the case of Chile. Chile presents an interesting setting to study this relationship since it underwent a massive trade liberalization that significantly exposed its plants to competition from abroad during the late 1970s and early 1980s. Methodologically, I approach this question in two steps. In the first step, I estimate a production function to obtain a measure of plant productivity. I estimate the production function semiparametrically to correct for the presence of selection and simultaneity biases in the estimates of the input coefficients required to construct a productivity measure. I explicitly incorporate plant exit in the estimation to correct for the selection problem induced by liquidated plants. These methodological aspects are important in obtaining a reliable plant-level productivity measure based on consistent estimates of the input coefficients. In the second step, I identify the impact of trade liberalization on plants' productivity in a regression framework allowing variation in productivity over time and across traded- and nontraded-goods sectors. Using plant-level panel data on Chilean manufacturers, I find evidence of within plant productivity improvements that can be attributed to a liberalized trade policy, especially for the plants in the import-competing sector. In many cases, aggregate productivity improvements stem from the reshuffling of resources and output from less to more efficient producers.",2000,57,1998,146,4,10,23,47,65,72,92,84,122,131
003a49aa331f59d72b40ca9d86d665ad42f3ef4e,"Molecular studies of drought stress in plants use a variety of strategies and include different species subjected to a wide range of water deficits. Initial research has by necessity been largely descriptive, and relevant genes have been identified either by reference to physiological evidence or by differential screening. A large number of genes with a potential role in drought tolerance have been described, and major themes in the molecular response have been established. Particular areas of importance are sugar metabolism and late-embryogenesis-abundant (LEA) proteins. Studies have begun to examine mechanisms that control the gene expression, and putative regulatory pathways have been established. Recent attempts to understand gene function have utilized transgenic plants. These efforts are of clear agronomic importance.",1996,208,2237,103,6,34,52,71,66,82,119,66,105,87
451358bc1b585bc3fa95f1d78b29b055278b2bd0,"The evolutionary response of plants to herbivory is constrained by the availability of resources in the environment. Woody plants adapted to low-resource environments have intrinsically slow growth rates that limit their capacity to grow rapidly beyond the reach of most browsing mammals. Their low capacity to acquire resources limits their potential for compensatory growth which would otherwise enable them to replace tissue destroyed by browsing. Plants adapted to low-resource environments have responded to browsing by evolving strong constitutive defenses with relatively low ontogenetic plasticity. Because nutrients are often more limiting than light in boreal forests, slowly growing boreal forest trees utilize carbon-based rather than nitrogen-based defenses. More rapidly growing shade-intolerant trees that are adapted to growth in high-resource environments are selected for competitive ability and can grow rapidly beyond the range of most browsing mammals. Moreover, these plants have the carbon and nutrient reserves necessary to replace tissue lost to browsing through compensatory growth. However, because browsing of juvenile plants reduces vertical growth and thus competitive ability, these plants are selected for resistance to browsing during the juvenile growth phase. Consequently, early successional boreal forest trees have responded to browsing by evolving strong defenses during juvenility only. Because severe pruning causes woody plants to revert to a juvenile form, resistance of woody plants to hares increases after severe hare browsing as occurs during hare population outbreaks. This increase in browsing resistance may play a significant role in boreal forest plant-hare interactions. Unlike woody plants, graminoids retain large reserves of carbon and nutrients below ground in both low-resource and high-resource environments and can respond to severe grazing through compensatory growth. These fundamental differences between the response of woody plants and graminoids to vertebrate herbivory suggest that the dynamics of browsing systems and grazing systems are qualitatively different.",1983,107,2362,132,7,7,17,16,24,32,26,27,41,37
69ff421246a51fb9d8f3d10a8227221ed532960a,"Methods of analysis for soils plants and waters , Methods of analysis for soils plants and waters , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,2254,168,17,19,21,20,33,16,16,19,25,13
d4b6194ba708d6c9c0f36b9294fec65b12e16a30,"This paper reviews the plant geography, ecology, metal tolerance and phytochcmistry of terrestrial higher plants which arc able to accumulate metallic elements in their dry matter to an exceptional degree. The review is limited to the elements Co, Cu, Cr, Pb. Mn. Ni and Zn. Hyperaccumulators of Co, Cu, Cr, Pb and Ni arc here defined as plants containing over 1000 u.g/g (ppm) of any of these elements in the dry matter; for Mn and Zn, the criterion is 10,000 u.g/g (1%). A unifying feature of hypcraccumula ting plants is their general restriction to mineralized soils and specific rock types. Lists of hypcraccumula ting species arc presented for the elements considered. These suggest that the phenomenon is widespread throughout the plant kingdom. For example, 145 hyper-accumulators of nickel are reported: these arc distributed among 6 supcrordcrs, 17 orders, and 22 families and include herbs, shrubs and trees from both the temperate and tropical zones. Although some phylogcnetic relationships emerge, the evolutionary significance of metal hyperaccumulation remains obscure. Phytochemical studies however suggest that hyperaccumulation is closely linked to the mechanism of metal tolerance involved in the successful colonization of metalliferous and otherwise phytotoxic soils. The potentialities of hyperaccumula ting plants in biorccovcry and soil detoxification arc indicated.",1989,58,2313,157,1,0,5,4,5,12,7,9,20,16
fcb343d0e7a762992cb20d618fa4253040a0ff80,,1971,0,2579,178,1,5,8,15,30,14,12,17,36,18
dbe6c36aac8d85dc67c944079e02d5f6d6000657,"Publisher Summary This chapter focuses on evolutionary significance of phenotypic plasticity in plants. The expression of an individual genotype is modified by its environment. The amount by which it can be modified is termed its plasticity. This plasticity can be either morphological or physiological; these are interrelated. The plasticity of a character is related to the general pattern of its development, and apart from this, that plasticity is a general property of the whole genotype. Plasticity of a character appears to be specific for that character, specific in relation to particular environmental influences, specific in direction, under genetic control not necessarily related to heterozygosity, and able to be radically altered by selection. Because plants are static organisms, plasticity is of marked adaptive value in a great number of situations. Examples of all these situations in plant species are discussed. They indicate that adaptation by plasticity is a widespread and important phenomenon in plants and has evolved differently in different species. The mechanisms involved in plasticity are varied. At one extreme, the character shows a continuous range of modification dependent on the intensity of the environmental stimulus. At the other, the character shows only two discrete modifications. The stimulus causing these modifications may be direct or indirect. The mechanisms found can be related to the particular environmental situation involved.",1965,70,2777,113,0,1,1,5,4,7,3,4,3,5
129c03c968f58f85a9f7afeae38c2fdc1cc704a9,"We reconcile trade theory with plant-level export behavior, extending the Ricardian model to accommodate many countries, geographic barriers, and imperfect competition. Our model captures qualitatively basic facts about U.S. plants: (i) productivity dispersion, (ii) higher productivity among exporters, (iii) the small fraction who export, (iv) the small fraction earned from exports among exporting plants, and (v) the size advantage of exporters. Fitting the model to bilateral trade among the United States and 46 major trade partners, we examine the impact of globalization and dollar appreciation on productivity, plant entry and exit, and labor turnover in U.S. manufacturing. (JEL F11, F17, O33)",2004,21,1459,272,26,40,49,80,103,96,107,124,98,103
88792e3cf04d925fd6c073599a179d857e173ed6,"Sugars not only fuel cellular carbon and energy metabolism but also play pivotal roles as signaling molecules. The experimental amenability of yeast as a unicellular model system has enabled the discovery of multiple sugar sensors and signaling pathways. In plants, different sugar signals are generated by photosynthesis and carbon metabolism in source and sink tissues to modulate growth, development, and stress responses. Genetic analyses have revealed extensive interactions between sugar and plant hormone signaling, and a central role for hexokinase (HXK) as a conserved glucose sensor. Diverse sugar signals activate multiple HXK-dependent and HXK-independent pathways and use different molecular mechanisms to control transcription, translation, protein stability and enzymatic activity. Important and complex roles for Snf1-related kinases (SnRKs), extracellular sugar sensors, and trehalose metabolism in plant sugar signaling are now also emerging.",2006,158,1814,123,11,75,94,103,114,120,146,133,165,131
a14bbdbbf49575f81b460473784080de623c7534,"Toxic metal pollution of waters and soils is a major environmental problem, and most conventional remediation approaches do not provide acceptable solutions. The use of specially selected and engineered metal-accumulating plants for environmental clean-up is an emerging technology called phytoremediation. Three subsets of this technology are applicable to toxic metal remediation: (1) Phytoextraction—the use of metal-accumulating plants to remove toxic metals from soil; (2) Rhizoflltration—the use of plant roots to remove toxic metals from polluted waters; and (3) Phytostabilization—the use of plants to eliminate the bioavailability of toxic metals hi soils. Biological mechanisms of toxic metal uptake, translocation and resistance as well as strategies for improving phytoremediation are also discussed.",1995,67,2177,88,1,12,27,18,35,39,39,46,67,76
271fb86a07b21eb8060c3272d6c298bfb1d59189,"Woody plants such as trees have a significant economic and climatic influence on global economies and ecologies. This completely revised classic book is an up-to-date synthesis of the intensive research devoted to woody plants published in the second edition, with additional important aspects from the authors' previous book, ""Growth Control in Woody Plants"". Intended primarily as a reference for researchers, the interdisciplinary nature of the book makes it useful to a broad range of scientists and researchers from agroforesters, agronomists, and arborists to plant pathologists and soil scientists. This third edition provides crutial updates to many chapters, including: responses of plants to elevated CO2; the process and regulation of cambial growth; photoinhibition and photoprotection of photosynthesis; nitrogen metabolism and internal recycling, and more.Revised chapters focus on emerging discoveries of the patterns and processes of woody plant physiology. This is the only book to provide recommendations for the use of specific management practices and experimental procedures and equipment. Interdisciplinary approach will appeal to a broad range of scientists, researchers, and growers. It is thoroughly updated with the latest research devoted to woody plants.",1983,0,2528,55,17,33,30,36,36,38,29,37,38,49
807d6057c5f59e97dfe4aa288f455c2cd0badc3a,"Abstract The paper summarizes present knowledge in the field of higher plant responses to cadmium, an important environmental pollutant. The principal mechanisms reviewed here include phytochelatin-based sequestration and compartmentalization processes, as well as additional defense mechanisms, based on cell wall immobilization, plasma membrane exclusion, stress proteins, stress ethylene, peroxidases, metallothioneins, etc. An analysis of data taken from the international literature has been carried out, in order to highlight possible ‘qualitative’ and ‘quantitative’ differences in the response of wild-type (non-tolerant) plants to chronic and acute cadmium stress. The dose-response relationships indicate that plant response to low and high cadmium level exposures is a very complex phenomenon indeed: cadmium evokes a number of parallel and/or consecutive events at molecular, physiological and morphological levels. We propose that, above all in response to acute cadmium stress, various mechanisms might operate both in an additive and in a potentiating way. Thus, a holistic and integrated approach seems to be necessary in the study of the response of higher plants to cadmium. This multi-component model, which we would call ‘fan-shaped’ response, may accord with the Selyean ‘general adaptation syndrome’ hypothesis. While cadmium detoxification is a complex phenomenon, probably under polygenic control, cadmium ‘real’ tolerance—found in mine plants or in plant systems artificially grown under long-term selection pressure, exposed to high levels of cadmium—seems to be a simpler phenomenon, possibly involving only monogenic/oligogenic control. We conclude that, following a ‘pyramidal’ model, (adaptive) tolerance is supported by (constitutive) detoxification mechanisms, which in turn rely on (constitutive) homeostatic processes. The shift between homeostasis and ‘fan-shaped’ response can be rapid and involve quick changes in (poly)gene expression. Differently, the slow shift from ‘fan-shaped’ response to ‘real’ cadmium tolerance is caused and affected by long-term selection pressure, which may increase the frequency (and promote the expression) of one or a few tolerance gene(s).",1999,169,2160,82,0,10,14,27,38,63,71,74,102,112
70c0541452254eaed66b19526884bcc9f179bb82,"In this review we describe and discuss several approaches to selecting higher plants as candidates for drug development with the greatest possibility of success. We emphasize the role of information derived from various systems of traditional medicine (ethnomedicine) and its utility for drug discovery purposes. We have identified 122 compounds of defined structure, obtained from only 94 species of plants, that are used globally as drugs and demonstrate that 80% of these have had an ethnomedical use identical or related to the current use of the active elements of the plant. We identify and discuss advantages and disadvantages of using plants as starting points for drug development, specifically those used in traditional medicine.",2001,118,1987,51,0,5,5,8,19,16,30,43,43,62
b7f38626c1ffbc8acfcc6a4d4655c796debe6ea4,"MicroRNAs (miRNAs) are an extensive class of ~22-nucleotide noncoding RNAs thought to regulate gene expression in metazoans. We find that miRNAs are also present in plants, indicating that this class of noncoding RNA arose early in eukaryotic evolution. In this paper 16 Arabidopsis miRNAs are described, many of which have differential expression patterns in development. Eight are absolutely conserved in the rice genome. The plant miRNA loci potentially encode stem-loop precursors similar to those processed by Dicer (a ribonuclease III) in animals. Mutation of an Arabidopsis Dicer homolog, CARPEL FACTORY, prevents the accumulation of miRNAs, showing that similar mechanisms direct miRNA processing in plants and animals. The previously described roles of CARPEL FACTORY in the development of Arabidopsis embryos, leaves, and floral meristems suggest that the miRNAs could play regulatory roles in the development of plants as well as animals.",2002,62,1648,134,17,59,90,89,61,87,76,75,77,97
1744b271a1564af41e34302e6f51c0cc3dbd0659,"Revue bibliographique suggerant que, au moins pour la croissance vegetative les plantes fonctionnent conformement aux theoremes economiques: optimiser les profits et repartir de facon optimale les ressources",1985,99,2217,80,0,2,18,25,22,26,35,39,28,43
caf3dfcf243a415c5dd4f337c8560e4bc04d2fa2,"Multicellular eukaryotes produce small RNA molecules (approximately 21–24 nucleotides) of two general types, microRNA (miRNA) and short interfering RNA (siRNA). They collectively function as sequence-specific guides to silence or regulate genes, transposons, and viruses and to modify chromatin and genome structure. Formation or activity of small RNAs requires factors belonging to gene families that encode DICER (or DICER-LIKE [DCL]) and ARGONAUTE proteins and, in the case of some siRNAs, RNA-dependent RNA polymerase (RDR) proteins. Unlike many animals, plants encode multiple DCL and RDR proteins. Using a series of insertion mutants of Arabidopsis thaliana, unique functions for three DCL proteins in miRNA (DCL1), endogenous siRNA (DCL3), and viral siRNA (DCL2) biogenesis were identified. One RDR protein (RDR2) was required for all endogenous siRNAs analyzed. The loss of endogenous siRNA in dcl3 and rdr2 mutants was associated with loss of heterochromatic marks and increased transcript accumulation at some loci. Defects in siRNA-generation activity in response to turnip crinkle virus in dcl2 mutant plants correlated with increased virus susceptibility. We conclude that proliferation and diversification of DCL and RDR genes during evolution of plants contributed to specialization of small RNA-directed pathways for development, chromatin structure, and defense.",2004,104,1512,193,46,122,111,104,111,102,91,101,108,78
294ec060bca8f76d34862776da6aedf6cfa80849,"Various novel techniques including ultrasound-assisted extraction, microwave-assisted extraction, supercritical fluid extraction, and accelerated solvent extraction have been developed for the extraction of nutraceuticals from plants in order to shorten the extraction time, decrease the solvent consumption, increase the extraction yield, and enhance the quality of extracts. A critical review was conducted to introduce and compare the conventional Soxhlet extraction and the new alternative methods used for the extraction of nutraceuticals from plants. The practical issues of each extraction method were discussed. Potential uses of those methods for the extraction of nutraceuticals from plant materials was finally summarized.",2006,88,1620,82,3,13,22,28,39,68,90,113,144,162
a7f35dad58665f866e497f5a3af9fda98a9c0649,"PHOTO PROTECTION 604 Prevention oj Excessive Light Absorption... 604 Removal of Excess Excitation Energy Directly within the Light-Capturing System ......... ...... . . ..... ..... . .... . ..... ...... .... . .. . .. . . ..... . . . ... ... . 604 Removal oj Active Oxygen Formed in the Photochemical Apparatus ........ . . .. . . . . . . 605 INACTIV A TIONiTURNOVER OF PS II 606 THE XANTHOPHYLL CYCLE AND THERMAL ENERGY DISSIPATION: A PHOTOPROTECTIVE RESPONSE 608 Characteristics oj the Xanthophyll Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . 608 Association Among the De-epoxidized State oj the Xanthophyll Cycle, Thermal Energy Dissipation. and Photoprotection .. .. . . . .. . . ...... .. .. ... ... 609 Operation of the Xanthophyll Cycle in the Field . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .... . . . .. . . . . 611 CONCLUSIONS 618",1992,30,2307,43,3,22,70,67,94,66,66,71,63,74
666deb16b072ba41d9acbb3c7f59eb22f0c77b4c,,1982,0,2488,19,8,24,30,35,40,44,40,37,39,42
12c7d7db0d7a5943a413db2e6c38d387c8c28070,"A new and relatively simple equation for the soil-water content-pressure head curve, 8(h), is described in this paper. The particular form of the equation enables one to derive closedform analytical expressions for the relative hydraulic conductivity, Kr, when substituted in the predictive conductivity models of N.T. Burdine or Y. Mualem. The resulting expressions for Kr(h) contain three independent parameters which may be obtained by fitting the proposed soil-water retention model to experimental data. Results obtained with the closed-form analytical expressions based on the Mualem theory are compared with observed hydraulic conductivity data for five soils with a wide range of hydraulic properties. The unsaturated hydraulic conductivity is predicted well in four out of five cases. It is found that a reasonable description of the soil-water retention curve at low water contents is important for an accurate prediction of the unsaturated hydraulic conductivity. Additional Index Words: soil-water diffusivity, soil-water retention curve. van Genuchten, M. Th. 1980. A closed-form equation for predicting the hydraulic conductivity of unsaturated soils. Soil Sci. Soc. Am. J. 44:892-898. T USE OF NUMERICAL MODELS for simulating fluid flow and mass transport in the unsaturated zone has become increasingly popular the last few years. Recent literature indeed demonstrates that much effort is put into the development of such models (Reeves and Duguid, 1975; Segol, 1976; Vauclin et al., 1979). Unfortunately, it appears that the ability to fully characterize the simulated system has not kept pace with the numerical and modeling expertise. Probably the single most important factor limiting the successful application of unsaturated flow theory to actual field problems is the lack of information regarding the parameters entering the governing transfer equations. Reliable estimates of the unsaturated hydraulic conductivity are especially difficult to obtain, partly because of its extensive variability in the field, and partly because measuring this parameter is time-consuming and expensive. Several investigators have, for these reasons, used models for calculating the unsaturated conductivity from the more easily measured soil-water retention curve. Very popular among these models has been the Millington-Quirk method (Millington and Quirk, 1961), various forms of which have been applied with some success in a number of studies (cf. Jackson et al., 1965; Jackson, 1972; Green and Corey, 1971; Bruce, 1972). Unfortunately, this method has the disadvantage of producing tabular results which, for example when applied to nonhomogeneous soils in multidimensional unsaturated flow models, are quite tedious to use. Closed-form analytical expressions for predicting 1 Contribution from the U. S. Salinity Laboratory, AR-SEA, USDA, Riverside, CA 92501. Received 29 June 1979. Approved 19 May I960. 'Soil Scientist, Dep. of Soil and Environmental Sciences, University of California, Riverside, CA 92521. The author is located at the U. S. Salinity Lab., 4500 Glenwood Dr., Riverside, CA 92502. the unsaturated hydraulic conductivity have also been developed. For example, Brooks and Corey (1964) and Jeppson (1974) each used an analytical expression for the conductivity based on the Burdine theory (Burdine, 1953). Brooks and Corey (1964, 1966) obtained fairly accurate predictions with their equations, even though a discontinuity is present in the slope of both the soil-water retention curve and the unsaturated hydraulic conductivity curve at some negative value of the pressure head (this point is often referred to as the bubbling pressure). Such a discontinuity sometimes prevents rapid convergence in numerical saturated-unsaturated flow problems. It also appears that predictions based on the Brooks and Corey equations are somewhat less accurate than those obtained with various forms of the (modified) Millington-Quirk method. Recently Mualem (1976a) derived a new model for predicting the hydraulic conductivity from knowledge of the soil-water retention curve and the conductivity at saturation. Mualem's derivation leads to a simple integral formula for the unsaturated hydraulic conductivity which enables one to derive closed-form analytical expressions, provided suitable equations for the soil-water retention curves are available. It is the purpose of this paper to derive such expressions using an equation for the soil-water retention curve which is both continuous and has a continuous slope. The resulting conductivity models generally contain three independent parameters which may be obtained by matching the proposed soil-water retention curve to experimental data. Results obtained with the closedform equations based on the Mualem theory will be compared with observed data for a few soils having widely varying hydraulic properties. THEORETICAL Equations Based on Mualem's Model The following equation was derived by Mualem (1976a) for predicting the relative hydraulic conductivity (Kr) from knowledge of the soil-water retention curve",1980,12,20504,1315,0,0,0,0,0,0,0,0,0,0
681a42d80a5dd02d2917a7ec4af079916c576f29,,1954,18,9874,1205,3,1,1,1,4,4,1,1,3,4
77d574bda2cc7edd4129da81a5f1aeff0c45572a,Chapter 1 The Biosphere Chapter 2 The Anthroposphere Introduction Air Pollution Water Pollution Soil Plants Chapter 3 Soils and Soil Processes Introduction Weathering Processes Pedogenic Processes Chapter 4 Soil Constituents Introduction Trace Elements Minerals Organic Matter Organisms in Soils Chapter 5 Trace Elements in Plants Introduction Absorption Translocation Availability Essentiality and Deficiency Toxicity and Tolerance Speciation Interaction Chapter 6 Elements of Group 1 (Previously Group Ia) Introduction Lithium Rubidium Cesium Chapter 7 Elements of Group 2 (Previously Group IIa) Beryllium Strontium Barium Radium Chapter 8 Elements of Group 3 (Previously Group IIIb) Scandium Yttrium Lanthanides Actinides Chapter 9 Elements of Group 4 (Previously Group IVb) Titanium Zirconium Hafnium Chapter 10 Elements of Group 5 (Previously Group Vb) Vanadium Niobium Tantalum Chapter 11 Elements of Group 6 (Previously Group VIb) Chromium Molybdenum Tungsten Chapter 12 Elements of Group 7 (Previously Group VIIb) Manganese Technetium Rhenium Chapter 13 Elements of Group 8 (Previously Part of Group VIII) Iron Ruthenium Osmium Chapter 14 Elements of Group 9 (Previously Part of Group VIII) Cobalt Rhodium Iridium Chapter 15 Elements of Group 10 (Previously Part of Group VIII) Nickel Palladium Platinum Chapter 16 Elements of Group 11 (Previously Group Ib) Copper Silver Gold Chapter 17 Trace Elements of Group 12 (Previously of Group IIb) Zinc Cadmium Mercury Chapter 18 Elements of Group 13 (Previously Group IIIa) Boron Aluminum Gallium Indium Thallium Chapter 19 Elements of Group I4 (Previously Group IVa) Silicon Germanium Tin Lead Chapter 20 Elements of Group 15 (Previously Group Va) Arsenic Antimony Bismuth Chapter 21 Elements of Group 16 (Previously Group VIa) Selenium Tellurium Polonium Chapter 22 Elements of Group 17 (Previously Group VIIa) Fluorine Chlorine Bromine Iodine,1984,813,7166,397,0,2,10,14,13,11,19,19,21,21
b258961fa2f5b4d3171133e23c2c56765bd5b179,"This introduction to modern soil chemistry describes chemical processes in soils in terms of established principles of inorganic, organic, and physical chemistry. The text provides an understanding of the structure of the solid mineral and organic materials from which soils are formed, and explains such important processes as cation exchange, chemisorption and physical absorption of organic and inorganic ions and molecules, soil acidification and weathering, oxidation-reduction reactions, and development of soil alkalinity and swelling properties. Environmental rather than agricultural topics are emphasized, with individual chapters on such pollutants as heavy metals, trace elements, and inorganic chemicals.",1994,0,6422,344,29,37,60,73,67,82,120,160,163,226
d159981f36764af240862124706dc9c23dd685f6,"DETERMINATION OF TOTAL, ORGANIC, AND AVAILABLE FORMS OF PHOSPHORUS IN SOILS ROGER BRAY;L. KURTZ; Soil Science",1945,0,6270,446,0,0,0,0,0,2,1,2,2,2
e06bd6aaf990dd41c71447a154e8d8cb62965705,"Chemical equilibria in soils , Chemical equilibria in soils , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1979,0,3794,445,0,3,8,22,29,27,34,35,35,66
4b365de4963549b572861e79ce0f89200c07c00b,"Summary 
The water-stability of aggregates in many soils is shown to depend on organic materials. The organic binding agents have been classified into (a) transient, mainly polysaccharides, (b), temporary, roots and fungal hyphae, and (c) persistent, resistant aromatic components associated with polyvalent metal cations, and strongly sorbed polymers. The effectiveness of various binding agents at different stages in the structural organization of aggregates is described and forms the basis of a model which illustrates the architecture of an aggregate. Roots and hyphae stabilize macro-aggregates, defined as > 250 μm diameter; consequently, macroaggregation is controlled by soil management (i.e. crop rotations), as management influences the growth of plant roots, and the oxidation of organic carbon. The water-stability of micro-aggregates depends on the persistent organic binding agents and appears to be a characteristic of the soil, independent of management.",1982,81,5073,270,0,2,15,12,12,19,22,26,21,28
3bca850920c736324a97fbcb8ec57a895ceb31b4,"Fifteen or more elements present in rocks and soils normally in very small amounts are essential for plant and/or animal nutrition. By the nature of their low abundance in natural uncontaminated earth materials or plants, they are known as trace elements, minor elements or micro-nutrients. Boron, copper, iron, manganese, molybdenum, silicon, vanadium and zinc are required by plants; copper, cobalt, iodine, iron, manganese, molybdenum, selenium and zinc by animals. In addition essential roles of arsenic, fluorine, nickel, silicon, tin and vanadium have in recent years been established in animal nutrition.",1980,80,3261,595,0,1,1,0,0,0,1,2,1,2
b6b67088f42337785438e8f40dfa89bb314d8d0d,"Keywords: Mecanique des sols ; Sols non satures Reference Record created on 2004-09-07, modified on 2016-08-08",1993,0,3168,322,0,6,13,17,15,23,22,40,51,57
3a5e696a03d8052d46136bff8d64a28f8935a531,"Spatial distributions of soil properties at the field and watershed scale may affect yield potential, hydrologic responses, and transport of herbicides and NO to surface or groundwater. Our research describes field-scale distributions and spatial trends for 28 different soil parameters at two sites within a watershed in central Iowa. Two of 27 parameters measured at one site and 10 of 14 parameters measured at the second site were normally distributed. Spatial variability was investigated using semivariograms and the ratio of nugget to total semivariance, expressed as a percentage, was used to classify spatial dependence. A ratio of 75% indicated weak spatial dependence. Twelve parameters at Site one, including organic C, total N, pH, and macroaggregation, and four parameters at Site two, including organic C and total N, were strongly spatially dependent. Six parameters at Site one, including biomass C and N, bulk density, and denitrification, and 9 parameters at Site two, including biomass C and N and bulk density, were moderately spatially dependent. Three parameters at Site one, including NO N and ergosterol, and one parameter at Site two, mineral-associated N, were weakly spatially dependent. Distributions of exchangeable Ca and Mg at Site one were not spatially dependent. Spatial distributions for some soil properties were similar for both field sites. We will be able to exploit these similarities to improve our ability to extrapolate information taken from one field to other fields within similar landscapes.",1994,0,2948,290,0,3,6,7,5,16,19,29,27,33
1049a0a79408aedf617568b5bda2268bda3a4520,"The relationship between soil structure and the ability of soil to stabilize soil organic matter (SOM) is a key element in soil C dynamics that has either been overlooked or treated in a cursory fashion when developing SOM models. The purpose of this paper is to review current knowledge of SOM dynamics within the framework of a newly proposed soil C saturation concept. Initially, we distinguish SOM that is protected against decomposition by various mechanisms from that which is not protected from decomposition. Methods of quantification and characteristics of three SOM pools defined as protected are discussed. Soil organic matter can be: (1) physically stabilized, or protected from decomposition, through microaggregation, or (2) intimate association with silt and clay particles, and (3) can be biochemically stabilized through the formation of recalcitrant SOM compounds. In addition to behavior of each SOM pool, we discuss implications of changes in land management on processes by which SOM compounds undergo protection and release. The characteristics and responses to changes in land use or land management are described for the light fraction (LF) and particulate organic matter (POM). We defined the LF and POM not occluded within microaggregates (53–250 μm sized aggregates as unprotected. Our conclusions are illustrated in a new conceptual SOM model that differs from most SOM models in that the model state variables are measurable SOM pools. We suggest that physicochemical characteristics inherent to soils define the maximum protective capacity of these pools, which limits increases in SOM (i.e. C sequestration) with increased organic residue inputs.",2002,163,2993,247,2,8,21,37,70,81,81,108,122,113
650f5a4464b11b4247f1c7bdbf8fe63eaf71b0ea,"Summary 
The soil is important in sequestering atmospheric CO2 and in emitting trace gases (e.g. CO2, CH4 and N2O) that are radiatively active and enhance the ‘greenhouse’ effect. Land use changes and predicted global warming, through their effects on net primary productivity, the plant community and soil conditions, may have important effects on the size of the organic matter pool in the soil and directly affect the atmospheric concentration of these trace gases. 
 
A discrepancy of approximately 350 × 1015 g (or Pg) of C in two recent estimates of soil carbon reserves worldwide is evaluated using the geo-referenced database developed for the World Inventory of Soil Emission Potentials (WISE) project. This database holds 4353 soil profiles distributed globally which are considered to represent the soil units shown on a 1/2° latitude by 1/2° longitude version of the corrected and digitized 1:5 M FAO–UNESCO Soil Map of the World. 
 
Total soil carbon pools for the entire land area of the world, excluding carbon held in the litter layer and charcoal, amounts to 2157–2293 Pg of C in the upper 100 cm. Soil organic carbon is estimated to be 684–724 Pg of C in the upper 30 cm, 1462–1548 Pg of C in the upper 100 cm, and 2376–2456 Pg of C in the upper 200 cm. Although deforestation, changes in land use and predicted climate change can alter the amount of organic carbon held in the superficial soil layers rapidly, this is less so for the soil carbonate carbon. An estimated 695–748 Pg of carbonate-C is held in the upper 100 cm of the world's soils. Mean C: N ratios of soil organic matter range from 9.9 for arid Yermosols to 25.8 for Histosols. Global amounts of soil nitrogen are estimated to be 133–140 Pg of N for the upper 100 cm. Possible changes in soil organic carbon and nitrogen dynamics caused by increased concentrations of atmospheric CO2 and the predicted associated rise in temperature are discussed.",1996,36,2950,278,2,1,11,10,21,30,28,42,61,46
78d44e2458c4e583549f1dfc9e87fda3551717a0,,1954,0,5549,154,3,1,4,8,6,9,15,22,5,20
f9a6ed1439078a7bdcd97cb70c0c3087c2e86c86,"A simple, rapid method for bacterial lysis and direct extraction of DNA from soils with minimal shearing was developed to address the risk of chimera formation from small template DNA during subsequent PCR. The method was based on lysis with a high-salt extraction buffer (1.5 M NaCl) and extended heating (2 to 3 h) of the soil suspension in the presence of sodium dodecyl sulfate (SDS), hexadecyltrimethylammonium bromide, and proteinase K. The extraction method required 6 h and was tested on eight soils differing in organic carbon, clay content, and pH, including ones from which DNA extraction is difficult. The DNA fragment size in crude extracts from all soils was > 23 kb. Preliminary trials indicated that DNA recovery from two soils seeded with gram-negative bacteria was 92 to 99%. When the method was tested on all eight unseeded soils, microscopic examination of indigenous bacteria in soil pellets before and after extraction showed variable cell lysis efficiency (26 to 92%). Crude DNA yields from the eight soils ranged from 2.5 to 26.9 micrograms of DNA g-1, and these were positively correlated with the organic carbon content in the soil (r = 0.73). DNA yields from gram-positive bacteria from pure cultures were two to six times higher when the high-salt-SDS-heat method was combined with mortar-and-pestle grinding and freeze-thawing, and most DNA recovered was of high molecular weight. Four methods for purifying crude DNA were also evaluated for percent recovery, fragment size, speed, enzyme restriction, PCR amplification, and DNA-DNA hybridization. In general, all methods produced DNA pure enough for PCR amplification. Since soil type and microbial community characteristics will influence DNA recovery, this study provides guidance for choosing appropriate extraction and purification methods on the basis of experimental goals.",1996,24,2709,236,1,11,24,36,32,48,40,75,73,105
875a4cb721e48f0208b0d30c6f40fb85eed5d338,"The nature and properties of soils , The nature and properties of soils , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1952,5,5295,62,1,1,2,1,1,0,4,3,3,3
491811b26f736b116e079166fe8bbec865fcf5d1,"Many physical, chemical, mineralogical, and biological soil properties can be affected by forest fires. The effects are chiefly a result of burn severity, which consists of peak temperatures and duration of the fire. Climate, vegetation, and topography of the burnt area control the resilience of the soil system; some fire-induced changes can even be permanent. Low to moderate severity fires, such as most of those prescribed in forest management, promote renovation of the dominant vegetation through elimination of undesired species and transient increase of pH and available nutrients. No irreversible ecosystem change occurs, but the enhancement of hydrophobicity can render the soil less able to soak up water and more prone to erosion. Severe fires, such as wildfires, generally have several negative effects on soil. They cause significant removal of organic matter, deterioration of both structure and porosity, considerable loss of nutrients through volatilisation, ash entrapment in smoke columns, leaching and erosion, and marked alteration of both quantity and specific composition of microbial and soil-dwelling invertebrate communities. However, despite common perceptions, if plants succeed in promptly recolonising the burnt area, the pre-fire level of most properties can be recovered and even enhanced. This work is a review of the up-to-date literature dealing with changes imposed by fires on properties of forest soils. Ecological implications of these changes are described.",2005,127,1968,244,5,22,41,52,63,72,111,107,132,148
a6d59382b4f1f32c7f7d67140233d62c037885a4,"Abstract. Rapid turnover of organic matter leads to a low efficiency of organic fertilizers applied to increase and sequester C in soils of the humid tropics. Charcoal was reported to be responsible for high soil organic matter contents and soil fertility of anthropogenic soils (Terra Preta) found in central Amazonia. Therefore, we reviewed the available information about the physical and chemical properties of charcoal as affected by different combustion procedures, and the effects of its application in agricultural fields on nutrient retention and crop production. Higher nutrient retention and nutrient availability were found after charcoal additions to soil, related to higher exchange capacity, surface area and direct nutrient additions. Higher charring temperatures generally improved exchange properties and surface area of the charcoal. Additionally, charcoal is relatively recalcitrant and can therefore be used as a long-term sink for atmospheric CO2. Several aspects of a charcoal management system remain unclear, such as the role of microorganisms in oxidizing charcoal surfaces and releasing nutrients and the possibilities to improve charcoal properties during production under field conditions. Several research needs were identified, such as field testing of charcoal production in tropical agroecosystems, the investigation of surface properties of the carbonized materials in the soil environment, and the evaluation of the agronomic and economic effectiveness of soil management with charcoal.",2002,103,2298,191,0,9,9,15,21,27,27,50,85,104
787cd29e9b984f5ce7978705bf02f4a57c4419b6,1. The Chemical Composition of Soils 2. Soil Minerals 3. Soil Humus 4. The Soil Solution 5. Mineral Stability and Weathering 6. Oxidation-Reduction Reactions 7. Soil Particle Surface Charge 8. Soil Adsorption Phenomena 9. Exchangeable Ions 10. Colloidal Phenomena 11. Soil Acidity 12. Soil Salinity,2008,0,2194,161,99,93,108,113,110,129,123,138,127,120
dffa1dcb0ae3868bc3165195b6ba9384dd18ead0,"Ammonia oxidation is the first step in nitrification, a key process in the global nitrogen cycle that results in the formation of nitrate through microbial activity. The increase in nitrate availability in soils is important for plant nutrition, but it also has considerable impact on groundwater pollution owing to leaching. Here we show that archaeal ammonia oxidizers are more abundant in soils than their well-known bacterial counterparts. We investigated the abundance of the gene encoding a subunit of the key enzyme ammonia monooxygenase (amoA) in 12 pristine and agricultural soils of three climatic zones. amoA gene copies of Crenarchaeota (Archaea) were up to 3,000-fold more abundant than bacterial amoA genes. High amounts of crenarchaeota-specific lipids, including crenarchaeol, correlated with the abundance of archaeal amoA gene copies. Furthermore, reverse transcription quantitative PCR studies and complementary DNA analysis using novel cloning-independent pyrosequencing technology demonstrated the activity of the archaea in situ and supported the numerical dominance of archaeal over bacterial ammonia oxidizers. Our results indicate that crenarchaeota may be the most abundant ammonia-oxidizing organisms in soil ecosystems on Earth.",2006,31,2105,171,9,54,104,118,144,170,161,198,171,159
22edc4e85055faba4573f6eaa5e5247a668a6a48,"Following disastrous earthquakes in Alaska and in Niigata, Japan in 1964, Professors H. B. Seed and I. M. Idriss developed and published a methodology termed the ''simplified procedure'' for evaluating liquefaction resistance of soils. This procedure has become a standard of practice throughout North America and much of the world. The methodology which is largely empirical, has evolved over years, primarily through summary papers by H. B. Seed and his colleagues. No general review or update of the procedure has occurred, however, since 1985, the time of the last major paper by Professor Seed and a report from a National Research Council workshop on liquefaction of soils. In 1996 a workshop sponsored by the National Center for Earthquake Engineering Research (NCEER) was convened by Professors T. L. Youd and I. M. Idriss with 20 experts to review developments over the previous 10 years. The purpose was to gain consensus on updates and augmen- tations to the simplified procedure. The following topics were reviewed and recommendations developed: (1) criteria based on standard penetration tests; (2) criteria based on cone penetration tests; (3) criteria based on shear-wave velocity measurements; (4) use of the Becker penetration test for gravelly soil; (4) magnitude scaling factors; (5) correction factors for overburden pressures and sloping ground; and (6) input values for earthquake magnitude and peak acceleration. Probabilistic and seismic energy analyses were reviewed but no recommen- dations were formulated.",2001,65,1926,184,2,28,26,53,52,88,69,111,68,86
deb86b1cce80e72fcc163bafd0da980c936d2012,"The Paper presents a constitutive model for describing the stress-strain behaviour of partially saturated soils. The model is formulated within the framework of hardening plasticity using two independent sets of stress variables: the excess of total stress over air pressure and the suction. The model is able to represent, in a consistent and unified manner, many of the fundamental features of the behaviour of partially saturated soils which had been treated separately by previously proposed models. On reaching saturation, the model becomes a conventional critical state model. Because experimental evidence is still limited, the model has been kept as simple as possible in order to provide a basic framework from which extensions are possible. The model is intended for partially saturated soils which are slightly or moderately expansive. After formulating the model for isotropic and triaxial stress states, typical predictions are described and compared, in a qualitative way, with characteristic trends of the...",1990,52,2059,214,0,2,1,3,3,9,7,10,17,13
81ff9d3d3e0691e05708cfd13a2b9606f92671e0,"Dissolved organic matter (DOM) in soils plays an important role in the biogeochemistry of carbon, nitrogen, and phosphorus, in pedogenesis, and in the transport of pollutants in soils. The aim of this review is to summarize the recent literature about controls on DOM concentrations and fluxes in soi",2000,189,1962,183,1,10,18,51,51,55,69,70,83,88
7d6408373d574f25320a461d397c5178064ff94f,"Summary 
Mechanisms for C stabilization in soils have received much interest recently due to their relevance in the global C cycle. Here we review the mechanisms that are currently, but often contradictorily or inconsistently, considered to contribute to organic matter (OM) protection against decomposition in temperate soils: (i) selective preservation due to recalcitrance of OM, including plant litter, rhizodeposits, microbial products, humic polymers, and charred OM; (ii) spatial inaccessibility of OM against decomposer organisms due to occlusion, intercalation, hydrophobicity and encapsulation; and (iii) stabilization by interaction with mineral surfaces (Fe-, Al-, Mn-oxides, phyllosilicates) and metal ions. Our goal is to assess the relevance of these mechanisms to the formation of soil OM during different stages of decomposition and under different soil conditions. The view that OM stabilization is dominated by the selective preservation of recalcitrant organic components that accumulate in proportion to their chemical properties can no longer be accepted. In contrast, our analysis of mechanisms shows that: (i) the soil biotic community is able to disintegrate any OM of natural origin; (ii) molecular recalcitrance of OM is relative, rather than absolute; (iii) recalcitrance is only important during early decomposition and in active surface soils; while (iv) during late decomposition and in the subsoil, the relevance of spatial inaccessibility and organo-mineral interactions for SOM stabilization increases. We conclude that major difficulties in the understanding and prediction of SOM dynamics originate from the simultaneous operation of several mechanisms. We discuss knowledge gaps and promising directions of future research.",2006,173,2041,90,10,21,55,76,77,80,109,138,172,185
69ff421246a51fb9d8f3d10a8227221ed532960a,"Methods of analysis for soils plants and waters , Methods of analysis for soils plants and waters , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,2254,168,17,19,21,20,33,16,16,19,25,13
ed7f45fc78cfd694ed285e17590058c6c7aa2e62,"Publisher Summary This chapter discusses the chemistry of submerged soils. The chemical changes in the submerged materials influence: (a) the character of the sediment or soil that forms, (b) the suitability of wet soils for crops, (c) the distribution of plant species around lakes and streams and in estuaries, deltas, and marine flood plains, (d) the quality and quantity of aquatic life, and (e) the capacity of lakes and seas to serve as sinks for terrestrial wastes. The single electrochemical property that serves to distinguish a submerged soil from a well-drained soil is its redox potential. The redox potential of a soil or sediment provides a quick, useful, semiquantitative measure of its oxidation–reduction status. Two recent developments have stimulated interest in the chemistry of submerged soils: the breeding of lowland rice varieties, with a high yield potential, and the pollution of streams, lakes, and seas, by domestic, agricultural, and industrial wastes. The chemistry of submerged soils is valuable: (a) in understanding the soil problems, limiting the performance of high-yielding rice varieties, and (b) in assessing the role of lake, estuarine, and ocean sediments as reservoirs of nutrients for aquatic plants and as sinks for terrestrial wastes.",1972,228,2547,149,0,1,5,5,14,8,14,15,23,25
03dd37fab5a385901b9b1fdf3546ad22ab7372d7,"Black Carbon (BC) may significantly affect nutrient retention and play a key role in a wide range of biogeochemical processes in soils, especially for nutrient cycling. Anthrosols from the Brazilian Amazon (ages between 600 and 8700 yr BP) with high contents of biomassderived BC had greater potential cation exchange capacity (CEC measured at pH 7) per unit organic C than adjacent soils with low BC contents.Synchrotron-based near edge X-ray absorption fine structure (NEXAFS) spectroscopy coupled with scanning transmission X-ray microscopy (STXM) techniques explained the source of the higher surface charge of BC compared with non-BC by mapping crosssectional areas of BC particles with diameters of 10 to 50 mm for C forms. The largest cross-sectional areas consisted of highly aromatic or only slightly oxidized organic C most likely originating from the BC itself with a characteristic peak at 286.1 eV, which could not be found in humic substance extracts, bacteria or fungi. Oxidation significantly increased from the core of BC particles to their surfaces as shown by the ratio of carboxyl-C/aromatic-C. Spotted and non-continuous distribution patterns of highly oxidized C functional groups with distinctly different chemical signatures on BC particle surfaces (peak shift at 286.1 eV to a higher energy of 286.7 eV) indicated that non-BC may be adsorbed on the surfaces of BC particles creating highly oxidized surface. As a consequence of both oxidation of the BC particles themselves and adsorption of organic matter to BC surfaces, the charge density (potential CEC per unit surface area) was greater in BC-rich Anthrosols than adjacent soils. Additionally, a high specific surface area was attributable to the presence of BC, which may contribute to the high CEC found in soils that are rich in BC.",2006,79,1781,117,1,10,21,33,63,97,103,118,165,141
dd2599bfca43ca735635b1d7acd901f164dd7462,"This paper reviews the importance of large continuous openings (macropores) on water flow in soils. The presence of macropores may lead to spatial concentrations of water flow through unsaturated soil that will not be described well by a Darcy approach to flow through porous media. This has important implications for the rapid movement of solutes and pollutants through soils. Difficulties in defining what constitutes a macropore and the limitations of current nomenclature are reviewed. The influence of macropores on infiltration and subsurface storm flow is discussed on the basis of both experimental evidence and theoretical studies. The limitations of models that treat macropores and matrix porosity as separate flow domains is stressed. Little-understood areas are discussed as promising lines for future research. In particular, there is a need for a coherent theory of flow through structured soils that would make the macropore domain concept redundant.",1982,146,2429,111,2,3,12,14,20,19,30,34,47,39
953466d63062ba223e7c3ae797789abdecf0f61d,"Consumption of food crops contaminated with heavy metals is a major food chain route for human exposure. We studied the health risks of heavy metals in contaminated food crops irrigated with wastewater. Results indicate that there is a substantial buildup of heavy metals in wastewater-irrigated soils, collected from Beijing, China. Heavy metal concentrations in plants grown in wastewater-irrigated soils were significantly higher (P<or=0.001) than in plants grown in the reference soil, and exceeded the permissible limits set by the State Environmental Protection Administration (SEPA) in China and the World Health Organization (WHO). Furthermore, this study highlights that both adults and children consuming food crops grown in wastewater-irrigated soils ingest significant amount of the metals studied. However, health risk index values of less than 1 indicate a relative absence of health risks associated with the ingestion of contaminated vegetables.",2008,30,1768,104,3,17,29,51,52,87,127,160,175,200
acfbec4f4f335cf6ec5ae3c8f12b9ae8a7aac988,"The percentage of soil pore space filled with water (percent water-filled pores, % WFP), as determined by water content and total porosity, appears to be closely related to soil microbial activity under different tillage regimes. Soil incubated in the laboratory at 60% WFP supported maximum aerobic microbial activity as determined by CO production and O uptake. In the field, % WFP of surface no-tillage soils (0–75 mm) at four U.S. locations averaged 62% at time of sampling, whereas that for plowed soils was 44%. This difference in % WFP was reflected in 3.4 and 9.4 times greater CO and NO production, respectively, from surface no-tillage soils over a 24-h period as compared to plowed soils. At a depth of 75 to 150 mm, % WFP values increased in both no-tillage and plowed soils, averaging approximately 70% for no tillage compared with 50 to 60% for plowed soils. Production of CO in the plowed soils was enhanced by the increased % WFP, resulting in little or no difference in CO production between tillage treatments. Nitrous oxide production, however, remained greater under no-tillage conditions. Substantially greater amounts of NO were produced from the N-fertilized soils, regardless of tillage practice. Production of CO and NO was primarily related to the % WFP of tillage treatments although, in several instances, soil-water-soluble C and NO levels were important as well. Calculations of relative aerobic microbial activity between no-tillage and plowed soils, based on differences in % WFP relative to maximum activity at 60%, indicated linear relationships for CO and NO production between WFP values of 30 to 70%. Below 60% WFP, water limits microbial activity, but above 60%, aerobic microbial activity decreases—apparently the result of reduced aeration.",1984,0,2030,155,0,0,5,2,7,11,6,9,9,10
407fb40e2516b60a73c3e4de0c9c31a463781303,"GENERAL PRINCIPLES. Soil Processes and the Behavior of Metals (B. Alloway). The Origins of Heavy Metals in Soils (B. Alloway). Methods of Analysis for Heavy Metals in Soils (A. Ure). INDIVIDUAL ELEMENTS. Arsenic (P. O'Neill). Cadmium (B. Alloway). Chromium and Nickel (S. McGrath & S. Smith). Copper (D. Baker). Lead (B. Davies). Manganese and Cobalt (K. Smith). Mercury (E. Steinnes). Selenium (R. Neal). Zinc (L. Kiekens). Other Metals and Metalloids (K. Jones, et al.). Appendices. Index.",1990,0,2328,58,0,4,8,10,9,17,29,32,36,44
dfe78c310e3630930aa812fcced5727f07dd57bf,"The oxidation potential of dithionite (Na 2 S 2 O 4 ) increases from 0.37 V to 0.73 V with increase in pH from 6 to 9, because hydroxyl is consumed during oxidation of dithionite. At the same time the amount of iron oxide dissolved in 15 minutes falls off (from 100 percent to less than 1 percent extracted) with increase in pH from 6 to 12 owing to solubility product relationships of iron oxides. An optimum pH for maximum reaction kinetics occurs at approximately pH 7.3. A buffer is needed to hold the pH at the optimum level because 4 moles of OH are used up in reaction with each mole of Na 2 S 2 O 4 oxidized. Tests show that NaHCO 3 effectively serves as a buffer in this application. Crystalline hematite dissolved in amounts of several hundred milligrams in 2 min. Crystalline geothite dissolved more slowly, but dissolved during the two or three 15 min treatments normally given for iron oxide removal from soils and clays. A series of methods for the extraction of iron oxides from soils and clays was tested with soils high in free iron oxides and with nontronite and other iron-bearing clays. It was found that the bicarbonate-buffered Na 2 S 2 O 4 -citrate system was the most effective in removal of free iron oxides from latosolic soils, and the least destructive of iron silicate clays as indicated by least loss in cation exchange capacity after the iron oxide removal treatment. With soils the decrease was very little but with the very susceptible Woody district nontronite, the decrease was aboout 17 percent as contrasted to 35-80 percent with other methods.",1960,12,2359,138,0,0,0,1,1,2,1,4,2,4
6e8bcf5b3dab5f62e9b452560c8ed3c260837ff6,"The article focuses on adsorption of heavy metal ions on soils and soils constituents such as clay minerals, metal (hydr)oxides, and soil organic matter. Empirical and mechanistic model approaches for heavy metal adsorption and parameter determination in such models have been reviewed. Sorption mechanisms in soils, the influence of surface functional groups and surface complexation as well as parameters influencing adsorption are discussed. The individual adsorption behavior of Cd, Cr, Pb, Cu, Mn, Zn and Co on soils and soil constituents is reviewed.",2004,153,1490,102,2,16,22,40,40,58,62,70,67,95
951324d6a74f63b2aa347dbff53a3fc9c03be901,"Abstract An increasing body of evidence suggests that microorganisms are far more sensitive to heavy metal stress than soil animals or plants growing on the same soils. Not surprisingly, most studies of heavy metal toxicity to soil microorganisms have concentrated on effects where loss of microbial function can be observed and yet such studies may mask underlying effects on biodiversity within microbial populations and communities. The types of evidence which are available for determining critical metal concentrations or loadings for microbial processes and populations in agricultural soil are assessed, particularly in relation to the agricultural use of sewage sludge. Much of the confusion in deriving critical toxic concentrations of heavy metals in soils arises from comparison of experimental results based on short-term laboratory ecotoxicological studies with results from monitoring of long-term exposures of microbial populations to heavy metals in field experiments. The laboratory studies in effect measure responses to immediate, acute toxicity (disturbance) whereas the monitoring of field experiments measures responses to long-term chronic toxicity (stress) which accumulates gradually. Laboratory ecotoxicological studies are the most easily conducted and by far the most numerous, but are difficult to extrapolate meaningfully to toxic effects likely to occur in the field. Using evidence primarily derived from long-term field experiments, a hypothesis is formulated to explain how microorganisms may become affected by gradually increasing soil metal concentrations and this is discussed in relation to defining “safe” or “critical” soil metal loadings for soil protection.",1998,175,1747,85,0,11,23,35,39,44,44,64,85,69
c94b1551d5d90d8d26412216974e3b0a2ce48281,,1954,0,2124,285,1,1,0,0,3,4,0,1,1,0
51f2add629f5db184c076840bc99739a5bfe9909,"Everyone who grows plants, whether a single geranium in a flower pot or hundreds of acres of corn or cotton, is aware of the importance of water for successful growth. Water supply not only affects the yield of gardens and field crops, but also controls the distribution of plants over the earth's surface, ranging from deserts and grasslands to rain forests, depending on the amount and seasonal distribution of precipitation. However, few people understand 'fully why water is so important for plant growth. This book attempts to explain its importance by showing how water affects the physiological processes that control the quantity and quality of growth. It is a useful introduction for students, teachers, and investigators in both basic and applied plant science, including botanists, crop scientists, foresters, horticulturists, soil scientists, and even gardeners and farmers who desire a better understanding of how their plants grow. An attempt has been made to present the information in terms intelligible to readers with various backgrounds. If the treatment of some topics seems inadequate to specialists in certain fields, they are reminded that the book was not written for specialists, but as an introduction to the broad field of plant water relations. As an aid in this respect, a laboratory manual is available with detailed instructions for some of the more complex methods (J. S. Boyer in ""Measuring the Water Status of Plants and Soils,"" Academic Press, San Diego, 1995).",1995,1,1892,111,2,9,26,24,44,45,50,60,58,69
6841dcd0ece52ad28cc1498dcb5398dd444445c2,"Acid soils significantly limit crop production worldwide because approximately 50% of the world's potentially arable soils are acidic. Because acid soils are such an important constraint to agriculture, understanding the mechanisms and genes conferring tolerance to acid soil stress has been a focus of intense research interest over the past decade. The primary limitations on acid soils are toxic levels of aluminum (Al) and manganese (Mn), as well as suboptimal levels of phosphorous (P). This review examines our current understanding of the physiological, genetic, and molecular basis for crop Al tolerance, as well as reviews the emerging area of P efficiency, which involves the genetically based ability of some crop genotypes to tolerate P deficiency stress on acid soils. These are interesting times for this field because researchers are on the verge of identifying some of the genes that confer Al tolerance in crop plants; these discoveries will open up new avenues of molecular/physiological inquiry that should greatly advance our understanding of these tolerance mechanisms. Additionally, these breakthroughs will provide new molecular resources for improving crop Al tolerance via both molecular-assisted breeding and biotechnology.",2004,172,1394,123,1,30,46,51,54,61,73,74,83,111
cc7fb0b67a1e770e07955892ab8194ddc4e222ac,"This work corroborates the hierarchical conceptual model for soil aggregate structure presented by Tisdall and Oades (1982), extends it to North American grassland soils, and elaborates on the aspects relating to the influence of cultivation upon losses of soil organic matter. Aggregate size distributions observed for our soils are very similar to those of Australian soils, indicating that the microaggregate-macroaggregate model may hold for a wide array of grassland soils from around the world. The use of two wetting treatments prior to sieving demonstrated that the native sod soil had the same general structural characteristics as cultivated soil but the macroaggregates were more stable. When slaked, native and cultivated soil present in the microaggregate size classes had less organic C, N, and P than the soil remaining as macroaggregates, even when expressed on a sand-free basis. Moreover, the C/N, C/P, and N/P ratios of microaggregates were narrower than those of macroaggregate size classes. Much more C and N was lost than P under the conditions of this study. Element-specific differences in microbial catabolic processes vs. extracellular enzyme activity and its attendant feedback controls are postulated to account for this difference. When the macroaggregates were crushed to the size of microaggregates, mineralizable C as a percent of the total organic C was generally greater for macro- than microaggregates early in the incubation for the cultivated soil and throughout most of the incubation for the native sod soil. Mineralizable N as a percent of the total organic N was greatest in macroaggregates even when the macroaggregates were not crushed. The macroaggregate-micraggregate conceptual model is applied to help explain accumulation of soil organic matter under native conditions and its loss upon cultivation.",1986,0,1811,129,0,3,5,6,11,8,4,13,10,23
1a67ab9f0d9a347799abe52490c3047b6e1a2cd7,"The surface chemistry of soils , The surface chemistry of soils , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1984,0,1868,135,0,5,7,10,12,22,35,39,32,32
5d5cb5a54e39b37831450df4bb6c6dac89f490c2,"Abstract Methane emission by soils results from antagonistic but correlated microbial activities. Methane is produced in the anaerobic zones of submerged soils by methanogens and is oxidised into CO2 by methanotrophs in the aerobic zones of wetland soils and in upland soils. Methanogens and methanotrophs are ubiquitous in soils where they remain viable under unfavourable conditions. Methane transfer from the soil to the atmosphere occurs mostly through the aerenchyma of aquatic plants, but also by diffusion and as bubbles escaping from wetland soils. Methane sources are mainly wetlands. However 60 to more than 90 % of CH4 produced in the anaerobic zones of wetlands is reoxidised in their aerobic zones (rhizosphere and oxidised soil-water interface). Methane consumption occurs in most soils and exhibits a broad range of values. Highest consumption rates or potentials are observed in soils where methanogenesis is or has been effective and where CH4 concentration is or has been much higher than in the atmosphere (ricefields, swamps, landfills, etc.). Aerobic soils consume atmospheric CH4 but their activities are very low and the micro-organisms involved are largely unknown. Methane emissions by cultivated or natural wetlands are expressed in mg CH4·m–2·h–1 with a median lower than 10 mg CH4·m–2·h–1. Methanotrophy in wetlands is most often expressed with the same unit. Methane oxidation by aerobic upland soils is rarely higher than 0.1 mg CH4·m–2·h–1. Forest soils are the most active, followed by grasslands and cultivated soils. Factors that favour CH4 emission from cultivated wetlands are mostly submersion and organic matter addition. Intermittent drainage and utilisation of the sulphate forms of N-fertilisers reduce CH4 emission. Methane oxidation potential of upland soils is reduced by cultivation, especially by ammonium N-fertiliser application.",2001,273,1533,95,0,5,13,13,22,27,26,47,48,77
2633282f6235c29b4d1372812fa5bdf086a1cb23,"A method is described for the rapid and objective estimation of the amount of carbon in the living, non-resting microbial biomass of soils. The method, which is based on the initial respiratory response of microbial populations to amendment with an excess of a carbon and energy source, was quantified using an expanded version of Jenkinson's technique. 
 
The simultaneous application of the two methods to 50 soil samples showed a highly significant correlation (r = 0.96) between both. From this correlation it could be deduced that at 22°C, a substrate-induced maximal respiratory rate of 1 ml CO2· h−1 corresponds to c. 40 mg microbial biomass C. Evidence supporting these results was obtained from pure culture studies. The various soil types investigated were collected from agricultural as well as forest sites and they contained between 15 and 240 mg microbial C·100g dry soil−1. The respiratory method provides reproducible estimates of biomass size within 1–3 h after soil amendment. It can be combined without difficulty with a selective inhibition method for determination of bacterial and fungal contributions to soil metabolism.",1978,21,2045,100,0,5,6,9,13,6,16,15,14,18
8cd4a09f1e4b725f6781a81c4f5fa35599e0668d,,1993,0,1645,40,0,1,3,6,9,9,13,16,25,11
b9fbab019d1e01db32f3a8545cd1790773df7dd6,,1972,0,1755,194,0,1,4,3,4,5,5,1,7,10
bc56504f48727d4c7fde58e23d12cebd3e4c64ab,"Soils and Geomorphology, now in its third edition, remains popular among soil scientists, geomorphologists, geologists, geographers, and archeologists. While retaining the useful ""factors of soil formation format, "" it has been extensively revised, incorporating a considerable amount of new research and offering a greater number of topics and examples - particularly in the chapters ""Weathering and Soil Development with Time"" and ""Topography: Soil Relations with Time in Different Climatic Settings."" Greater emphasis is placed on the role of dust in pedogenesis, and new data are included on tropical soil development, global soil-loess relations, neotectonics, and reduction processes. Soils and Geomorphology is an ideal text for advanced undergraduate and graduate students in courses on pedology, soil science. Quaternary geology, archeology, and sedimentary petrology.",1984,0,1683,114,3,6,13,22,23,18,38,26,29,29
1dfa7a5c4f638a15432488d31bf4669e2e93b11d,"The oxidation potential of dithionite (Na2S2O4) increases from 0.37 V to 0.73 V with increase in pH from 6 to 9, because hydroxyl is consumed during oxidation of dithionite. At the same time the amount of iron oxide dissolved in 15 minutes falls off (from 100 percent to less than 1 percent extracted) with increase in pH from 6 to 12 owing to solubility product relationships of iron oxides. An optimum pH for maximum reaction kinetics occurs at approximately pH 7.3. A buffer is needed to hold the pH at the optimum level because 4 moles of OH are used up in reaction with each mole of Na2S2O4 oxidized. Tests show that NaHCO3 effectively serves as a buffer in this application. Crystalline hematite dissolved in amounts of several hundred milligrams in 2 min. Crystalline goethite dissolved more slowly, but dissolved during the two or three 15 min treatments normally given for iron oxide removal from soils and clays. A series of methods for the extraction of iron oxides from soils and clays was tested with soils high in free iron oxides and with nontronite and other iron-bearing clays. It was found that the bicarbonate-buffered Na2S2O4-citrate system was the most effective in removal of free iron oxides from latosolic soils, and the least destructive of iron silicate clays as indicated by least loss in cation exchange capacity after the iron oxide removal treatment. With soils the decrease was very little but with the very susceptible Woody district nontronite, the decrease was about 17 percent as contrasted to 35–80 percent with other methods.",1958,12,2396,11,1,1,0,0,3,1,3,3,4,4
fb47ada634cd93e4c94306698e926e7becbd94a6,"Tillage intensity affects soil structure and the loss of soil organic C and N. We hypothesized that no-tillage (NT) and conventional tillage (CT) differentially affect three physically defined particulate organic matter (POM) fractions. A grassland-derived Haplustoll was separated into aggregates by wet sieving. Free light fraction (LF) and intra-aggregate POM (iPOM) were isolated. Natural abundance 13 C was measured for whole soil C, free LF C, and iPOM C. The mean residence time of soil C under CT (44 yr) was 1.7 times less than in NT (73 yr). The amount of free LF C was 174, 196, and 474 g C m -2 for CT, NT, and NS, respectively. Total iPOM C amounts in CT, NT, and NS were 193, 337, and 503 g C m -2 , respectively. The level of fine iPOM C (53-250 μm) level in macroaggregates (250-2000 μm) obtained after slaking was five times greater in NT vs. CT and accounted for 47.3% of the difference in total POM C between NT and CT. The amount of coarse iPOM C (250-2000 μm) was only 2.4 times greater and accounted for only 21% of the difference in total POM C. Sequestration of iPOM was observed in NT vs. CT, but free LF was not influenced by differential tillage. We conclude that differences in aggregate turnover largely control the difference in fine iPOM in CT vs. NT and consequently SOM loss is affected by both the amount of aggregation and aggregate turnover.",1998,2,1340,97,0,3,12,16,21,16,33,33,62,53
34a4f048e2a0ed58108273922019b82df7017d33,"In the next decades, many soils will be subjected to increased drying/wetting cycles or modified water availability considering predicted global changes in precipitation and evapotranspiration. These changes may affect the turnover of C and N in soils, but the direction of changes is still unclear. The aim of the review is the evaluation of involved mechanisms, the intensity, duration and frequency of drying and wetting for the mineralization and fluxes of C and N in terrestrial soils. Controversial study results require a reappraisal of the present understanding that wetting of dry soils induces significant losses of soil C and N. The generally observed pulse in net C and N mineralization following wetting of dry soil (hereafter wetting pulse) is short-lived and often exceeds the mineralization rate of a respective moist control. Accumulated microbial and plant necromass, lysis of live microbial cells, release of compatible solutes and exposure of previously protected organic matter may explain the additional mineralization during wetting of soils. Frequent drying and wetting diminishes the wetting pulse due to limitation of the accessible organic matter pool. Despite wetting pulses, cumulative C and N mineralization (defined here as total net mineralization during drying and wetting) are mostly smaller compared with soil with optimum moisture, indicating that wetting pulses cannot compensate for small mineralization rates during drought periods. Cumulative mineralization is linked to the intensity and duration of drying, the amount and distribution of precipitation, temperature, hydrophobicity and the accessible pool of organic substrates. Wetting pulses may have a significant impact on C and N mineralization or flux rates in arid and semiarid regions but have less impact in humid and subhumid regions on annual time scales. Organic matter stocks are progressively preserved with increasing duration and intensity of drought periods; however, fires enhance the risk of organic matter losses under dry conditions. Hydrophobicity of organic surfaces is an important mechanism that reduces C and N mineralization in topsoils after precipitation. Hence, mineralization in forest soils with hydrophobic organic horizons is presumably stronger limited than in grassland or farmland soils. Even in humid regions, suboptimal water potentials often restrict microbial activity in topsoils during growing seasons. Increasing summer droughts will likely reduce the mineralization and fluxes of C and N whereas increasing summer precipitation could enhance the losses of C and N from soils.",2009,109,848,57,7,41,43,62,60,66,62,82,98,91
1d847d795ed5bc2941e2f023275f7fa7844834ef,"* Soil fungi play a major role in ecological and biogeochemical processes in forests. Little is known, however, about the structure and richness of different fungal communities and the distribution of functional ecological groups (pathogens, saprobes and symbionts). * Here, we assessed the fungal diversity in six different forest soils using tag-encoded 454 pyrosequencing of the nuclear ribosomal internal transcribed spacer-1 (ITS-1). No less than 166 350 ITS reads were obtained from all samples. In each forest soil sample (4 g), approximately 30 000 reads were recovered, corresponding to around 1000 molecular operational taxonomic units. * Most operational taxonomic units (81%) belonged to the Dikarya subkingdom (Ascomycota and Basidiomycota). Richness, abundance and taxonomic analyses identified the Agaricomycetes as the dominant fungal class. The ITS-1 sequences (73%) analysed corresponded to only 26 taxa. The most abundant operational taxonomic units showed the highest sequence similarity to Ceratobasidium sp., Cryptococcus podzolicus, Lactarius sp. and Scleroderma sp. * This study validates the effectiveness of high-throughput 454 sequencing technology for the survey of soil fungal diversity. The large proportion of unidentified sequences, however, calls for curated sequence databases. The use of pyrosequencing on soil samples will accelerate the study of the spatiotemporal dynamics of fungal communities in forest ecosystems.",2009,80,813,27,1,47,55,71,93,76,88,82,55,66
456ec824f30047dab257b2eafda1531382a09b76,"This paper reviews the factors affecting trace metal behaviour in estuarine and riverine floodplain soils and sediments. Spatial occurrence of processes affecting metal mobility and availability in floodplains are largely determined by the topography. At the oxic-anoxic interface and in the anoxic layers of floodplain soils, especially redox-sensitive processes occur, which mainly result in the inclusion of metals in precipitates or the dissolution of metal-containing precipitates. Kinetics of these processes are of great importance for these soils as the location of the oxic-anoxic interface is subject to change due to fluctuating water table levels. Other important processes and factors affecting metal mobility in floodplain soils are adsorption/desorption processes, salinity, the presence of organic matter, sulphur and carbonates, pH and plant growth. Many authors report highly significant correlations between cation exchange capacity, clay or organic matter contents and metal contents in floodplain soils. Iron and manganese (hydr)oxides were found to be the main carriers for Cd, Zn and Ni under oxic conditions, whereas the organic fraction was most important for Cu. The mobility and availability of metals in a floodplain soil can be significantly reduced by the formation of metal sulphide precipitates under anoxic conditions. Ascending salinity in the flood water promotes metal desorption from the floodplain soil in the absence of sulphides, hence increases total metal concentrations in the water column. The net effect of the presence of organic matter can either be a decrease or an increase in metal mobility, whereas the presence of carbonates in calcareous floodplain soils or sediments constitutes an effective buffer against a pH decrease. Moreover, carbonates may also directly precipitate metals. Plants can affect the metal mobility in floodplain soils by oxidising their rhizosphere, taking up metals, excreting exudates and stimulating the activity of microbial symbionts in the rhizosphere.",2009,140,841,24,12,30,37,44,61,76,69,87,80,87
0a6740b013faad421dee8c298308c4285401c6ca,"This report describes the RETC computer code for analyzing the soil water retention and hydraulic conductivity functions of unsaturated soils. These hydraulic properties are key parameters in any quantitative description of water flow into and through the unsaturated zone of soils. The program uses the parametric models of Brooks-Corey and van Genuchten to represent the soil water retention curve, and the theoretical pore-size distribution models of Mualem and Burdine to predict the unsaturated hydraulic conductivity function from observed soil water retention data. The report gives a detailed discussion of the different analytical expressions used for quantifying the soil water retention and hydraulic conductivity functions. A brief review is also given of the nonlinear least-squares parameter optimization method used for estimating the unknown coefficients in the hydraulic models. Several examples are presented to illustrate a variety of program options. The program may be used to predict the hydraulic conductivity from observed soil water retention data assuming that one observed conductivity value (not necessarily at saturation) is available. The program also permits one to fit analytical functions simultaneously to observed water retention and hydraulic conductivity data. The report serves as both a user manual and reference document. Detailed information is given on the computer program along with instructions for data input preparation and sample input and output files. A listing of the source code is also provided.",1992,48,1470,93,1,2,9,13,18,19,23,19,25,33
c13860422445582992a23fdef379b0663d4ac066,"A simple, practical procedure for representing the nonlinear, stress-dependent, inelastic stress-strain behavior of soils was developed. The values of the required parameters employed in the stress-strain relationship may be derived from the results of standard triaxial tests on plane strain compression tests involving primary loading, unloading, and reloading. Comparisons of calculated and measured strains in specimens of dense and loose silica sand showed that the relationship was capable of accurately representing the behavior of this sand under complex triaxial loading conditions, and analyses of the behavior of footings on sand and clay showed that finite element stress analyses conducted using this relationship were in good agreement with empirical observations and applicable theories.",1970,0,1754,113,2,0,1,0,4,6,3,4,5,7
731ec456aea2958bb13c1bb72f21b42a484ed6f4,"The oxidation of ammonia to nitrate, known as nitrification, is a key process in the nitrogen cycle. Real-time polymerase chain-reaction measurements show that nitrification is driven by bacteria rather than archaea in nitrogen-rich grassland soils in New Zealand. The oxidation of ammonia to nitrate, nitrification, is a key process in the nitrogen cycle. Ammonia-oxidizing archaea are present in large numbers in the ocean1,2,3 and soils4,5,6, suggesting a potential role for archaea, in addition to bacteria, in the global nitrogen cycle7,8. However, the importance of archaea to nitrification in agricultural soils is not well understood4. Here, we examine the contribution of ammonia-oxidizing archaea and bacteria to nitrification in six grassland soils in New Zealand using a quantitative polymerase chain reaction. We show that although ammonia-oxidizing archaea are present in large numbers in these soils, neither their abundance nor their activity increased with the application of an ammonia substrate, suggesting that their abundance was not related to the rate of nitrification. In contrast, the number of ammonia-oxidizing bacteria increased 3.2–10.4-fold and their activity increased 177-fold, in response to ammonia additions. Indeed, we find a significant relationship between the abundance of ammonia-oxidizing bacteria and the rate of nitrification. We suggest that nitrification is driven by bacteria rather than archaea in these nitrogen-rich grassland soils.",2009,21,693,40,3,21,53,49,57,63,60,67,67,67
023735e8dd24147dfc7d154457541489d5770440,A CRITICAL EXAMINATION OF A RAPID METHOD FOR DETERMINING ORGANIC CARBON IN SOILS—EFFECT OF VARIATIONS IN DIGESTION CONDITIONS AND OF INORGANIC SOIL CONSTITUENTS ALLAN WALKLEY; Soil Science,1947,0,2130,91,0,0,0,0,0,1,0,1,4,0
88d69927068fa9bd09b912f3dce2508ab869a00e,"Iron and aluminum were determined in acid ammonium oxalate extracts and in dithionite–citrate–bicarbonate extracts of a wide range of Canadian soils, several oxide and silicate minerals, and some amorphous preparations of iron or aluminum and silica. The oxalate extraction dissolved much of the iron and aluminum from the amorphous materials but very little from crystalline oxides, whereas the dithionite extraction dissolved a large proportion of the crystalline iron oxides as well as much of the amorphous materials. Oxalate-extractable iron and aluminum gave a useful indication of Bf horizon development in many soils, even if the parent materials were high in iron oxides. In one class of Gleysolic soils, however, the Bfg horizons were high in dithionite-extractable iron and low in oxalate-extractable iron. An accumulation of goethite was found in the Bfg horizon of some of these soils. In some other Gleysolic soils iron was depleted in the A horizon but there was no horizon of iron accumulation. Extractio...",1966,2,1788,125,0,1,6,6,3,7,7,8,5,4
e3a3d0e56552b17c719c5a50dabc10ae471bf821,"Stochastic modeling of soil water fluxes in the absence of measured hydraulic parameters requires a knowledge of the expected distribution of the hydraulic parameters in different soil types. Predictive relationships describing the hydraulic parameter distributions must be developed based on the common descriptors of the physical properties of soils (e.g., texture, structure, particle size distribution). Covariation among the hydraulic parameters within these relationships must be identified. Data for 1448 soil samples were examined in an evaluation of the usefulness of qualitative descriptors as predictors of soil hydraulic behavior. Analysis of variance and multiple linear regression techniques were used to derive quantitative expressions for the moments of the hydraulic parameters as functions of the particle size distributions (percent sand, silt, and clay content) of soils. Discriminant analysis suggests that the covariation of the hydraulic parameters can be used to construct a classification scheme based on the hydraulic behavior of soils that is analogous to the textural classification scheme based on the sand, silt, and clay content of soils.",1984,14,1375,141,0,1,2,4,0,7,5,12,6,5
018ea63975eb722527bc9bf203b8cc3b55e06e8c,"Veterinary pharmaceuticals (VPs) are used in large amounts in modern husbandry. Due to their use pattern, they possess a potential for reaching the soil environment. To assess their mobility in soil, the literature on sorption of chemicals used as VPs is reviewed and put into perspective of their physicochemical properties. The compilation of sorption coefficients to soil solids (Kd,solid) demonstrates that these chemicals display a wide range of mobility (0.2 < Kd,solid < 6,000 L/kg). Partition coefficients for association of tetracycline and quinolone carboxylic acid VPs to dissolved organic matter (Kd,DOM) vary between 100 and 50,000 L/kg. The variation in Kd,solid for a given compound in different soils can be significant. For most of the compounds, the variation is not considerably lower for the organic carbon-normalized sorption coefficient Koc. In addition, prediction of log Koc by log Kow leads to significant underestimation of log Koc and log Kd,DOM values. This suggests that mechanisms other than hydrophobic partitioning play a significant role in sorption of VPs. A number of hydrophobicity-independent mechanisms such as cation exchange, cation bridging at clay surfaces, surface complexation, and hydrogen bonding appear to be involved. These processes are not accounted for by organic carbon normalization, suggesting that this data treatment is conceptually inappropriate and fails to describe the sorption behavior. Moreover, prediction of log Koc based on the hydrophobicity parameter log Kow is not successful.",2001,0,1150,102,0,1,11,29,50,39,47,55,43,65
41a88a490d7ba9e383ecb16c4290083413a08258,"This book is a rigorous exposition of formal languages and models of computation, with an introduction to computational complexity. The authors present the theory in a concise and straightforward manner, with an eye out for the practical applications. Exercises at the end of each chapter, including some that have been solved, help readers confirm and enhance their understanding of the material. This book is appropriate for upper-level computer science undergraduates who are comfortable with mathematical arguments.",1979,1,14120,881,0,0,6,3,2,5,3,3,7,6
3c6f487f79b23dcf2149b994d2eed3fc9f29795b,"This special issue of Mathematical Structures in Computer Science contains several contributions related to the modern field of Quantum Information and Quantum Computing. The first two papers deal with entanglement. The paper by R. Mosseri and P. Ribeiro presents a detailed description of the two- and three-qubit geometry in Hilbert space, dealing with the geometry of fibrations and discrete geometry. The paper by J.-G.Luque et al. is more algebraic and considers invariants of pure k-qubit states and their application to entanglement measurement.",2007,0,15206,2014,3,3,13,65,237,907,1102,1054,1100,1035
71b2135b1298939385fb85ccd061e57789287975,"Abstract The paper reviews the problem of making numerical predictions of turbulent flow. It advocates that computational economy, range of applicability and physical realism are best served at present by turbulence models in which the magnitudes of two turbulence quantities, the turbulence kinetic energy k and its dissipation rate ϵ, are calculated from transport equations solved simultaneously with those governing the mean flow behaviour. The width of applicability of the model is demonstrated by reference to numerical computations of nine substantially different kinds of turbulent flow.",1990,27,10424,424,0,0,103,154,136,155,180,162,178,139
961e2156d523e3901c491cc2a1f65764c976fc44,"Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.",1995,55,5841,949,1,7,48,77,90,99,134,142,158,178
6012f4ad15ba4eb7888e43daf69f11041ab56dbd,"From the Publisher: 
""An IEEE reprinting of this classic 1968 edition, FIELD COMPUTATION BY MOMENT METHODS is the first book to explore the computation of electromagnetic fields by the most popular method for the numerical solution to electromagnetic field problems. It presents a unified approach to moment methods by employing the concepts of linear spaces and functional analysis. Written especially for those who have a minimal amount of experience in electromagnetic theory, this book illustrates theoretical and mathematical concepts to prepare all readers with the skills they need to apply the method of moments to new, engineering-related problems.Written especially for those who have a minimal amount of experience in electromagnetic theory, theoretical and mathematical concepts are illustrated by examples that prepare all readers with the skills they need to apply the method of moments to new, engineering-related problems.""",1968,0,6324,348,0,10,9,12,26,33,23,24,32,28
8665c9b459e4161825baf1f25b5141f41a5085ff,"The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.",1990,33,4017,330,4,24,48,55,63,72,146,149,133,140
638df1b831feb3647a9bf5496780b38890573d4d,"gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear",1989,0,5464,490,18,24,46,47,84,81,81,85,82,52
2a193b9417a4aaf35bcad9152cf35f78dc5906a9,"The tools of molecular biology were used to solve an instance of the directed Hamiltonian path problem. A small graph was encoded in molecules of DNA, and the ""operations"" of the computation were performed with standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computations at the molecular level.",1994,37,4010,272,3,36,52,77,69,95,92,105,120,133
74e92100fbe99fbe2de8243ff489ec7d53d8c3ae,"In a recent paper, Bai and Perron (1998) considered theoretical issues related to the limiting distribution of estimators and test statistics in the linear model with multiple structural changes. In this companion paper, we consider practical issues for the empirical applications of the procedures. We first address the problem of estimation of the break dates and present an efficient algorithm to obtain global minimizers of the sum of squared residuals. This algorithm is based on the principle of dynamic programming and requires at most least-squares operations of order O(T 2) for any number of breaks. Our method can be applied to both pure and partial structural-change models. Secondly, we consider the problem of forming confidence intervals for the break dates under various hypotheses about the structure of the data and the errors across segments. Third, we address the issue of testing for structural changes under very general conditions on the data and the errors. Fourth, we address the issue of estimating the number of breaks. We present simulation results pertaining to the behavior of the estimators and tests in finite samples. Finally, a few empirical applications are presented to illustrate the usefulness of the procedures. All methods discussed are implemented in a GAUSS program available upon request for non-profit academic use.",1998,26,4438,628,1,1,2,11,18,29,48,85,90,126
5a8d0d7094c356e3b851fd66bd929ed0e56aabfd,,1977,0,5328,362,16,18,16,35,37,50,41,58,58,53
2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea,"A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.<<ETX>>",1994,39,5368,192,7,33,58,57,77,65,89,93,117,115
650003ab3505441cd8e828ca26d2aa2faf24d2ff,,1988,0,4012,250,0,0,3,4,21,30,38,57,104,109
c86900fc9f326e9861adc395c7e4be93dcdb8c2c,"Recent advances in cDNA and oligonucleotide DNA arrays have made it possible to measure the abundance of mRNA transcripts for many genes simultaneously. The analysis of such experiments is nontrivial because of large data size and many levels of variation introduced at different stages of the experiments. The analysis is further complicated by the large differences that may exist among different probes used to interrogate the same gene. However, an attractive feature of high-density oligonucleotide arrays such as those produced by photolithography and inkjet technology is the standardization of chip manufacturing and hybridization process. As a result, probe-specific biases, although significant, are highly reproducible and predictable, and their adverse effect can be reduced by proper modeling and analysis methods. Here, we propose a statistical model for the probe-level data, and develop model-based estimates for gene expression indexes. We also present model-based methods for identifying and handling cross-hybridizing probes and contaminating array regions. Applications of these results will be presented elsewhere.",2001,13,3357,237,28,77,159,248,301,314,333,287,281,244
054b680165a7325569ca6e63028ca9cee7f3ac9a,"Quantum computers promise to increase greatly the efficiency of solving problems such as factoring large integers, combinatorial optimization and quantum physics simulation. One of the greatest challenges now is to implement the basic quantum-computational elements in a physical system and to demonstrate that they can be reliably and scalably controlled. One of the earliest proposals for quantum computation is based on implementing a quantum bit with two optical modes containing one photon. The proposal is appealing because of the ease with which photon interference can be observed. Until now, it suffered from the requirement for non-linear couplings between optical modes containing few photons. Here we show that efficient quantum computation is possible using only beam splitters, phase shifters, single photon sources and photo-detectors. Our methods exploit feedback from photo-detectors and are robust against errors from photon loss and detector inefficiency. The basic elements are accessible to experimental investigation with current technology.",2001,56,4188,110,23,56,121,134,174,155,185,194,180,195
a6e1b6ed82a5286baf684f538dab54c8235b05d4,We propose an implementation of a universal set of one- and two-quantum-bit gates for quantum computation using the spin states of coupled single-electron quantum dots. Desired operations are effected by the gating of the tunneling barrier between neighboring dots. Several measures of the gate quality are computed within a recently derived spin master equation incorporating decoherence caused by a prototypical magnetic environment. Dot-array experiments that would provide an initial demonstration of the desired nonequilibrium spin dynamics are proposed.,1997,33,4647,89,4,10,29,55,86,161,179,195,208,274
0b7cd3a0975a9fb228dacf1c63615c98cf07591e,"Topological quantum computation has emerged as one of the most exciting approaches to constructing a fault-tolerant quantum computer. The proposal relies on the existence of topological states of matter whose quasiparticle excitations are neither bosons nor fermions, but are particles known as non-Abelian anyons, meaning that they obey non-Abelian braiding statistics. Quantum information is stored in states with multiple quasiparticles, which have a topological degeneracy. The unitary gate operations that are necessary for quantum computation are carried out by braiding quasiparticles and then measuring the multiquasiparticle states. The fault tolerance of a topological quantum computer arises from the nonlocal encoding of the quasiparticle states, which makes them immune to errors caused by local perturbations. To date, the only such topological states thought to have been found in nature are fractional quantum Hall states, most prominently the $\ensuremath{\nu}=5∕2$ state, although several other prospective candidates have been proposed in systems as disparate as ultracold atoms in optical lattices and thin-film superconductors. In this review article, current research in this field is described, focusing on the general theoretical concepts of non-Abelian statistics as it relates to topological quantum computation, on understanding non-Abelian quantum Hall states, on proposed experiments to detect non-Abelian anyons, and on proposed architectures for a topological quantum computer. Both the mathematical underpinnings of topological quantum computation and the physics of the subject are addressed, using the $\ensuremath{\nu}=5∕2$ fractional quantum Hall state as the archetype of a non-Abelian topological state enabling fault-tolerant quantum computation.",2007,577,3299,71,9,47,83,97,142,139,181,211,230,254
68db0fc648885fa43fd8a055f45a4c4debfe1f3d,"Abstract : A foundational model of concurrency is developed in this thesis. It examines issues in the design of parallel systems and show why the actor model is suitable for exploiting large-scale parallelism. Concurrency in actors is constrained only by the availability of hardware resources and by the logical dependence inherent in the computation. Unlike dataflow and functional programming, however, actors are dynamically reconfigurable and can model shared resources with changing local state. Concurrency is spawned in actors using asynchronous message-passing, pipelining, and the dynamic creation of actors. The author defines an abstract actor machine and provide a minimal programming language for it. A more expressive language, which includes higher level constructs such as delayed and eager evaluation, can be defined in terms of the primitives. Examples are given to illustrate the ease with which concurrent data and control structures can be programmed. This thesis deals with some central issues in distributed computing. Specifically, problems of divergence and deadlock are addressed. Additional keywords: Object oriented programming; Semantics.",1990,44,3165,198,56,88,103,117,102,121,125,99,108,99
4c7671550671deba9ec318d867522897f20e19ba,"The usual general-purpose computing automaton (e.g.. a Turing machine) is logically irreversible- its transition function lacks a single-valued inverse. Here it is shown that such machines may he made logically reversible at every step, while retainillg their simplicity and their ability to do general computations. This result is of great physical interest because it makes plausible the existence of thermodynamically reversible computers which could perform useful computations at useful speed while dissipating considerably less than kT of energy per logical step. In the first stage of its computation the logically reversible automaton parallels the corresponding irreversible automaton, except that it saves all intermediate results, there by avoiding the irreversible operation of erasure. The second stage consists of printing out the desired output. The third stage then reversibly disposes of all the undesired intermediate results by retracing the steps of the first stage in backward order (a process which is only possible because the first stage has been carried out reversibly), there by restoring the machine (except for the now-written output tape) to its original condition. The final machine configuration thus contains the desired output and a reconstructed copy of the input, but no other undesired data. The foregoing results are demonstrated explicitly using a type of three-tape Turing machine. The biosynthesis of messenger RNA is discussed as a physical example of reversible computation.",1973,5,3397,175,1,2,2,5,2,3,2,2,3,12
ac145c160754421f43463a1d2826693f9d2422cb,"A general procedure for the solution of problems in structural dynamics is described herein. The method is capable of application to structures of any degree of complication, with any relationship ...",1959,0,4352,207,0,0,0,1,0,0,1,1,0,0
99303872c16d47af2042c5303cd5da863a1e5cfb,We implement the efficient line-of-sight method to calculate the anisotropy and polarization of the cosmic microwave background for scalar and tensor modes in almost Friedmann-Robertson-Walker models with positive spatial curvature. We present new results for the polarization power spectra in such models.,1999,33,2992,257,0,6,5,19,27,20,27,52,70,93
1d41d6ec4805f80b84a1ccd17f6753ba71e107f7,"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.",2005,0,2291,486,1,1,14,26,33,43,59,77,122,128
4cf4429f11acb8a51a362cbcf3713c06bba5aec7,"We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.",2002,47,2553,325,1,12,15,21,25,31,56,79,139,137
7a23da2c14f9355dd63a434b62cf5b28aeebc305,"Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.",1985,36,3054,157,0,5,5,29,22,45,44,30,30,31
e0535dedb8607d83cd2614317c99913378e89e26,"A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.",2002,43,2756,196,15,24,37,66,77,85,83,132,111,111
84205f6b64cf84f52defc13d22d19d8bb5814503,"A two-dimensional quantum system with anyonic excitations can be considered as a quantum 
computer. Unitary transformations can be performed by moving the excitations around 
each other. Measurements can be performed by joining excitations in pairs and observing the 
result of fusion. Such computation is fault-tolerant by its physical nature.",1997,23,2710,173,0,0,1,5,16,10,24,30,50,56
9ff69c38a1b618f5806d08cfba9a4a0f8c5d5fb4,"To understand the economic value of computers, one must broaden the traditional definition of both the technology and its effects. Case studies and firm-level econometric evidence suggest that: 1) organizational ""investments"" have a large influence on the value of IT investments; and 2) the benefits of IT investment are often intangible and disproportionately difficult to measure. Our analysis suggests that the link between IT and increased productivity emerged well before the recent surge in the aggregate productivity statistics and that the current macroeconomic productivity revival may in part reflect the contributions of intangible capital accumulated in the past.",2000,161,2954,152,18,78,114,120,118,117,136,154,146,131
59b447f58246fbbdffd5e896f83a3a142eca5cf1,"We show that a set of gates that consists of all one-bit quantum gates (U(2)) and the two-bit exclusive-or gate (that maps Boolean values (x,y) to (x,x ⊕y)) is universal in the sense that all unitary operations on arbitrarily many bits n (U(2 n )) can be expressed as compositions of these gates. We investigate the number of the above gates required to implement other gates, such as generalized Deutsch-Toffoli gates, that apply a specific U(2) transformation to one input bit if and only if the logical AND of all remaining input bits is satisfied. These gates play a central role in many proposed constructions of quantum computational networks. We derive upper and lower bounds on the exact number of elementary gates required to build up a variety of two- and three-bit quantum gates, the asymptotic number required for n-bit Deutsch-Toffoli gates, and make some observations about the number required for arbitrary n-bit unitary operations.",1995,78,2919,168,11,41,27,62,57,84,79,109,122,102
48e992a734ef6ecbc9d5aeb3fc9135bbee531e07,"Preface 1. Introduction to competitive analysis: the list accessing problem 2. Introduction to randomized algorithms: the list accessing problem 3. Paging: deterministic algorithms 4. Paging: randomized algorithms 5. Alternative models for paging: beyond pure competitive analysis 6. Game theoretic foundations 7. Request - answer games 8. Competitive analysis and zero-sum games 9. Metrical task systems 10. The k-server problem 11. Randomized k-server algorithms 12. Load-balancing 13. Call admission and circuit-routing 14. Search, trading and portfolio selection 15. Competitive analysis and decision making under uncertainty Appendices Bibliography Index.",1998,0,2505,167,12,34,58,70,87,94,119,110,121,101
12f852849fe813eddc17208c30cf97000736da87,"From the Publisher: 
Michael Sipser's philosophy in writing this book is simple: make the subject interesting and relevant, and the students will learn. His emphasis on unifying computer science theory - rather than offering a collection of low-level details - sets the book apart, as do his intuitive explanations. Throughout the book, Sipser - a noted authority on the theory of computation - builds students' knowledge of conceptual tools used in computer science, the aesthetic sense they need to create elegant systems, and the ability to think through problems on their own. INTRODUCTION TO THE THEORY OF COMPUTATION provides a mathematical treatment of computation theory grounded in theorems and proofs. Proofs are presented with a ""proof idea"" component to reveal the concepts underpinning the formalism. Algorithms are presented using prose instead of pseudocode to focus attention on the algorithms themselves, rather than on specific computational models. Topic coverage, terminology, and order of presentation are traditional for an upper-level course in computer science theory. Users of the Preliminary Edition (now out of print) will be interested to note several new chapters on complexity theory: Chapter 8 on space complexity; Chapter 9 on provable intractability, and Chapter 10 on advanced topics, including approximation algorithms, alternation, interactive proof systems, cryptography, and parallel computing.",1996,4,2233,156,3,3,7,11,34,21,33,45,59,48
4b6fb8bbff6cc0cd84f8cc36625c8b6f47874135,"List of Figures. List of Tables. Preface. Contributing Authors. Series Foreword. Part I: Foundations. 1. An Introduction to Evolutionary Algorithms J.A. Lozano. 2. An Introduction to Probabilistic Graphical Models P. Larranaga. 3. A Review on Estimation of Distribution Algorithms P. Larranaga. 4. Benefits of Data Clustering in Multimodal Function Optimization via EDAs J.M. Pena, et al. 5. Parallel Estimation of Distribution Algorithms J.A. Lozano, et al. 6. Mathematical Modeling of Discrete Estimation of Distribution Algorithms C. Gonzalez, et al. Part II: Optimization. 7. An Empiricial Comparison of Discrete Estimation of Distribution Algorithms R. Blanco., J.A. Lozano. 8. Results in Function Optimization with EDAs in Continuous Domain E. Bengoetxea, et al. 9. Solving the 0-1 Knapsack Problem with EDAs R. Sagarna, P. Larranaga. 10. Solving the Traveling Salesman Problem with EDAs V. Robles, et al. 11. EDAs Applied to the Job Shop Scheduling Problem J.A. Lozano, A. Mendiburu. 12. Solving Graph Matching with EDAs Using a Permutation-Based Representation E. Bengoetxea, et al. Part III: Machine Learning. 13. Feature Subset Selection by Estimation of Distribution Algorithms I. Inza, et al. 14. Feature Weighting for Nearest Neighbor by EDAs I. Inza, et al. 15. Rule Induction by Estimation of Distribution Algorithms B. Sierra, et al. 16. Partial Abductive Inference in Bayesian Networks: An Empirical Comparison Between GAs and EDAs L.M. de Campos, et al.17. Comparing K-Means, GAs and EDAs in Partitional Clustering J. Roure, et al. 18. Adjusting Weights in Artificial Neural Networks using Evolutionary Algorithms C. Cotta, et al. Index.",2001,32,2083,197,6,25,43,60,85,105,112,153,151,145
f54ba433ce93b894a4a30c994ae5602898448c9c,"Mathematical Background Topics from Linear Algebra Single Objective Linear Programming Determining all Alternative Optima Comments about Objective Row Parametric Programming Utility Functions, Nondominated Criterion Vectors and Efficient Points Point Estimate Weighted-sums Approach Optimal Weighting Vectors, Scaling and Reduced Feasible Region Methods Vector-Maximum Algorithms Goal Programming Filtering and Set Discretization Multiple Objective Linear Fractional Programming Interactive Procedures Interactive Weighted Tchebycheff Procedure Tchebycheff/Weighted-Sums Implementation Applications Future Directions Index.",1989,0,2815,59,39,30,42,51,55,49,50,53,81,59
889de2dc91109b767e71a9b3cffa6e0a22f8a79c,"Every function of <italic>n</italic> inputs can be efficiently computed by a complete network of <italic>n</italic> processors in such a way that:<list><item>If no faults occur, no set of size <italic>t</italic> < <italic>n</italic>/2 of players gets any additional information (other than the function value),
</item><item>Even if Byzantine faults are allowed, no set of size <italic>t</italic> < <italic>n</italic>/3 can either disrupt the computation or get additional information.
</item></list>
Furthermore, the above bounds on <italic>t</italic> are tight!",1988,18,2330,155,8,15,20,18,12,12,21,15,27,25
3371d7f50540dc8e157df672c71bd0317047c3a2,"A thorough and elegant treatment of the theory of matrix functions and numerical methods for computing them, including an overview of applications, new and unpublished research results, and improved algorithms. Key features include a detailed treatment of the matrix sign function and matrix roots; a development of the theory of conditioning and properties of the Frechet derivative; Schur decomposition; block Parlett recurrence; a thorough analysis of the accuracy, stability, and computational cost of numerical methods; general results on convergence and stability of matrix iterations; and a chapter devoted to the f(A)b problem. Ideal for advanced courses and for self-study, its broad content, references and appendix also make this book a convenient general reference. Contains an extensive collection of problems with solutions and MATLAB implementations of key algorithms.",2008,22,1764,175,23,37,62,99,82,132,160,145,165,169
77d202f90a7af3b0a42c548f614eb03819cce5b3,"The density functional theory (DFT) computation of electronic structure, total energy and other properties of materials, is a field in constant progress. In order to stay at the forefront of knowledge, a DFT software project can benefit enormously from widespread collaboration, if handled properly. Also, modern software engineering concepts can considerably ease its development. The ABINIT project relies upon these ideas: freedom of sources, reliability, portability, and self-documentation are emphasised, in the development of a sophisticated plane-wave pseudopotential code. We describe ABINITv3.0, distributed under the GNU General Public License. The list of ABINITv3.0 capabilities is presented, as well as the different software techniques that have been used until now: PERL scripts and CPP directives treat a unique set of FORTRAN90 source files to generate sequential (or parallel) object code for many different platforms; more than 200 automated tests secure existing capabilities; strict coding rules are followed; the documentation is extensive, including online help files, tutorials, and HTML-formatted sources. (C) 2002 Elsevier Science B.V. All rights reserved.",2002,57,2254,32,5,35,55,96,136,137,189,177,150,140
a146d6514cb7db6eb511047abbc6983b04a077bb,"Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a “high-input regime” in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50–100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10–50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons—that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources.",1998,161,2094,136,6,71,56,71,63,95,80,80,73,93
bc271b77fca08772b916c59ee4d9ecdf5380d4c6,"In information processing, as in physics, our classical world view provides an incomplete approximation to an underlying quantum reality. Quantum effects like interference and entanglement play no direct role in conventional information processing, but they can—in principle now, but probably eventually in practice—be harnessed to break codes, create unbreakable codes, and speed up otherwise intractable computations.",2000,203,2458,30,36,67,93,115,115,101,133,141,126,144
37d15f157f8e8a80a5ae0b23d28e6b7b0a8b8b26,"From the Publisher: 
Many scientists and engineers now use the paradigms of evolutionary computation (genetic agorithms, evolution strategies, evolutionary programming, genetic programming, classifier systems, and combinations or hybrids thereof) to tackle problems that are either intractable or unrealistically time consuming to solve through traditional computational strategies. Recently there have been vigorous initiatives to promote cross-fertilization between the EC paradigms, and also to combine these paradigms with other approaches such as neural networks to create hybrid systems with enhanced capabilities. To address the need for speedy dissemination of new ideas in these fields, and also to assist in cross-disciplinary communications and understanding, Oxford University Press and the Institute of Physics have joined forces to create a major reference publication devoted to EC fundamentals, models, algorithms and applications. This work is intended to become the standard reference resource for the evolutionary computation community. The Handbook of Evolutionary Computation will be available in loose-leaf print form, as well as in an electronic version that combines both CD-ROM and on-line (World Wide Web) acess to its contents. Regularly published supplements will be available on a subscription basis.",1997,0,2053,99,13,19,39,64,51,57,104,90,117,113
57dc98cfb48247b400cc8decb93380e022864905,,1994,0,2472,45,121,134,190,166,144,148,115,106,113,104
2bbf413f36f366fa73da4dc028a32131b5d205d6,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this �eld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.",2005,398,1911,121,16,67,147,188,202,206,228,165,158,136
ea31017b8dcbd399368fd1c2c27c2dce114e7311,"Neuronal gamma-band synchronization is found in many cortical areas, is induced by different stimuli or tasks, and is related to several cognitive capacities. Thus, it appears as if many different gamma-band synchronization phenomena subserve many different functions. I argue that gamma-band synchronization is a fundamental process that subserves an elemental operation of cortical computation. Cortical computation unfolds in the interplay between neuronal dynamics and structural neuronal connectivity. A core motif of neuronal connectivity is convergence, which brings about both selectivity and invariance of neuronal responses. However, those core functions can be achieved simultaneously only if converging neuronal inputs are functionally segmented and if only one segment is selected at a time. This segmentation and selection can be elegantly achieved if structural connectivity interacts with neuronal synchronization. I propose that this process is at least one of the fundamental functions of gamma-band synchronization, which then subserves numerous higher cognitive functions.",2009,97,1444,69,21,55,94,118,129,115,154,152,131,123
a2b5abb2e8ef4935c5e651fde5e3f3f007c5d9f9,"From the Publisher: 
In this revised and significantly expanded second edition, distinguished scientist David B. Fogel presents the latest advances in both the theory and practice of evolutionary computation to help you keep pace with developments in this fast-changing field.. ""In-depth and updated, Evolutionary Computation shows you how to use simulated evolution to achieve machine intelligence. You will gain current insights into the history of evolutionary computation and the newest theories shaping research. Fogel carefully reviews the ""no free lunch theorem"" and discusses new theoretical findings that challenge some of the mathematical foundations of simulated evolution. This second edition also presents the latest game-playing techniques that combine evolutionary algorithms with neural networks, including their success in playing competitive checkers. Chapter by chapter, this comprehensive book highlights the relationship between learning and intelligence.. ""Evolutionary Computation features an unparalleled integration of history with state-of-the-art theory and practice for engineers, professors, and graduate students of evolutionary computation and computer science who need to keep up-to-date in this developing field.",1995,0,2114,72,21,36,68,72,71,63,92,128,87,93
692dceed6973b708ff6b2032da9a1f35963aa634,A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.,1992,5,2096,72,4,8,20,24,32,33,50,48,48,44
1a9c2fa2d12609af7a489648fe68a416075cac00,"The method of characteristics used for numerical computation of solutions of fluid dynamical equations is characterized by a large degree of non standardness and therefore is not suitable for automatic computation on electronic computing machines, especially for problems with a large number of shock waves and contact discontinuities. In 1950 v. Neumann and Richtmyer proposed to use, for the solution of fluid dynamics equations, difference equations into which viscosity was introduced artificially; this has the effect of smearing out the shock wave over several mesh points. Then, it was proposed to proceed with the computations across the shock waves in the ordinary manner. In 1954, Lax published the ""triangle'' scheme suitable for computation across the shock"" waves. A deficiency of this scheme is that it does not allow computation with arbitrarily fine time steps (as compared with the space steps divided by the sound speed) because it then transforms any initial data into linear functions. In addition, this scheme smears out contact discontinuities. The purpose of this paper is to choose a scheme which is in some sense best and which still allows computation across the shock waves. This choice is made for linear equations and then by analogy the scheme is applied to the general equations of fluid dynamics. Following this scheme we carried out a large number of computations on Soviet electronic computers. For a check, some of these computations were compared with the computations carried out by the method of characteristics. The agreement of results was fully satisfactory.",1959,2,2761,73,0,0,0,3,0,1,2,1,1,3
6bc70aec3415944b461bef65b8444cd84d11667c,"A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5.",1993,49,1815,129,22,70,65,109,100,91,91,80,75,54
7f2210ff39ef9669f2a84db611c80c4b28f9fffc,"Abstract The λ-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with λ-terms. However, if one goes further and uses βη-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values ) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations , that provide a correct basis for proving equivalence of programs for a wide range of notions of computation .",1991,61,1839,166,8,14,32,21,28,36,52,57,50,51
26b7c4232872cc2327029b5354a41fde703f8e02,"After a brief introduction to the principles and promise of quantum information processing, the requirements for the physical implementation of quantum computation are discussed. These five requirements, plus two relating to the communication of quantum information, are extensively ex- plored and related to the many schemes in atomic physics, quantum optics, nuclear and electron magnetic resonance spectroscopy, superconducting electronics, and quantum-dot physics, for achiev- ing quantum computing. I. INTRODUCTION � The advent of quantum information processing, as an abstract concept, has given birth to a great deal of new thinking, of a very concrete form, about how to create physical computing devices that operate in the hitherto unexplored quantum mechanical regime. The efforts now underway to produce working laboratory devices that perform this profoundly new form of information pro- cessing are the subject of this book. In this chapter I provide an overview of the common objectives of the investigations reported in the remain- der of this special issue. The scope of the approaches, proposed and underway, to the implementation of quan- tum hardware is remarkable, emerging from specialties in atomic physics (1), in quantum optics (2), in nuclear (3) and electron (4) magnetic resonance spectroscopy, in su- perconducting device physics (5), in electron physics (6), and in mesoscopic and quantum dot research (7). This amazing variety of approaches has arisen because, as we will see, the principles of quantum computing are posed using the most fundamental ideas of quantum mechanics, ones whose embodiment can be contemplated in virtually every branch of quantum physics. The interdisciplinary spirit which has been fostered as a result is one of the most pleasant and remarkable fea- tures of this field. The excitement and freshness that has been produced bodes well for the prospect for discovery, invention, and innovation in this endeavor.",2000,60,1844,70,15,20,28,30,63,59,77,45,61,84
82b0507c2d6fddb1651b255209365cf1ce406ba2,"Preface to the English Edition. Preface to the German Edition. Real Interval Arithmetic. Further Concepts and Properties. Interval Evaluation and Range of Real Functions. Machine Interval Arithmetic. Complex Interval Arithmetic. Metric, Absolute, Value, and Width in. Inclusion of Zeros of a Function of One Real Variable. Methods for the Simultaneous Inclusion of Real Zeros of Polynomials. Methods for the Simultaneous Inclusion of Complex Zeros of Polynomials. Interval Matrix Operations. Fixed Point Iteration for Nonlinear Systems of Equations. Systems of Linear Equations Amenable to Interation. Optimality of the Symmetric Single Step Method with Taking Intersection after Every Component. On the Feasibility of the Gaussian Algorithm for Systems of Equations with Intervals as Coefficients. Hansen's Method. The Procedure of Kupermann and Hansen. Ireation Methods for the Inclusion of the Inverse Matrix and for Triangular Decompositions. Newton-like Methods for Nonlinear Systems of Equations. Newton-like Methods without Matrix Inversions. Newton-like Methods for Particular Systems of Nonlinear Equations. Newton-like Total step and Single Step Methods. Appendix A. The Order of Convergence of Iteration Methods in vn(Ic) and Mmn(iC) ). Appendix B. Realizations of Machine Interval Arithmetics in ALGOL 60. Appendix C. ALGOL Procedures. Bibliography. Index of Notation. Subject Index.",1983,1,1959,176,0,3,15,8,18,24,17,28,41,26
3500d8d44c28c316970d857b665fb42b100049c7,"To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ""Bayes' optimal"". This leads to the ""Bayesian coding hypothesis"": that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.",2004,67,1815,67,1,12,15,40,47,56,61,62,103,112
b65e7a34b0bab3f293eb26985a87d81bbced3311,"Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.",2003,48,1499,144,1,19,30,48,97,99,120,106,102,114
c3e9cc58291f6e94ab9c47bbb8b4fa79b1e3613d,"The Third Edition of this internationally acclaimed publication provides the latest theory and techniques for using simulated evolution to achieve machine intelligence. As a leading advocate for evolutionary computation, the author has successfully challenged the traditional notion of artificial intelligence, which essentially programs human knowledge fact by fact, but does not have the capacity to learn or adapt as evolutionary computation does.",1997,0,1649,97,41,57,52,43,41,75,48,72,82,98
2ff90c6a5d1bd6fc61f3cca76c2eeb78fc7d54f6,"A comprehensive, self-contained treatment presenting general results of the theory. Establishes a geometric intuition and a working facility with specific geometric practices. Emphasizes applications through the study of interesting examples and the development of computational tools. Coverage ranges from analytic to geometric. Treats basic techniques and results of complex manifold theory, focusing on results applicable to projective varieties, and includes discussion of the theory of Riemann surfaces and algebraic curves, algebraic surfaces and the quadric line complex as well as special topics in complex manifolds.",1978,0,7109,500,1,6,15,29,46,56,49,39,62,73
4713a6bea2d6e55c0a88e90fa0b25f60a4e6faf3,Theoretical background Perturbation theory Error analysis Solution of linear algebraic equations Hermitian matrices Reduction of a general matrix to condensed form Eigenvalues of matrices of condensed forms The LR and QR algorithms Iterative methods Bibliography Index.,1966,0,7566,293,15,27,52,46,58,97,98,96,103,88
b07c157e7d40e06a4f2d486b16d5180d8b24acb9,Graphs.- Groups.- Transitive Graphs.- Arc-Transitive Graphs.- Generalized Polygons and Moore Graphs.- Homomorphisms.- Kneser Graphs.- Matrix Theory.- Interlacing.- Strongly Regular Graphs.- Two-Graphs.- Line Graphs and Eigenvalues.- The Laplacian of a Graph.- Cuts and Flows.- The Rank Polynomial.- Knots.- Knots and Eulerian Cycles.- Glossary of Symbols.- Index.,2001,7,7147,357,37,32,80,99,158,173,247,322,370,364
81ff15981f785c7c4ac18a73d459fd0739105d8e,"Introduction to Algebraic Geometry.By Serge Lang. Pp. xi + 260. (Addison–Wesley: Reading, Massachusetts, 1972.)",1973,0,5735,237,0,1,2,1,1,0,2,6,4,6
0ca045681b4362909427784e83be74cdbde4be5a,Introduction.- Elementary Definitions.- I Basic Constructions.- II Dimension Theory.- III Homological Methods.- Appendices.- Hints and Solutions for Selected Exercises.- References.- Index of Notation.- Index.,1995,12,4938,369,2,32,65,85,98,120,133,177,180,185
32b5178547b79d384afad2c7abb6c64f1697617c,,1973,3,3490,254,0,0,0,1,1,1,0,0,1,0
c01d227077dae98b82c68d9871e8d28dc24a0887,"A description of 148 algorithms fundamental to number-theoretic computations, in particular for computations related to algebraic number theory, elliptic curves, primality testing and factoring. The first seven chapters guide readers to the heart of current research in computational algebraic number theory, including recent algorithms for computing class groups and units, as well as elliptic curve computations, while the last three chapters survey factoring and primality testing methods, including a detailed description of the number field sieve algorithm. The whole is rounded off with a description of available computer packages and some useful tables, backed by numerous exercises. Written by an authority in the field, and one with great practical and teaching experience, this is certain to become the standard and indispensable reference on the subject.",1993,229,2775,210,3,13,27,41,64,80,66,97,76,94
803205059d331dfcd09654b64caefb3a2c92a7cf,"The name Algebraic Geometry comes from the fact that in this part of Mathematics one tries to study geometric objects (mainly) through algebraic techniques. This combination of algebra and geometry is extremely fruitful, and as a result the field of Algebraic Geometry has become big and very diverse. There are many connections to other areas/techniques in mathematics, such as Number Theory, Differential Geometry, Topology, Category Theory, Cryptography, Mathematical Physics, and so on. All in all, it’s better to think of Algebraic Geometry as indicating a sub-area of mathematics as a whole, rather than a very precisely defined subfield.",1977,1,3137,190,0,1,1,2,3,7,5,9,10,11
3db3ba525ec64cc778f2f48df05c4332a079cfde,,1978,7,3507,113,5,8,10,12,21,22,17,39,44,55
b2f46402f60220d4140cc76a3555be5cc678f41f,"Elements of Algebraic Topology provides the most concrete approach to the subject. With coverage of homology and cohomology theory, universal coefficient theorems, Kunneth theorem, duality in manifolds, and applications to classical theorems of point-set topology, this book is perfect for comunicating complex topics and the fun nature of algebraic topology for beginners.",1984,0,2516,227,0,1,2,4,8,11,10,12,15,25
f00b6c02a96dab7a27ab3d69570360d85b5f04ba,,1948,0,4595,85,0,0,0,1,0,0,2,0,0,1
4fd5f6b7c036c7522d82f48e7cfd384e38328d6f,I: Algebraic Integers.- II: The Theory of Valuations.- III: Riemann-Roch Theory.- IV: Abstract Class Field Theory.- V: Local Class Field Theory.- VI: Global Class Field Theory.- VII: Zeta Functions and L-series.,1999,7,2632,114,46,70,60,80,80,104,100,94,104,114
bc7e0aa5bcbdacef451d001fed342da78465fbb7,"This is the revised edition of Berlekamp's famous book, ""Algebraic Coding Theory,"" originally published in 1968, wherein he introduced several algorithms which have subsequently dominated engineering practice in this field. One of these is an algorithm for decoding Reed-Solomon and Bose–Chaudhuri–Hocquenghem codes that subsequently became known as the Berlekamp–Massey Algorithm. Another is the Berlekamp algorithm for factoring polynomials over finite fields, whose later extensions and embellishments became widely used in symbolic manipulation systems. Other novel algorithms improved the basic methods for doing various arithmetic operations in finite fields of characteristic two. Other major research contributions in this book included a new class of Lee metric codes, and precise asymptotic results on the number of information symbols in long binary BCH codes.Selected chapters of the book became a standard graduate textbook.Both practicing engineers and scholars will find this book to be of great value.",1984,0,2778,141,23,29,23,32,36,38,34,59,32,58
4ac0eda2d75cce43fa56e60d22580c9b8f196e03,I De Rham Theory.- II The ?ech-de Rham Complex.- III Spectral Sequences and Applications.- IV Characteristic Classes.- References.- List of Notations.,1982,0,2796,138,0,1,7,15,21,19,19,20,18,25
789dedd8d7f377792a8bf37a7bf91a8336daf876,"SUNDIALS is a suite of advanced computational codes for solving large-scale problems that can be modeled as a system of nonlinear algebraic equations, or as initial-value problems in ordinary differential or differential-algebraic equations. The basic versions of these codes are called KINSOL, CVODE, and IDA, respectively. The codes are written in ANSI standard C and are suitable for either serial or parallel machine environments. Common and notable features of these codes include inexact Newton-Krylov methods for solving large-scale nonlinear systems; linear multistep methods for time-dependent problems; a highly modular structure to allow incorporation of different preconditioning and/or linear solver methods; and clear interfaces allowing for users to provide their own data structures underneath the solvers. We describe the current capabilities of the codes, along with some of the algorithms and heuristics used to achieve efficiency and robustness. We also describe how the codes stem from previous and widely used Fortran 77 solvers, and how the codes have been augmented with forward and adjoint methods for carrying out first-order sensitivity analysis with respect to model parameters or initial conditions.",2005,85,2055,121,12,16,32,48,49,55,97,146,165,169
300b625e8ea2c03ce03796adbfb48baf3f3dd2b2,"This book, along with volume I, which appeared previously, presents a survey of the structure and representation theory of semi groups. Volume II goes more deeply than was possible in volume I into the theories of minimal ideals in a semi group, inverse semi groups, simple semi groups, congruences on a semi group, and the embedding of a semi group in a group. Among the more important recent developments of which an extended treatment is presented are B. M. Sain's theory of the representations of an arbitrary semi group by partial one-to-one transformations of a set, L. Redei's theory of finitely generated commutative semi groups, J. M. Howie's theory of amalgamated free products of semi groups, and E. J. Tully's theory of representations of a semi group by transformations of a set. Also a full account is given of Malcev's theory of the congruences on a full transformation semi group.",1964,19,2904,174,16,20,24,27,30,48,62,86,74,66
e13bc77990767b1b2a0994c83fce751072109dc8,Preface.- Guide to the Reader.- Chapter IX. The Hilbert Scheme.- Chapter X. Nodal curves.- Chapter XI. Elementary deformation theory and some applications.- Chapter XII. The moduli space of stable curves.- Chapter XIII. Line bundles on moduli.- Chapter XIV. The projectivity of the moduli space of stable curves.- Chapter XV. The Teichmuller point of view.- Chapter XVI. Smooth Galois covers of moduli spaces.- Chapter XVII. Cycles on the moduli spaces of stable curves.- Chapter XVIII. Cellular decomposition of moduli spaces.- Chapter XIX. First consequences of the cellular decomposition .- Chapter XX. Intersection theory of tautological classes.- Chapter XXI. Brill-Noether theory on a moving curve.- Bibliography.- Index.,1985,4,2160,171,2,10,13,20,23,18,22,26,34,25
4ab74f65af82281e7ecd7cc90968e7c8e59ad421,,2004,4,1743,137,21,45,62,59,87,89,106,110,120,134
5a4a9e10adf1c6e8bc2ede3f69efc3e7ed08e7b9,"The theory of algebraic function fields has its origins in number theory, complex analysis (compact Riemann surfaces), and algebraic geometry. Since about 1980, function fields have found surprising applications in other branches of mathematics such as coding theory, cryptography, sphere packings and others. The main objective of this book is to provide a purely algebraic, self-contained and in-depth exposition of the theory of function fields. This new edition, published in the series Graduate Texts in Mathematics, has been considerably expanded. Moreover, the present edition contains numerous exercises. Some of them are fairly easy and help the reader to understand the basic material. Other exercises are more advanced and cover additional material which could not be included in the text. This volume is mainly addressed to graduate students in mathematics and theoretical computer science, cryptography, coding theory and electrical engineering.",1993,0,1802,141,2,3,12,17,24,34,54,54,56,51
9e717abebb98da646af51345b4758ab7bf760b73,"1. Ordered Fields, Real Closed Fields.- 2. Semi-algebraic Sets.- 3. Real Algebraic Varieties.- 4. Real Algebra.- 5. The Tarski-Seidenberg Principle as a Transfer Tool.- 6. Hilbert's 17th Problem. Quadratic Forms.- 7. Real Spectrum.- 8. Nash Functions.- 9. Stratifications.- 10. Real Places.- 11. Topology of Real Algebraic Varieties.- 12. Algebraic Vector Bundles.- 13. Polynomial or Regular Mappings with Values in Spheres.- 14. Algebraic Models of C? Manifolds.- 15. Witt Rings in Real Algebraic Geometry.- Index of Notation.",1992,68,1856,135,0,1,1,1,1,1,2,8,22,33
144d1caa9efec96a0e2b97b9328f2bda640bed2d,Conventions and notation background material from algebraic geometry general notions associated with algebraic groups homogeneous spaces solvable groups Borel subgroups reductive groups rationality questions.,1991,3,1820,132,25,24,16,25,23,37,24,47,36,40
05a5def3c0a1b291f0136bcba0d72fd72c906dfd,"We take a new look at the issue of network capacity. It is shown that network coding is an essential ingredient in achieving the capacity of a network. Building on recent work by Li et al., who examined the network capacity of multicast networks, we extend the network coding framework to arbitrary networks and robust networking. For networks which are restricted to using linear network codes, we find necessary and sufficient conditions for the feasibility of any given set of connections over a given network. We also consider the problem of network recovery for nonergodic link failures. For the multicast setup we prove that there exist coding strategies that provide maximally robust networks and that do not require adaptation of the network interior to the failure pattern in question. The results are derived for both delay-free networks and networks with delays.",2003,20,1735,90,6,22,48,85,91,114,153,194,197,186
2cbe6ffbe39403981cdbec6878c98cb8e9d86253,"We give a summary, without proofs, of basic properties of linear algebraic groups, with particular emphasis on reductive algebraic groups.",2005,19,1529,50,62,74,90,97,93,61,85,64,72,59
ff6ef8c9b8435f44ce222b0982c19d65900e0f92,"Algebraic Geometry is the study of systems of polynomial equations in one or more variables, asking such questions as: Does the The denominator is taking on this, book interested. This book for grbner bases which printing you can spend a new section. A geometric theorem the first four chapters build on algorithms. For each command in geometric object called! The last digit displayed is no, doubt the computational. Darren glass is the version of arrow in maximum number systems. And reduce springer ebooks are several appendices on many solutions of well. In recent years that assume very little is available as the dictionary relating last. David cox founded the study of typographical errors are purchase you must. In alphabetical order whereas the rank of algebraic geometry they. Furthermore your ebooks across numerous devices such topics as the entire lab. Cox john little from ideals and maple package although there are a dictionary relating. Although the value of algebraic object, is not compatible with visa mastercard american mathematical. The corresponding algebraic geometry and resultants which reveals the solutions how. To use their solutions how can directly download your ebook reader to use. The algorithmic roots of pictures parametrizations and errata. Appendix contains a seminar supervised by the other theorems. He lives in the focus squarely, on. Algebraic object is an important part of north.",2007,0,1543,67,65,77,90,75,67,92,105,89,92,84
5f614aa79aa8f4cea05cc7f7db11dcaee873b6d7,"Abstract In the course of closing supersymmetry on parallel M2 branes up to a gauge transformation, following the suggestion in [J. Bagger, N. Lambert, Phys. Rev. D 75 (2007) 045020, hep-th/0611108 ] of incorporating a gauge field which only has topological degrees of freedom, we are led to assume a certain algebraic structure for the low energy theory supposedly living on parallel M2 branes.",2007,23,1036,79,3,194,154,119,106,91,84,53,44,57
f70e49433a0198a73a559a0429e5a98ee82fc66c,,1973,17,1698,186,2,6,5,11,8,7,19,15,25,19
0fbf98f1fec4d33c4fc615655d2cfc1968cb1cf4,List of symbols and acronyms List of iterative algorithm templates List of direct algorithms List of figures List of tables 1: Introduction 2: A brief tour of Eigenproblems 3: An introduction to iterative projection methods 4: Hermitian Eigenvalue problems 5: Generalized Hermitian Eigenvalue problems 6: Singular Value Decomposition 7: Non-Hermitian Eigenvalue problems 8: Generalized Non-Hermitian Eigenvalue problems 9: Nonlinear Eigenvalue problems 10: Common issues 11: Preconditioning techniques Appendix: of things not treated Bibliography Index .,2000,0,1430,84,11,16,41,46,63,75,80,86,78,64
899a5e04fcb72c3c2b2c615ea21effeabe7099e9,Introduction 1. Some topics in commutative algebra 2. General Properties of schemes 3. Morphisms and base change 4. Some local properties 5. Coherent sheaves and Cech cohmology 6. Sheaves of differentials 7. Divisors and applications to curves 8. Birational geometry of surfaces 9. Regular surfaces 10. Reduction of algebraic curves Bibilography Index,2002,0,1033,275,0,10,8,15,12,18,39,48,61,60
c5aeb5d078cfa7d676b967cdaa4ff96b8a53f2f0,1. Preliminaries from the theory of matrices 2. Indefinite scalar products 3. Skew-symmetric scalar products 4. Matrix theory and control 5. Linear matrix equations 6. Rational matrix functions 7. Geometric theory: the complex case 8. Geometric theory: the real case 9. Constructive existence and comparison theorems 10. Hermitian solutions and factorizations of rational matrix functions 11. Perturbation theory 12. Geometric theory for the discrete algebraic Riccati equation 13. Constructive existence and comparison theorems 14. Perturbation theory for discrete algebraic Riccati equations 15. Discrete algebraic Riccati equations and matrix pencils 16. Linear-quadratic regulator problems 17. The discrete Kalman filter 18. The total least squares technique 19. Canonical factorization 20. Hoo control problems 21. Contractive rational matrix functions 22. The matrix sign function 23. Structured stability radius Bibliography List of notations Index,1995,8,1354,119,1,4,28,30,27,35,51,43,44,47
6bfd3b8242c1a62c6776e7e99116e3f994b2b0c7,I. Hilbert Schemes and Chow Varieties.- II. Curves on Varieties.- III. The Cone Theorem and Minimal Models.- IV. Rationally Connected Varieties.- V. Fano Varieties.- VI. Appendix.- References.,1995,9,1364,86,8,15,14,28,29,35,24,34,46,56
7b4d6b67a7e7814b7c85a8a8ac0b699dc8b689dd,"1. Finite and infinite words J. Berstel and D. Perrin 2. Sturmian words J. Berstel and P. Seebold 3. Unavoidable patterns J. Cassaigne 4. Sesquipowers A. De Luca and S. Varricchio 5. The plactic monoid A. Lascoux, B. Leclerc and J.-Y. Thibon 6. Codes V. Bruyere 7. Numeration systems C. Frougny 8. Periodicity F. Mignosi and A. Restivo 9. Centralisers of noncommutative series and polynomials C. Reutenauer 10. Transformations on words and q-calculus D. Foata and G.-N. Han 11. Statistics on permutations and words J. Desarmenien 12. Makanin's algorithm V. Diekert 13. Independent systems of equations T. Harju, J. Karhumaki and W. Plandowski.",2002,0,1193,122,35,44,51,62,51,81,85,79,60,64
4eb0f06e6198aa342666c8048968fe8f156fe4b7,"Since a nondeterministic and concurrent program may, in general, communicate repeatedly with its environment, its meaning cannot be presented naturally as an input/output function (as is often done in the denotational approach to semantics). In this paper, an alternative is put forth. First, a definition is given of what it is for two programs or program parts to be equivalent for all observers; then two program parts are said to be observation congruent if they are, in all program contexts, equivalent. The behavior of a program part, that is, its meaning, is defined to be its observation congruence class.
The paper demonstrates, for a sequence of simple languages expressing finite (terminating) behaviors, that in each case observation congruence can be axiomatized algebraically. Moreover, with the addition of recursion and another simple extension, the algebraic language described here becomes a calculus for writing and specifying concurrent programs and for proving their properties.",1985,19,1490,145,8,10,14,18,29,32,40,41,37,50
f8b12b0a138271c245097f2c70ed9a0b61873968,"In this paper we have discussed what appears to be a superior implementation of the Algebraic Reconstruction Technique (ART). The method is based on 1) simultaneous application of the error correction terms as computed by ART for all rays in a given projection; 2) longitudinal weighting of the correction terms back-distributed along the rays; and 3) using bilinear elements for discrete approximation to the ray integrals of a continuous image. Since this implementation generates a good reconstruction in only one iteration, it also appears to have a computational advantage over the more traditional implementation of ART. Potential applications of this implementation include image reconstruction in conjunction with ray tracing for ultrasound and microwave tomography in which the curved nature of the rays leads to a non-uniform ray density across the image.",1984,36,1338,137,2,1,1,3,1,3,3,3,1,2
47f0e75ad13de3c21c8f7b1f76ccc04dd09f8847,"One or more than made up, by fred woodward under the best uses of systems. 157 rule generalizes easily to undergraduates this has changed. The copyright page the 5th printing you can always re. This text like harris' book is the computations lists of bezout's theorem some. The algorithmic roots of algebraic object, called a close relationship between ideals. Many of polynomial equations in geometric, object called a more than you. Then for teaching purposes and varieties. The solutions and reduce even without copy. Although the division algorithm for polynomials in mind following files warning electronic. We must also use their solutions sent in the rank of systems and resultants. In preparing a geometric theorem although. Some reason the following should be extremely useful to words. The many pages 510 and mxgb which reveals the tutorial you. It is the dean of mathematics in computational methods have length this volume. There are old the book bases, which computes a graduate. The book we are old it, is particularly important part of cash rewards for releases. Appendix contains a variety the main commands for instance. When requesting a variety the buchberger criterion as matrix formed. I do not made up by symbolic math packages? There is not just finished using algebraic geometry on this book but elegant introductions. In geometric theorem and if so, how to systems of can they work. Lists of computers in the right amount maple. The main tools used by hyeyoun chung for the second printing of modern. In a geometric object is highly accessible. Then for academic affairs of algebraic, geometry a level appropriate to undergraduates this version.",1997,0,1240,93,15,17,34,27,22,40,36,27,37,49
0eb88d98afa3de00800412739bc1177727bdbc70,"1: Affine and Projective Varieties. 2: Regular Functions and Maps. 3: Cones, Projections, and More About Products. 4: Families and Parameter Spaces. 5: Ideals of Varieties, Irreducible Decomposition. 6: Grassmannians and Related Varieties. 7: Rational Functions and Rational Maps. 8: More Examples. 9: Determinantal Varieties. 10: Algebraic Groups. 11: Definitions of Dimension and Elementary Examples. 12: More Dimension Computations. 13: Hilbert Functions and Polynomials. 14: Smoothness and Tangent Spaces. 15: Gauss Maps, Tangential and Dual Varieties. 16: Tangent Spaces to Grassmannians. 17: Further Topics Involving Smoothness and Tangent Spaces. 18: Degree. 19: Further Examples and Applications of Degree. 20: Singular Points and Tangent Cones. 21: Parameter Spaces and Moduli Spaces. 22: Quadrics.",1995,0,1300,93,11,8,14,19,21,14,38,33,28,56
dc743b17e0450c207c48df61e729f2717803df24,"Introduction.- Solving Polynomial Equations.- Resultants.- Computation in Local Rings.- Modules.- Free Resolutions.- Polytopes, Resultants, and Equations.- Integer Programming, Combinatorics, and Splines.- Algebraic Coding Theory.- The Berlekamp-Massey-Sakata Decoding Algorithm.",1998,0,1178,97,3,13,22,25,38,39,54,51,62,61
c4152fdff9d1f7da881a1c1c8abedd0759d68bcb,,1968,0,1748,57,0,4,11,17,32,57,34,27,50,27
945298ccb0399eebbb865c8995bc2c4a37cabccb,"Since it was first published in 1967, Simplicial Objects in Algebraic Topology has been the standard reference for the theory of simplicial sets and their relationship to the homotopy theory of topological spaces. J. Peter May gives a lucid account of the basic homotopy theory of simplicial sets (discrete analogs of topological spaces) which have played a central role in algebraic topology ever since their introduction in the late 1940s. ""Simplicial Objects in Algebraic Topology presents much of the elementary material of algebraic topology from the semi-simplicial viewpoint. It should prove very valuable to anyone wishing to learn semi-simplicial topology. [May] has included detailed proofs, and he has succeeded very well in the task of organizing a large body of previously scattered material.""--Mathematical Review",1993,0,1188,140,14,11,25,27,36,24,36,20,48,26
3d9994c4f730afcec714bc5c2057723c4532d52a,Part I. General theory: Schemes Group schemes and representations Induction and injective modules Cohomology Quotients and associated sheaves Factor groups Algebras of distributions Representations of finite algebraic groups Representations of Frobenius kernels Reduction mod $p$ Part II. Representations of reductive groups: Reductive group Simple $G$-modules Irreducible representations of the Frobenius kernels Kempf's vanishing theorem The Borel-Bott-Weil theorem and Weyl's character formula The linkage principle The translation functors Filtrations of Weyl modules Representations of $G_rT$ and $G_rB$ Geometric reductivity and other applications of the Steinberg modules Injective $G_r$-modules Cohomology of the Frobenius kernels Schubert schemes Line bundles on Schubert schemes Truncated categories and Schur algebras Results over the integers Lusztig's conjecture and some consequences Radical filtrations and Kazhdan-Lusztig polynomials Tilting modules Frobenius splitting Frobenius splitting and good filtrations Representations of quantum groups References List of notations Index.,1987,7,1229,122,1,3,5,9,17,16,18,24,19,22
bfcdca580a3212c88366907287fe0ab51177e642,"The second volume of Shafarevich's introductory book on algebraic geometry focuses on schemes, complex algebraic varieties and complex manifolds. As with first volume the author has revised the text and added new material. Although the material is more advanced than in Volume 1 the algebraic apparatus is kept to a minimum making the book accessible to non-specialists. It can be read independently of the first volume and is suitable for beginning graduate students.",1974,1,1514,105,0,2,3,4,6,8,10,10,9,12
52b4d3640a77ef604f5b28816ed28de04ea520e2,"This is a corrected printing of the second edition of Lang's well-known textbook. It covers all of the basic material of classical algebraic number theory, giving the student the background necessary for the study of further topics in algebraic number theory, such as cyclotomic fields, or modular forms. Part I introduces some of the basic ideas of the theory: number fields, ideal classes, ideles and adeles, and zeta functions. It also contains a version of a Riemann-Roch theorem in number fields, proved by Lang in the very first version of the book in the sixties. This version can now be seen as a precursor of Arakelov theory. Part II covers class field theory, and Part III is devoted to analytic methods, including an exposition of Tate's thesis, the Brauer-Siegel theorem, and Weil's explicit formulas. The second edition contains corrections, as well as several additions to the previous edition, and the last chapter on explicit formulas has been rewritten.",1971,0,1565,39,7,8,8,10,14,19,10,11,15,22
4a6982d524475eeceec82affe5621b5a445665c3,Algebraic geometry affine algebraic groups lie algebras homogeneous spaces chracteristic 0 theory semisimple and unipoten elements solvable groups Borel subgroups centralizers of Tori structure of reductive groups representations and classification of semisimple groups survey of rationality properties.,1975,0,1471,66,0,0,5,11,9,13,13,16,18,14
267f5e3270718ef8995d35ffa4e788ced2d2379e,"The optimal control of linear systems with respect to quadratic performance criteria over an infinite time interval is treated. Both the case in which the terminal state is free and that in which the terminal state is constrained to be zero are treated. The integrand of the performance criterion is allowed to be fully quadratic in the control and the state without necessarily satisfying the definiteness conditions which are usually assumed in the standard regulator problem. Frequency-domain and time-domain conditions for the existence of solutions are derived. The algebraic Riccati equation is then examined, and a complete classification of all its solutions is presented. It is finally shown how the optimal control problems introduced in the beginning of the paper may be solved analytically via the algebraic Riccati equation.",1971,17,1401,88,0,5,10,12,13,15,5,11,14,13
d820147f1672a527320eb2c821974582d19f6585,"There appeared in 1976 an expository paper by the present author [52] entitled ""What is a group ringV This question, rhetorical as it is, may nevertheless be answered directly by saying that for a group G over an integral domain R the group ring R(G) is a free unitary iî-module over the elements of G as a basis and in which the multiplication on G is extended linearly to yield an associative multiplication on R(G), R(G) becoming a ring with an identity. While this may answer the question, the underlying aim of the author is evidently to draw attention to this particular ring R(G) which, over the past decade and especially when G is infinite, has come to be intensively studied [51]. In the main R(G) is studied under the assumption that R is a field K and so, although K(G) is nowadays commonly called a group ring, K{G) in an older and more informative terminology is a linear associative algebra. The group ring K(G) of a finite group G over a field of characteristic 0 is semisimple. Over a sufficiently large extension K of the rational field Q there is a well-known theory of group characters by whose means, for example, explicit characterisations of the primitive central idempotents of K(G) are obtainable. Over a field of prime characteristic p and for G finite the Jacobson radical JK(G) of K(G) may be nontrivial but, since around 1940 [7], the development of Brauer's theory of modular characters has again, for a sufficiently large extension of the prime field GF(p), yielded characterisations of the primitive central idempotents [44]. All of this work, for which the text of Curtis and Reiner is a well-known reference [10], depends heavily on the finiteness of G. Passman's book is concerned with the case of a group ring K(G) in which G is potentially infinite and for which, in consequence, ordinary or modular character theory is of little help. The bulk of the work on infinite group rings has been done in the period 1967-1977, a major, if not the major, contributor being the present author. Prior to the mid-1960s the earliest significant work was due to Jennings [33], who for a finite /?-group G over a field K of characteristic p obtained group-theoretic descriptions of the dimension subgroups Dn(K(G)) formally defined from the ring structure as",1977,36,1396,71,3,6,14,20,25,27,19,28,20,22
1cc9d7704fa6c0a109af0bf9443de4a70e02cc0e,"For any arbitrary algebraic curve, we define an infinite sequence of invariants. We study their properties, in particular their variation under a variation of the curve, and their modular properties. We also study their limits when the curve becomes singular. In addition we find that they can be used to define a formal series, which satisfies formally an Hirota equation, and we thus obtain a new way of constructing a tau function attached to an algebraic curve. These invariants are constructed in order to coincide with the topological expansion of a matrix formal integral, when the algebraic curve is chosen as the large N limit of the matrix model's spectral curve. Surprisingly, we find that the same invariants also give the topological expansion of other models, in particular the matrix model with an external field, and the so-called double scaling limit of matrix models, i.e. the (p,q) minimal models of conformal field theory. As an example to illustrate the efficiency of our method, we apply it to the Kontsevitch integral, and we give a new and extremely easy proof that Kontsevitch integral depends only on odd times, and that it is a KdV tau-function.",2007,94,639,118,15,31,41,44,43,34,35,40,51,52
ee787c6090c333eb83786ea48e3e1b1ae6499662,"This paper is an attempt at developing a theory of algebraic systems that would correspond in a natural fashion to the No-valued propositional calculus(2). For want of a better name, we shall call these algebraic systems MV-algebras where MV is supposed to suggest many-valued logics. It is known that the classical two-valued logic gives rise to the study of Boolean algebras and, as can be expected, every Boolean algebra will be an MValgebra whereas the converse does not hold. However, many results for Boolean algebras can be appropriately carried over to MV-algebras, although in some cases the proofs become more subtle and delicate. The motivation behind the present study is to find a proof of the completeness of the Novalued logic by using some algebraic results concerning MV-algebras; more specifically, it is known that the completeness of the two-valued logic is a consequence of the Boolean prime ideal theorem and we wish to exploit just some such corresponding result for MV-algebras(3). It will be seen that our effort in duplicating this result is only partially successful. In the first four sections of this paper we present various theorems concerning both the arithmetic in MV-algebras and the structure of these algebras. In the last section we give some applications of our results to the study of completeness of NO-valued logic and some related topics. We point out here that the treatment of MV-algebras as given here is not meant to be complete and exhaustive. 1. Axioms of MV-algebras and some elementary consequences. An MV",1958,9,1341,106,0,1,0,0,1,4,1,0,0,0
cbc7177361265780a0bc83e9ae4bab18713fec14,Algebraically Closed Fields.- Real Closed Fields.- Semi-Algebraic Sets.- Algebra.- Decomposition of Semi-Algebraic Sets.- Elements of Topology.- Quantitative Semi-algebraic Geometry.- Complexity of Basic Algorithms.- Cauchy Index and Applications.- Real Roots.- Cylindrical Decomposition Algorithm.- Polynomial System Solving.- Existential Theory of the Reals.- Quantifier Elimination.- Computing Roadmaps and Connected Components of Algebraic Sets.- Computing Roadmaps and Connected Components of Semi-algebraic Sets.,2003,146,888,105,7,24,32,59,47,58,60,59,52,52
aae1777885238424443e7e87928e7f2c43b4807a,1. Introduction 2. Linear Algebra Review 3. Analysis of First-order Information 4. Second-order Information in Linear Systems 5. Covariance Controllers 6.Covariance Upper Boundary Controllers 7. H-Controllers 8. Model Reduction 9. Unified Perspective 10. Projection Methods 11. Successive Centring Methods 12. A: Linear Algebra Basics 13. B: Calculus of Vectors and Matrices 14. C: Balanced Model Reduction,1998,142,1098,81,8,8,13,25,26,35,53,61,65,64
8ceaf15653e991dd134580d0d17bffb49223d43b,Introduction. 1. Basic notions. 2. Chang completeness theorem. 3. Free MV-algebras. 4. Lukasiewicz INFINITY-valued calculus. 5. Ulam's game. 6. Lattice-theoretical properties. 7. MV-algebras and l-groups. 8. Varieties of MV-algebras. 9. Advanced topics. 10. Further Readings. Bibliography. Index.,1999,0,1020,69,5,7,29,27,42,37,49,53,54,61
47c6c119740fc7aed4478fc44b4713dd754db0e5,"This book provides a careful and detailed algebraic introduction to Grothendieck’s local cohomology theory, and provides many illustrations of applications of the theory in commutative algebra and in the geometry of quasi-affine and quasi-projective varieties. Topics covered include Castelnuovo–Mumford regularity, the Fulton–Hansen connectedness theorem for projective varieties, and connections between local cohomology and both reductions of ideals and sheaf cohomology. It is designed for graduate students who have some experience of basic commutative algebra and homological algebra, and also for experts in commutative algebra and algebraic geometry.",1998,74,960,73,5,2,5,11,27,17,28,38,36,41
6f08ecc00cc86835fb0b3f9cbda78932d9db07f5,"Given an error-correcting code over strings of length n and an arbitrary input string also of length n, the list decoding problem is that of finding all codewords within a specified Hamming distance from the input string. We present an improved list decoding algorithm for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon codes reduces to the following ""curve-fitting"" problem over a field F: given n points ((x/sub i//spl middot/y/sub i/))/sub i=1//sup n/, x/sub i/, y/sub i//spl isin/F, and a degree parameter k and error parameter e, find all univariate polynomials p of degree at most k such that y/sub i/=p(x/sub i/) for all but at most e values of i/spl isin/(1,...,n). We give an algorithm that solves this problem for e 1/3, where the result yields the first asymptotic improvement in four decades. The algorithm generalizes to solve the list decoding problem for other algebraic codes, specifically alternant codes (a class of codes including BCH codes) and algebraic-geometry codes. In both cases, we obtain a list decoding algorithm that corrects up to n-/spl radic/(n(n-d')) errors, where n is the block length and d' is the designed distance of the code. The improvement for the case of algebraic-geometry codes extends the methods of Shokrollahi and Wasserman (see in Proc. 29th Annu. ACM Symp. Theory of Computing, p.241-48, 1998) and improves upon their bound for every choice of n and d'. We also present some other consequences of our algorithm including a solution to a weighted curve-fitting problem, which may be of use in soft-decision decoding algorithms for Reed-Solomon codes.",1999,106,951,81,6,19,21,33,41,52,58,62,62,61
8d03e9bccb5efb02fee168ab968c6cfa8884905c,Concepts of Algebraic Topology.- Overture.- Cell Complexes.- Homology Groups.- Concepts of Category Theory.- Exact Sequences.- Homotopy.- Cofibrations.- Principal ?-Bundles and Stiefel-Whitney Characteristic Classes.- Methods of Combinatorial Algebraic Topology.- Combinatorial Complexes Melange.- Acyclic Categories.- Discrete Morse Theory.- Lexicographic Shellability.- Evasiveness and Closure Operators.- Colimits and Quotients.- Homotopy Colimits.- Spectral Sequences.- Complexes of Graph Homomorphisms.- Chromatic Numbers and the Kneser Conjecture.- Structural Theory of Morphism Complexes.- Using Characteristic Classes to Design Tests for Chromatic Numbers of Graphs.- Applications of Spectral Sequences to Hom Complexes.,2007,0,500,43,5,7,10,27,30,39,42,34,42,42
d870a6062777934b402107324c7b3fbf8f0338f7,"(Chapter Heading): Algebraic Number Theory. Algebraic Groups. Algebraic Groups over Locally Compact Fields. Arithmetic Groups and Reduction Theory. Adeles. Galois Cohomology. Approximation in Algebraic Groups. Class Numbers andClass Groups of Algebraic Groups. Normal Structure of Groups of Rational Points of Algebraic Groups. Appendix A. Appendix B: Basic Notation. Algebraic Number Theory: Algebraic Number Fields, Valuations, and Completions. Adeles and Ideles Strong and Weak Approximation The Local-Global Principle. Cohomology. Simple Algebras over Local Fields. Simple Algebras over Algebraic Number Fields. Algebraic Groups: Structural Properties of Algebraic Groups. Classification of K-Forms Using Galois Cohomology. The Classical Groups. Some Results from Algebraic Geometry. Algebraic Groups over Locally Compact Fields: Topology and Analytic Structure. The Archimedean Case. The Non-Archimedean Case. Elements of Bruhat-Tits Theory. Results Needed from Measure Theory. Arithmetic Groups and Reduction Theory: Arithmetic Groups. Overview of Reduction Theory: Reduction in GLn(R).Reduction in Arbitrary Groups. Group-Theoretic Properties of Arithmetic Groups. Compactness of GR/GZ. The Finiteness of the Volume of GR/GZ. Concluding Remarks on Reduction Theory. Finite Arithmetic Groups. Adeles: Basic Definitions. Reduction Theory for GA Relative to GK. Criteria for the Compactness and the Finiteness of Volume of GA/GK. Reduction Theory for S-Arithmetic Subgroups. Galois Cohomology: Statement of the Main Results. Cohomology of Algebraic Groups over Finite Fields. Galois Cohomology of Algebraic Tori. Finiteness Theorems for Galios Cohomology. Cohomology of Semisimple Algebraic Groups over Local Fields and Number Fields. Galois Cohomology and Quadratic, Hermitian, and Other Forms. Proof of Theorems 6.4 and 6.6: Classical Groups. Proof of Theorems 6.4 and 6.6: Exceptional Groups. Approximation in Algebraic Groups: Strong and Weak Approximation in Algebraic Varieties. The Kneser-Tits Conjecture. Weak Approximation in Algebraic Groups. The Strong Approximation Theorem. Generalization of the Strong Approximation Theorem. Class Numbers and Class Groups of Algebraic Groups: Class Numbers of Algebraic Groups and Number of Classes in a Genus. Class Numbers and Class Groups of Semisimple Groups of Noncompact Type The Realization Theorem. Class Numbers of Algebraic Groups of Compact Type. Estimating the Class Number for Reductive Groups. The Genus Problem. Normal Subgroup Structure of Groups of Rational Points of Algebraic Groups: Main Conjecture and Results. Groups of Type An. The Classical Groups. Groups Split over a Quadratic Extension. The Congruence Subgroup Problem (A Survey). Appendices: Basic Notation. Bibliography. Index.",1992,27,1055,73,1,3,3,5,15,10,20,17,19,24
1c621ae00fa4080fd7e7465c8b6109baefb7609b,"The book is meant to serve two purposes. The first and more obvious one is to present state of the art results in algebraic research into residuated structures related to substructural logics. The second, less obvious but equally important, is to provide a reasonably gentle introduction to algebraic logic. At the beginning, the second objective is predominant. Thus, in the first few chapters the reader will find a primer of universal algebra for logicians, a crash course in nonclassical logics for algebraists, an introduction to residuated structures, an outline of Gentzen-style calculi as well as some titbits of proof theory - the celebrated Hauptsatz, or cut elimination theorem, among them. These lead naturally to a discussion of interconnections between logic and algebra, where we try to demonstrate how they form two sides of the same coin.We envisage that the initial chapters could be used as a textbook for a graduate course, perhaps entitled Algebra and Substructural Logics. As the book progresses the first objective gains predominance over the second. Although the precise point of equilibrium would be difficult to specify, it is safe to say that we enter the technical part with the discussion of various completions of residuated structures. These include Dedekind-McNeille completions and canonical extensions. Completions are used later in investigating several finiteness properties such as the finite model property, generation of varieties by their finite members, and finite embeddability. The algebraic analysis of cut elimination that follows, also takes recourse to completions.Decidability of logics, equational and quasi-equational theories comes next, where we show how proof theoretical methods like cut elimination are preferable for small logics/theories, but semantic tools like Rabin's theorem work better for big ones. Then we turn to Glivenko's theorem, which says that a formula is an intuitionistic tautology if and only if its double negation is a classical one. We generalise it to the substructural setting, identifying for each substructural logic its Glivenko equivalence class with smallest and largest element. This is also where we begin investigating lattices of logics and varieties, rather than particular examples.We continue in this vein by presenting a number of results concerning minimal varieties/maximal logics. A typical theorem there says that for some given well-known variety its subvariety lattice has precisely such-and-such number of minimal members (where values for such-and-such include, but are not limited to, continuum, countably many and two). In the last two chapters we focus on the lattice of varieties corresponding to logics without contraction. In one we prove a negative result: that there are no nontrivial splittings in that variety. In the other, we prove a positive one: that semisimple varieties coincide with discriminator ones. Within the second, more technical part of the book another transition process may be traced. Namely, we begin with logically inclined technicalities and end with algebraically inclined ones. Here, perhaps, algebraic rendering of Glivenko theorems marks the equilibrium point, at least in the sense that finiteness properties, decidability and Glivenko theorems are of clear interest to logicians, whereas semisimplicity and discriminator varieties are universal algebra par exellence. It is for the reader to judge whether we succeeded in weaving these threads into a seamless fabric. This book: Considers both the algebraic and logical perspective within a common framework; Is written by experts in the area; Is easily accessible to graduate students and researchers from other fields; Includes results summarized in tables and diagrams to provide an overview of the area; Is useful as a textbook for a course in algebraic logic, with exercises and suggested research directions; And provides a concise introduction to the subject and leads directly to research topics. The ideas from algebra and logic are developed hand-in-hand and the connections are shown in every level.",2007,0,653,43,13,22,40,30,37,43,36,61,49,50
35f43533e1ada1e7984c1072e474f62d671ae5c2,,1998,0,1010,50,1,4,8,14,13,22,19,25,31,29
5fff88a6f7051834a326daf399834e3713758810,Introduction The quintic threefold Toric geometry Mirror symmetry constructions Hodge theory and Yukawa couplings Moduli spaces Gromov-Witten invariants Quantum cohomology Localization Quantum differential equations The mirror theorem Conclusion Singular varieties Physical theories Bibliography Index.,1999,3,917,82,14,27,34,29,27,31,36,44,39,61
4651fe8028f2467c7ada51df8178095db3a337a9,"Introduction Chapter 1: Preparatory material 1. Multiplicative sequences 2. Sheaves 3. Fibre bundles 4. Characteristic classes Chapter 2: The cobordism ring 5. Pontrjagin numbers 6. The ring /ss(/Omega) /oplus //Varrho 7. The cobordism ring /omega 8. The index of a 4k-dimensional manifold 9. The virtual index Chapter 3: The Todd genus 10. Definiton of the Todd genus 11. The virutal generalised Todd genus 12. The t-characteristic of a GL(q, C)-bundle 13. Split manifolds and splitting methods 14. Multiplicative properties of the Todd genus Chapter 4: The Riemann-Roch theorem for algebraic manifolds 15. Cohomology of Compact complex manifolds 16. Further properties of the (/chi)x characteristics 17. The virtual (/chi)x characteristics 18. Some fundamental theorems of Kodaira 19. The virtual (/chi)x characteristics for algebraic manifolds 20. The Riemann-Roch theorem for algebraic manifolds and complex analytic line bundles 21. The Riemann-Roch theorem for algebraic manifolds and complex analytic vector bundles Appendix 1 by R.L.E. Schwarzenberger 22. Applications of the Riemann-Roch theorem 23. The Riemann-Roch theorem of Grothendieck 24. The Grothendieck ring of continuous vector bundles 25. The Atijah-Singer index theorem 26. Integrality theorems for differentiable manifolds Appendix 2 by A. Borel A spectral sequence for complex analytic bundles Bibliography Index",1966,0,1163,93,2,3,6,8,10,13,15,15,18,16
54946e3ef95b6eb93d89346a768a9c73053870d0,"This is the first book to present an up-to-date and self-contained account of Algebraic Complexity Theory that is both comprehensive and unified. Requiring of the reader only some basic algebra and offering over 350 exercises, it is well-suited as a textbook for beginners at graduate level. With its extensive bibliography covering about 500 research papers, this text is also an ideal reference book for the professional researcher. The subdivision of the contents into 21 more or less independent chapters enables readers to familiarize themselves quickly with a specific topic, and facilitates the use of this book as a basis for complementary courses in other areas such as computer algebra.",1997,0,862,55,8,12,15,28,24,18,27,33,41,41
4e64fc252a1da5d5b92dccd4bf0c8cbb7909e846,"These notes describe a general procedure for calculating the Betti numbers of the projective quotient varieties that geometric invariant theory associates to reductive group actions on nonsingular complex projective varieties. These quotient varieties are interesting in particular because of their relevance to moduli problems in algebraic geometry. The author describes two different approaches to the problem. One is purely algebraic, while the other uses the methods of symplectic geometry and Morse theory, and involves extending classical Morse theory to certain degenerate functions.",1984,17,1011,135,1,5,2,8,2,7,4,10,7,16
0ef81dc9c352e8eefd783b28754c3398982e56ea,Introduction Classical theory of symmetric bilinear forms and quadratic forms: Bilinear forms Quadratic forms Forms over rational function fields Function fields of quadrics Bilinear and quadratic forms and algebraic extensions $u$-invariants Applications of the Milnor conjecture On the norm residue homomorphism of degree two Algebraic cycles: Homology and cohomology Chow groups Steenrod operations Category of Chow motives Quadratic forms and algebraic cycles: Cycles on powers of quadrics The Izhboldin dimension Application of Steenrod operations The variety of maximal totally isotropic subspaces Motives of quadrics Appendices Bibliography Notation Terminology.,2008,165,417,22,2,23,30,27,22,24,34,41,38,40
e1bb5e0f7258cd2bbc8260b7ebebdef3d86b7d08,"Algebraic Theory of Processes provides the first general and systematic introduction to the semantics of concurrent systems, a relatively new research area in computer science. It develops the mathematical foundations of the algebraic approach to the formal semantics of languages and applies these ideas to a particular semantic theory of distributed processes. The book is unique in developing three complementary views of the semantics of concurrent processes: a behavioral view where processes are deemed to be equivalent if they cannot be distinguished by any experiment; a denotational model where processes are interpreted as certain kinds of trees; and a proof-theoretic view where processes may be transformed into equivalent processes using valid equations or transformations. It is an excellent guide on how to reason about and relate behavioral, denotational, and proof-theoretical aspects of languages in general: all three views are developed for a sequence of increasingly complex algebraic languages for concurrency and in each case they are shown to be equivalent. Algebraic Theory of Processes is a valuable source of information for theoretical computer scientists, not only as an elegant and comprehensive introduction to the field but also in its discussion of the author's own theory of the behavioral semantics of processes (""testing equivalence"") and original results in example languages for distributed processes, It is self-contained; the problems addressed are motivated from the standpoint of computer science, and all the required algebraic concepts are covered. There are exercises at the end of each chapter.",1988,0,1082,80,5,16,24,46,47,65,64,82,76,55
d3d1a29dfd7e8201159e7f56ce1f5045bb72e7de,,1984,0,1128,95,1,3,5,9,11,16,15,19,18,13
20d59aee913e9bad6257645cc56dc3e494539705,Introduction 1. The resultant 2. The resolvent 3. General properties of modules 4. The inverse system.,1972,0,625,45,1,2,5,3,2,0,6,2,1,1
3a1aa32196a173ff7df3b4463026fc2cf3a3db00,"Cryptography and the Methodology of Provable Security.- Dynamical Systems Generated by Rational Functions.- Homotopy Methods for Equations over Finite Fields.- Three Constructions of Authentication/Secrecy Codes.- The Jacobi Model of an Elliptic Curve and Side-Channel Analysis.- Fast Point Multiplication on Elliptic Curves through Isogenies.- Interpolation of the Elliptic Curve Diffie-Hellman Mapping.- An Optimized Algebraic Method for Higher Order Differential Attack.- Fighting Two Pirates.- Copyright Control and Separating Systems.- Unconditionally Secure Homomorphic Pre-distributed Commitments.- A Class of Low-Density Parity-Check Codes Constructed Based on Reed-Solomon Codes with Two Information Symbols.- Relative Duality in MacWilliams Identity.- Good Expander Graphs and Expander Codes: Parameters and Decoding.- On the Covering Radius of Certain Cyclic Codes.- Unitary Error Bases: Constructions, Equivalence, and Applications.- Differentially 2-Uniform Cocycles - The Binary Case.- The Second and Third Generalized Hamming Weights of Algebraic Geometry Codes.- Error Correcting Codes over Algebraic Surfaces.- A Geometric View of Decoding AG Codes.- Performance Analysis of M-PSK Signal Constellations in Riemannian Varieties.- Improvements to Evaluation Codes and New Characterizations of Arf Semigroups.- Optimal 2-Dimensional 3-Dispersion Lattices.- On g-th MDS Codes and Matroids.- On the Minimum Distance of Some Families of ?2 k-Linear Codes.- Quasicyclic Codes of Index ? over F q Viewed as F q[x]-Submodules of F q ?[x]/?x m?1?.- Fast Decomposition of Polynomials with Known Galois Group.",1999,20,231,0,0,0,0,1,0,0,0,2,15,36
05ee3c8db4cd44e36cc08a71d279c2aa1ed55b14,"We consider the problem of information flow in networks. In particular, we relate the question whether a set of desired connections can be accommodated in a network to the problem of finding a point on a variety defined over a suitable field. This approach lends itself to the derivation of a number of theorems concerning the feasibility of a communication scenario involving failures.",2001,16,797,80,0,5,7,11,13,31,39,57,101,83
66628267fb55ca3821d7153447d46681f7972418,,2006,143,526,63,22,27,43,26,27,27,32,28,24,30
2132660aafb39e1675f6707fbb1888b50c6be3cf,"William S. Massey Professor Massey, born in Illinois in 1920, received his bachelor's degree from the University of Chicago and then served for four years in the U.S. Navy during World War II. After the War he received his Ph.D. from Princeton University and spent two additional years there as a post-doctoral research assistant. He then taught for ten years on the faculty of Brown University, and moved to his present position at Yale in 1960. He is the author of numerous research articles on algebraic topology and related topics. This book developed from lecture notes of courses taught to Yale undergraduate and graduate students over a period of several years.",1977,0,1105,64,4,1,2,1,0,4,0,1,3,2
4b02aaf96017ed8a088281eb7f474fef90bb26be,Introduction. Part I. The Topology of Algebraic Varieties: 1. The Lefschetz theorem on hyperplane sections 2. Lefschetz pencils 3. Monodromy 4. The Leray spectral sequence Part II. Variations of Hodge Structure: 5. Transversality and applications 6. Hodge filtration of hypersurfaces 7. Normal functions and infinitesimal invariants 8. Nori's work Part III. Algebraic Cycles: 9. Chow groups 10. Mumford' theorem and its generalisations 11. The Bloch conjecture and its generalisations References Index.,2002,0,773,38,2,1,12,12,17,26,28,31,30,42
3d273d185d3f805f866b22d485c135c5e7a3bd4b,,1993,7,879,92,0,1,6,10,12,14,16,21,16,21
526fe83e0eb8aec7eda4ab84311f20c0dc807e42,"A classical construction of stream ciphers is to combine several LFSRs and a highly non-linear Boolean function f . Their security is usually analysed in terms of correlation attacks, that can be seen as solving a system of multivariate linear equations, true with some probability. At ICISC’02 this approach is extended to systems of higher-degree multivariate equations, and gives an attack in 2 for Toyocrypt, a Cryptrec submission. In this attack the key is found by solving an overdefined system of algebraic equations. In this paper we show how to substantially lower the degree of these equations by multiplying them by well-chosen multivariate polynomials. Thus we are able to break Toyocrypt in 2 CPU clocks, with only 20 Kbytes of keystream, the fastest attack proposed so far. We also successfully attack the Nessie submission LILI-128, within 2 CPU clocks (not the fastest attack known). In general, we show that if the Boolean function uses only a small subset (e.g. 10) of state/LFSR bits, the cipher can be broken, whatever is the Boolean function used (worst case). Our new general algebraic attack breaks stream ciphers satisfying all the previously known design criteria in at most the square root of the complexity of the previously known generic attack.",2003,58,672,74,7,36,44,61,35,30,41,37,44,50
9f4e2e3332afbeef3989529dc386095aa3b43083,,1987,0,966,77,0,0,2,0,2,3,3,7,6,10
590bea27f256997456cee4ccaa0ebce50ee4ddc1,"A classical construction of stream ciphers is to combine several LFSRs and a highly non-linear Boolean function f. Their security is usually analysed in terms of correlation attacks, that can be seen as solving a system of multivariate linear equations, true with some probability. At ICISC'02 this approach is extended to systems of higher-degree multivariate equations, and gives an attack in 292 for Toyocrypt, a Cryptrec submission. In this attack the key is found by solving an overdefined system of algebraic equations. In this paper we show how to substantially lower the degree of these equations by multiplying them by well-chosen multivariate polynomials. Thus we are able to break Toyocrypt in 249 CPU clocks, with only 20 Kbytes of keystream, the fastest attack proposed so far. We also successfully attack the Nessie submission LILI-128, within 257 CPU clocks (not the fastest attack known). In general, we show that if the Boolean function uses only a small subset (e.g. 10) of state/LFSR bits, the cipher can be broken, whatever is the Boolean function used (worst case). Our new general algebraic attack breaks stream ciphers satisfying all the previously known design criteria in at most the square root of the complexity of the previously known generic attack.",2003,42,724,39,10,41,37,60,35,43,53,42,51,56
16bdf50188d5009cdbc7d95c6b01f3ded8bb7a84,Introduction Definition of vertex algebras Vertex algebras associated to Lie algebras Associativity and operator product expansion Applications of the operator product expansion Modules over vertex algebras and more examples Vertex algebra bundles Action of internal symmetries Vertex algebra bundles: Examples Conformal blocks I Conformal blocks II Free field realization I Free field realization II The Knizhnik-Zamolodchikov equations Solving the KZ equations Quantum Drinfeld-Sokolov reduction and $\mathcal{W}$-algebras Vertex Lie algebras and classical limits Vertex algebras and moduli spaces I Vertex algebras and moduli spaces II Chiral algebras Factorization Appendix Bibliography Index List of frequently used notation.,2000,241,748,58,5,8,16,31,34,30,40,29,47,25
c8f0c9f2785339b8351c8b35b2f1cff2ae312621,"We consider the problem of tracing algebraic curves by computer, using a numerical technique augmented by symbolic computations. In particular, all singularities are analyzed cOITectly. The methods presented find application in solid modeling and robotics.",1988,17,926,80,13,7,11,15,23,12,11,16,23,20
1bb74b2a6c57d083b7c9bf9569296683ff94dc16,"The book is meant to serve two purposes. The first and more obvious one is to present state of the art results in algebraic research into residuated structures related to substructural logics. The second, less obvious but equally important, is to provide a reasonably gentle introduction to algebraic logic. At the beginning, the second objective is predominant. Thus, in the first few chapters the reader will find a primer of universal algebra for logicians, a crash course in nonclassical logics for algebraists, an introduction to residuated structures, an outline of Gentzen-style calculi as well as some titbits of proof theory - the celebrated Hauptsatz, or cut elimination theorem, among them. These lead naturally to a discussion of interconnections between logic and algebra, where we try to demonstrate how they form two sides of the same coin. We envisage that the initial chapters could be used as a textbook for a graduate course, perhaps entitled Algebra and Substructural Logics. As the book progresses the first objective gains predominance over the second. Although the precise point of equilibrium would be difficult to specify, it is safe to say that we enter the technical part with the discussion of various completions of residuated structures. These include Dedekind-McNeille completions and canonical extensions. Completions are used later in investigating several finiteness properties such as the finite model property, generation of varieties by their finite members, and finite embeddability. The algebraic analysis of cut elimination that follows, also takes recourse to completions. Decidability of logics, equational and quasi-equational theories comes next, where we show how proof theoretical methods like cut elimination are preferable for small logics/theories, but semantic tools like Rabin's theorem work better for big ones. Then we turn to Glivenko's theorem, which says that a formula is an intuitionistic tautology if and only if its double negation is a classical one. We generalise it to the substructural setting, identifying for each substructural logic its Glivenko equivalence class with smallest and largest element. This is also where we begin investigating lattices of logics and varieties, rather than particular examples. We continue in this vein by presenting a number of results concerning minimal varieties/maximal logics. A typical theorem there says that for some given well-known variety its subvariety lattice has precisely such-and-such number of minimal members (where values for such-and-such include, but are not limited to, continuum, countably many and two). In the last two chapters we focus on the lattice of varieties corresponding to logics without contraction. In one we prove a negative result: that there are no nontrivial splittings in that variety. In the other, we prove a positive one: that semisimple varieties coincide with discriminator ones. Within the second, more technical part of the book another transition process may be traced. Namely, we begin with logically inclined technicalities and end with algebraically inclined ones. Here, perhaps, algebraic rendering of Glivenko theorems marks the equilibrium point, at least in the sense that finiteness properties, decidability and Glivenko theorems are of clear interest to logicians, whereas semisimplicity and discriminator varieties are universal algebra par exellence. It is for the reader to judge whether we succeeded in weaving these threads into a seamless fabric. - Considers both the algebraic and logical perspective within a common framework. - Written by experts in the area. - Easily accessible to graduate students and researchers from other fields. - Results summarized in tables and diagrams to provide an overview of the area. - Useful as a textbook for a course in algebraic logic, with exercises and suggested research directions. - Provides a concise introduction to the subject and leads directly to research topics. - The ideas from algebra and logic are developed hand-in-hand and the connections are shown in every level.",2007,5,406,41,5,7,24,31,29,34,38,46,49,44
2101e5811b162f3fde5a4e19623c4235f34ac418,"Algebraic K-theory describes a branch of algebra that centers about two functors. K0 and K1, which assign to each associative ring ? an abelian group K0? or K1? respectively. Professor Milnor sets out, in the present work, to define and study an analogous functor K2, also from associative rings to abelian groups. Just as functors K0 and K1 are important to geometric topologists, K2 is now considered to have similar topological applications. The exposition includes, besides K-theory, a considerable amount of related arithmetic.",1971,0,952,65,1,3,18,10,12,17,20,16,28,18
d0f1dadc9db7e67e22d61633af43c4e32ae77ad3,"Algebraic Cryptanalysis bridges the gap between a course in cryptography, and being able to read the cryptanalytic literature. This book is divided into three parts: Part One covers the process of turning a cipher into a system of equations; Part Two covers finite field linear algebra; Part Three covers the solution of Polynomial Systems of Equations, with a survey of the methods used in practice, including SAT-solvers and the methods of Nicolas Courtois. Topics include: Analytic Combinatorics, and its application to cryptanalysis The equicomplexity of linear algebra operations Graph coloring Factoring integers via the quadratic sieve, with its applications to the cryptanalysis of RSA Algebraic Cryptanalysis is designed for advanced-level students in computer science and mathematics as a secondary text or reference book for self-guided study. This book is suitable for researchers in Applied Abstract Algebra or Algebraic Geometry who wish to find more applied topics or practitioners working for security and communications companies.",2009,0,143,6,5,7,11,16,14,16,8,9,12,13
18c24ec71e4afb3109ee15d58d2a1b8fee53bd8f,"The aim of this book is to present fundamentals of algebraic specifications with respect to the following three aspects: fundamentals in the sense of a carefully motivated introduction to algebraic specifications, which is easy to understand for computer scientists and mathematicians; fundamentals in the sense of mathematical theories which are the basis for precise definitions, constructions, results, and correctness proofs; and fundamentals in the sense of concepts, which are introduced on a conceptual level and formalized in mathematical terms. The book is equally suitableas a text book for graduate courses and as a reference for researchers and system developers.",1985,0,919,33,1,3,13,18,25,45,46,63,44,48
da523d111d849f76e9afdb193f2e3758dae9ccb6,"A polynomial-time soft-decision decoding algorithm for Reed-Solomon codes is developed. This list-decoding algorithm is algebraic in nature and builds upon the interpolation procedure proposed by Guruswami and Sudan(see ibid., vol.45, p.1757-67, Sept. 1999) for hard-decision decoding. Algebraic soft-decision decoding is achieved by means of converting the probabilistic reliability information into a set of interpolation points, along with their multiplicities. The proposed conversion procedure is shown to be asymptotically optimal for a certain probabilistic model. The resulting soft-decoding algorithm significantly outperforms both the Guruswami-Sudan decoding and the generalized minimum distance (GMD) decoding of Reed-Solomon codes, while maintaining a complexity that is polynomial in the length of the code. Asymptotic analysis for alarge number of interpolation points is presented, leading to a geo- metric characterization of the decoding regions of the proposed algorithm. It is then shown that the asymptotic performance can be approached as closely as desired with a list size that does not depend on the length of the code.",2000,49,601,74,0,9,22,15,36,38,35,47,39,34
367b521d93b9ec91a020e4d654e75bb43b937b12,"Abstract : Explicit algebraic stress models that are valid for three-dimensional turbulent flows in noninertial frames are systematically derived from a hierarchy of second-order closure models. This represents a generalization of the model derived by Pope (J. Fluid Mech. 72, 331 (1975)) who based his analysis on the Launder, Reece and Rodi model restricted to two-dimensional turbulent flows in an inertial frame. The relationship between the new models and traditional algebraic stress models -- as well as anistropic eddy viscosity models -- is theoretically established. The need for regularization is demonstrated in an effort to explain why traditional algebraic stress models have failed in complex flows. It is also shown that these explicit algebraic stress models can shed new light on what second-order closure models predict for the equilibrium states of homogeneous turbulent flows and can serve as a useful alternative in practical computations.",1992,32,839,61,0,2,6,15,24,31,46,43,51,40
83061ff85d00edafadce62d9ed34fadedfd89967,"This paper reviews and compares constructive and algebraic approaches in the study of rough sets. In the constructive approach, one starts from a binary relation and defines a pair of lower and upper approximation operators using the binary relation. Different classes of rough set algebras are obtained from different types of binary relations. In the algebraic approach, one defines a pair of dual approximation operators and states axioms that must be satisfied by the operators. Various classes of rough set algebras are characterized by different sets of axioms. Axioms of approximation operators guarantee the existence of certain types of binary relations producing the same operators.",1998,68,745,39,0,0,1,1,1,5,14,28,29,37
5be0913fa18db2c8a4fcd66fc0dfb4ef340dbf9b,"MANY factors may influence the total milk yield of a single lactation, but the general shape of the curve, defined by the locus of weekly yield, remains substantially unchanged. Economically, the configuration of the curve is important, for the animal which produces milk at a moderate level steadily throughout her lactation is to be preferred to one which produces a great deal of milk at her peak but little thereafter (see Cersovsky1 for a review of the literature).",1967,3,955,106,0,1,1,0,0,1,0,1,0,1
ddccaf679f5149035c5644221a8fccccd1e23ff3,"I Preliminaries on Categories, Abelian Groups, and Homotopy.- x1 Categories and Functors.- x2 Abelian Groups (Exactness, Direct Sums, Free Abelian Groups).- x3 Homotopy.- II Homology of Complexes.- x1 Complexes.- x2 Connecting Homomorphism, Exact Homology Sequence.- x3 Chain-Homotopy.- x4 Free Complexes.- III Singular Homology.- x1 Standard Simplices and Their Linear Maps.- x2 The Singular Complex.- x3 Singular Homology.- x4 Special Cases.- x5 Invariance under Homotopy.- x6 Barycentric Subdivision.- x7 Small Simplices. Excision.- x8 Mayer-Vietoris Sequences.- IV Applications to Euclidean Space.- x1 Standard Maps between Cells and Spheres.- x2 Homology of Cells and Spheres.- x3 Local Homology.- x4 The Degree of a Map.- x5 Local Degrees.- x6 Homology Properties of Neighborhood Retracts in ?n.- x7 Jordan Theorem, Invariance of Domain.- x8 Euclidean Neighborhood Retracts (ENRs).- V Cellular Decomposition and Cellular Homology.- x1 Cellular Spaces.- x2 CW-Spaces.- x3 Examples.- x4 Homology Properties of CW-Spaces.- x5 The Euler-Poincare Characteristic.- x6 Description of Cellular Chain Maps and of the Cellular Boundary Homomorphism.- x7 Simplicial Spaces.- x8 Simplicial Homology.- VI Functors of Complexes.- x1 Modules.- x2 Additive Functors.- x3 Derived Functors.- x4 Universal Coefficient Formula.- x5 Tensor and Torsion Products.- x6 Horn and Ext.- x7 Singular Homology and Cohomology with General Coefficient Groups.- x8 Tensorproduct and Bilinearity.- x9 Tensorproduct of Complexes. Kunneth Formula.- x10 Horn of Complexes. Homotopy Classification of Chain Maps.- x11 Acyclic Models.- x12 The Eilenberg-Zilber Theorem. Kunneth Formulas for Spaces.- VII Products.- x1 The Scalar Product.- x2 The Exterior Homology Product.- x 3 The Interior Homology Product (Pontijagin Product).- x 4 Intersection Numbers in ?n.- x5 The Fixed Point Index.- x6 The Lefschetz-Hopf Fixed Point Theorem.- x7 The Exterior Cohomology Product.- x 8 The Interior Cohomology Product (?-Product).- x 9 ?-Products in Projective Spaces. Hopf Maps and Hopf Invariant.- x10 Hopf Algebras.- x11 The Cohomology Slant Product.- x12 The Cap-Product (?-Product).- x 13 The Homology Slant Product, and the Pontijagin Slant Product.- VIII Manifolds.- x1 Elementary Properties of Manifolds.- x2 The Orientation Bundle of a Manifold.- x3 Homology of Dimensions ? n in n-Manifolds.- x4 Fundamental Class and Degree.- x5 Limits.- x6 ?ech Cohomology of Locally Compact Subsets of ?n.- x7 Poincare-Lefschetz Duality.- x8 Examples, Applications.- x9 Duality in ?-Manifolds.- x10 Transfer.- x11 Thom Class, Thom Isomorphism.- x12 The Gysin Sequence. Examples.- x13 Intersection of Homology Classes.- Appendix: Kan- and ?ech-Extensions of Functors.- x1 Limits of Functors.- x2 Polyhedrons under a Space, and Partitions of Unity.- x3 Extending Functors from Polyhedrons to More General Spaces.",1972,0,983,52,0,1,10,9,7,5,13,11,23,14
da5e6420a063c9a315eb55695af6a26daa3d1ccb,"This paper is a survey of the second smallest eigenvalue of the Laplacian of a graph G, best-known as the algebraic connectivity of G, denoted a(G). Emphasis is given on classifications of bounds to algebraic connectivity as a function of other graph invariants, as well as the applications of Fiedler vectors (eigenvectors related to a(G)) on trees, on hard problems in graphs and also on the combinatorial optimization problems. Besides, limit points to a(G) and characterizations of extremal graphs to a(G) are described, especially those for which the algebraic connectivity is equal to the vertex connectivity.",2007,85,343,5,5,5,11,21,18,19,25,21,30,36
5d54560c6c88eecccf18cdce4255ce63cc91cb36,"The thermodynamic properties of 154 mineral end-members, 13 silicate liquid end-members and 22 aqueous fluid species are presented in a revised and updated data set. The use of a temperature-dependent thermal expansion and bulk modulus, and the use of high-pressure equations of state for solids and fluids, allows calculation of mineral-fluid equilibria to 100 kbar pressure or higher. A pressure-dependent Landau model for order-disorder permits extension of disordering transitions to high pressures, and, in particular, allows the alpha-beta quartz transition to be handled more satisfactorily. Several melt end- members have been included to enable calculation of simple phase equilibria and as a first stage in developing melt mixing models in NCKFMASH. The simple aqueous species density model has been extended to enable speciation calculations and mineral solubility determination involving minerals and aqueous species at high temperatures and pressures. The data set has also been improved by incorporation of many new phase equilibrium constraints, calorimetric studies and new measurements of molar volume, thermal expansion and compressibility. This has led to a significant improvement in the level of agreement with the available experimental phase equilibria, and to greater flexibility in calculation of complex mineral equilibria. It is also shown that there is very good agreement between the data set and the most recent available calorimetric data. kinetics which apply to determining directly the greatest majority of such equilibria in the laboratory, for forming solid solutions, and inclusion of aqueous and silicate melt species), and provides uncertainties especially at lower temperatures, as well as the diYculty of establishing reversals of reactions involving solid allowing the likely uncertainties on the results of thermodynamic calculations to be estimated. This is a solutions. The levels of precision and accuracy required of thermodynamic data in order to be able to forward- critical issue in that calculations using data sets should always involve uncertainty propagation to help evalu- model synthetic and natural mineral assemblages mean that the continuing upgrading and expansion of the ate the results. Because the experimental phase equilib- ria involve overlapping subsets of compositional space, data set by incorporation of new phase equilibrium constraints, calorimetry and new measurements of the derived thermodynamic data are highly correlated, and it is only the inclusion of the correlations which molar volume, thermal expansion and compressibility are more than justified. enables the reliable calculation of uncertainties on mineral reactions to be performed. Earlier work on mineral thermodynamic data sets for rock-forming minerals includes compilations of The thermodynamic data extraction involves using weighted least squares on the diVerent types of data",2004,350,4003,392,139,122,146,177,188,211,195,227,238,227
4cc29c9fe3c2d0381649465a7be3ab0a662fbcc4,"An improved dynamic programming algorithm is reported for RNA secondary structure prediction by free energy minimization. Thermodynamic parameters for the stabilities of secondary structure motifs are revised to include expanded sequence dependence as revealed by recent experiments. Additional algorithmic improvements include reduced search time and storage for multibranch loop free energies and improved imposition of folding constraints. An extended database of 151,503 nt in 955 structures? determined by comparative sequence analysis was assembled to allow optimization of parameters not based on experiments and to test the accuracy of the algorithm. On average, the predicted lowest free energy structure contains 73 % of known base-pairs when domains of fewer than 700 nt are folded; this compares with 64 % accuracy for previous versions of the algorithm and parameters. For a given sequence, a set of 750 generated structures contains one structure that, on average, has 86 % of known base-pairs. Experimental constraints, derived from enzymatic and flavin mononucleotide cleavage, improve the accuracy of structure predictions.",1999,117,3639,378,15,94,170,234,227,228,236,269,250,230
41078349f5d8c2f32518bcccf4a0cb1f577789a1,"In 1995, the International Association for the Properties of Water and Steam (IAPWS) adopted a new formulation called “The IAPWS Formulation 1995 for the Thermodynamic Properties of Ordinary Water Substance for General and Scientific Use”, which we abbreviate to IAPWS-95 formulation or IAPWS-95 for short. This IAPWS-95 formulation replaces the previous formulation adopted in 1984. This work provides information on the selected experimental data of the thermodynamic properties of water used to develop the new formulation, but information is also given on newer data. The article presents all details of the IAPWS-95 formulation, which is in the form of a fundamental equation explicit in the Helmholtz free energy. The function for the residual part of the Helmholtz free energy was fitted to selected data for the following properties: (a) thermal properties of the single-phase region (pρT) and of the vapor–liquid phase boundary (pσρ′ρ″T), including the phase-equilibrium condition (Maxwell criterion), and (b) t...",2002,7,3342,161,11,39,62,90,83,101,106,117,153,158
943c326ce0555c66d68b4955e1f7f489b7fc8f35,"We demonstrate in this work that the surface tension, water‐organic solvent, transfer‐free energies and the thermodynamics of melting of linear alkanes provide fundamental insights into the nonpolar driving forces for protein folding and protein binding reactions. We first develop a model for the curvature dependence of the hydrophobic effect and find that the macroscopic concept of interfacial free energy is applicable at the molecular level. Application of a well‐known relationship involving surface tension and adhesion energies reveals that dispersion forces play little or no net role in hydrophobic interactions; rather, the standard model of disruption of water structure (entropically driven at 25°C) is correct. The hydrophobic interaction is found, in agreement with the classical picture, to provide a major driving force for protein folding. Analysis of the melting behavior of hydrocarbons reveals that close packing of the protein interior makes only a small free energy contribution to folding because the enthalpic gain resulting from increased dispersion interactions (relative to the liquid) is countered by the freezing of side chain motion. The identical effect should occur in association reactions, which may provide an enormous simplification in the evaluation of binding energies. Protein binding reactions, even between nearly planar or concave/convex interfaces, are found to have effective hydrophobicities considerably smaller than the prediction based on macroscopic surface tension. This is due to the formation of a concave collar region that usually accompanies complex formation. This effect may preclude the formation of complexes between convex surfaces.",1991,34,5119,147,0,9,14,41,110,164,234,335,401,466
2267b5833829a82c017a6a9aa907a775d724748e,"A critical discussion is given of the use of local compositions for representation of excess Gibbs energies of liquid mixtures. A new equation is derived, based on Scott's two-liquid model and on an assumption of nonrandomness similar to that used by Wilson. For the same activity coefficients at infinite dilution, the Gibbs energy of mixing is calculated with the new equation as well as the equations of van Laar, Wilson, and Heil; these four equations give similar results for mixtures of moderate nonideality but they differ appreciably for strongly nonideal systems, especially for those with limited miscibility. The new equation contains a nonrandomness parameter α12 which makes it applicable to a large variety of mixtures. By proper selection of α12, the new equation gives an excellent representation of many types of liquid mixtures while other local composition equations appear to be limited to specific types. Consideration is given to prediction of ternary vapor-liquid and ternary liquid-liquid equilibria based on binary data alone.",1968,0,4945,138,0,0,2,1,2,4,2,7,3,11
0fa6ad7b5fcef5e18c205182a2096cf29cd8e775,,2007,0,2935,171,18,35,62,105,123,194,231,267,247,285
98d6e196543b719f98acb4aa1b2c28e09a63b84e,"A revised regular solution-type thermodynamic model for twelve-component silicate liquids in the system SiO2-TiO2-Al2O3-Fe2O3-Cr2O3-FeO-MgO-CaO-Na2O-K2O-P2O5-H2O is calibrated. The model is referenced to previously published standard state thermodynamic properties and is derived from a set of internally consistent thermodynamic models for solid solutions of the igneous rock forming minerals, including: (Mg,Fe2+,Ca)-olivines, (Na,Mg,Fe2+,Ca)M2 (Mg,Fe2+, Ti, Fe3+, Al)M1 (Fe3+, Al,Si)2TETO6-pyroxenes, (Na,Ca,K)-feldspars, (Mg,Fe2+) (Fe3+, Al, Cr)2O4-(Mg,Fe2+)2 TiO4 spinels and (Fe2+, Mg, Mn2+)TiO3-Fe2O3 rhombohedral oxides. The calibration utilizes over 2,500 experimentally determined compositions of silicate liquids coexisting at known temperatures, pressures and oxygen fugacities with apatite ±feldspar ±leucite ±olivine ±pyroxene ±quartz ±rhombohedral oxides ±spinel ±whitlockite ±water. The model is applicable to natural magmatic compositions (both hydrous and anhydrous), ranging from potash ankaratrites to rhyolites, over the temperature (T) range 900°–1700°C and pressures (P) up to 4 GPa. The model is implemented as a software package (MELTS) which may be used to simulate igneous processes such as (1) equilibrium or fractional crystallization, (2) isothermal, isenthalpic or isochoric assimilation, and (3) degassing of volatiles. Phase equilibria are predicted using the MELTS package by specifying bulk composition of the system and either (1) T and P, (2) enthalpy (H) and P, (3) entropy (S) and P, or (4) T and volume (V). Phase relations in systems open to oxygen are determined by directly specifying the fo2 or the T-P-fo2 (or equivalently H-P-fo2, S-P-fo2, T-V-fo2) evolution path. Calculations are performed by constrained minimization of the appropriate thermodynamic potential. Compositions and proportions of solids and liquids in the equilibrium assemblage are computed.",1995,118,2316,219,9,20,21,43,42,45,50,71,72,77
82b9504d35dc4739a1cd3bd2b4f90ac645659116,"Abstract A thermodynamic theory of “weak” ferromagnetism of α-Fe 2 O 3 , MnCO 3 and CoCO 3 is developed on the basis of landau's theory of phase transitions of the second kind. It is shown that the “weak” ferromagnetism is due to the relativistic spin-lattice and the magnetic dipole interactions. A strong dependence of the properties of “weak” ferromagnetics on the magnetic crystalline symmetry is noted and the behaviour of these ferromagnetics in a magnetic field is studied.",1958,6,3469,56,2,2,4,8,11,13,5,8,7,11
f00b6c02a96dab7a27ab3d69570360d85b5f04ba,,1948,0,4595,85,0,0,0,1,0,0,2,0,0,1
879e8d7778c0ab1479339fe29d3cc4ded78fe4e5,"The probability of a configuration is given in classical theory by the Boltzmann formula exp [— V/hT] where V is the potential energy of this configuration. For high temperatures this of course also holds in quantum theory. For lower temperatures, however, a correction term has to be introduced, which can be developed into a power series of h. The formula is developed for this correction by means of a probability function and the result discussed.",1932,0,5250,45,0,0,0,0,1,0,1,1,0,0
d40fb11afbe101d34f78b27bc448fea20dd3f23f,"A report about values for the entropy, molar volume, and for the enthalpy and Gibbs energy of formation for the elements and minerals and substances at 298.15 K.",1995,0,2584,150,81,69,65,63,58,68,75,65,67,119
5f9604e8f60943e397cebc107f06acc0a5169074,"Abstract : This volume, together with its companion, Selected Values of Thermodynamic Properties of the Elements, represents a complete revision of the work, Selected Values of Thermodynamic Properties of Metals and Alloys, by Hultgren, Orr, Anderson, and Kelley, published in 1963 by John Wiley and Sons, New York. The work should cover pertinent data available at the date printed on the first page of each system. Inspection will show that many or most of the selected values differ from the 1963 edition; many of the differences are substantial. This shows progress in measurement and, at the same time, hints of uncertainties still present.",1973,0,2730,101,4,3,16,32,37,37,48,52,59,65
3c2c03f28eac064da546f33d3372e1088471b3b0,"Recent advances in theoretical geochemistry permit calculation of the standard molal thermodynamic properties of a wide variety of minerals, gases, aqueous species, and reactions from 1 to 5000 bar and 0 to 1000°C. The SUPCRT92 software package facilitates practical application of these recent theories, equations, and data to define equilibrium constraints on geochemical processes in a wide variety of geologic systems. 
 
The SUPCRT92 package is composed of three interactive FORTRAN 77 programs, SUPCRT92, MPRONS92, and CPRONS92, and a sequential-access thermodynamic database, SPRONS92.DAT. The SUPCRT92 program reads or permits user-generation of its two input files, CON and RXN, retrieves data from the direct-access equivalent of SPRONS92.DAT, calculates the standard molal Gibbs free energy, enthalpy, entropy, heat capacity, and volume of each reaction specified on the RXN file through a range of conditions specified on the CON file, and writes the calculated reaction properties to the output TAB file and, optionally, to PLT files that facilitate their graphical depiction. Calculations can be performed along the liquid side of the H2O vaporization boundary by specifying either temperature (T) or pressure (P), and in the single-phase regions of fluid H2O by specifying either T and P, T and H2O density, T and log K, or P and log K. SPRONS92.DAT, which contains standard molal thermodynamic properties at 25°C and 1 bar, equation-of-state parameters, heat capacity coefficients, and phase transition data for approximately 500 minerals, gases, and aqueous species, can be augmented or otherwise modified using MPRONS92, and converted to its direct-access equivalent using CPRONS92.",1992,41,2070,104,11,22,31,38,25,42,52,32,56,57
0b9193580334b1529287aed6a91d1221f09126dd,"An updated genome‐scale reconstruction of the metabolic network in Escherichia coli K‐12 MG1655 is presented. This updated metabolic reconstruction includes: (1) an alignment with the latest genome annotation and the metabolic content of EcoCyc leading to the inclusion of the activities of 1260 ORFs, (2) characterization and quantification of the biomass components and maintenance requirements associated with growth of E. coli and (3) thermodynamic information for the included chemical reactions. The conversion of this metabolic network reconstruction into an in silico model is detailed. A new step in the metabolic reconstruction process, termed thermodynamic consistency analysis, is introduced, in which reactions were checked for consistency with thermodynamic reversibility estimates. Applications demonstrating the capabilities of the genome‐scale metabolic model to predict high‐throughput experimental growth and gene deletion phenotypic screens are presented. The increased scope and computational capability using this new reconstruction is expected to broaden the spectrum of both basic biology and applied systems biology studies of E. coli metabolism.",2007,86,1404,120,6,67,86,122,120,137,122,142,125,99
09cae7408f2ded45a53bcea325e8b3480a71009d,,1988,0,1893,158,1,11,25,46,47,47,59,69,54,54
c5cfe026fce5d5626f876dc8f465f2224c4a22d8,"The models recognize that ZrSiO4, ZrTiO4,  and TiSiO4, but not ZrO2 or TiO2, are independently variable phase components in zircon. Accordingly, the equilibrium controlling the Zr content of rutile coexisting with zircon is ZrSiO4 = ZrO2 (in rutile) +  SiO2. The equilibrium controlling the Ti content of zircon is either ZrSiO4 + TiO2 = ZrTiO4 + SiO2 or TiO2 + SiO2 = TiSiO4, depending whether Ti substitutes for Si or Zr. The Zr content of rutile thus depends on the activity of SiO2$$(a_{\text{SiO}_{2}})$$ as well as T, and the Ti content of zircon depends on $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ as well as T. New and published experimental data confirm the predicted increase in the Zr content of rutile with decreasing $$a_{\text{SiO}_{2}},$$ and unequivocally demonstrate that the Ti content of zircon increases with decreasing $$a_{\text{SiO}_{2}}$$. The substitution of Ti in zircon therefore is primarily for Si. Assuming a constant effect of P, unit $$a_{\text{ZrSiO}_{4}},$$ and that $$a_{\text{ZrO}_{2}}$$ and $$a_{\text{ZrTiO}_{4}}$$ are proportional to ppm Zr in rutile and ppm Ti in zircon, [log(ppm Zr-in-rutile) + log$$a_{\text{SiO}_{2}}$$] = A1 + B1/T(K) and  [log(ppm Ti-in-zircon) + log$$a_{\text{SiO}_{2}}$$ − log$$a_{\text{TiO}_{2}}$$] = A2 + B2/T, where the A and B are constants. The constants were derived from published and new data from experiments with $$a_{\text{SiO}_{2}}$$ buffered by either quartz or zircon + zirconia, from experiments with $$a_{\text{SiO}_{2}}$$ defined by the Zr content of rutile, and from well-characterized natural samples. Results are A1 = 7.420 ± 0.105;  B1 = −4,530 ± 111;  A2 = 5.711 ± 0.072;  B2 = −4,800 ± 86 with activity referenced to α-quartz and rutile at P and T of interest. The zircon thermometer may now be applied to rocks without quartz and/or rutile, and the rutile thermometer applied to rocks without quartz, provided that $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ are estimated. Maximum uncertainties introduced to zircon and rutile thermometry by unconstrained $$a_{\text{SiO}_{2}}$$ and $$a_{\text{TiO}_{2}}$$ can be quantitatively assessed and are ≈60 to 70°C at 750°C. A preliminary assessment of the dependence of the two thermometers on P predicts that an uncertainty of ±1 GPa introduces an additional uncertainty at 750°C of ≈50°C for the Ti-in-zircon thermometer and of ≈70 to 80°C for the Zr-in-rutile thermometer.",2007,14,1241,138,7,17,37,44,44,37,48,82,86,101
c3ccd84ec1d1453bc5766234a00c4d899dd5ca31,,1971,0,2415,102,17,11,31,37,38,42,52,48,62,48
683fb521e4e1a08954db267952f9878a37adbca9,"Abstract A numerical model for the simulation of sea ice circulation and thickness over a seasonal cycle is presented. This model is used to investigate the effects of ice dynamics on Arctic ice thickness and air-sea heat flux characteristics by carrying out several numerical simulations over the entire Arctic Ocean region. The essential idea in the model is to couple the dynamics to the ice thickness characteristics by allowing the ice interaction to become stronger as the ice becomes thicker and/or contains a lower areas percentage of thin ice. The dynamics in turn causes high oceanic heat losses in regions of ice divergence and reduced heat losses in regions of convergence. TO model these effects consistently the ice is considered to interact in a plastic manner with the plastic strength chosen to depend on the ice thickness and concentration. The thickness and concentration, in turn, evolve according to continuity equations which include changes in ice mass and percent of open water due to advection, ...",1979,0,1891,143,5,6,14,10,14,14,9,12,20,14
e821a03f897328fb0589f47c3014a1d4d1d87469,"The structural properties of free nanoclusters are reviewed. Special attention is paid to the interplay of energetic, thermodynamic, and kinetic factors in the explanation of cluster structures that are actually observed in experiments. The review starts with a brief summary of the experimental methods for the production of free nanoclusters and then considers theoretical and simulation issues, always discussed in close connection with the experimental results. The energetic properties are treated first, along with methods for modeling elementary constituent interactions and for global optimization on the cluster potential-energy surface. After that, a section on cluster thermodynamics follows. The discussion includes the analysis of solid-solid structural transitions and of melting, with its size dependence. The last section is devoted to the growth kinetics of free nanoclusters and treats the growth of isolated clusters and their coalescence. Several specific systems are analyzed.",2005,467,1338,17,6,45,76,90,87,67,103,109,109,101
f1ce2a74f0fa4bb31690efcfa62dabf9fee7ff58,"Abstract : Contents: Methods of evaluation, atomic weights, fundamental constants, symbols and units, general references, and properties of the elements.",1973,0,1981,29,0,5,18,29,47,43,65,68,63,44
f3f6b18aa6349d904a719688f605e836dd3a4ec8,"The behavior of any system at high enough temperatures approaches that of its classical counterpart. The probability of any configurational position is then proportional to exp—U/kT, with U the potential energy. Wigner has shown that quantum‐mechanical deviations, as the temperature is lowered, may be approximated by multiplication of this with 1–f, where f is a function proportional to h2 and having terms in T−2 and terms in T−3. This type of approximation is unsatisfactory for a system of many degrees of freedom, that is, one of many dependent molecules. For such a system it is shown that instead of Wigner's approximation we may replace the classical potential U in the exponent by U—kTf, where f is the same as Wigner's function.",1947,1,2049,149,0,2,2,0,2,0,0,0,1,0
bbe9992b30d352501716cf3978a79e6026704bc0,,1991,0,1557,9,0,17,45,63,59,78,57,65,82,63
fa8787db8f8d438383b10293a947a4e42f97eca2,"Motivated by the computation of scattering amplitudes at strong coupling, we consider minimal area surfaces in AdS5 which end on a null polygonal contour at the boundary. We map the classical problem of finding the surface into an SU(4) Hitchin system. The polygon with six edges is the first non-trivial example. For this case, we write an integral equation which determines the area as a function of the shape of the polygon. The equations are identical to those of the Thermodynamics Bethe Ansatz. Moreover, the area is given by the free energy of this TBA system. The high temperature limit of the TBA system can be exactly solved. It leads to an explicit expression for a special class of hexagonal contours.",2009,83,215,25,5,47,18,18,20,14,11,22,10,14
6f826d2863352923695839a35530043e6de2faed,,1947,0,1986,100,0,0,1,1,2,0,5,4,3,4
baf71c23b505f4f3bcf0e45d737087be13514116,"This study presents ISORROPIA II, a thermo- dynamic equilibrium model for the K + -Ca 2+ -Mg 2+ -NH + - Na + -SO 2 -NO 3 -Cl -H2O aerosol system. A comprehen- sive evaluation of its performance is conducted against water uptake measurements for laboratory aerosol and predictions of the SCAPE2 thermodynamic module over a wide range of atmospherically relevant conditions. The two models agree well, to within 13% for aerosol water content and total PM mass, 16% for aerosol nitrate and 6% for aerosol chloride and ammonium. Largest discrepancies were found under condi- tions of low RH, primarily from differences in the treatment of water uptake and solid state composition. In terms of com- putational speed, ISORROPIA II was more than an order of magnitude faster than SCAPE2, with robust and rapid con- vergence under all conditions. The addition of crustal species does not slow down the thermodynamic calculations (com- pared to the older ISORROPIA code) because of optimiza- tions in the activity coefficient calculation algorithm. Based on its computational rigor and performance, ISORROPIA II appears to be a highly attractive alternative for use in large scale air quality and atmospheric transport models.",2007,91,1058,40,4,8,16,14,33,49,42,54,73,100
1ebb1c0989f5e437e7222ad0ebd4808b62bd358a,,1985,0,1561,14,0,17,27,43,43,46,54,52,40,48
5588a41780e2f397d1d8f5b7195f3426ea863581,"The application of the conventional permeability equations to the study of biological membranes leads often to contradictions. It is shown that the equations generally used, based on two permeability coefficients—the solute permeability coefficient and the water permeability coefficient—are incompatible with the requirements of thermodynamics of irreversible processes. 
 
The inconsistencies are removed by a thermodynamic treatment, following the approach of Staverman, which leads to a three coefficient system taking into account the interactions: solute-solvent, solute-membrane and solvent-membrane. 
 
The equations derived here have been applied to various permeability measurements found in the literature, such as: the penetration of heavy water into animal cells, permeability of blood vessels, threshold concentration of plasmolysis and relaxation experiments with artificial membranes. 
 
It is shown how the pertinent coefficients may be derived from the experimental data and how to choose suitable conditions in order to obtain all the required information on the permeability of the membranes. 
 
The significance of these coefficients for the elucidation of membrane structure is pointed out.",1958,10,1848,72,0,1,4,0,3,5,8,7,14,14
d8c6a43ba76ee72c7dd3fc5911a3e565e1ad472d,The temporal correlations of thermodynamic concentration fluctuations have been measured in a chemically reactive system at equilibrium by observing fluctuations of the fluorescence of a reaction product. The experiment yields the chemical rate constants and diffusion coefficients and shows the coupling among them. Data are reported for binding of ethidium bromide to DNA.,1972,0,1553,62,0,4,8,4,5,4,6,4,2,4
ba5c27fbbb26094b3fc844bb0afb0491e0dfca83,"Abstract : Recommended values are provided for chemical thermodynamic properties of inorganic substances and for organic substances usually containing only one or two carbon atoms. Where available, values are given for the enthalpy of formation, Gibbs energy of formation, entropy, and heat capacity at 298.15 K (25 C), the enthalpy difference between 298.15 and 0 K and the enthalpy of formation at 0 K. All values are given in SI units and are for a standard state pressure of 100 000 pascal. This volume is a new collective edition of 'Selected Values of Chemical Thermodynamic Properties,' which was issued serially as National Bureau of Standards Technical Notes 270-1 (1965) to 270-8 (1981). Values are given for properties of gaseous, liquid and crystalline substances, for solutions in water, and for mixed aqueous and organic solutions. Values are not given for alloys or other solid solutions, fused salts or for substances of undefined composition. Compounds of the transuranium elements are not included. (Author)",1982,0,1286,70,3,2,4,6,6,8,12,12,13,10
a43f2b4724d9e4a38de0ef8f3bbf51337a71d5fe,An equation of state is proposed for the mixture of hard spheres based on an averaging process over the two results of the solution of the Percus–Yevick integral equation for the mixture of hard spheres. Compressibility and other equilibrium properties of the binary mixtures of hard spheres are calculated and they are compared with the related machine‐calculated (Monte Carlo and molecular dynamics) data. The comparison shows excellent agreement between the proposed equation of state and the machine‐calculated data.,1971,15,1621,11,1,4,4,4,4,3,12,4,3,12
3c8406f9435c0d9797509a02ac92a3344a5052d4,"We present, as a progress report, a revised and much enlarged version of the thermodynamic dataset given earlier (Holland & Powell, 1985). This new set includes data for 123 mineral and fluid end-members made consistent with over 200 P–T–XCO2–fO2 phase equilibrium experiments. Several improvements and advances have been made, in addition to the increased coverage of mineral phases: the data are now presented in three groups ranked according to reliability; a large number of iron-bearing phases has been included through experimental and, in some cases, natural Fe:Mg partitioning data; H2O and CO2 contents of cordierites are accounted for with the solution model of Kurepin (1985); simple Landau theory is used to model lambda anomalies in heat capacity and the Al/Si order–disorder behaviour in some silicates, and Tschermak-substituted end-members have been derived for iron and magnesium end-members of chlorite, talc, muscovite, biotite, pyroxene and amphibole. 
 
 
 
For the subset of data which overlap those of Berman (1988), it is encouraging to find both (1) very substantial agreement between the two sets of thermodynamic data and (2) that the two sets reproduce the phase equilibrium experimental brackets to a very similar degree of accuracy. The main differences in the two datasets involve size (123 as compared to 67 end-members), the methods used in data reduction (least squares as compared to linear programming), and the provision for estimation of uncertainties with this dataset. For calculations on mineral assemblages in rocks, we aim to maximize the information available from the dataset, by combining the equilibria from all the reactions which can be written between the end-members in the minerals. For phase diagram calculations, we calculate the compositions of complex solid solutions (together with P and T) involved in invariant, univariant and divariant assemblages. Moreover we strongly believe in attempting to assess the probable uncertainties in calculated equilibria and hence provide a framework for performing simple error propagation in all calculations in thermocalc, the computer program we offer for an effective use of the dataset and the calculation methods we advocate.",1990,203,1164,59,17,36,40,49,55,64,41,60,75,42
01f6eab4e5853e7b6085f3417fa6c637218f166a,"A computationally efficient and rigorous thermodynamic model that predicts the physical state and composition of inorganic atmospheric aerosol is presented. One of the main features of the model is the implementation of mutual deliquescence of multicomponent salt particles, which lowers the deliquescence point of the aerosol phase.The model is used to examine the behavior of four types of tropospheric aerosol (marine, urban, remote continental and non-urban continental), and the results are compared with the predictions of two other models currently in use. The results of all three models were generally in good agreement. Differences were found primarily in the mutual deliquescence humidity regions, where the new model predicted the existence of water, and the other two did not. Differences in the behavior (speciation and water absorbing properties) between the aerosol types are pointed out. The new model also needed considerably less CPU time, and always shows stability and robust convergence.",1998,36,1135,116,1,6,3,9,19,23,22,64,46,60
a27bff788a802242f1196246cae8b6e9e0e5dd6e,"v. 1. Elements O, HD, T, F, Cl, Br, I, He, Ne, Ar, Kr, Xe, Rn, S, N, P, and their compounds. pt. 1. Methods and computation. pt. 2. Tables v. 2. Elements C, Si, Ge, Sn, Pb, and their compounds. pt. 1. Methods and computation. pt. 2. Tables v. 3. Elements B, Al, Ga, In, Tl, Be, Mg, Ca, Sr, Ba, and their compounds. pt. 1. Methods and computation. pt. 2. Tables.",1994,0,1167,18,29,27,26,42,41,31,39,47,42,40
ff7302c1a174654b7912ccc18a6c2cbdc0320613,"We use the string hypothesis for the mirror theory to derive the Thermodynamic Bethe Ansatz equations for the AdS5 × S5 mirror model. We further demonstrate how these equations can be used to construct the associated Y-system recently discussed in the literature, putting particular emphasis on the assumptions and the range of validity of the corresponding construction.",2009,66,418,14,35,57,43,37,31,29,40,36,28,29
d891e37d4c5b6490a0832b9da4c027464db6641c,"The composition of the phase assemblage and pore solution of Portland cements hydrated between 0-60°C were modeled as a function of time and temperature. Results of thermodynamic modeling showed good agreement with experimental data gained at 5, 20, and 50°C. At 5 and 20°C, a similar phase assemblage was calculated to be present, while at ~50°C, thermodynamic calculations predicted conversion of ettringite and monocarbonate to monosulphate. Modeling showed that in Portland cements having an Al2O3/SO3 ratio of > 1.3 (bulk weight), above 50°C monosulphate and monocarbonate are present. In Portland cements containing less Al (Al2O3/SO3 < 1.3), above 50°C monosulphate and small amounts of ettringite are expected to persist. A good correlation between calculated porosity and measured compressive strength was observed.",2008,83,652,26,8,15,33,35,32,43,46,60,62,77
85071fc496825e30a60ede2eebde09717071b679,"Iron oxides occur ubiquitously in environmental, geological, planetary, and technological settings. They exist in a rich variety of structures and hydration states. They are commonly fine-grained (nanophase) and poorly crystalline. This review summarizes recently measured thermodynamic data on their formation and surface energies. These data are essential for calculating the thermodynamic stability fields of the various iron oxide and oxyhydroxide phases and understanding their occurrence in natural and anthropogenic environments. The competition between surface enthalpy and the energetics of phase transformation leads to the general conclusion that polymorphs metastable as micrometer-sized or larger crystals can often be thermodynamically stabilized at the nanoscale. Such size-driven crossovers in stability help to explain patterns of occurrence of different iron oxides in nature.",2008,29,599,16,11,24,34,49,47,53,39,52,50,47
71ea4c3aeb896a8af6eccce9eba9a0a6cfe7a6d4,"The production of functional molecular architectures through self-assembly is commonplace in biology, but despite advances, it is still a major challenge to achieve similar complexity in the laboratory. Self-assembled structures that are reproducible and virtually defect free are of interest for applications in three-dimensional cell culture, templating, biosensing and supramolecular electronics. Here, we report the use of reversible enzyme-catalysed reactions to drive self-assembly. In this approach, the self-assembly of aromatic short peptide derivatives provides a driving force that enables a protease enzyme to produce building blocks in a reversible and spatially confined manner. We demonstrate that this system combines three features: (i) self-correction--fully reversible self-assembly under thermodynamic control; (ii) component-selection--the ability to amplify the most stable molecular self-assembly structures in dynamic combinatorial libraries; and (iii) spatiotemporal confinement of nucleation and structure growth. Enzyme-assisted self-assembly therefore provides control in bottom-up fabrication of nanomaterials that could ultimately lead to functional nanostructures with enhanced complexities and fewer defects.",2009,30,412,1,11,35,32,41,32,44,41,32,35,33
dbe4b7e3dcfb57ab565bbd90474e77e154e3b2e8,"Improved thermodynamic parameters for prediction of RNA duplex formation are derived from optical melting studies of 90 oligoribonucleotide duplexes containing only Watson-Crick base pairs. To test end or base composition effects, new sets of duplexes are included that have identical nearest neighbors, but different base compositions and therefore different ends. Duplexes with terminal GC pairs are more stable than duplexes with the same nearest neighbors but terminal AU pairs. Penalizing terminal AU base pairs by 0.45 kcal/mol relative to terminal GC base pairs significantly improves predictions of DeltaG degrees37 from a nearest-neighbor model. A physical model is suggested in which the differential treatment of AU and GC ends accounts for the dependence of the total number of Watson-Crick hydrogen bonds on the base composition of a duplex. On average, the new parameters predict DeltaG degrees37, DeltaH degrees, DeltaS degrees, and TM within 3.2%, 6.0%, 6.8%, and 1.3 degreesC, respectively. These predictions are within the limit of the model, based on experimental results for duplexes predicted to have identical thermodynamic parameters.",1998,135,1021,48,0,6,23,18,20,23,34,44,58,60
e1b96d80e424f4a1976f4abd07eabef6fa52afc7,"The first use of microalgae by humans dates back 2000 years to the Chinese, who used Nostoc to survive during famine. However, microalgal biotechnology only really began to develop in the middle of the last century. Nowadays, there are numerous commercial applications of microalgae. For example, (i) microalgae can be used to enhance the nutritional value of food and animal feed owing to their chemical composition, (ii) they play a crucial role in aquaculture and (iii) they can be incorporated into cosmetics. Moreover, they are cultivated as a source of highly valuable molecules. For example, polyunsaturated fatty acid oils are added to infant formulas and nutritional supplements and pigments are important as natural dyes. Stable isotope biochemicals help in structural determination and metabolic studies. Future research should focus on the improvement of production systems and the genetic modification of strains. Microalgal products would in that way become even more diversified and economically competitive.",2006,105,3421,171,1,15,45,71,134,161,217,290,284,361
8a7f8f9227f8b843fbbe6c0c991670bbc8f9832f,"Using bank-level data for 80 countries in the year's 1988-95, this article shows that differences in interest margins and bank profitability reflect a variety of determinants: bank characteristics, macroeconomic conditions, explicit and implicit bank taxation, deposit insurance regulation, overall financial structure, and underlying legal and institutional indicators. A larger ratio of bank assets to gross domestic product and a lower market concentration ratio lead to lower margins and profits, controlling for differences in bank activity, leverage, and the macroeconomic environment. Foreign banks have higher margins and profits than domestic banks in developing countries, while the opposite holds in industrial countries. Also, there is evidence that the corporate tax burden is fully passed onto bank customers, while higher reserve requirements are not, especially in developing countries.",1999,53,2226,211,10,6,26,42,32,35,51,51,49,54
1ffbf5ada9068d39951c01689978ca5adae9eed2,Entrepreneurship has been the engine propelling much of the growth of the business sector as well as a driving force behind the rapid expansion of the social sector. This article offers a comparative analysis of commercial and social entrepreneurship using a prevailing analytical model from commercial entrepreneurship. The analysis highlights key similarities and differences between these two forms of entrepreneurship and presents a framework on how to approach the social entrepreneurial process more systematically and effectively. We explore the implications of this analysis of social entrepreneurship for both practitioners and researchers.,2006,30,2040,160,8,13,31,48,81,104,117,133,160,163
3e2ab599c6a7fb19db5e3ccd8c2aaf5ea61678ba,"Many microorganisms, especially bacteria, produce biosurfactants when grown on water-immiscible substrates. Biosurfactants are more effective, selective, environmentally friendly, and stable than many synthetic surfactants. Most common biosurfactants are glycolipids in which carbohydrates are attached to a long-chain aliphatic acid, while others, like lipopeptides, lipoproteins, and heteropolysaccharides, are more complex. Rapid and reliable methods for screening and selection of biosurfactant-producing microorganisms and evaluation of their activity have been developed. Genes involved in rhamnolipid synthesis (rhlAB) and regulation (rhlI and rhlR) in Pseudomonas aeruginosa are characterized, and expression of rhlAB in heterologous hosts is discussed. Genes for surfactin production (sfp, srfA, and comA) in Bacillus spp. are also characterized. Fermentative production of biosurfactants depends primarily on the microbial strain, source of carbon and nitrogen, pH, temperature, and concentration of oxygen and metal ions. Addition of water-immiscible substrates to media and nitrogen and iron limitations in the media result in an overproduction of some biosurfactants. Other important advances are the use of water-soluble substrates and agroindustrial wastes for production, development of continuous recovery processes, and production through biotransformation. Commercialization of biosurfactants in the cosmetic, food, health care, pulp- and paper-processing, coal, ceramic, and metal industries has been proposed. However, the most promising applications are cleaning of oil-contaminated tankers, oil spill management, transportation of heavy crude oil, enhanced oil recovery, recovery of crude oil from sludge, and bioremediation of sites contaminated with hydrocarbons, heavy metals, and other pollutants. Perspectives for future research and applications are also discussed.",1997,229,1403,95,1,8,12,17,20,26,24,36,34,36
f42239347b34560fdedad44f302e52cb7144b1e4,"Abstract Surfactants are surface-active compounds capable of reducing surface and interfacial tension at the interfaces between liquids, solids and gases, thereby allowing them to mix or disperse readily as emulsions in water or other liquids. The enormous market demand for surfactants is currently met by numerous synthetic, mainly petroleum-based, chemical surfactants. These compounds are usually toxic to the environment and non-biodegradable. They may bio-accumulate and their production, processes and by-products can be environmentally hazardous. Tightening environmental regulations and increasing awareness for the need to protect the ecosystem have effectively resulted in an increasing interest in biosurfactants as possible alternatives to chemical surfactants. Biosurfactants are amphiphilic compounds of microbial origin with considerable potential in commercial applications within various industries. They have advantages over their chemical counterparts in biodegradability and effectiveness at extreme temperature or pH and in having lower toxicity. Biosurfactants are beginning to acquire a status as potential performance-effective molecules in various fields. At present biosurfactants are mainly used in studies on enhanced oil recovery and hydrocarbon bioremediation. The solubilization and emulsification of toxic chemicals by biosurfactants have also been reported. Biosurfactants also have potential applications in agriculture, cosmetics, pharmaceuticals, detergents, personal care products, food processing, textile manufacturing, laundry supplies, metal treatment and processing, pulp and paper processing and paint industries. Their uses and potential commercial applications in these fields are reviewed.",2000,187,1459,31,0,5,21,22,28,32,38,56,79,67
62cf465bbf50dae7906de5492d721452086c9744,"Abstract. In response to the rapidly growing field of proteomics, the use of recombinant proteins has increased greatly in recent years. Recombinant hybrids containing a polypeptide fusion partner, termed affinity tag, to facilitate the purification of the target polypeptides are widely used. Many different proteins, domains, or peptides can be fused with the target protein. The advantages of using fusion proteins to facilitate purification and detection of recombinant proteins are well-recognized. Nevertheless, it is difficult to choose the right purification system for a specific protein of interest. This review gives an overview of the most frequently used and interesting systems: Arg-tag, calmodulin-binding peptide, cellulose-binding domain, DsbA, c-myc-tag, glutathione S-transferase, FLAG-tag, HAT-tag, His-tag, maltose-binding protein, NusA, S-tag, SBP-tag, Strep-tag, and thioredoxin.",2002,174,1307,53,2,7,38,50,59,58,75,77,97,81
1204f9d5f1a1ad3f78d44cd06712e033305f9a96,"The commercial culture of microalgae is now over 30 years old with the main microalgal species grown being Chlorella and Spirulina for health food, Dunaliella salina for β-carotene, Haematococcus pluvialis for astaxanthin and several species for aquaculture. The culture systems currently used to grow these algae are generally fairly unsophisticated. For example, Dunaliella salina is cultured in large (up to approx. 250 ha) shallow open-air ponds with no artificial mixing. Similarly, Chlorella and Spirulina also are grown outdoors in either paddle-wheel mixed ponds or circular ponds with a rotating mixing arm of up to about 1 ha in area per pond. The production of microalgae for aquaculture is generally on a much smaller scale, and in many cases is carried out indoors in 20-40 1 carboys or in large plastic bags of up to approximately 1000 1 in volume. More recently, a helical tubular photobioreactor system, the BIOCOIL™, has been developed which allows these algae to be grown reliably outdoors at high cell densities in semi-continuous culture. Other closed photobioreactors such as fiat panels are also being developed. The main problem facing the commercialisation of new microalgae and microalgal products is the need for closed culture systems and the fact that these are very capital intensive. The high cost of microalgal culture systems relates to the need for light and the relatively slow growth rate of the algae. Although this problem has been avoided in some instances by growing the algae heterotrophically, not all algae or algal products can be produced this way.",1999,32,1184,73,0,0,13,9,16,8,14,22,16,39
b60577c146c6d3492d8200dafe462f89864f060d,"Summary The foregoing examples illustrate how the theory developed here may be employed to make estimates concerning the condition of a commercial marine fishery. The examples employed, although having perhaps as complete information as any available for this purpose, leave something to be desired. In particular, in both of these examples, very little or no data are available concerning intensity of fishing and abundance for the early period of development of the fishery, well before the maximum catches are reached. A great deal of precision would be added to the estimate if such information were available. We may emphasize, therefore, the desirability of obtaining detailed information on the total catch and catch-per-unit-of-effort from as early in the development of a commercial fishery as may be possible. Measurements of fishing mortality rates at more than one level of population would also be desirable, since they would make possible verification of the adequacy of the form of equation (13a) for describing the changes in population under the joint influences of growth and fishing. In order to apply the theory developed here to the tropical tuna fishery, it will be necessary to compile statistics of catch, abundance and intensity of fishing over a considerable series of years, beginning as early in the history of the fishery as possible. This task is well under way. It will also be necessary to obtain some estimate of the rate of fishing mortality, or to devise some other means of estimating the constant k 2 . Estimation of fishing mortality from tagging promises to be a difficult problem for the tunas. Exploration of other means of obtaining the relationship between U and P appears, therefore, to constitute an important line of investigation.",1991,8,1382,84,16,16,8,12,19,18,15,25,24,27
7562d6af07c5449a681939842f99809e22a4b53e,"This is a textbook on the structural analysis and design of highway pavements. It presents the theory of pavement design and reviews the methods developed by several organizations, such as the American Association of State Highway and Transportation Officials (AASHTO), the Asphalt Institute (AI), and the Portland Cement Association (PCA). It can be used for an undergraduate course by skipping the appendices or as an advanced graduate course by including them. The book is organized in 13 chapters. Chapter 1 introduces the historical development of pavement design, the major road tests, the various design factors, and the differences in design concepts among highway pavements, airport pavements, and railroad trackbeds. Chapter 2 discusses stresses and strains in flexible pavements. Chapter 3 presents the KENLAYER computer program, based on Burmister's layered theory, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis on the effect of various factors on pavement responses. Chapter 4 discusses stresses and deflections in rigid pavements due to curling, loading, and friction, as well as the design of dowels and joints. Influence charts for determining stresses and deflections are also presented. Chapter 5 presents the KENSLABS computer program, based on the finite element method, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis. Chapter 6 discusses the concept of equivalent single-wheel and single-axle loads and the prediction of traffic. Chapter 7 describes the material characterization for mechanistic-empirical methods of pavement design including the determination of resilient modulus, fatigue and permanent deformation properties, and the modulus of subgrade reaction. Chapter 8 outlines the subdrainage design including general principles, drainage materials, and design procedures. Chapter 9 discusses pavement performance including distress, serviceability, skid resistance, nondestructive testing, and the evaluation of pavement performance. Chapter 10 illustrates the reliability concept of pavement design in which the variabilities of traffic, material, and geometric parameters are all taken into consideration. A probabilistic procedure, developed by Rosenblueth, is described and two probabilistic computer programs including VESYS for flexible pavements and PMRPD for rigid pavements are discussed. Chapter 11 outlines an idealistic mechanistic method of flexible pavement design and presents in detail the AI method and the AASHTO method, as well as the design of flexible pavement shoulders. Chapter 12 outlines an idealistic mechanistic method of rigid pavement design and presents in detail the PCA method and the AASHTO method. The design of continuous reinforced concrete pavements and rigid pavement shoulders is also included. Chapter 13 outlines the design of overlay on both flexible and rigid pavements including the AASHTO, AI, and PCA procedures. An Author Index and a Subject Index are provided.",1992,0,2550,357,0,0,4,7,11,10,22,17,29,31
c03ecc07b6ae40e0a0b5ec27bb7a65b4da1d5ebc,"List of Tables. List of Examples. Preface. 1 The Kinematics and Dynamics of Aircraft Motion. 1.1 Introduction. 1.2 Vector Kinematics. 1.3 Matrix Analysis of Kinematics. 1.4 Geodesy, Earth's Gravitation, Terrestrial Navigation. 1.5 Rigid-Body Dynamics. 1.6 Summary. 2 Modeling the Aircraft. 2.1 Introduction. 2.2 Basic Aerodynamics. 2.3 Aircraft Forces and Moments. 2.4 Static Analysis. 2.5 The Nonlinear Aircraft Model. 2.6 Linear Models and the Stability Derivatives. 2.7 Summary. 3 Modeling, Design, and Simulation Tools. 3.1 Introduction. 3.2 State-Space Models. 3.3 Transfer Function Models. 3.4 Numerical Solution of the State Equations. 3.5 Aircraft Models for Simulation. 3.6 Steady-State Flight. 3.7 Numerical Linearization. 3.8 Aircraft Dynamic Behavior. 3.9 Feedback Control. 3.10 Summary. 4 Aircraft Dynamics and Classical Control Design. 4.1 Introduction. 4.2 Aircraft Rigid-Body Modes. 4.3 The Handling-Qualities Requirements. 4.4 Stability Augmentation. 4.5 Control Augmentation Systems. 4.6 Autopilots. 4.7 Nonlinear Simulation. 4.8 Summary. 5 Modern Design Techniques. 5.1 Introduction. 5.2 Assignment of Closed-Loop Dynamics. 5.3 Linear Quadratic Regulator with Output Feedback. 5.4 Tracking a Command. 5.5 Modifying the Performance Index. 5.6 Model-Following Design. 5.7 Linear Quadratic Design with Full State Feedback. 5.8 Dynamic Inversion Design. 5.9 Summary. 6 Robustness and Multivariable Frequency-Domain Techniques. 6.1 Introduction. 6.2 Multivariable Frequency-Domain Analysis. 6.3 Robust Output-Feedback Design. 6.4 Observers and the Kalman Filter. 6.5 LQG/Loop-Transfer Recovery. 6.6 Summary. 7 Digital Control. 7.1 Introduction. 7.2 Simulation of Digital Controllers. 7.3 Discretization of Continuous Controllers. 7.4 Modified Continuous Design. 7.5 Implementation Considerations. 7.6 Summary. Appendix A F-16 Model. Appendix B Software. Index.",1992,0,2821,213,3,4,5,14,17,22,23,25,40,40
83a5c1cfb2be4dc4ff62c57309d89bc2dd01ad12,"* Design - A Separate Discipline * Overview of the Design Process * Sizing from a Conceptual Sketch * Airfoil and Geometry Selection * Thrust-to-Weight Ratio and Wing Loading * Initial Sizing * Configuration Layout and Loft * Special Considerations in Configuration Layout * Crew Station, Passengers, and Payload * Propulsion and Fuel System Integration * Landing Gear and Subsystems * Intermission: Step-by-Step Development of a New Design * Aerodynamics * Propulsion * Structures and Loads * Weights * Stability, Control, and Handling Qualities * Performance and Flight Mechanics * Cost Analysis * Sizing and Trade Studies * Design of Unique Aircraft Concepts * Conceptual Design Examples * Appendix A: Unit Conversion * Appendix B: Standard Atmosphere.",1989,27,2376,239,0,0,1,12,3,7,6,11,19,26
7c854f6ad964decd450acefbc52ea96b7570dce7,"A method has been described for the isolation of DNA from micro-organisms which yields stable, biologically active, highly polymerized preparations relatively free from protein and RNA. Alternative methods of cell disruption and DNA isolation have been described and compared. DNA capable of transforming homologous strains has been used to test various steps in the procedure and preparations have been obtained possessing high specific activities. Representative samples have been characterized for their thermal stability and sedimentation behaviour.",1961,22,9927,226,5,37,58,86,111,150,154,191,175,211
37126de1bbf70ba0349d8b72af8bb9357c3b12cf,"Today's surface ocean is saturated with respect to calcium carbonate, but increasing atmospheric carbon dioxide concentrations are reducing ocean pH and carbonate ion concentrations, and thus the level of calcium carbonate saturation. Experimental evidence suggests that if these trends continue, key marine organisms—such as corals and some plankton—will have difficulty maintaining their external calcium carbonate skeletons. Here we use 13 models of the ocean–carbon cycle to assess calcium carbonate saturation under the IS92a ‘business-as-usual’ scenario for future emissions of anthropogenic carbon dioxide. In our projections, Southern Ocean surface waters will begin to become undersaturated with respect to aragonite, a metastable form of calcium carbonate, by the year 2050. By 2100, this undersaturation could extend throughout the entire Southern Ocean and into the subarctic Pacific Ocean. When live pteropods were exposed to our predicted level of undersaturation during a two-day shipboard experiment, their aragonite shells showed notable dissolution. Our findings indicate that conditions detrimental to high-latitude ecosystems could develop within decades, not centuries as suggested previously.",2005,63,4073,358,11,62,108,153,218,253,259,337,360,335
f32b0b1ea1e0282ab5c0f8ca23c295f3c3b69198,"Molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes (particularly so among microorganisms). Consequently, the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level. Molecular comparisons show that life on this planet divides into three primary groupings, commonly known as the eubacteria, the archaebacteria, and the eukaryotes. The three are very dissimilar, the differences that separate them being of a more profound nature than the differences that separate typical kingdoms, such as animals and plants. Unfortunately, neither of the conventionally accepted views of the natural relationships among living systems--i.e., the five-kingdom taxonomy or the eukaryote-prokaryote dichotomy--reflects this primary tripartite division of the living world. To remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a ""domain."" Life on this planet would then be seen as comprising three domains, the Bacteria, the Archaea, and the Eucarya, each containing two or more kingdoms. (The Eucarya, for example, contain Animalia, Plantae, Fungi, and a number of others yet to be defined). Although taxonomic structure within the Bacteria and Eucarya is not treated herein, Archaea is formally subdivided into the two kingdoms Euryarchaeota (encompassing the methanogens and their phenotypically diverse relatives) and Crenarchaeota (comprising the relatively tight clustering of extremely thermophilic archaebacteria, whose general phenotype appears to resemble most the ancestral phenotype of the Archaea.",1990,50,5773,374,4,57,89,118,106,112,143,127,151,146
05509d207766c7bac327e55ddca03212c48faca0,"Multicellular organisms live, by and large, harmoniously with microbes. The cornea of the eye of an animal is almost always free of signs of infection. The insect flourishes without lymphocytes or antibodies. A plant seed germinates successfully in the midst of soil microbes. How is this accomplished? Both animals and plants possess potent, broad-spectrum antimicrobial peptides, which they use to fend off a wide range of microbes, including bacteria, fungi, viruses and protozoa. What sorts of molecules are they? How are they employed by animals in their defence? As our need for new antibiotics becomes more pressing, could we design anti-infective drugs based on the design principles these molecules teach us?",2002,101,7070,271,57,133,191,218,263,241,295,347,382,416
6dd84bd0edbafc92d03c57d6bea2d4e07093ea80,,1963,0,4357,459,0,4,4,5,3,8,5,12,9,11
816d9b199d07340238fd263a6369271c556b6ca8,"Interactions between organisms are a major determinant of the distribution and abundance of species. Ecology textbooks (e.g., Ricklefs 1984, Krebs 1985, Begon et al. 1990) summarise these important interactions as intra- and interspecific competition for abiotic and biotic resources, predation, parasitism and mutualism. Conspicuously lacking from the list of key processes in most text books is the role that many organisms play in the creation, modification and maintenance of habitats. These activities do not involve direct trophic interactions between species, but they are nevertheless important and common. The ecological literature is rich in examples of habitat modification by organisms, some of which have been extensively studied (e.g. Thayer 1979, Naiman et al. 1988).",1994,89,5139,165,2,2,14,27,39,61,56,63,64,92
af47ec845f1b60e2117bb1a093de224429786e8c,"Functional partnerships between proteins are at the core of complex cellular phenotypes, and the networks formed by interacting proteins provide researchers with crucial scaffolds for modeling, data reduction and annotation. STRING is a database and web resource dedicated to protein–protein interactions, including both physical and functional interactions. It weights and integrates information from numerous sources, including experimental repositories, computational prediction methods and public text collections, thus acting as a meta-database that maps all interaction evidence onto a common set of genomes and proteins. The most important new developments in STRING 8 over previous releases include a URL-based programming interface, which can be used to query STRING from other resources, improved interaction prediction via genomic neighborhood in prokaryotes, and the inclusion of protein structures. Version 8.0 of STRING covers about 2.5 million proteins from 630 organisms, providing the most comprehensive view on protein–protein interactions currently available. STRING can be reached at http://string-db.org/.",2008,39,2214,201,0,96,239,282,225,195,179,201,221,124
36e2c49a23cb62bc5f47c82603373970e975b87d,"The rhizosphere encompasses the millimeters of soil surrounding a plant root where complex biological and ecological processes occur. This review describes recent advances in elucidating the role of root exudates in interactions between plant roots and other plants, microbes, and nematodes present in the rhizosphere. Evidence indicating that root exudates may take part in the signaling events that initiate the execution of these interactions is also presented. Various positive and negative plant-plant and plant-microbe interactions are highlighted and described from the molecular to the ecosystem scale. Furthermore, methodologies to address these interactions under laboratory conditions are presented.",2006,209,3163,137,9,38,63,86,108,122,169,242,244,261
dddb532a1fa61531bf24b3a4bd0f9a81b747aa43,"Physical ecosystem engineers are organisms that directly or indirectly control the availability of resources to other organisms by causing physical state changes in biotic or abiotic materials. Physical ecosystem engineering by organisms is the physical modification, maintenance, or creation of habitats. Ecological effects of engineers on many other species occur in virtually all ecosystems because the physical state changes directly create nonfood resources such as living space, directly control abiotic resources, and indirectly modulate abiotic forces that, in turn, affect resource use by other organisms. Trophic interactions and resource competition do not constitute engineering. Engineering can have significant or trivial effects on other species, may involve the physical structure of an organism (like a tree) or structures made by an organism (like a beaver dam), and can, but does not invariably, have feedback effects on the engineer. We argue that engineering has both negative and positive effects on species richness and abundances at small scales, but the net effects are probably positive at larger scales encompassing engineered and nonengineered environments in ecological and evolutionary space and time. Models of the population dynamics of engineers suggest that the engineer/habitat equilibrium is often, but not always, locally stable and may show long-term cycles, with potential ramifications for community and ecosystem stability. As yet, data adequate to parameterize such a model do not exist for any engineer species. Because engineers control flows of energy and materials but do not have to participate in these flows, energy, mass, and stoichiometry do not appear to be useful in predicting which engineers will have big effects. Empirical observations suggest some potential generalizations about which species will be important engineers in which ecosystems. We point out some of the obvious, and not so obvious, ways in which engineering and trophic relations interact, and we call for greater research on physical ecosystem engineers, their impacts, and their interface with trophic relations.",1997,69,2035,73,2,6,26,28,34,41,55,59,45,101
2105cf67e643f73b0812ddd3252d23b812330fb3,"Investigating diversity in asexual organisms using molecular markers involves the assignment of individuals to clonal lineages and the subsequent analysis of clonal diversity. Assignment is possible using a distance matrix in combination with a user-specified threshold, defined as the maximum distance between two individuals that are considered to belong to the same clonal lineage. Analysis of clonal diversity requires tests for differences in diversity and clonal composition between populations. We developed two programs, GENOTYPE and GENODIVE for such analyses of clonal diversity in asexually reproducing organisms. Additionally, genotype can be used for detecting genotyping errors in studies of sexual organisms.",2004,9,1704,73,0,3,10,11,16,30,33,60,101,124
279f02e2e821df40bb7dea6f9afc1bd0e2d530e4,"The helix-loop-helix (HLH) family of transcriptional regulatory proteins are key players in a wide array of developmental processes. Over 240 HLH proteins have been identified to date in organisms ranging from the yeast Saccharomyces cerevisiae to humans (6). Studies in Xenopus laevis, Drosophila melanogaster, and mice have convincingly demonstrated that HLH proteins are intimately involved in developmental events such as cellular differentiation, lineage commitment, and sex determination. In yeast, HLH proteins regulate several important metabolic pathways, including phosphate uptake and phospholipid biosynthesis (19, 67, 112). In multicellular organisms, HLH factors are required for a multitude of important developmental processes, including neurogenesis, myogenesis, hematopoiesis, and pancreatic development (12, 86, 127, 179). The purpose of this review is to examine the structure and functional properties of HLH proteins. 
 
 
 
E-box sites: elements mediating cell-type-specific gene transcription. 
Gene transcription of the immunoglobulin heavy-chain (IgH) gene has long been known to be regulated, in part, by a cis-acting DNA element known as the IgH intronic enhancer (109, 156). By in vivo methylation protection assays, a number of sites were identified in both the IgH and the kappa light-chain gene enhancers which were specifically protected in B cells but not in nonlymphoid cells (41). These elements shared a signature motif which consisted of the core hexanucleotide sequence, CANNTG, and were subsequently dubbed E boxes (41). A total of five E-box elements are present in the IgH gene enhancer: μE1, μE2, μE3, μE4, and μE5. The Ig kappa enhancer also contains three cannonical E boxes, designated κE1, κE2, and κE3. E-box sites have been subsequently found in B-cell-specific promoter and enhancer elements, including a subset of Ig light-chain gene promoters, the IgH and Ig light-chain 3′ enhancers, and, more recently, the λ5 promoter (110, 118, 156). 
 
E-box elements have also been identified in promoter and enhancer elements that regulate muscle-, neuron-, and pancreas-specific gene expression. For example, in muscle, the muscle creatine kinase gene, acetylcholine receptor genes α and δ, and the myosin light-chain gene all require E-box elements for full activity (27, 51, 85). A number of genes whose expression is limited to the pancreas also require E-box sites for proper expression. The insulin and somatostatin genes, for example, contain E-box sites that, when multimerized, are sufficient to regulate pancreatic β-cell-specific gene expression (168). More recently, E-box regulatory sites have been identified in a number of neuron-specific genes, including the opsin, hippocalcin, beta 2 subunit of the neuronal nicotinic acetylcholine receptor, and muscarinic acetylcholine receptor genes (1, 21, 52, 125). 
 
 
 
 
E-box sites: cognate recognition sequence for HLH proteins. 
Two proteins, termed E12 and E47, were originally identified as binding to the κE2/μE5 site (65, 102). They have a region of homology with the Drosophila Daughterless protein, the myogenic differentiation factor MyoD, members of the achaete-scute gene complex, and the Myc family of transcription factors (102). This stretch of conserved residues, known as the Myc homology region, appeared to be critical for the DNA binding properties of E12 and E47 (102). The E12 and E47 proteins, which differ only within this Myc homology region, arise by alternative splicing of the E2A gene (157). This conserved sequence, which was modeled as two amphipathic alpha helices separated by a flexible loop structure, was named the HLH motif and shown to function as a dimerization domain. 
 
 
 
 
The HLH structure. 
The solution structure of the basic HLH (bHLH)-leucine zipper (LZ) factor Max first confirmed the existence of the HLH motif (44). Subsequently, the three-dimensional structure of the E47 bHLH polypeptide bound to its E-box recognition site, CACCTG, has been solved at 2.8-Å resolution (38). A number of interesting features were revealed from analysis of the E47 crystal structure. The E47 dimer forms a parallel, four-helix bundle which allows the basic region to contact the major groove (38). In addition to the basic region, residues in the loop and helix 2 also make contact with DNA (38). Stable interaction of the HLH domain is favored by van der Waals interactions between conserved hydrophobic residues (38). The E47 dimer is centered over the E box, with each monomer interacting with either a CAC or CAG half-site. A glutamate present in the basic region of each subunit makes contact with the cytosine and adenine bases in the E-box half-site. An adjacent arginine residue stabilizes the position of the glutamate by direct interaction with these nucleotides and additionally the phosphodiester backbone. Both the glutamate and the arginine residues are conserved in most bHLH proteins, consistent with a role in specific DNA binding (6, 38, 102). 
 
 
 
 
Classification of the HLH proteins. 
Owing to the large number of HLH proteins that have been described, a classification scheme that was based upon tissue distribution, dimerization capabilities, and DNA-binding specificities was devised (Fig. ​(Fig.1)1) (101). Class I HLH proteins, also known as the E proteins, include E12, E47, HEB, E2-2, and Daughterless. These proteins are expressed in many tissues and capable of forming either homo- or heterodimers (103). The DNA-binding specificity of class I proteins is limited to the E-box site. Class II HLH proteins, which include members such as MyoD, myogenin, Atonal, NeuroD/BETA2, and the achaete-scute complex, show a tissue-restricted pattern of expression. With few exceptions, they are incapable of forming homodimers and preferentially heterodimerize with the E proteins. Class I-class II heterodimers can bind both canonical and noncanonical E-box sites (103). Class III HLH proteins include the Myc family of transcription factors, TFE3, SREBP-1, and the microphthalmia-associated transcription factor, Mi. Proteins of this class contain an LZ adjacent to the HLH motif (66, 177). Class IV HLH proteins define a family of molecules, including Mad, Max, and Mxi, that are capable of dimerizing with the Myc proteins or with one another (7, 22, 174). A group of HLH proteins that lack a basic region, including Id and emc, define the class V HLH proteins (18, 39, 47). Class V members are negative regulators of class I and class II HLH proteins (18, 39, 47). Class VI HLH proteins have as their defining feature a proline in their basic region. This group includes the Drosophila proteins Hairy and Enhancer of split (76, 141). Finally, the class VII HLH proteins are categorized by the presence of the bHLH-PAS domain and include members such as the aromatic hydrocarbon receptor (AHR), the AHR nuclear-translocator (Arnt), hypoxia-inducible factor 1α, and the Drosophila Single-minded and Period proteins (34). 
 
 
 
FIG. 1 
 
Multiple sequence alignment and classification of some representative members of the HLH family of transcription factors. Shown is a dendrogram created by aligning the sequences of the indicated HLH proteins by the Clustal W algorithm (160). 
 
 
 
Recently, another classification method of HLH proteins has been described (6). Based on the amino acid sequences of 242 HLH proteins, a phylogenetic tree was created to group family members according to evolutionary relationships (6). Four major groups, A through D, which comprise more than 24 protein families were identified (6). The groupings were based upon DNA-binding specificity as well as conservation of amino acids at certain positions (6). As the number of HLH proteins continues to grow, this evolutionary or “natural” classification may provide a more accurate and convenient means of categorization.",2000,193,1642,121,24,58,74,77,104,71,64,70,83,88
b39c51b5e5f106430dfa6a234fcc671b7782e45e,"In the last 10 years, a large family of secreted signaling molecules has been discovered that appear to mediate many key events in normal growth and development. The family is known as the TGF-p superfamily (Massague 1990), a name taken from the first member of the family to be isolated (transforming growth factor-^l). This name is somewhat misleading, because TGF-p 1 has a large number of effects in different systems (Spom and Roberts 1992). It actually inhibits the proliferation of many different cell lines, and its original ""transforming"" activity may be due to secondary effects on matrix pro­ duction and synthesis of other growth factors (Moses et al. 1990). The two dozen other members of the TGF-p superfamily have a remarkable range of activities. In Diosophila, a TGF-p-related gene is required for dorsoventral axis formation in early embryos, communication between tissue layers in gut development, and correct proximal distal patterning of adult appendages. In Xenopus, a TGF-p-related gene is expressed specifically at one end of fertilized eggs and may function in early signaling events that lay out the basic body plan. In mammals, TGF-p-related molecules have been found that control sexual development, pituitary hormone production, and the creation of bones and cartilage. The recognition of TGF-p superfamily members in many different organ­ isms and contexts provides one of the major unifying themes in recent molecular studies of animal growth and development. The rough outlines of the TGF-p family were first rec­ ognized in the 1980s. Since that time, a number of ex­ cellent reviews have appeared that summarize the prop­ erties of different family members (Ying 1989; Massague 1990; Lyons et al. 1991; Spom and Roberts 1992). Here, I will focus on four areas that have seen major progress in the last 3 years: structural characterization of the signal­ ing molecule, isolation of new family members, cloning of receptor molecules, and new genetic tests of the func­ tions of these factors in different organisms.",1994,106,1983,45,31,91,159,151,161,143,121,109,86,71
3fa772c422d9bd85e451822e4fcc58c98d5c481d,"Reactive oxygen species (ROS) have multifaceted roles in the orchestration of plant gene expression and gene-product regulation. Cellular redox homeostasis is considered to be an ""integrator"" of information from metabolism and the environment controlling plant growth and acclimation responses, as well as cell suicide events. The different ROS forms influence gene expression in specific and sometimes antagonistic ways. Low molecular antioxidants (e.g., ascorbate, glutathione) serve not only to limit the lifetime of the ROS signals but also to participate in an extensive range of other redox signaling and regulatory functions. In contrast to the low molecular weight antioxidants, the ""redox"" states of components involved in photosynthesis such as plastoquinone show rapid and often transient shifts in response to changes in light and other environmental signals. Whereas both types of ""redox regulation"" are intimately linked through the thioredoxin, peroxiredoxin, and pyridine nucleotide pools, they also act independently of each other to achieve overall energy balance between energy-producing and energy-utilizing pathways. This review focuses on current knowledge of the pathways of redox regulation, with discussion of the somewhat juxtaposed hypotheses of ""oxidative damage"" versus ""oxidative signaling,"" within the wider context of physiological function, from plant cell biology to potential applications.",2009,440,1176,70,16,73,98,122,126,107,109,109,81,90
5f74244d62bdc42a8ef6606495508e40d647366f,"The potential of oxygen free radicals and other reactive oxygen species (ROS) to damage tissues and cellular components, called oxidative stress, in biological systems has become a topic of significant interest for environmental toxicology studies. The balance between prooxidant endogenous and exogenous factors (i.e., environmental pollutants) and antioxidant defenses (enzymatic and nonenzymatic) in biological systems can be used to assess toxic effects under stressful environmental conditions, especially oxidative damage induced by different classes of chemical pollutants. The role of these antioxidant systems and their sensitivity can be of great importance in environmental toxicology studies. In the past decade, numerous studies on the effects of oxidative stress caused by some environmental pollutants in terrestrial and aquatic species were published. Increased numbers of agricultural and industrial chemicals are entering the aquatic environment and being taken up into tissues of aquatic organisms. Transition metals, polycyclic aromatic hydrocarbons, organochlorine and organophosphate pesticides, polychlorinated biphenyls, dioxins, and other xenobiotics play important roles in the mechanistic aspects of oxidative damage. Such a diverse array of pollutants stimulate a variety of toxicity mechanisms, such as oxidative damage to membrane lipids, DNA, and proteins and changes to antioxidant enzymes. Although there are considerable gaps in our knowledge of cellular damage, response mechanisms, repair processes, and disease etiology in biological systems, free radical reactions and the production of toxic ROS are known to be responsible for a variety of oxidative damages leading to adverse health effects and diseases. In the past decade, mammalian species were used as models for the study of molecular biomarkers of oxidative stress caused by environmental pollutants to elucidate the mechanisms underlying cellular oxidative damage and to study the adverse effects of some environmental pollutants with oxidative potential in chronic exposure and/or sublethal concentrations. This review summarizes current knowledge and advances in the understanding of such oxidative processes in biological systems. This knowledge is extended to specific applications in aquatic organisms because of their sensitivity to oxidative pollutants, their filtration capacity, and their potential for environmental toxicology studies.",2006,164,1365,71,7,10,33,40,63,71,72,96,104,128
175152bdf0eeeab0cc4fa457784dd8ebdda132a6,"The paper that follows is based on notes taken by Dr. R. S. Pierce on five lectures given by the author at the California Institute of Technology in January 1952. They have been revised by the author but they reflect, apart from minor changes, the lectures as they were delivered. The subject-matter, as the title suggests, is the role of error in logics, or in the physical implementation of logics—–in automatasynthesis. Error is viewed, therefore, not as an extraneous and misdirected or misdirecting accident, but as an essential part of the process under consideration—–its importance in the synthesis of automata being fully comparable to that of the factor which is normally considered, the intended and correct logical structure. Our present treatment of error is unsatisfactory and ad hoc. It is the author’s conviction, voiced over many years, that error should be treated by thermodynamical methods, and be the subject of a thermodynamical theory, as information has been, by the work of L. Szilard and C. E. Shannon (cf. 5.2). The present treatment falls far short of achieving this, but it assembles, it is hoped, some of the building materials, which will have to enter into the final structure. The author wants to express his thanks to K. A. Brueckner and M. Gell-Mann, then at the University of Illinois, to whose discussions in 1951 he owes some important stimuli on this subject; to Dr. R. S. Pierce at the California Institute of Technology, on whose excellent notes this exposition is based; and to the California Institute of Technology, whose invitation to deliver these lectures combined with the very warm reception by the audience, caused him to write this paper in its present form, and whose cooperation in connection with the present publication is much appreciated.",1956,7,2192,147,0,2,8,4,2,9,7,7,2,4
9932ff03877ee134b4fa753c7555ae6281b6ad3e,"Ocean-going ships carry, as ballast, seawater that is taken on in port and released at subsequent ports of call. Plankton samples from Japanese ballast water released in Oregon contained 367 taxa. Most taxa with a planktonic phase in their life cycle were found in ballast water, as were all major marine habitat and trophic groups. Transport of entire coastal planktonic assemblages across oceanic barriers to similar habitats renders bays, estuaries, and inland waters among the most threatened ecosystems in the world. Presence of taxonomically difficult or inconspicuous taxa in these samples suggests that ballast water invasions are already pervasive.",1993,29,1626,42,4,11,22,31,25,47,37,63,52,60
1666fad6fbd651cada1d4ad8eb5831d7f5fbafb0,"Fractal-like networks effectively endow life with an additional fourth spatial dimension. This is the origin of quarter-power scaling that is so pervasive in biology. Organisms have evolved hierarchical branching networks that terminate in size-invariant units, such as capillaries, leaves, mitochondria, and oxidase molecules. Natural selection has tended to maximize both metabolic capacity, by maximizing the scaling of exchange surface areas, and internal efficiency, by minimizing the scaling of transport distances and times. These design principles are independent of detailed dynamics and explicit models and should apply to virtually all organisms.",1999,6,1428,86,3,28,33,50,43,71,66,89,60,61
c18600920e1b9bfd04a6c7baa82c0ad239aea803,,2001,106,1477,80,0,14,12,20,31,42,43,63,72,86
9233c0090a4a0f804f4a294775d395bb55400e2d,"Many ecosystem services are delivered by organisms that depend on habitats that are segregated spatially or temporally from the location where services are provided. Management of mobile organisms contributing to ecosystem services requires consideration not only of the local scale where services are delivered, but also the distribution of resources at the landscape scale, and the foraging ranges and dispersal movements of the mobile agents. We develop a conceptual model for exploring how one such mobile-agent-based ecosystem service (MABES), pollination, is affected by land-use change, and then generalize the model to other MABES. The model includes interactions and feedbacks among policies affecting land use, market forces and the biology of the organisms involved. Animal-mediated pollination contributes to the production of goods of value to humans such as crops; it also bolsters reproduction of wild plants on which other services or service-providing organisms depend. About one-third of crop production depends on animal pollinators, while 60-90% of plant species require an animal pollinator. The sensitivity of mobile organisms to ecological factors that operate across spatial scales makes the services provided by a given community of mobile agents highly contextual. Services vary, depending on the spatial and temporal distribution of resources surrounding the site, and on biotic interactions occurring locally, such as competition among pollinators for resources, and among plants for pollinators. The value of the resulting goods or services may feed back via market-based forces to influence land-use policies, which in turn influence land management practices that alter local habitat conditions and landscape structure. Developing conceptual models for MABES aids in identifying knowledge gaps, determining research priorities, and targeting interventions that can be applied in an adaptive management context.",2007,199,1172,51,7,36,59,52,62,80,98,107,85,95
04b68c1b6ee148d7b1332198d03b571728792bc8,"We have carried out detailed statistical analyses of integral membrane proteins of the helix‐bundle class from eubacterial, archaean, and eukaryotic organisms for which genome‐wide sequence data are available. Twenty to 30% of all ORFs are predicted to encode membrane proteins, with the larger genomes containing a higher fraction than the smaller ones. Although there is a general tendency that proteins with a smaller number of transmembrane segments are more prevalent than those with many, uni‐cellular organisms appear to prefer proteins with 6 and 12 transmembrane segments, whereas Caenorhabditis elegansandHomo sapienshave a slight preference for proteins with seven transmembrane segments. In all organisms, there is a tendency that membrane proteins either have manytransmembrane segments with short connecting loops or few transmembrane segments with large extra‐membraneous domains. Membrane proteins from all organisms studied, except possibly the archaeon Methanococcus jannaschii, follow the so‐called “positive‐inside” rule; i.e., they tend to have a higher frequency of positively charged residues in cytoplasmic than in extra‐cytoplasmic segments.",1998,45,1444,26,7,20,35,32,34,31,42,48,72,79
42eb47797fe541dd1e9ebe46f31df3cbda9e66ec,"Searches for genes involved in the ageing process have been made in genetically tractable model organisms such as yeast, the nematode Caenorhabditis elegans , Drosophila melanogaster fruitflies and mice. These genetic studies have established that ageing is indeed regulated by specific genes, and have allowed an analysis of the pathways involved, linking physiology, signal transduction and gene regulation. Intriguing similarities in the phenotypes of many of these mutants indicate that the mutations may also perturb regulatory systems that control ageing in higher organisms.",2000,104,1290,31,0,42,108,86,100,93,65,65,62,71
830fdb7b17cfa7b96693c41f04416c05ccbf34e6,"A full description of a protein's function requires knowledge of all partner proteins with which it specifically associates. From a functional perspective, ‘association’ can mean direct physical binding, but can also mean indirect interaction such as participation in the same metabolic pathway or cellular process. Currently, information about protein association is scattered over a wide variety of resources and model organisms. STRING aims to simplify access to this information by providing a comprehensive, yet quality-controlled collection of protein–protein associations for a large number of organisms. The associations are derived from high-throughput experimental data, from the mining of databases and literature, and from predictions based on genomic context analysis. STRING integrates and ranks these associations by benchmarking them against a common reference set, and presents evidence in a consistent and intuitive web interface. Importantly, the associations are extended beyond the organism in which they were originally described, by automatic transfer to orthologous protein pairs in other organisms, where applicable. STRING currently holds 730 000 proteins in 180 fully sequenced organisms, and is available at http://string.embl.de/.",2004,22,1220,64,3,37,63,80,47,43,55,57,60,64
9c82f82047b5dd52fda35fbca3622cbe11ce3b8e,"Large proteins are usually expressed in a eukaryotic system while smaller ones are expressed in prokaryotic systems. For proteins that require glycosylation, mammalian cells, fungi or the baculovirus system is chosen. The least expensive, easiest and quickest expression of proteins can be carried out in Escherichia coli. However, this bacterium cannot express very large proteins. Also, for S-S rich proteins, and proteins that require post-translational modifications, E. coli is not the system of choice. The two most utilized yeasts are Saccharomyces cerevisiae and Pichia pastoris. Yeasts can produce high yields of proteins at low cost, proteins larger than 50 kD can be produced, signal sequences can be removed, and glycosylation can be carried out. The baculoviral system can carry out more complex post-translational modifications of proteins. The most popular system for producing recombinant mammalian glycosylated proteins is that of mammalian cells. Genetically modified animals secrete recombinant proteins in their milk, blood or urine. Similarly, transgenic plants such as Arabidopsis thaliana and others can generate many recombinant proteins.",2009,141,818,52,6,31,49,55,67,83,74,89,99,84
24971a32453170ec29c7335e2afb72f426bfce24,"The organization of biological activities into daily cycles is universal in organisms as diverse as cyanobacteria, fungi, algae, plants, flies, birds and man. Comparisons of circadian clocks in unicellular and multicellular organisms using molecular genetics and genomics have provided new insights into the mechanisms and complexity of clock systems. Whereas unicellular organisms require stand-alone clocks that can generate 24-hour rhythms for diverse processes, organisms with differentiated tissues can partition clock function to generate and coordinate different rhythms. In both cases, the temporal coordination of a multi-oscillator system is essential for producing robust circadian rhythms of gene expression and biological activity.",2005,166,1216,55,7,52,63,71,63,65,69,83,69,79
71c5df87d5e6e7e5a00dc7c3668ac7248814a33f,"Models that describe the spread of invading organisms often assume that the dispersal distances of propagules are normally distributed. In contrast, measured dispersal curves are typically leptokurtic, not normal. In this paper, we consider a class of models, integrodifference equations, that directly incorporate detailed dispersal data as well as population growth dynamics. We provide explicit formulas for the speed of invasion for compensatory growth and for different choices of the propagule redistribution kernel and apply these formulas to the spread of D. pseudoobscura. We observe that: (1) the speed of invasion of a spreading population is extremely sensitive to the precise shape of the redistribution kernel and, in particular, to the tail of the distribution; (2) fat-tailed kernels can generate accelerating invasions rather than constant-speed travelling waves; (3) normal redistribution kernels (and by inference, many reaction-diffusion models) may grossly underestimate rates of spread of invading populations in comparison with models that incorporate more realistic leptokurtic distributions; and (4) the relative superiority of different redistribution kernels depends, in general, on the precise magnitude of the net reproductive rate. The addition of an Allee effect to an integrodifference equation may decrease the overall rate of spread. An Allee effect may also introduce a critical range; the population must surpass this spatial thresh-old in order to invade successfully. Fat-tailed kernels and Allee effects provide alternative explanations for the accelerating rates of spread observed for many invasions.",1996,0,1298,97,0,8,11,23,25,33,38,54,50,47
f9df3e2cc08af8119d6234f81147628080bc343f,"Choices of synonymous codons in unicellular organisms are here reviewed, and differences in synonymous codon usages between Escherichia coli and the yeast Saccharomyces cerevisiae are attributed to differences in the actual populations of isoaccepting tRNAs. There exists a strong positive correlation between codon usage and tRNA content in both organisms, and the extent of this correlation relates to the protein production levels of individual genes. Codon-choice patterns are believed to have been well conserved during the course of evolution. Examination of silent substitutions and tRNA populations in Enterobacteriaceae revealed that the evolutionary constraint imposed by tRNA content on codon usage decelerated rather than accelerated the silent-substitution rate, at least insofar as pairs of taxonomically related organisms were examined. Codon-choice patterns of multicellular organisms are briefly reviewed, and diversity in G+C percentage at the third position of codons in vertebrate genes--as well as a possible causative factor in the production of this diversity--is discussed.",1985,38,1605,70,0,8,16,20,29,24,25,13,29,30
216fce085f7112eeec2a9b9252b7f8735d2b846f,,1991,25,2444,28,28,46,62,56,49,28,47,33,29,31
d2167c06eaa23893331ab0ae062e57ce22b89e29,"UNLABELLED
Biologists and other scientists routinely need to know times of divergence between species and to construct phylogenies calibrated to time (timetrees). Published studies reporting time estimates from molecular data have been increasing rapidly, but the data have been largely inaccessible to the greater community of scientists because of their complexity. TimeTree brings these data together in a consistent format and uses a hierarchical structure, corresponding to the tree of life, to maximize their utility. Results are presented and summarized, allowing users to quickly determine the range and robustness of time estimates and the degree of consensus from the published literature.


AVAILABILITY
TimeTree is available at http://www.timetree.net",2006,12,1039,115,2,2,9,11,38,55,79,82,130,123
b6297363c4b33f0b882170add35c8e5cfa25a79d,"Unprecedented development along tropical shorelines is causing severe degradation of coral reefs primarily from increases in sedimentation. Sediment particles smother reef organisms and reduce light available for photosynthesis. Excessive sedmentation can adversely affect the structure and function of the coral reef ecosystem by altering both physical and biological processes. Mean sediment rates and suspended sediment concentrations for reefs not subject to stresses from human activities are < 1 to ca 10 mg cm-* d-' and < 10 mg I-', respectively. Chronic rates and concentrations above these values are 'hlgh'. Heavy sedmentation is associated with fewer coral species, less live coral, lower coral growth rates, greater abundance of branching forms, reduced coral recruitment, decreased calcification, decreased net productivity of corals, and slower rates of reef accretion. Coral species have different capabilities of clearing themselves of sediment particles or surviving lower light levels. Sedlment rejection is a function of morphology, orientation, growth habit, and behavior; and of the amount and type of se lment . Coral growth rates are not simple indicators of sediment levels. Decline of tropical fisheries is partially attributable to deterioration of coral reefs, seagrass beds, and mangroves from sedimentation. Sedimentation can alter the complex interactions between fish and their reef habitat. For example, sedimentation can lull major reef-building corals, leading to eventual collapse of the reef framework. A decline in the amount of shelter the reef provides leads to reductions in both number of individuals and number of species of fish. Currently, we are unable to rigorously predict the responses of coral reefs and reef organisms to excessive sedimentation from coastal development and other sources. Given information on the amount of sediment which will be introduced into the reef environment, the coral community composition, the depth of the reef, the percent coral cover, and the current patterns, we should be able to predict the consequences of a particular activity. Models of physical processes (e.g. sediment transport) must be complemented with better understanding of organism and ecosystem responses to sediment stress. Specifically, we need data on the threshold levels for reef orgarusms and for the reef ecosystem as a whole the levels above which sedimentation has lethal effects for particular species and above which normal functioning of the reef ceases. Additional field studies on the responses of reef organisms to both temgenous and calcium carbonate sediments are necessary. To effectively assess trends on coral reefs, e.g. changes in abundance and spatial arrangement of dominant benthic organisms, scientists must start using standardized monitoring methods. Long-term data sets are critical for tracking these complex ecosystems.",1990,109,1322,116,1,3,9,6,9,8,9,18,17,19
d6dc753be567413c42f02f02f35da476221575ca,"(2002). The Effects of Harmful Algal Blooms on Aquatic Organisms. Reviews in Fisheries Science: Vol. 10, No. 2, pp. 113-390.",2002,1964,1156,85,0,10,24,37,49,54,64,59,64,66
f90500d0f00a6c20303c28076eb0818f90e70ec2,"Although the nonlinear optical effect known as second-harmonic generation (SHG) has been recognized since the earliest days of laser physics and was demonstrated through a microscope over 25 years ago, only in the past few years has it begun to emerge as a viable microscope imaging contrast mechanism for visualization of cell and tissue structure and function. Only small modifications are required to equip a standard laser-scanning two-photon microscope for second-harmonic imaging microscopy (SHIM). Recent studies of the three-dimensional in vivo structures of well-ordered protein assemblies, such as collagen, microtubules and muscle myosin, are beginning to establish SHIM as a nondestructive imaging modality that holds promise for both basic research and clinical pathology. Thus far the best signals have been obtained in a transmitted light geometry that precludes in vivo measurements on large living animals. This drawback may be addressed through improvements in the collection of SHG signals via an epi-illumination microscope configuration. In addition, SHG signals from certain membrane-bound dyes have been shown to be highly sensitive to membrane potential. Although this indicates that SHIM may become a valuable tool for probing cell physiology, the small signal size would limit the number of photons that could be collected during the course of a fast action potential. Better dyes and optimized microscope optics could ultimately lead to the imaging of neuronal electrical activity with SHIM.",2003,31,1194,23,1,17,26,47,71,66,66,84,62,97
ddf280021cb2ebce1d42dc28d6b6897fd3163eaa,"Metallic nanoparticles are among the most widely used types of engineered nanomaterials; however, little is known about their environmental fate and effects. To assess potential environmental effects of engineered nanometals, it is important to determine which species are sensitive to adverse effects of various nanomaterials. In the present study, zebrafish, daphnids, and an algal species were used as models of various trophic levels and feeding strategies. To understand whether observed effects are caused by dissolution, particles were characterized before testing, and particle concentration and dissolution were determined during exposures. Organisms were exposed to silver, copper, aluminum, nickel, and cobalt as both nanoparticles and soluble salts as well as to titanium dioxide nanoparticles. Our results indicate that nanosilver and nanocopper cause toxicity in all organisms tested, with 48-h median lethal concentrations as low as 40 and 60 microg/L, respectively, in Daphnia pulex adults, whereas titanium dioxide did not cause toxicity in any of the tests. Susceptibility to nanometal toxicity differed among species, with filter-feeding invertebrates being markedly more susceptible to nanometal exposure compared with larger organisms (i.e., zebrafish). The role of dissolution in observed toxicity also varied, being minor for silver and copper but, apparently, accounting for most of the toxicity with nickel. Nanoparticulate forms of metals were less toxic than soluble forms based on mass added, but other dose metrics should be developed to accurately assess concentration-response relationships for nanoparticle exposures.",2008,21,793,52,1,21,39,72,69,87,84,81,66,76
a49524b0834aceaff2fab2897dc6399d970d1bad,"Semiquantitative mineral analysis has been done by X-ray diffraction on the < 2 μ- and 2–20 μ-size fractions of approximately five hundred Recent deep-sea core samples from the Atlantic, Antarctic, western Indian Oceans, and adjacent seas. Relative abundances of montmorillonite, illite, kaolinite, chlorite, gibbsite, quartz, amphibole, clinoptilolite-heulandite(?), and pyrophyllite(?) were determined. Mixed-layer clay minerals, feldspars, and dolomite were also observed but not quantitatively evaluated. From the patterns of mineral distribution, the following conclusions appear warranted:

Most Recent Atlantic Ocean deep-sea clay is detritus from the continents. The formation of minerals in situ on the ocean bottom is relatively unimportant in the Atlantic but may be significant in parts of the southwestern Indian Ocean.

Mineralogical analysis of the fine fraction of Atlantic Ocean deep-sea sediments is a useful indicator of sediment provenance. Kaolinite, gibbsite, pyrophyllite, mixed-layer minerals, and chlorite contribute the most unequivocal provenance information because they have relatively restricted loci of continental origin.

Topographic control over mineral distribution by the Mid-Atlantic Ridge in the North Atlantic Ocean precludes significant eolian transport by the jet stream and emphasizes the importance of transport to and within that part of the deep-sea by processes operative at or near the sediment-water interface.

Transport of continent-derived sediment to the equatorial Atlantic is primarily by rivers draining from South America and by rivers and wind from Africa.

The higher proportion of kaolinite and gibbsite in deep-sea sediments adjacent to small tropical South American rivers reflects a greater intensity of lateritic weathering than is observed near the mouths of the larger rivers. This may be explained by a greater variety of pedogenic conditions in the larger drainage basins, resulting in an assemblage with proportionately less lateritic material in the detritus transported by the larger rivers despite their quantitatively greater influence on deep-sea sediment accumulation.

In the South Atlantic Ocean, the fine-fraction mineral assemblage of surface sediment in the Argentine Basin is sufficiently unlike that adjacent to the mouth of the Rio de la Plata to preclude it as a major Recent sediment source for that basin. The southern Argentine Continental Shelf, the Scotia Ridge, and the Weddell Sea arc mineralogically more likely immediate sources. Transport from the Weddell Sea by the Antarctic Bottom Water may be responsible for the northward transport of fine-fraction sediment along parts of the western South Atlantic as far north as the Equator.",1965,34,1866,105,2,2,1,10,8,11,14,13,13,18
9ce09564b4745f9196609163a7c94ba4b7079a12,"We summarize progress with respect to (1) different approaches to isolate, extract, and quantify organo-mineral compounds from soils, (2) types of mineral surfaces and associated interactions, (3) the distribution and function of soil biota at organo-mineral surfaces, (4) the distribution and content of organo-mineral associations, and (5) the factors controlling the turnover of organic matter (OM) in organo-mineral associations from temperate soils. Physical fractionation achieves a rough separation between plant residues and mineral-associated OM, which makes density or particle-size fractionation a useful pretreatment for further differentiation of functional fractions. A part of the OM in organo-mineral associations resists different chemical treatments, but the data obtained cannot readily be compared among each other, and more research is necessary on the processes underlying resistance to treatments for certain OM components. Studies using physical-fractionation procedures followed by soil-microbiological analyses revealed that organo-mineral associations spatially isolate C sources from soil biota, making quantity and quality of OM in microhabitats an important factor controlling community composition. The distribution and activity of soil microorganisms at organo-mineral surfaces can additionally be modified by faunal activities. Composition of OM in organo-mineral associations is highly variable, with loamy soils having generally a higher contribution of polysaccharides, whereas mineral-associated OM in sandy soils is often more aliphatic. Though highly reactive towards Fe oxide surfaces, lignin and phenolic components are usually depleted in organo-mineral associations. Charred OM associated with the mineral surface contributes to a higher aromaticity in heavy fractions. The relative proportion of OC bound in organo-mineral fractions increases with soil depth. Likewise does the strength of the bonding. Organic molecules sorbed to the mineral surfaces or precipitated by Al are effectively stabilized, indicated by reduced susceptibility towards oxidative attack, higher thermal stability, and lower bioavailability. At higher surface loading, organic C is much better bioavailable, also indicated by little 14C age. In the subsurface horizons of the soils investigated in this study, Fe oxides seem to be the most important sorbents, whereas phyllosilicate surfaces may be comparatively more important in topsoils. Specific surface area of soil minerals is not always a good predictor for C-stabilization potentials because surface coverage is discontinuous. Recalcitrance and accessibility/aggregation seem to determine the turnover dynamics in fast and intermediate cycling OM pools, but for long-term OC preservation the interactions with mineral surfaces, and especially with Fe oxide surfaces, are a major control in all soils investigated here.",2008,158,743,32,12,18,41,23,41,45,53,62,64,71
482a64aea935eca6bda8bb0f73af42e05595b569,"Martian aqueous mineral deposits have been examined and characterized using data acquired during Mars Reconnaissance Orbiter's (MRO) primary science phase, including Compact Reconnaissance Imaging Spectrometer for Mars hyperspectral images covering the 0.4–3.9 μm wavelength range, coordinated with higher–spatial resolution HiRISE and Context Imager images. MRO's new high-resolution measurements, combined with earlier data from Thermal Emission Spectrometer; Thermal Emission Imaging System; and Observatoire pour la Mineralogie, L'Eau, les Glaces et l'Activitie on Mars Express, indicate that aqueous minerals are both diverse and widespread on the Martian surface. The aqueous minerals occur in 9–10 classes of deposits characterized by distinct mineral assemblages, morphologies, and geologic settings. Phyllosilicates occur in several settings: in compositionally layered blankets hundreds of meters thick, superposed on eroded Noachian terrains; in lower layers of intracrater depositional fans; in layers with potential chlorides in sediments on intercrater plains; and as thousands of deep exposures in craters and escarpments. Carbonate-bearing rocks form a thin unit surrounding the Isidis basin. Hydrated silica occurs with hydrated sulfates in thin stratified deposits surrounding Valles Marineris. Hydrated sulfates also occur together with crystalline ferric minerals in thick, layered deposits in Terra Meridiani and in Valles Marineris and together with kaolinite in deposits that partially infill some highland craters. In this paper we describe each of the classes of deposits, review hypotheses for their origins, identify new questions posed by existing measurements, and consider their implications for ancient habitable environments. On the basis of current data, two to five classes of Noachian-aged deposits containing phyllosilicates and carbonates may have formed in aqueous environments with pH and water activities suitable for life.",2009,190,492,18,7,34,41,50,45,59,42,49,38,27
c9d582a8e074e3b0cd3ea21e6795947d3485614d,"Sequential density fractionation separated soil particles into “light” predominantly mineral-free organic matter vs. increasingly “heavy” organo-mineral particles in four soils of widely differing mineralogy. With increasing particle density C concentration decreased, implying that the soil organic matter (OM) accumulations were thinner. With thinner accumulations we saw evidence for both an increase in 14C-based mean residence time (MRT) of the OM and a shift from plant to microbial origin.Evidence for the latter included: (1) a decrease in C/N, (2) a decrease in lignin phenols and an increase in their oxidation state, and (3) an increase in δ13C and δ15N. Although bulk-soil OM levels varied substantially across the four soils, trends in OM composition and MRT across the density fractions were similar. In the intermediate density fractions (~1.8–2.6 g cm−3), most of the reactive sites available for interaction with organic molecules were provided by aluminosilicate clays, and OM characteristics were consistent with a layered mode of OM accumulation. With increasing density (lower OM loading) within this range, OM showed evidence of an increasingly microbial origin. We hypothesize that this microbially derived OM was young at the time of attachment to the mineral surfaces but that it persisted due to both binding with mineral surfaces and protection beneath layers of younger, less microbially processed C. As a result of these processes, the OM increased in MRT, oxidation state, and degree of microbial processing in the sequentially denser intermediate fractions. Thus mineral surface chemistry is assumed to play little role in determining OM composition in these intermediate fractions. As the separation density was increased beyond ~2.6 g cm−3, mineralogy shifted markedly: aluminosilicate clays gave way first to light primary minerals including quartz, then at even higher densities to various Fe-bearing primary minerals. Correspondingly, we observed a marked drop in δ15N, a weaker decrease in extent of microbial processing of lignin phenols, and some evidence of a rise in C/N ratio. At the same time, however, 14C-based MRT time continued its increase. The increase in MRT, despite decreases in degree of microbial alteration, suggests that mineral surface composition (especially Fe concentration) plays a strong role in determining OM composition across these two densest fractions.",2009,104,267,11,3,4,14,24,22,26,22,12,30,45
7539d262794f8a3538d24b01c04ac62728edc664,"Banded iron-formations (BIFs) occur in the Precambrian geologic record over a wide time span. Beginning at 3.8 Ga (Isua, West Greenland), they are part of Archean cratons and range in age from about 3.5 until 2.5 Ga. Their overall volume reaches a maximum at about 2.5 Ga (iron-formations in the Hamersley Basin of Western Australia) and they disappear from the geologic record at about 1.8 Ga, only to reappear between 0.8 and 0.6 Ga.

The stratigraphic sequences in which BIFs occur are highly variable. Most Archean iron-formations are part of greenstone belts that have been deformed, metamorphosed, and dismembered. This makes reconstruction of the basinal setting of such BIFs very difficult. The general lack of metamorphism and deformation of extensive BIFs of the Hamersley Range of Western Australia and the Transvaal Supergroup of South Africa allow for much better evaluations of original basinal settings. Most Archean iron-formations show fine laminations and/or microbanding. Such microbanding is especially well developed in the Brockman Iron Formation of Western Australia, where it has been interpreted as chemical varves, or annual layers of sedimentation. BIFs ranging in age from 2.2 Ga to about 1.8 Ga (e.g., those of the Lake Superior region, U.S.A., Labrador Trough, Canada, and the Nabberu Basin of Western Australia) commonly exhibit granular textures and lack microbanding.

The mineralogy of the least metamorphosed BIFs consists of combinations of the following minerals: chert, magnetite, hematite, carbonates (most commonly siderite and members of the dolomite-ankerite series), greenalite, stilpnomelane, and riebeckite, and locally pyrite. Minnesotaite is a common, very low-grade metamorphic reaction product. The Eh-pH stability fields of the above minerals (and/or their precursors) indicate anoxic conditions for the original depositional environment.

The average bulk chemistry of BIFs, from 3.8 through 1.8 Ga in age, is very similar. They are rich in total Fe (ranging from about 20 to 40 wt%) and SiO2 (ranging from 43 to 56 wt%). CaO and MgO contents range from 1.75 to 9.0 and from 1.20 to 6.7 wt%, respectively. Al2O3 contents are very low, ranging from 0.09 to 1.8 wt%. These chemical values show that they are clean chemical sediments devoid of detrital input. Only the Neoproterozoic iron-formations (of 0.8 to 0.6 Ga in age) have very different mineralogical and chemical make-ups. They consist mainly of chert and hematite, with minor carbonates.

The rare-earth element profiles of almost all BIFs,with generally pronounced positive Eu anomalies, indicate that the source of Fe and Si was the result of deep ocean hydrothermal activity admixed with sea water.

The prograde metamorphism of iron-formations produces sequentially Fe-amphiboles, then Fe-pyroxenes, and finally (at highest grade) Fe-olivine-containing assemblages. Such metamorphic reactions are isochemical except for decarbonation and dehydration.

The common fine lamination (and/or microbanding) as well as the lack of detrital components in most BIFs suggest that such are the result of deposition, below wave base, in the deeper parts of ocean basins. Those with granular textures are regarded as the result of deposition in shallow water, platformal areas. Carbon isotope data suggest that for a long period of time (from Archean to Early Proterozoic) the ocean basins were stratified with respect to δ13C (in carbonates) as well as organic carbon content. In Middle Proterozoic time (when granular BIFs appear) this stratification diminishes and is lost.

The Neoproterozoic BIFs occur in stratigraphic sequences with glaciomarine deposits. These BIFs are the result of anoxic conditions that resulted from the stagnation in the oceans beneath a near-global ice cover, referred to as “Snowball Earth.”

All of the most “primary” mineral assemblages appear to be the result of chemical precipitation under anoxic conditions. There are, as yet, no data to support that BIF precipitation was linked directly to microbial activity. The relative abundance of BIF throughout the Precambrian record is correlated with a possible curve for the evolution of the O2 content in the Precambrian atmosphere.",2005,73,623,105,0,3,14,16,19,20,39,49,52,53
100988c875624abaaa080647e562d6eecaf2919c,"The bulk of the comet 81P/Wild 2 (hereafter Wild 2) samples returned to Earth by the Stardust spacecraft appear to be weakly constructed mixtures of nanometer-scale grains, with occasional much larger (over 1 micrometer) ferromagnesian silicates, Fe-Ni sulfides, Fe-Ni metal, and accessory phases. The very wide range of olivine and low-Ca pyroxene compositions in comet Wild 2 requires a wide range of formation conditions, probably reflecting very different formation locations in the protoplanetary disk. The restricted compositional ranges of Fe-Ni sulfides, the wide range for silicates, and the absence of hydrous phases indicate that comet Wild 2 experienced little or no aqueous alteration. Less abundant Wild 2 materials include a refractory particle, whose presence appears to require radial transport in the early protoplanetary disk.",2006,36,549,7,6,33,61,47,55,47,52,33,32,42
fdfa44946e50645c1e34535cbc9718fd0cc4d6d9,"The Mineralogy of Scotland.Dr. M. Forster Heddle. J. G. Goodchild. Reprinted under authority of Alex. Thoms by the Council of University College, Dundee, assisted by D. E. I. Innes. Vol. 1. Pp. lviii + 148 + 51 plates + 4 maps. Vol. 2. Pp. viii + 250 + plates 52−103 + 7 maps. (Dundee: Frank Russell, 1923–1924.) 15s.",1925,0,52,2,0,0,0,0,0,0,0,0,0,1
2920e21cfec576cc47c1e5385f0dd7426ff514b1,,2009,0,215,25,11,13,17,25,14,11,15,18,10,9
705b68cee8112e8b16a4aeb49c011f79dd8eb39a,"SECTION 1: CRYSTALLOGRAPHY AND CRYSTAL CHEMISTRY 1. Introduction 2. Crystallography 3. Crystal Chemistry 4. Crystal Structure 5. Crystal Growth SECTION 2: MINERAL PROPERTIES, STUDY, AND IDENTIFICATION 6. Physical Properties of Minerals 7. Optical Mineralogy 8. Intro to X-Ray Crystallography 9. Chemical Analysis of Minerals 10. Strategies for Study SECTION 3: MINERAL DESCRIPTIONS 11. Silicates 12. Framework Silicates 13. Sheet Silicates 14. Chain Silicates 15. Disilicates and Ring Silicates 16. Orthosilicates 17. Carbonates, Sulfates, Phosphates, Borates, Tungstates, and Molybdates 18. Oxides, Hydroxides, and Halides 19. Sulfides and Related Minerals 20. Native Elemetns Appendix A Effective Ionic Radii of the Elements Appendix B Determinative Tables",2008,0,281,33,8,15,19,10,17,30,25,18,29,22
a104e8ec068c7e684422cefad6abaf1ec85b2f63,"Abstract CM carbonaceous chondrites are samples of incompletely serpentinized primitive asteroids. Using position sensitive detector X-ray diffraction (PSD-XRD) and a pattern stripping technique, we quantify the modal mineralogy of CM2 chondrites: Mighei; Murray; Murchison; Nogoya and Cold Bokkeveld. There is a narrow range in the combined modal volume (vol%) of the most abundant phases Mg-serpentine (25–33%) and Fe-cronstedtite (43–50%). Cold Bokkeveld is anomalous in containing more Mg-serpentine (49–59%) than Fe-cronstedtite (19–27%). Even including Cold Bokkeveld, the range in modal total phyllosilicate is 73–79% (average = 75%). Total phyllosilicate abundance provides a non-ambiguous measure of the degree of aqueous alteration and indicates that these meteorites have all experienced essentially the same degree of aqueous alteration. This reflects pervasive hydration of matrix across CM2 samples. Apparent differences in the alteration of chondrules observed in petrographic studies represent various stages in the progression towards complete hydration of all components but are not manifest in significant differences in modal mineralogy. For all samples there is a limited range in olivine (6.9%) and pyroxene (5%) abundances. Modal abundances of the remaining identified phases also show a limited range: calcite (0–1.3%); gypsum (0–1.6%); magnetite (1.1–2.4%); pentlandite (0–2.1%) and pyrrhotite (1–3.8%). As expected, we observe a strong negative correlation in the modal abundance of anhydrous Fe–Mg silicates (olivine + pyroxene) and total phyllosilicate (Mg-serpentine + Fe-cronstedtite) consistent with the idea that phyllosilicate is forming by aqueous alteration of the anhydrous components. The negative correlation in the modal abundance between Mg–serpentine and Fe-cronstedtite indicates: (a) mineralogic transformation of Fe-cronstedtite to Mg-serpentine by fluid driven recrystallisation or (b) that these meteorites had different initial abundances of olivine and pyroxene. The observed positive correlation in the relative proportion of Mg-serpentine with increasing total phyllosilicate abundance reflects the evolution of increasingly Mg-rich phyllosilicate during aqueous alteration. Fe-cronstedtite is the dominant phyllosilicate, while CM chondrule olivines are forsteritic and will form Mg-serpentine during aqueous alteration. This implies that matrix olivine was more Fe-rich than chondrule olivine prior to aqueous alteration.",2009,41,158,11,2,4,7,10,11,16,16,10,11,18
6d264c16fee0695639fb1a8c2db322f135eae207,"[1] We present a new compilation of physical properties of minerals relevant to subduction zones and new phase diagrams for mid-ocean ridge basalt, lherzolite, depleted lherzolite, harzburgite, and serpentinite. We use these data to calculate H2O content, density and seismic wave speeds of subduction zone rocks. These calculations provide a new basis for evaluating the subduction factory, including (1) the presence of hydrous phases and the distribution of H2O within a subduction zone; (2) the densification of the subducting slab and resultant effects on measured gravity and slab shape; and (3) the variations in seismic wave speeds resulting from thermal and metamorphic processes at depth. In considering specific examples, we find that for ocean basins worldwide the lower oceanic crust is partially hydrated (<1.3 wt % H2O), and the uppermost mantle ranges from unhydrated to � 20% serpentinized (� 2.4 wt % H2O). Anhydrous eclogite cannot be distinguished from harzburgite on the basis of wave speeds, but its � 6% greater density may render it detectable through gravity measurements. Subducted hydrous crust in cold slabs can persist to several gigapascals at seismic velocities that are several percent slower than the surrounding mantle. Seismic velocities and VP/VS ratios indicate that mantle wedges locally reach 60–80% hydration. INDEX TERMS: 3040 Marine Geology and Geophysics: Plate tectonics (8150, 8155, 8157, 8158); 3660 Mineralogy and Petrology: Metamorphic petrology; 3919 Mineral Physics: Equations of state; 5199 Physical Properties of Rocks: General or miscellaneous; 8123 Tectonophysics: Dynamics, seismotectonics; KEYWORDS: subduction, seismic velocities, mineral physics, H2O",2003,116,704,46,11,16,30,40,25,31,44,39,31,34
40212ef74b5aab99708213b5b5238c31e2cc0699,3 is an imaging spectrometer that operates from the visible into the near-infrared (0.42-3.0 μm) where highly diagnostic mineral absorption bands occur. Over the course of the mission M 3 will provide low resolution spectro- scopic data for the entire lunar surface at 140 m/pixel (86 spectral channels) to be used as a base-map and high spectral resolution science data (80 m/pixel; 260 spectral channels) for 25-50% of the surface. The de- tailed mineral assessment of different lunar terrains provided by M 3 is principal information needed for understanding the geologic evolution of the lunar crust and lays the foundation for focused future in-depth exploration of the Moon.,2009,9,183,6,2,5,13,8,23,20,17,18,23,11
a736c328bb96b05847dccad89ca19a5fe890bf65,"Failure of Established Firms Author(s): Rebecca M. Henderson and Kim B. Clark Source: Administrative Science Quarterly, Vol. 35, No. 1, Special Issue: Technology, Organizations, and Innovation (Mar., 1990), pp. 9-30 Published by: Sage Publications, Inc. on behalf of the Johnson Graduate School of Management, Cornell University Stable URL: http://www.jstor.org/stable/2393549 . Accessed: 24/07/2013 03:49",1990,54,7812,460,5,7,17,23,32,42,55,63,75,76
359ae2261d4a478e5d79e95a01a36054badb0b1b,"Multiple organ failure (MOF) is a major cause of morbidity and mortali ty in the critically ill patient. Emerging in the 1970s, the concept of MOF was linked to modern developments in intensive care medicine [1]. Although an uncontrolled infection can lead to MOF [2], such a phenomenon is not always found. A number of mediators and the persistence of tissue hypoxia have been incriminated in the development of MOF [3]. The gut has been cited as a possible ""moto r "" of MOF [4]. Nevertheless, our knowledge regarding the pathophysiology of MOF remains limited. Furthermore, the development of new therapeutic interventions aiming at a reduction of the incidence and severity of organ failure calls for a better definition of the severity of organ dysfunction/failure to quantify the severity of illness. Accordingly, it is important to set some simple but objective criteria to define the degree of organ dysfunction/failure. The evolution of our knowledge of organ dysfunction/failure led us to establish several principles: 1. Organ dysfunction/failure is a process rather than an event. Hence, it should be seen as a continuum and should not be described simply as ""present"" or ""absent~' Hence, the assessment should be based on a scale. 2. The time factor is fundamental for several reasons: (a) Development and similarly resolution of organ failure may take some time. Patients dying early may not have time to develop organ dysfunction/failure. (b) The time course of organ dysfunction/failure can be mult imodal during a complex clinical course, what is sometimes referred to as a ""multiple-hit"" scenario. (c) Time evaluation allows a greater understanding of the disease process as a natural process or under the influence of therapeutic interventions. The collection of data on a daily basis seems adequate. 3. The evaluation of organ dysfunction/failure should be based on a limited number of simple but objective variables that are easily and routinely measured in every institution. The collection of this information should not impose any intervention beyond what is routinely performed in every ICU. The variables used should as much as possible be independent of therapy, since therapeutic management may vary from one institution to another and even from one patient to another (Table 1). Until recently, none of the existing systems describing organ failure met these criteria, since they were based on categorial definitions or described organ failure as present or absent [5-7] . The ESICM organized a consensus meeting in Paris in October 1994 to create a so-called sepsis-related organ failure assessment (SOFA) score, to describe quantitatively and as objectively as possible the degree of organ dysfunction/failure over time in groups of patients or even in individual patients (Fig. 1). There are two major applications of such a SOFA score: 1. To improve our Understanding of the natural history of organ dysfunction/failure and the interrelation between the failure of the various organs.",1996,21,7333,313,1,0,6,5,6,14,18,103,116,130
42401f196bbb049b66cc44b399e587248f5c3dff,"Chapter 5, “Inference Procedures for Log-Location-Scale Distributions,” is concerned with likelihood-based inference under various censoring schemes for the important case where the logarithm of the lifetime is modeled with a location-scale distribution. The Weibull, extreme value, lognormal, and loglogistic distributions are discussed. In addition, two distributions with an additional parameter, the generalized log-Burr and the generalized log-gamma, are discussed. Chapter 6, “Parametric Regression Models,” presents models where one or more distribution parameters are modeled as a linear function of some regression parameters. The most popular of such models, the accelerated failure time model, where the location parameter in a location-scale distribution is modeled as a linear function of regression parameters, is discussed in detail. Extensions of this model in which the distribution is allowed to be the generalized log-Burr or the generalized log-gamma and the scale parameter is modeled with regression parameters are discussed. Also discussed in detail are graphical modelchecking techniques for assessing regression model  t. Chapter 7, “Semiparametric Multiplicative Hazards Regression Models,” focuses on the proportional hazards model. Chapter 8, “Rank-Type and Other Semiparametric Procedures for Log-Location-Scale Models,” presents the accelerated failure time analog to the models of Chapter 7. The regression model has a location-scale form, but no speci c distribution is assumed. Chapter 9, “Multiple Modes of Failure,” gives a careful treatment of the analysis of data when failures can occur due to multiple failure modes. Chapter 10, “Goodness-of-Fit Tests,” begins with a general discussion of methods for testing goodness of  t. Some speci c tests of  t for the exponential, Weibull, extreme value, normal, and lognormal are then presented for the case of type 2 censoring or complete data. Tests of  t in regression models are brie y discussed. Simulation methods for overcoming restrictions on the testing procedures are discussed in each case. Finally, Chapter 11, “Beyond Univariate Survival Analysis,” introduces multivariate lifetime data, discussing clustered lifetimes, event history data, and data where an internal process is related to the lifetime of interest. For those working with reliability data, the books by Meeker and Escobar (1998) or Nelson (1982) may be more useful as primary references because of their emphasis on engineering methodology and applications. However, this book complements the other references well, and merits a place on the bookshelf of anyone concerned with the analysis of lifetime data from any  eld.",2003,3,4800,369,196,241,250,228,244,259,218,250,248,248
98dc78fb9285fbe9f937d4a99399f17e8fbde744,"Since 1973 technological, political, regulatory, and economic forces have been changing the worldwide economy in a fashion comparable to the changes experienced during the nineteenth century Industrial Revolution. As in the nineteenth century, we are experiencing declining costs, increasing average (but decreasing marginal) productivity of labor, reduced growth rates of labor income, excess capacity, and the requirement for downsizing and exit. The last two decades indicate corporate internal control systems have failed to deal effectively with these changes, especially slow growth and the requirement for exit. The next several decades pose a major challenge for Western firms and political systems as these forces continue to work their way through the worldwide economy. Copyright 1993 by American Finance Association.",1993,229,6733,586,6,21,25,42,55,73,64,91,103,100
e787079a38dde55ce53eb60f3a3b5e640055cd85,"At the turn of the century, ratio analysis was in its embryonic state. It began with the development of a single ratio, the current ratio,' for a single purpose-the evaluation of credit-worthiness. Today ratio analysis involves the use of several ratios by a variety of users-including credit lenders, credit-rating agencies, investors, and management.2 In spite of the ubiquity of ratios, little effort has been directed toward the formal empirical verification of their usefulness. The usefulness of ratios can only be tested with regard to some particular purpose. The purpose chosen here was the prediction of failure, since ratios are currently in widespread use as predictors of failure. This is not the only possible use of ratios but is a starting point from which to build an empirical verification of ratio analysis. ""Failure"" is defined as the inability of a firm to pay its financial obligations as they mature. Operationally, a firm is said to have failed when any of the following events have occurred: bankruptcy, bond default, an overdrawn bank account, or nonpayment of a preferred stock dividend.3 A ""financial ratio"" is a quotient of two numbers, where both num-",1966,0,4079,375,0,0,0,0,4,5,1,6,4,5
e47eca9c090528f046ba16fd0f640cd1f177a91a,"BACKGROUND
Cardiac resynchronization reduces symptoms and improves left ventricular function in many patients with heart failure due to left ventricular systolic dysfunction and cardiac dyssynchrony. We evaluated its effects on morbidity and mortality.


METHODS
Patients with New York Heart Association class III or IV heart failure due to left ventricular systolic dysfunction and cardiac dyssynchrony who were receiving standard pharmacologic therapy were randomly assigned to receive medical therapy alone or with cardiac resynchronization. The primary end point was the time to death from any cause or an unplanned hospitalization for a major cardiovascular event. The principal secondary end point was death from any cause.


RESULTS
A total of 813 patients were enrolled and followed for a mean of 29.4 months. The primary end point was reached by 159 patients in the cardiac-resynchronization group, as compared with 224 patients in the medical-therapy group (39 percent vs. 55 percent; hazard ratio, 0.63; 95 percent confidence interval, 0.51 to 0.77; P<0.001). There were 82 deaths in the cardiac-resynchronization group, as compared with 120 in the medical-therapy group (20 percent vs. 30 percent; hazard ratio 0.64; 95 percent confidence interval, 0.48 to 0.85; P<0.002). As compared with medical therapy, cardiac resynchronization reduced the interventricular mechanical delay, the end-systolic volume index, and the area of the mitral regurgitant jet; increased the left ventricular ejection fraction; and improved symptoms and the quality of life (P<0.01 for all comparisons).


CONCLUSIONS
In patients with heart failure and cardiac dyssynchrony, cardiac resynchronization improves symptoms and the quality of life and reduces complications and the risk of death. These benefits are in addition to those afforded by standard pharmacologic therapy. The implantation of a cardiac-resynchronization device should routinely be considered in such patients.",2005,43,5764,172,140,325,420,464,441,424,483,406,405,365
b79c6f35101e635171a1f24452bb56ba32ee7f24,"BACKGROUND
The prevalence of heart failure with preserved ejection fraction may be changing as a result of changes in population demographics and in the prevalence and treatment of risk factors for heart failure. Changes in the prevalence of heart failure with preserved ejection fraction may contribute to changes in the natural history of heart failure. We performed a study to define secular trends in the prevalence of heart failure with preserved ejection fraction among patients at a single institution over a 15-year period.


METHODS
We studied all consecutive patients hospitalized with decompensated heart failure at Mayo Clinic Hospitals in Olmsted County, Minnesota, from 1987 through 2001. We classified patients as having either preserved or reduced ejection fraction. The patients were also classified as community patients (Olmsted County residents) or referral patients. Secular trends in the type of heart failure, associated cardiovascular disease, and survival were defined.


RESULTS
A total of 6076 patients with heart failure were discharged over the 15-year period; data on ejection fraction were available for 4596 of these patients (76 percent). Of these, 53 percent had a reduced ejection fraction and 47 percent had a preserved ejection fraction. The proportion of patients with the diagnosis of heart failure with preserved ejection fraction increased over time and was significantly higher among community patients than among referral patients (55 percent vs. 45 percent). The prevalence rates of hypertension, atrial fibrillation, and diabetes among patients with heart failure increased significantly over time. Survival was slightly better among patients with preserved ejection fraction (adjusted hazard ratio for death, 0.96; P=0.01). Survival improved over time for those with reduced ejection fraction but not for those with preserved ejection fraction.


CONCLUSIONS
The prevalence of heart failure with preserved ejection fraction increased over a 15-year period, while the rate of death from this disorder remained unchanged. These trends underscore the importance of this growing public health problem.",2006,34,3566,189,18,96,159,184,187,241,239,293,280,284
cf04075b17dbae3fc6bc9a3a2f9673f9fad192ea,"BACKGROUND
Sudden death from cardiac causes remains a leading cause of death among patients with congestive heart failure (CHF). Treatment with amiodarone or an implantable cardioverter-defibrillator (ICD) has been proposed to improve the prognosis in such patients.


METHODS
We randomly assigned 2521 patients with New York Heart Association (NYHA) class II or III CHF and a left ventricular ejection fraction (LVEF) of 35 percent or less to conventional therapy for CHF plus placebo (847 patients), conventional therapy plus amiodarone (845 patients), or conventional therapy plus a conservatively programmed, shock-only, single-lead ICD (829 patients). Placebo and amiodarone were administered in a double-blind fashion. The primary end point was death from any cause.


RESULTS
The median LVEF in patients was 25 percent; 70 percent were in NYHA class II, and 30 percent were in class III CHF. The cause of CHF was ischemic in 52 percent and nonischemic in 48 percent. The median follow-up was 45.5 months. There were 244 deaths (29 percent) in the placebo group, 240 (28 percent) in the amiodarone group, and 182 (22 percent) in the ICD group. As compared with placebo, amiodarone was associated with a similar risk of death (hazard ratio, 1.06; 97.5 percent confidence interval, 0.86 to 1.30; P=0.53) and ICD therapy was associated with a decreased risk of death of 23 percent (0.77; 97.5 percent confidence interval, 0.62 to 0.96; P=0.007) and an absolute decrease in mortality of 7.2 percentage points after five years in the overall population. Results did not vary according to either ischemic or nonischemic causes of CHF, but they did vary according to the NYHA class.


CONCLUSIONS
In patients with NYHA class II or III CHF and LVEF of 35 percent or less, amiodarone has no favorable effect on survival, whereas single-lead, shock-only ICD therapy reduces overall mortality by 23 percent.",2005,24,5637,165,194,358,355,371,406,353,374,368,380,374
bb0f33d05addb6bc93e6e53f0cc37b28be47009a,"BACKGROUND
We tested the hypothesis that prophylactic cardiac-resynchronization therapy in the form of biventricular stimulation with a pacemaker with or without a defibrillator would reduce the risk of death and hospitalization among patients with advanced chronic heart failure and intraventricular conduction delays.


METHODS
A total of 1520 patients who had advanced heart failure (New York Heart Association class III or IV) due to ischemic or nonischemic cardiomyopathies and a QRS interval of at least 120 msec were randomly assigned in a 1:2:2 ratio to receive optimal pharmacologic therapy (diuretics, angiotensin-converting-enzyme inhibitors, beta-blockers, and spironolactone) alone or in combination with cardiac-resynchronization therapy with either a pacemaker or a pacemaker-defibrillator. The primary composite end point was the time to death from or hospitalization for any cause.


RESULTS
As compared with optimal pharmacologic therapy alone, cardiac-resynchronization therapy with a pacemaker decreased the risk of the primary end point (hazard ratio, 0.81; P=0.014), as did cardiac-resynchronization therapy with a pacemaker-defibrillator (hazard ratio, 0.80; P=0.01). The risk of the combined end point of death from or hospitalization for heart failure was reduced by 34 percent in the pacemaker group (P<0.002) and by 40 percent in the pacemaker-defibrillator group (P<0.001 for the comparison with the pharmacologic-therapy group). A pacemaker reduced the risk of the secondary end point of death from any cause by 24 percent (P=0.059), and a pacemaker-defibrillator reduced the risk by 36 percent (P=0.003).


CONCLUSIONS
In patients with advanced heart failure and a prolonged QRS interval, cardiac-resynchronization therapy decreases the combined risk of death from any cause or first hospitalization and, when combined with an implantable defibrillator, significantly reduces mortality.",2004,33,4955,155,60,293,342,335,353,354,353,370,358,332
bc20b0d8037780f976453089b84dc40c19e5958a,"BACKGROUND
Patients with congestive heart failure have a high mortality rate and are also hospitalized frequently. We studied the effect of an angiotensin-converting-enzyme inhibitor, enalapril, on mortality and hospitalization in patients with chronic heart failure and ejection fractions less than or equal to 0.35.


METHODS
Patients receiving conventional treatment for heart failure were randomly assigned to receive either placebo (n = 1284) or enalapril (n = 1285) at doses of 2.5 to 20 mg per day in a double-bind trial. Approximately 90 percent of the patients were in New York Heart Association functional classes II and III. The follow-up averaged 41.4 months.


RESULTS
There were 510 deaths in the placebo group (39.7 percent), as compared with 452 in the enalapril group (35.2 percent) (reduction in risk, 16 percent; 95 percent confidence interval, 5 to 26 percent; P = 0.0036). Although reductions in mortality were observed in several categories of cardiac deaths, the largest reduction occurred among the deaths attributed to progressive heart failure (251 in the placebo group vs. 209 in the enalapril group; reduction in risk, 22 percent; 95 percent confidence interval, 6 to 35 percent). There was little apparent effect of treatment on deaths classified as due to arrhythmia without pump failure. Fewer patients died or were hospitalized for worsening heart failure (736 in the placebo group and 613 in the enalapril group; risk reduction, 26 percent; 95 percent confidence interval, 18 to 34 percent; P less than 0.0001).


CONCLUSIONS
The addition of enalapril to conventional therapy significantly reduced mortality and hospitalizations for heart failure in patients with chronic congestive heart failure and reduced ejection fractions.",1991,7,7365,96,13,95,173,162,229,245,283,321,351,400
bac638d3d7859fe294cb8c51d791a68f7273e6df,,2000,0,5056,182,140,200,210,270,283,296,318,255,253,270
9bdd1ee918c171b541c616704447b1e06b88f14a,"We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties—completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].",1996,88,2831,482,23,52,50,73,83,98,126,132,123,165
78058c51ef390c715b72cf5bf8339ce4c8d5646b,"BACKGROUND AND METHODS
Aldosterone is important in the pathophysiology of heart failure. In a doubleblind study, we enrolled 1663 patients who had severe heart failure and a left ventricular ejection fraction of no more than 35 percent and who were being treated with an angiotensin-converting-enzyme inhibitor, a loop diuretic, and in most cases digoxin. A total of 822 patients were randomly assigned to receive 25 mg of spironolactone daily, and 841 to receive placebo. The primary end point was death from all causes.


RESULTS
The trial was discontinued early, after a mean follow-up period of 24 months, because an interim analysis determined that spironolactone was efficacious. There were 386 deaths in the placebo group (46 percent) and 284 in the spironolactone group (35 percent; relative risk of death, 0.70; 95 percent confidence interval, 0.60 to 0.82; P<0.001). This 30 percent reduction in the risk of death among patients in the spironolactone group was attributed to a lower risk of both death from progressive heart failure and sudden death from cardiac causes. The frequency of hospitalization for worsening heart failure was 35 percent lower in the spironolactone group than in the placebo group (relative risk of hospitalization, 0.65; 95 percent confidence interval, 0.54 to 0.77; P<0.001). In addition, patients who received spironolactone had a significant improvement in the symptoms of heart failure, as assessed on the basis of the New York Heart Association functional class (P<0.001). Gynecomastia or breast pain was reported in 10 percent of men who were treated with spironolactone, as compared with 1 percent of men in the placebo group (P<0.001). The incidence of serious hyperkalemia was minimal in both groups of patients.


CONCLUSIONS
Blockade of aldosterone receptors by spironolactone, in addition to standard therapy, substantially reduces the risk of both morbidity and death among patients with severe heart failure.",1999,41,7306,107,14,194,278,272,344,374,394,424,374,362
25e84ecaac5834301db3f2087fa6b9bfff0c5cbb,"THE receptor tyrosine kinase Flk-1 (ref. 1) is believed to play a pivotal role in endothelial development. Expression of the Flk-1 receptor is restricted to endothelial cells and their embryonic precursors25, and is complementary to that of its ligand, vascular endothelial growth factor (VEGF)2,3, which is an endothelial-speci-fic mitogen6. Highest levels of flk-1 expression are observed during embryonic vasculogenesis and angiogenesis25, and during pathological processes associated with neovascularization, such as tumour angiogenesis7,8. Because flk-1 expression can be detected in presumptive mesodermal yolk-sac blood-island progenitors as early as 7.0 days postcoitum, Flk-1 may mark the putative common embryonic endothelial and haematopoietic precursor, the haemangioblast, and thus may also be involved in early haematopoiesis4. Here we report the generation of mice deficient in Flk-1 by disruption of the gene using homologous recombination in embryonic stem (ES) cells. Embryos homozygous for this mutation die in utero between 8.5 and 9.5 days post-coitum, as a result of an early defect in the development of haematopoietic and endothelial cells. Yolk-sac blood islands were absent at 7.5 days, organized blood vessels could not be observed in the embryo or yolk sac at any stage, and haematopoietic progenitors were severely reduced. These results indicate that Flk-1 is essential for yolk-sac blood-island formation and vasculogenesis in the mouse embryo.",1995,29,4027,182,7,74,109,156,179,199,225,216,204,229
4083ffd2cabfadd76d90175e4816d04202fe928a,"BACKGROUND
Metoprolol can improve haemodynamics in chronic heart failure, but survival benefit has not been proven. We investigated whether metoprolol controlled release/extended release (CR/XL) once daily, in addition to standard therapy, would lower mortality in patients with decreased ejection fraction and symptoms of heart failure.


METHODS
We enrolled 3991 patients with chronic heart failure in New York Heart Association (NYHA) functional class II-IV and with ejection fraction of 0.40 or less, stabilised with optimum standard therapy, in a double-blind randomised controlled study. Randomisation was preceded by a 2-week single-blind placebo run-in period. 1990 patients were randomly assigned metoprolol CR/XL 12.5 mg (NYHA III-IV) or 25.0 mg once daily (NYHA II) and 2001 were assigned placebo. The target dose was 200 mg once daily and doses were up-titrated over 8 weeks. Our primary endpoint was all-cause mortality, analysed by intention to treat.


FINDINGS
The study was stopped early on the recommendation of the independent safety committee. Mean follow-up time was 1 year. All-cause mortality was lower in the metoprolol CR/XL group than in the placebo group (145 [7.2%, per patient-year of follow-up]) vs 217 deaths [11.0%], relative risk 0.66 [95% CI 0.53-0.81]; p=0.00009 or adjusted for interim analyses p=0.0062). There were fewer sudden deaths in the metoprolol CR/XL group than in the placebo group (79 vs 132, 0.59 [0.45-0.78]; p=0.0002) and deaths from worsening heart failure (30 vs 58, 0.51 [0.33-0.79]; p=0.0023).


INTERPRETATION
Metoprolol CR/XL once daily in addition to optimum standard therapy improved survival. The drug was well tolerated.",1999,41,4569,58,46,258,306,245,324,300,297,266,234,199
7a363c89d5c789d84992991e059208262a8f4d55,"CONTEXT
Although acute renal failure (ARF) is believed to be common in the setting of critical illness and is associated with a high risk of death, little is known about its epidemiology and outcome or how these vary in different regions of the world.


OBJECTIVES
To determine the period prevalence of ARF in intensive care unit (ICU) patients in multiple countries; to characterize differences in etiology, illness severity, and clinical practice; and to determine the impact of these differences on patient outcomes.


DESIGN, SETTING, AND PATIENTS
Prospective observational study of ICU patients who either were treated with renal replacement therapy (RRT) or fulfilled at least 1 of the predefined criteria for ARF from September 2000 to December 2001 at 54 hospitals in 23 countries.


MAIN OUTCOME MEASURES
Occurrence of ARF, factors contributing to etiology, illness severity, treatment, need for renal support after hospital discharge, and hospital mortality.


RESULTS
Of 29 269 critically ill patients admitted during the study period, 1738 (5.7%; 95% confidence interval [CI], 5.5%-6.0%) had ARF during their ICU stay, including 1260 who were treated with RRT. The most common contributing factor to ARF was septic shock (47.5%; 95% CI, 45.2%-49.5%). Approximately 30% of patients had preadmission renal dysfunction. Overall hospital mortality was 60.3% (95% CI, 58.0%-62.6%). Dialysis dependence at hospital discharge was 13.8% (95% CI, 11.2%-16.3%) for survivors. Independent risk factors for hospital mortality included use of vasopressors (odds ratio [OR], 1.95; 95% CI, 1.50-2.55; P<.001), mechanical ventilation (OR, 2.11; 95% CI, 1.58-2.82; P<.001), septic shock (OR, 1.36; 95% CI, 1.03-1.79; P = .03), cardiogenic shock (OR, 1.41; 95% CI, 1.05-1.90; P = .02), and hepatorenal syndrome (OR, 1.87; 95% CI, 1.07-3.28; P = .03).


CONCLUSION
In this multinational study, the period prevalence of ARF requiring RRT in the ICU was between 5% and 6% and was associated with a high hospital mortality rate.",2005,35,3437,102,10,63,125,174,137,210,235,269,239,265
fb735932e3369d2e49852426648daecfa69d9051,"In view of the increasing incidence of stem-type femoral component loosening, a detailed retrospective radiographic zonal analysis of 389 total hip replacements indicated a 19.5% incidence (76 hips) of radiological evidences of mechanical looseness, i.e., fractured acrylic cement and/or a radiolucent gap at the stem-cement or cement-bone interfaces. Detailed serial radiographic examination demonstrated progressive loosening in 56 of the 76 hips and these were categorized into mechanical modes of failure. The 4 modes of failure characterizing stem-type component progressive loosening mechanisms consisted of stem pistoning within the acrylic (3.3%), cement-embedded stem pistoning with the femur (5.1%), medial midstem pivot (2.5%), calcar pivot (0.7%) and bending (fatigue) cantilever (3.3%).",1979,0,3835,112,0,0,7,8,12,14,6,20,14,17
2fa8bf2cbb77fab5803e4ba6081017a22bad1a16,"ESC Committee for Practice Guidelines (CPG), Silvia G. Priori (Chairperson) (Italy), Jean-Jacques Blanc (France), Andrzej Budaj (Poland), John Camm (UK), Veronica Dean (France), Jaap Deckers (The Netherlands), Kenneth Dickstein (Norway), John Lekakis (Greece), Keith McGregor (France), Marco Metra (Italy), Joao Morais (Portugal), Ady Osterspey (Germany), Juan Tamargo (Spain), Jose Luis Zamorano (Spain)  Document Reviewers, Marco Metra (CPG Review Coordinator) (Italy), Michael Bohm (Germany), Alain Cohen-Solal (France), Martin Cowie (UK), Ulf Dahlstrom (Sweden), Kenneth Dickstein (Norway), Gerasimos S. Filippatos (Greece), Edoardo Gronda (Italy), Richard Hobbs (UK), John K. Kjekshus (Norway), John McMurray (UK), Lars Ryden (Sweden), Gianfranco Sinagra (Italy), Juan Tamargo (Spain), Michal Tendera (Poland), Dirk van Veldhuisen (The Netherlands), Faiez Zannad (France)

Guidelines and Expert Consensus Documents aim to present all the relevant evidence on a particular issue in order to help physicians to weigh the benefits and risks of a particular diagnostic or therapeutic procedure. They should be helpful in everyday clinical decision-making.

A great number of Guidelines and Expert Consensus Documents have been issued in recent years by the European Society of Cardiology (ESC) and by different organizations and other related societies. This profusion can put at stake the authority and validity of guidelines, which can only be guaranteed if they have been developed by an unquestionable decision-making process. This is one of the reasons why the ESC and others have issued recommendations for formulating and issuing Guidelines and Expert Consensus Documents.

In spite of the fact that standards for issuing good quality Guidelines and Expert Consensus Documents are well defined, recent surveys of Guidelines and Expert Consensus Documents published in peer-reviewed journals between 1985 and 1998 have shown that methodological standards were not complied with in the vast majority of cases. It is therefore of great importance that guidelines and recommendations are presented in formats that are …",2005,194,5758,102,127,325,416,452,466,399,449,458,682,712
b31e45f340ef52735be6ac2aeda5a20bf7922fdf,"In its earliest clinical phase, Alzheimer's disease characteristically produces a remarkably pure impairment of memory. Mounting evidence suggests that this syndrome begins with subtle alterations of hippocampal synaptic efficacy prior to frank neuronal degeneration, and that the synaptic dysfunction is caused by diffusible oligomeric assemblies of the amyloid β protein.",2002,86,3620,167,1,58,107,96,125,120,166,197,198,209
3b53332c38cf05279adb1765f95eb3bc765952e0,"A BSTRACT Background Previous studies have suggested that cardiac resynchronization achieved through atrialsynchronized biventricular pacing produces clinical benefits in patients with heart failure who have an intraventricular conduction delay. We conducted a double-blind trial to evaluate this therapeutic approach. Methods Four hundred fifty-three patients with moderate-to-severe symptoms of heart failure associated with an ejection fraction of 35 percent or less and a QRS interval of 130 msec or more were randomly assigned to a cardiac-resynchronization group (228 patients) or to a control group (225 patients) for six months, while conventional therapy for heart failure was maintained. The primary end points were the New York Heart Association functional class, quality of life, and the distance walked in six minutes. Results As compared with the control group, patients assigned to cardiac resynchronization experienced an improvement in the distance walked in six minutes (+39 vs. +10 m, P=0.005), functional class (P<0.001), quality of life (i18.0 vs. i9.0 points, P= 0.001), time on the treadmill during exercise testing (+81 vs. +19 sec, P=0.001), and ejection fraction (+4.6 percent vs. i0.2 percent, P<0.001). In addition, fewer patients in the group assigned to cardiac resynchronization than control patients required hospitalization (8 percent vs. 15 percent) or intravenous medications (7 percent vs. 15 percent) for the treatment of heart failure (P<0.05 for both comparisons). Implantation of the device was unsuccessful in 8 percent of patients and was complicated by refractory hypotension, bradycardia, or asystole in four patients (two of whom died) and by perforation of the coronary sinus requiring pericardiocentesis in two others. Conclusions Cardiac resynchronization results in significant clinical improvement in patients who have moderate-to-severe heart failure and an intraventricular conduction delay. (N Engl J Med 2002;346:1845-53.)",2002,45,3976,25,27,163,229,285,281,322,309,266,269,275
9fdbcdf8d92d68f4967732e5f6664dd208c8b139,"BACKGROUND
Controlled clinical trials have shown that beta-blockers can produce hemodynamic and symptomatic improvement in chronic heart failure, but the effect of these drugs on survival has not been determined.


METHODS
We enrolled 1094 patients with chronic heart failure in a double-blind, placebo-controlled, stratified program, in which patients were assigned to one of the four treatment protocols on the basis of their exercise capacity. Within each of the four protocols patients with mild, moderate, or severe heart failure with left ventricular ejection fractions < or = 0.35 were randomly assigned to receive either placebo (n = 398) or the beta-blocker carvedilol (n = 696); background therapy with digoxin, diuretics, and an angiotensin-converting-enzyme inhibitor remained constant. Patient were observed for the occurrence death or hospitalization for cardiovascular reasons during the following 6 months, after the beginning (12 months for the group with mild heart failure).


RESULTS
The overall mortality rate was 7.8 percent in the placebo group and 3.2 percent in the carvedilol group; the reduction in risk attributable to carvedilol was 65 percent (95 percent confidence interval, 39 to 80 percent; P < 0.001). This finding led the Data and Safety Monitoring Board to recommend termination of the study before its scheduled completion. In addition, as compared with placebo, carvedilol therapy was accompanied by a 27 percent reduction in the risk of hospitalization for cardiovascular causes (19.6 percent vs. 14.1 percent, P = 0.036), as well as a 38 percent reduction in the combined risk of hospitalization or death (24.6 percent vs, 15.8 percent, P < 0.001). Worsening heart failure as an adverse reaction during treatment was less frequent in the carvedilol than in the placebo group.


CONCLUSIONS
Carvedilol reduces the risk or death as well as the risk of hospitalization for cardiovascular causes in patients with heart failure who are receiving treatment with digoxin, diuretics, and an angiotensin-converting-enzyme inhibitor.",1996,35,3968,44,27,107,176,209,267,287,220,266,250,192
9b293c5d740d20866d9de079e08d7f6f39797728,"Heart failure develops when the heart cannot pump adequate amounts of blood for the body’s needs. The heart tries to compensate and work harder by dilating (enlargement of the heart chambers), by becoming hypertrophic (thickening of the heart walls), or by beating faster. In many countries, heart failure is a leading cause of death. For individuals older than 65 years, heart failure is the most common cause of hospitalization. Because the burden of heart failure is large and affects health care delivery worldwide, new treatments and methods to diagnose heart failure are being developed. The June 13, 2007, issue of JAMA includes an article about the role of a new type of treatment for heart failure using biventricular pacemakers.",1937,4,3764,22,0,0,0,0,0,0,0,0,0,0
d585c640ef7ee0e18269206d3568d0c94a69dd52,,1981,0,7394,12,13,39,57,69,82,100,104,127,143,137
13f06676916762d0076e777064d7ea250f3f1285,"To evaluate the influence of the angiotensin-converting enzyme inhibitor, enalapril (2.5 to 40 mg/day), on the prognosis of severe congestive heart failure, defined as New York Heart Association functional class IV, a double-blind study was undertaken in which 253 patients were randomized to receive either placebo (n = 126) or enalapril (n = 127) in addition to conventional treatment, including vasodilators. Follow-up averaged 188 days (range 1 day to 20 months). The reduction in crude mortality within 6 months (primary objective) was 40% in the enalapril-treated group (from 44 to 26%, p = 0.002) and within 1 year 31% (p = 0.001). By the end of the study, 68 subjects in the placebo group and 50 in the enalapril group had died--a reduction of 27% (p = 0.003). The entire reduction in total mortality (50%) was found in patients dying from progressive heart failure, whereas no difference was seen in the incidence of sudden cardiac death. There was a significant improvement in New York Heart Association classification in the enalapril group, together with a reduction in heart size and a reduced requirement for other heart failure medication. It is concluded that the addition of enalapril to conventional therapy in patients with severe congestive heart failure can reduce mortality and improve symptoms. The effect seems to be due to a reduction in death from progression of heart failure.",1988,13,3824,17,43,62,57,59,80,101,79,108,86,108
d3c92939c2d2b42ceb6e954709aaaa41c2103e14,"BACKGROUND
Implantable left ventricular assist devices have benefited patients with end-stage heart failure as a bridge to cardiac transplantation, but their long-term use for the purpose of enhancing survival and the quality of life has not been evaluated.


METHODS
We randomly assigned 129 patients with end-stage heart failure who were ineligible for cardiac transplantation to receive a left ventricular assist device (68 patients) or optimal medical management (61). All patients had symptoms of New York Heart Association class IV heart failure.


RESULTS
Kaplan-Meier survival analysis showed a reduction of 48 percent in the risk of death from any cause in the group that received left ventricular assist devices as compared with the medical-therapy group (relative risk, 0.52; 95 percent confidence interval, 0.34 to 0.78; P=0.001). The rates of survival at one year were 52 percent in the device group and 25 percent in the medical-therapy group (P=0.002), and the rates at two years were 23 percent and 8 percent (P=0.09), respectively. The frequency of serious adverse events in the device group was 2.35 (95 percent confidence interval, 1.86 to 2.95) times that in the medical-therapy group, with a predominance of infection, bleeding, and malfunction of the device. The quality of life was significantly improved at one year in the device group.


CONCLUSIONS
The use of a left ventricular assist device in patients with advanced heart failure resulted in a clinically meaningful survival benefit and an improved quality of life. A left ventricular assist device is an acceptable alternative therapy in selected patients who are not candidates for cardiac transplantation.",2001,29,3212,43,3,68,100,116,146,154,131,133,146,163
123ec3e65c7c3d16125abc7cbe2f325ce6c1024d,"BACKGROUND
It is not known whether the treatment of patients with asymptomatic left ventricular dysfunction reduces mortality and morbidity. We studied the effect of an angiotensin-converting--enzyme inhibitor, enalapril, on total mortality and mortality from cardiovascular causes, the development of heart failure, and hospitalization for heart failure among patients with ejection fractions of 0.35 or less who were not receiving drug treatment for heart failure.


METHODS
Patients were randomly assigned to receive either placebo (n = 2117) or enalapril (n = 2111) at doses of 2.5 to 20 mg per day in a double-blind trial. Follow-up averaged 37.4 months.


RESULTS
There were 334 deaths in the placebo group, as compared with 313 in the enalapril group (reduction in risk, 8 percent by the log-rank test; 95 percent confidence interval, -8 percent [an increase of 8 percent] to 21 percent; P = 0.30). The reduction in mortality from cardiovascular causes was larger but was not statistically significant (298 deaths in the placebo group vs. 265 in the enalapril group; risk reduction, 12 percent; 95 percent confidence interval, -3 to 26 percent; P = 0.12). When we combined patients in whom heart failure developed and those who died, the total number of deaths and cases of heart failure was lower in the enalapril group than in the placebo group (630 vs. 818; risk reduction, 29 percent; 95 percent confidence interval, 21 to 36 percent; P less than 0.001). In addition, fewer patients given enalapril died or were hospitalized for heart failure (434 in the enalapril group; vs. 518 in the placebo group; risk reduction, 20 percent; 95 percent confidence interval, 9 to 30 percent; P less than 0.001).


CONCLUSIONS
The angiotensin-converting--enzyme inhibitor enalapril significantly reduced the incidence of heart failure and the rate of related hospitalizations, as compared with the rates in the group given placebo, among patients with asymptomatic left ventricular dysfunction. There was also a trend toward fewer deaths due to cardiovascular causes among the patients who received enalapril.",1992,7,3558,39,11,102,101,119,139,145,160,169,183,176
3f1acb22acedf0a987ba6e0906a9bd10cb288b60,"BACKGROUND
Patients with advanced heart failure have improved survival rates and quality of life when treated with implanted pulsatile-flow left ventricular assist devices as compared with medical therapy. New continuous-flow devices are smaller and may be more durable than the pulsatile-flow devices.


METHODS
In this randomized trial, we enrolled patients with advanced heart failure who were ineligible for transplantation, in a 2:1 ratio, to undergo implantation of a continuous-flow device (134 patients) or the currently approved pulsatile-flow device (66 patients). The primary composite end point was, at 2 years, survival free from disabling stroke and reoperation to repair or replace the device. Secondary end points included survival, frequency of adverse events, the quality of life, and functional capacity.


RESULTS
Preoperative characteristics were similar in the two treatment groups, with a median age of 64 years (range, 26 to 81), a mean left ventricular ejection fraction of 17%, and nearly 80% of patients receiving intravenous inotropic agents. The primary composite end point was achieved in more patients with continuous-flow devices than with pulsatile-flow devices (62 of 134 [46%] vs. 7 of 66 [11%]; P<0.001; hazard ratio, 0.38; 95% confidence interval, 0.27 to 0.54; P<0.001), and patients with continuous-flow devices had superior actuarial survival rates at 2 years (58% vs. 24%, P=0.008). Adverse events and device replacements were less frequent in patients with the continuous-flow device. The quality of life and functional capacity improved significantly in both groups.


CONCLUSIONS
Treatment with a continuous-flow left ventricular assist device in patients with advanced heart failure significantly improved the probability of survival free from stroke and device failure at 2 years as compared with a pulsatile device. Both devices significantly improved the quality of life and functional capacity. (ClinicalTrials.gov number, NCT00121485.)",2009,42,2572,91,7,87,203,250,260,320,251,242,245,212
092cfb0ce325dc7d182ee8d42d7e13faf0caa99f,,1980,0,3177,194,0,0,3,2,2,4,3,5,4,8
bea542a2bfd13956f3ff878c887e163f928dd2b3,"Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide area traces, investigating a number of wide area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remote-login and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into ""connection bursts"", the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of wide area traffic. >",1995,34,3170,146,16,38,80,102,130,126,159,201,228,211
7e087b2c0c656f6d2e24849168a934bd018222ac,"Diastolic heart failure (DHF) currently accounts for more than 50% of all heart failure patients. DHF is also referred to as heart failure with normal left ventricular (LV) ejection fraction (HFNEF) to indicate that HFNEF could be a precursor of heart failure with reduced LVEF. Because of improved cardiac imaging and because of widespread clinical use of plasma levels of natriuretic peptides, diagnostic criteria for HFNEF needed to be updated. The diagnosis of HFNEF requires the following conditions to be satisfied: (i) signs or symptoms of heart failure; (ii) normal or mildly abnormal systolic LV function; (iii) evidence of diastolic LV dysfunction. Normal or mildly abnormal systolic LV function implies both an LVEF > 50% and an LV end-diastolic volume index (LVEDVI) <97 mL/m(2). Diagnostic evidence of diastolic LV dysfunction can be obtained invasively (LV end-diastolic pressure >16 mmHg or mean pulmonary capillary wedge pressure >12 mmHg) or non-invasively by tissue Doppler (TD) (E/E' > 15). If TD yields an E/E' ratio suggestive of diastolic LV dysfunction (15 > E/E' > 8), additional non-invasive investigations are required for diagnostic evidence of diastolic LV dysfunction. These can consist of blood flow Doppler of mitral valve or pulmonary veins, echo measures of LV mass index or left atrial volume index, electrocardiographic evidence of atrial fibrillation, or plasma levels of natriuretic peptides. If plasma levels of natriuretic peptides are elevated, diagnostic evidence of diastolic LV dysfunction also requires additional non-invasive investigations such as TD, blood flow Doppler of mitral valve or pulmonary veins, echo measures of LV mass index or left atrial volume index, or electrocardiographic evidence of atrial fibrillation. A similar strategy with focus on a high negative predictive value of successive investigations is proposed for the exclusion of HFNEF in patients with breathlessness and no signs of congestion. The updated strategies for the diagnosis and exclusion of HFNEF are useful not only for individual patient management but also for patient recruitment in future clinical trials exploring therapies for HFNEF.",2007,195,2463,131,17,109,176,180,192,229,216,208,255,210
6f8d79c9fa1bde18bad2fb5c5cf12b9e530d8829,"Procrastination is a prevalent and pernicious form of self-regulatory failure that is not entirely understood. Hence, the relevant conceptual, theoretical, and empirical work is reviewed, drawing upon correlational, experimental, and qualitative findings. A meta-analysis of procrastination's possible causes and effects, based on 691 correlations, reveals that neuroticism, rebelliousness, and sensation seeking show only a weak connection. Strong and consistent predictors of procrastination were task aversiveness, task delay, self-efficacy, and impulsiveness, as well as conscientiousness and its facets of self-control, distractibility, organization, and achievement motivation. These effects prove consistent with temporal motivation theory, an integrative hybrid of expectancy theory and hyperbolic discounting. Continued research into procrastination should not be delayed, especially because its prevalence appears to be growing.",2007,536,1666,337,7,23,25,33,58,80,94,129,138,160
bbd9f8a0f470a4295a3eea6a0ed41021cf2c0e77,"BACKGROUND
This trial was designed to determine whether cardiac-resynchronization therapy (CRT) with biventricular pacing would reduce the risk of death or heart-failure events in patients with mild cardiac symptoms, a reduced ejection fraction, and a wide QRS complex.


METHODS
During a 4.5-year period, we enrolled and followed 1820 patients with ischemic or nonischemic cardiomyopathy, an ejection fraction of 30% or less, a QRS duration of 130 msec or more, and New York Heart Association class I or II symptoms. Patients were randomly assigned in a 3:2 ratio to receive CRT plus an implantable cardioverter-defibrillator (ICD) (1089 patients) or an ICD alone (731 patients). The primary end point was death from any cause or a nonfatal heart-failure event (whichever came first). Heart-failure events were diagnosed by physicians who were aware of the treatment assignments, but they were adjudicated by a committee that was unaware of assignments.


RESULTS
During an average follow-up of 2.4 years, the primary end point occurred in 187 of 1089 patients in the CRT-ICD group (17.2%) and 185 of 731 patients in the ICD-only group (25.3%) (hazard ratio in the CRT-ICD group, 0.66; 95% confidence interval [CI], 0.52 to 0.84; P=0.001). The benefit did not differ significantly between patients with ischemic cardiomyopathy and those with nonischemic cardiomyopathy. The superiority of CRT was driven by a 41% reduction in the risk of heart-failure events, a finding that was evident primarily in a prespecified subgroup of patients with a QRS duration of 150 msec or more. CRT was associated with a significant reduction in left ventricular volumes and improvement in the ejection fraction. There was no significant difference between the two groups in the overall risk of death, with a 3% annual mortality rate in each treatment group. Serious adverse events were infrequent in the two groups.


CONCLUSIONS
CRT combined with ICD decreased the risk of heart-failure events in relatively asymptomatic patients with a low ejection fraction and wide QRS complex. (ClinicalTrials.gov number, NCT00180271.)",2009,21,2433,76,23,130,254,250,263,254,211,215,205,181
943458266045b452e982e66176dbc618ae4b56fc,"Customers often react strongly to service failures, so it is critical that an organization's recovery efforts be equally strong and effective. In this article, the authors develop a model of custom...",1999,82,2181,250,2,9,24,29,34,47,52,60,90,102
91cb62fc50c82202b4cf9f4bbfc757895ead6193,"Hemodynamics, plasma norepinephrine, and plasma renin activity were measured at supine rest in 106 patients (83 men and 23 women) with moderate to severe congestive heart failure. During follow-up lasting 1 to 62 months, 60 patients died (57 per cent); 47 per cent of the deaths were sudden, and 45 per cent were related to progressive heart failure. Statistically unrelated to the risk of mortality were cause of disease (60 patients had coronary disease, and 46 had cardiomyopathy), age (mean, 54.8 years), cardiac index (mean, 2.11 liters per minute per square meter of body-surface area), pulmonary wedge pressure (mean, 24.5 mm Hg), and mean arterial pressure (mean, 83.2 mm Hg). A multivariate analysis of the five significant univariate prognosticators--heart rate (mean, 84.4 beats per minute), plasma renin activity (mean, 15.4 ng per milliliter per hour), plasma norepinephrine (mean, 700 pg per milliliter), serum sodium (mean, 135.7 mmol per liter), and stroke-work index (mean, 21.0 g-meters per square meter)--found only plasma norepinephrine to be independently (P = 0.002) related to the subsequent risk of mortality. Norepinephrine was also higher in patients who died from progressive heart failure than in those who died suddenly. These data suggest that a single resting venous blood sample showing the plasma norepinephrine concentration provides a better guide to prognosis than other commonly measured indexes of cardiac performance.",1984,22,3054,51,2,12,38,49,52,53,55,54,65,61
d8acb4892c052331bcf246ca7f6821b74d745713,"BACKGROUND
Previous studies have suggested that cardiac resynchronization achieved through atrial-synchronized biventricular pacing produces clinical benefits in patients with heart failure who have an intraventricular conduction delay. We conducted a double-blind trial to evaluate this therapeutic approach.


METHODS
Four hundred fifty-three patients with moderate-to-severe symptoms of heart failure associated with an ejection fraction of 35 percent or less and a QRS interval of 130 msec or more were randomly assigned to a cardiac-resynchronization group (228 patients) or to a control group (225 patients) for six months, while conventional therapy for heart failure was maintained. The primary end points were the New York Heart Association functional class, quality of life, and the distance walked in six minutes.


RESULTS
As compared with the control group, patients assigned to cardiac resynchronization experienced an improvement in the distance walked in six minutes (+39 vs. +10 m, P=0.005), functional class (P<0.001), quality of life (-18.0 vs. -9.0 points, P= 0.001), time on the treadmill during exercise testing (+81 vs. +19 sec, P=0.001), and ejection fraction (+4.6 percent vs. -0.2 percent, P<0.001). In addition, fewer patients in the group assigned to cardiac resynchronization than control patients required hospitalization (8 percent vs. 15 percent) or intravenous medications (7 percent vs. 15 percent) for the treatment of heart failure (P<0.05 for both comparisons). Implantation of the device was unsuccessful in 8 percent of patients and was complicated by refractory hypotension, bradycardia, or asystole in four patients (two of whom died) and by perforation of the coronary sinus requiring pericardiocentesis in two others.


CONCLUSIONS
Cardiac resynchronization results in significant clinical improvement in patients who have moderate-to-severe heart failure and an intraventricular conduction delay.",2002,0,2613,113,11,110,116,143,145,172,173,206,188,195
48307473b7e660a7cedecac441f2b07c3bfcdfdb,"Why might firms be regarded as astutely managed at one point, yet subsequently lose their positions of industry leadership when faced with technological change? We present a model, grounded in a study of the world disk drive industry, that charts the process through which the demands of a firm's customers shape the allocation of resources in technological innovation—a model that links theories of resource dependence and resource allocation. We show that established firms led the industry in developing technologies of every sort—even radical ones—whenever the technologies addressed existing customers' needs. The same firms failed to develop simpler technologies that initially were only useful in emerging markets, because impetus coalesces behind, and resources are allocated to, programs targeting powerful customers. Projects targeted at technologies for which no customers yet exist languish for lack of impetus and resources. Because the rate of technical progress can exceed the performance demanded in a market, technologies which initially can only be used in emerging markets later can invade mainstream ones, carrying entrant firms to victory over established companies.",1996,33,2358,124,0,2,6,14,17,34,36,66,62,64
b38899169785497d4cfa21e8d8f0838216d6c7a9,"A topic that has received attention in both the statistical and medical literature is the estimation of the probability of failure for endpoints that are subject to competing risks. Despite this, it is not uncommon to see the complement of the Kaplan-Meier estimate used in this setting and interpreted as the probability of failure. If one desires an estimate that can be interpreted in this way, however, the cumulative incidence estimate is the appropriate tool to use in such situations. We believe the more commonly seen representations of the Kaplan-Meier estimate and the cumulative incidence estimate do not lend themselves to easy explanation and understanding of this interpretation. We present, therefore, a representation of each estimate in a manner not ordinarily seen, each representation utilizing the concept of censored observations being 'redistributed to the right.' We feel these allow a more intuitive understanding of each estimate and therefore an appreciation of why the Kaplan-Meier method is inappropriate for estimation purposes in the presence of competing risks, while the cumulative incidence estimate is appropriate.",1999,21,2480,76,1,26,33,37,55,51,73,72,102,127
3a220f28d076e62794ba70a0865713351b7205cf,"BACKGROUND
B-type natriuretic peptide is released from the cardiac ventricles in response to increased wall tension.


METHODS
We conducted a prospective study of 1586 patients who came to the emergency department with acute dyspnea and whose B-type natriuretic peptide was measured with a bedside assay. The clinical diagnosis of congestive heart failure was adjudicated by two independent cardiologists, who were blinded to the results of the B-type natriuretic peptide assay.


RESULTS
The final diagnosis was dyspnea due to congestive heart failure in 744 patients (47 percent), dyspnea due to noncardiac causes in 72 patients with a history of left ventricular dysfunction (5 percent), and no finding of congestive heart failure in 770 patients (49 percent). B-type natriuretic peptide levels by themselves were more accurate than any historical or physical findings or laboratory values in identifying congestive heart failure as the cause of dyspnea. The diagnostic accuracy of B-type natriuretic peptide at a cutoff of 100 pg per milliliter was 83.4 percent. The negative predictive value of B-type natriuretic peptide at levels of less than 50 pg per milliliter was 96 percent. In multiple logistic-regression analysis, measurements of B-type natriuretic peptide added significant independent predictive power to other clinical variables in models predicting which patients had congestive heart failure.


CONCLUSIONS
Used in conjunction with other clinical information, rapid measurement of B-type natriuretic peptide is useful in establishing or excluding the diagnosis of congestive heart failure in patients with acute dyspnea.",2002,37,2559,52,17,107,200,243,258,184,161,153,128,127
9bf25fb7735f03e0f3ec40565b475b8e82f10f1d,"BACKGROUND
To define better the efficacy of vasodilator therapy in the treatment of chronic congestive heart failure, we compared the effects of hydralazine and isosorbide dinitrate with those of enalapril in 804 men receiving digoxin and diuretic therapy for heart failure. The patients were randomly assigned in a double-blind manner to receive 20 mg of enalapril daily or 300 mg of hydralazine plus 160 mg of isosorbide dinitrate daily. The latter regimen was identical to that used with a similar patient population in the effective-treatment arm of our previous Vasodilator-Heart Failure Trial.


RESULTS
Mortality after two years was significantly lower in the enalapril arm (18 percent) than in the hydralazine-isosorbide dinitrate arm (25 percent) (P = 0.016; reduction in mortality, 28.0 percent), and overall mortality tended to be lower (P = 0.08). The lower mortality in the enalapril arm was attributable to a reduction in the incidence of sudden death, and this beneficial effect was more prominent in patients with less severe symptoms (New York Heart Association class I or II). In contrast, body oxygen consumption at peak exercise was increased only by hydralazine-isosorbide dinitrate treatment (P less than 0.05), and left ventricular ejection fraction, which increased with both regimens during the 2 years after randomization, increased more (P less than 0.05) during the first 13 weeks in the hydralazine-isosorbide dinitrate group.


CONCLUSIONS
The similar two-year mortality in the hydralazine-isosorbide dinitrate arms in our previous Vasodilator-Heart Failure Trial (26 percent) and in the present trial (25 percent), as compared with that in the placebo arm in the previous trial, (34 percent) and the further survival benefit with enalapril in the present trial (18 percent) strengthen the conclusion that vasodilator therapy should be included in the standard treatment for heart failure. The different effects of the two regimens (enalapril and hydralazine-isosorbide dinitrate) on mortality and physiologic end points suggest that the profile of effects might be enhanced if the regimens were used in combination.",1991,15,2672,34,14,78,128,103,140,136,114,137,133,148
c0a76925c5a5a52731283d799b65801b81cc9670,"Abstract The natural history of congestive heart failure was studied over a 16-year period in 5192 persons initially free of the disease. Over this period, overt evidence of congestive heart failure developed in 142 persons. In almost every five-year age group, from 30 to 62 years, the incidence rate was greater for men than for women. Although the usual etiologic precursors were found, the dominant one was clearly hypertension, which preceded failure in 75 per cent of the cases. Coronary heart disease was noted at an earlier examination in 39 per cent, but in 29 per cent of the cases it was accompanied by hypertension. Precursive rheumatic heart disease, noted in 21 per cent of cases of congestive heart failure, was accompanied by hypertension in 11 per cent. Despite modern management, congestive heart failure proved to be extremely lethal. The probability of dying within five years from onset of congestive heart failure was 62 per cent for men and 42 per cent for women.",1971,17,2976,55,0,6,2,6,3,7,2,7,5,4
fd97302716a810db9dd5ec9d6342ccf2324c5489,This review provides an update of previous estimates of first-year probabilities of contraceptive failure for all methods of contraception available in the United States. Estimates are provided of probabilities of failure during typical use (which includes both incorrect and inconsistent use) and during perfect use (correct and consistent use). The difference between these two probabilities reveals the consequences of imperfect use; it depends both on how unforgiving of imperfect use a method is and on how hard it is to use that method perfectly. These revisions reflect new research on contraceptive failure both during perfect use and during typical use.,2004,103,997,48,0,1,2,3,2,25,22,36,50,69
287c5eb5857102bff5f566ed9ed723d25a960328,"Objective. To establish a clinically relevant list with explicit criteria for pharmacologically inappropriate prescriptions in general practice for elderly people ≥70 years. Design. A three-round Delphi process for validating the clinical relevance of suggested criteria (n = 37) for inappropriate prescriptions to elderly patients. Setting. A postal consensus process undertaken by a panel of specialists in general practice, clinical pharmacology, and geriatrics. Main outcome measures. The Norwegian General Practice (NORGEP) criteria, a relevance-validated list of drugs, drug dosages, and drug combinations to be avoided in the elderly (≤70 years) patients. Results. Of the 140 invited panellists, 57 accepted to participate and 47 completed all three rounds of the Delphi process. The panellists reached consensus that 36 of the 37 suggested criteria were clinically relevant for general practice. Relevance of three of the criteria was rated significantly higher in Round 3 than in Round 1. At the end of the Delphi process, a significant difference between the different specialist groups’ scores was seen for only one of the 36 criteria. Conclusion. The NORGEP criteria may serve as rules of thumb for general practitioners (GPs) related to their prescribing practice for elderly patients, and as a tool for evaluating the quality of GPs’ prescribing in settings where access to clinical information for individual patients is limited, e.g. in prescription databases and quality improvement interventions.",2009,48,216,18,0,7,13,25,18,23,21,24,18,25
71de3ae53b4ad8557f6b203b3d9e555a66bbb651,"The epidemiology of immune thrombocytopenic purpura (ITP) is not well‐characterised in the general population. This study described the incidence and survival of ITP using the UK population‐based General Practice Research Database (GPRD). ITP patients first diagnosed in 1990–2005 were identified in the GPRD. Overall incidence rates (per 100 000 person‐years) and rates by age, sex, and calendar periods were calculated. Survival analysis was conducted using the Kaplan‐Meier and proportional hazard methods. A total of 1145 incident ITP patients were identified. The crude incidence was 3·9 (95% confidence interval [CI]: 3·7–4·1). Overall average incidence was statistically significantly higher in women (4·4, 95% CI: 4·1–4·7) compared to men (3·4; 95% CI: 3·1–3·7). Among men, incidence was bimodal with peaks among ages under 18 and between 75–84 years. The hazard ratio for death among ITP patients was 1·6 (95% CI: 1·3–1·9) compared to age‐ and sex‐matched comparisons. During follow‐up 139 cases died, of whom 75 had a computerised plausible cause of death. Death was related to bleeding in 13% and infection in 19% of these 75. In conclusion, ITP incidence varies with age and is higher in women than men. This potentially serious medical condition is associated with increased mortality in the UK.",2009,49,195,11,2,12,14,17,16,18,22,15,22,14
f8e5bbe7e6f67ad4f829e21c90a1d288a13e9825,"Ageing of the population in western societies and the rising costs of health and social care are refocusing health policy on health promotion and disability prevention among older people. However, efforts to identify at-risk groups of older people and to alter the trajectory of avoidable problems associated with ageing by early intervention or multidisciplinary case management have been largely unsuccessful. This paper argues that this failure arises from the dominance in primary care of a managerial perspective on health care for older people, and proposes instead the adoption of a clinical paradigm based on the concept of frailty. Frailty, in its simplest definition, is vulnerability to adverse outcomes. It is a dynamic concept that is different from disability and easy to overlook, but also easy to identify using heuristics (rules of thumb) and to measure using simple scales. Conceptually, frailty fits well with the biopsychosocial model of general practice, offers practitioners useful tools for patient care, and provides commissioners of health care with a clinical focus for targeting resources at an ageing population.",2009,72,198,3,3,10,16,19,20,10,22,17,20,14
91e0c86e47e15bee8a3b6203c7ffd650a3afc75f,"BACKGROUND
Although studies are available on patients' ideas, concerns, and expectations in primary care, there is a scarcity of studies that explore the triad of ideas, concerns, and expectations (ICE) in general practice consultations and the impact on medication prescribing.


AIM
To evaluate the presence of ICE and its relation to medication prescription.


DESIGN OF STUDY
Cross-sectional study.


SETTING
Thirty-six GP teaching practices affiliated with the University of Ghent, in Flanders, Belgium.


METHOD
Participants were all patients consulting on 30 May 2005, and their doctors. Reasons for an encounter (consultation or home visit) with new and follow-up contacts, the identification of ICE, and the prescription of medication were recorded by 36 trainee GPs undergoing observational training. The study included 613 consultations.


RESULTS
One, two, or three of the ICE components were expressed in 38.5%, 24.4%, and 20.1% (n = 236, 150, 123) of contacts respectively. On the other hand, in 17.0% (104/613) of all contacts, and in 22% (77/350) of the new contact reasons, no ICE was voiced, and the GPs operated without knowing this information about the patients. Mean number of ICE components per doctor and per contact was 1.54 (standard deviation = 0.54). A logistic regression analysis of the 350 new contacts showed that the presence of concerns (P = 0.037, odds ratio [OR] 1.73, 95% confidence interval [CI] = 1.03 to 2.9), and expectations (P = 0.009, OR = 2.0, 95% CI = 1.2 to 3.4) was associated with not prescribing new medication (dichotomised into the categories present/absent); however, other patient, doctor, and student variables were not significantly associated with medication prescription.


CONCLUSION
An association was found between the presence of concerns and/or expectations, and less medication prescribing. The data suggest that exploring ICE components may lead to fewer new medication prescriptions.",2009,45,93,1,6,4,3,3,6,9,8,5,8,9
ade4d85d0937c9445fc88a96feab1af55e785f19,"BACKGROUND
Health literacy is the ability to understand and interpret the meaning of health information in written, spoken or digital form and how this motivates people to embrace or disregard actions relating to health.


OBJECTIVE
This article aims to describe the concept of health literacy, its importance and its applications in the general practice setting.


DISCUSSION
Australia trails behind other western countries in practical applications of health literacy. Health literacy underpins the efficiency of consultations, health promotion efforts, and self management programs. Recognition of the health literacy status of individuals allows use of appropriate communication tools. This can save time and effort and improve patient satisfaction and health outcomes.",2009,23,119,5,2,4,11,12,8,11,12,7,6,10
e3283da7acdf880647c34b48e2951058abeba8f3,Objective: To evaluate the management of cardiovascular disease (CVD) risk in Australian general practice.,2009,31,116,2,2,10,4,14,10,11,16,6,9,11
9397e3d9a5556de8be5ed16bedfaf3d2e78d054c,"BACKGROUND
The extent to which a fear of needles influences health decisions remains largely unexplored. This study investigated the prevalence of fear of needles in a southeast Queensland community, described associated symptoms, and highlighted health care avoidance tendencies of affected individuals.


METHODS
One hundred and seventy-seven participants attending an outer urban general practice responded to a questionnaire on fear of needles, symptoms associated with needles and its influence on their use of medical care.


RESULTS
Twenty-two percent of participants reported a fear of needles. Affected participants were more likely than participants with no fear to report vasovagal symptoms, have had a previous traumatic needle experience (46.2 vs. 16.4%, p<0.001) and avoid medical treatment involving needles (20.5 vs. 2.3%, p<0.001).


DISCUSSION
Fear of needles is common and is associated with health care avoidance. Health professionals could better identify and manage patients who have a fear of needles by recognising associated vasovagal symptoms and past traumatic experiences.",2009,16,116,5,1,1,3,5,5,16,11,18,15,15
d29c2d9d76def6f0789b16d7fc9227dc0e08c3e2,"Over the past 5 years, general practice in the UK has undergone major change. Starting with the introduction of the new GMS contract in 2004, it has continued apace with the establishment of Postgraduate Medical Education Training Board, a GP training curriculum, and nMRCGP. The NHS is developing very differently in the four countries of the UK. Regulation of the profession is under review, and a system of relicensing, recertification, and revalidation is being introduced. The Essence project, initiated by RCGP Scotland in conjunction with International Futures Forum 4 years ago is a constructive response to these changes. It has included learning journeys, a discussion day for GPs, and commissioned short pieces of 100 words from GPs and patients. From an analysis of these, some characteristics of the essence of general practice have been defined. These include key roles and core personal qualities for GPs. It is argued that general practice has important and unique advantages - trust, coordination, continuity, flexibility, universal coverage, and leadership - which mean that it should continue to be central to the development of primary care throughout the UK.",2009,55,53,1,9,8,7,7,2,7,3,2,4,2
f66a0d9cfd4f5eb5d586f53732cfe2bb38b471cb,"BACKGROUND
In Australia, most medical students graduate without a firm career choice, with this decision being made during their early postgraduate years. Strategies addressing the current lack of meaningful exposure to general practice during these formative prevocational years are likely to be the most effective in increasing the proportion and number of entrants to general practice.


OBJECTIVE
This review summarises the influences of medical student selection criteria, curriculum, geographical location, timing and duration of general practice exposure and experience, prevocational experience, and vocational training, on an eventual choice of general practice as a career.


DISCUSSION
These are important influences on the complex process of career choice. Much research has focused on isolated interventions at one point along the pipeline. Varied and conflicting conclusions emerge from individual studies. In complex systems it is hard to understand the influence of an isolated intervention without looking at the system as a whole.",2009,28,60,2,0,2,12,13,4,6,4,8,2,3
f7d7a2c03932683dfc56baf3627f37c6c165aef3,"BackgroundGeneral practitioners sometimes base clinical decisions on gut feelings alone, even though there is little evidence of their diagnostic and prognostic value in daily practice. Research to validate the determinants and to assess the test properties of gut feelings requires precise and valid descriptions of gut feelings in general practice which can be used as a reliable measuring instrument. Research question: Can we obtain consensus on descriptions of two types of gut feelings: a sense of alarm and a sense of reassurance?MethodsQualitative research including a Delphi consensus procedure with a heterogeneous sample of 27 Dutch and Belgian GPs or ex-GPs involved in academic educational or research programmes.ResultsAfter four rounds, we found 70% or greater agreement on seven of the eleven proposed statements. A ""sense of alarm"" is defined as an uneasy feeling perceived by a GP as he/she is concerned about a possible adverse outcome, even though specific indications are lacking: There's something wrong here. This activates the diagnostic process by stimulating the GP to formulate and weigh up working hypotheses that might involve a serious outcome. A ""sense of alarm"" means that, if possible, the GP needs to initiate specific management to prevent serious health problems. A ""sense of reassurance"" is defined as a secure feeling perceived by a GP about the further management and course of a patient's problem, even though the doctor may not be certain about the diagnosis: Everything fits in.ConclusionThe sense of alarm and the sense of reassurance are well-defined concepts. These descriptions enable us to operationalise the concept of gut feelings in further research.",2009,21,77,4,0,11,5,3,5,4,4,5,9,9
b612a64c54e09baa3f094940bd5aa9870fd42df9,"BackgroundSchizophrenia patients frequently develop somatic co-morbidity. Core tasks for GPs are the prevention and diagnosis of somatic diseases and the provision of care for patients with chronic diseases. Schizophrenia patients experience difficulties in recognizing and coping with their physical problems; however GPs have neither specific management policies nor guidelines for the diagnosis and treatment of somatic co-morbidity in schizophrenia patients. This paper systematically reviews the prevalence and treatment of somatic co-morbidity in schizophrenia patients in general practice.MethodsThe MEDLINE, EMBASE, PsycINFO data-bases and the Cochrane Library were searched and original research articles on somatic diseases of schizophrenia patients and their treatment in the primary care setting were selected.ResultsThe results of this search show that the incidence of a wide range of diseases, such as diabetes mellitus, the metabolic syndrome, coronary heart diseases, and COPD is significantly higher in schizophrenia patients than in the normal population. The health of schizophrenic patients is less than optimal in several areas, partly due to their inadequate help-seeking behaviour. Current GP management of such patients appears not to take this fact into account. However, when schizophrenic patients seek the GP's help, they value the care provided.ConclusionSchizophrenia patients are at risk of undetected somatic co-morbidity. They present physical complaints at a late, more serious stage. GPs should take this into account by adopting proactive behaviour. The development of a set of guidelines with a clear description of the GP's responsibilities would facilitate the desired changes in the management of somatic diseases in these patients.",2009,50,115,0,4,8,14,9,13,9,9,9,10,10
6b25ca281526bf0a5fb955ad2617662d4e52718b,"BackgroundWith increasing rates of chronic disease associated with lifestyle behavioural risk factors, there is urgent need for intervention strategies in primary health care. Currently there is a gap in the knowledge of factors that influence the delivery of preventive strategies by General Practitioners (GPs) around interventions for smoking, nutrition, alcohol consumption and physical activity (SNAP). This qualitative study explores the delivery of lifestyle behavioural risk factor screening and management by GPs within a 45–49 year old health check consultation. The aims of this research are to identify the influences affecting GPs' choosing to screen and choosing to manage SNAP lifestyle risk factors, as well as identify influences on screening and management when multiple SNAP factors exist.MethodsA total of 29 audio-taped interviews were conducted with 15 GPs and one practice nurse over two stages. Transcripts from the interviews were thematically analysed, and a model of influencing factors on preventive care behaviour was developed using the Theory of Planned Behaviour as a structural framework.ResultsGPs felt that assessing smoking status was straightforward, however some found assessing alcohol intake only possible during a formal health check. Diet and physical activity were often inferred from appearance, only being assessed if the patient was overweight. The frequency and thoroughness of assessment were influenced by the GPs' personal interests and perceived congruence with their role, the level of risk to the patient, the capacity of the practice and availability of time. All GPs considered advising and educating patients part of their professional responsibility. However their attempts to motivate patients were influenced by perceptions of their own effectiveness, with smoking causing the most frustration. Active follow-up and referral of patients appeared to depend on the GPs' orientation to preventive care, the patient's motivation, and cost and accessibility of services to patients.ConclusionGeneral practitioner attitudes, normative influences from both patients and the profession, and perceived external control factors (time, cost, availability and practice capacity) all influence management of behavioural risk factors. Provider education, community awareness raising, support and capacity building may improve the uptake of lifestyle modification interventions.",2009,22,180,13,0,11,10,23,30,20,24,12,9,15
6624a88676463a96f3b67c7731d0ea8695f5a8df,"Traditionally, professional expertise has been judged by length of experience, reputation, and perceived mastery of knowledge and skill. Unfortunately, recent research demonstrates only a weak relationship between these indicators of expertise and actual, observed performance. In fact, observed performance does not necessarily correlate with greater professional experience. Expert performance can, however, be traced to active engagement in deliberate practice (DP), where training (often designed and arranged by their teachers and coaches) is focused on improving particular tasks. DP also involves the provision of immediate feedback, time for problem-solving and evaluation, and opportunities for repeated performance to refine behavior. In this article, we draw upon the principles of DP established in other domains, such as chess, music, typing, and sports to provide insight into developing expert performance in medicine.",2008,41,1186,52,2,9,30,46,66,99,99,109,110,107
7cfad0fc9e5413aa34324b776c027e5c1aa89b39,"Outline of a Theory of Practice is recognized as a major theoretical text on the foundations of anthropology and sociology. Pierre Bourdieu, a distinguished French anthropologist, develops a theory of practice which is simultaneously a critique of the methods and postures of social science and a general account of how human action should be understood. With his central concept of the habitus, the principle which negotiates between objective structures and practices, Bourdieu is able to transcend the dichotomies which have shaped theoretical thinking about the social world. The author draws on his fieldwork in Kabylia (Algeria) to illustrate his theoretical propositions. With detailed study of matrimonial strategies and the role of rite and myth, he analyses the dialectical process of the 'incorporation of structures' and the objectification of habitus, whereby social formations tend to reproduce themselves. A rigorous consistent materialist approach lays the foundations for a theory of symbolic capital and, through analysis of the different modes of domination, a theory of symbolic power.",1972,0,22478,1640,0,0,0,0,0,0,0,0,0,0
fe001eb4659b6fefa8974780bcfb8afc0eaa0e42,,1899,0,99,0,0,0,0,0,0,0,0,0,0,0
2f3d7e9a29b58ec04a0d52257984011b1154962d,"AbstractBackgroundThe impact of high physician workload and job stress on quality and outcomes of healthcare delivery is not clear. Our study explored whether high workload and job stress were associated with lower performance in general practices in the Netherlands.MethodsSecondary analysis of data from 239 general practices, collected in practice visits between 2003 to 2006 in the Netherlands using a comprehensive set of measures of practice management. Data were collected by a practice visitor, a trained non-physician observer using patients questionnaires, doctors and staff. For this study we selected five measures of practice performance as outcomes and six measures of GP workload and job stress as predictors. A total of 79 indicators were used out of the 303 available indicators. Random coefficient regression models were applied to examine associations.Results and discussionWorkload and job stress are associated with practice performance.
Workload: Working more hours as a GP was associated with more positive patient experiences of accessibility and availability (b = 0.16). After list size adjustment, practices with more GP-time per patient scored higher on GP care (b = 0.45). When GPs provided more than 20 hours per week per 1000 patients, patients scored over 80% on the Europep questionnaire for quality of GP care.
Job stress: High GP job stress was associated with lower accessibility and availability (b = 0.21) and insufficient practice management (b = 0.25). Higher GP commitment and more satisfaction with the job was associated with more prevention and disease management (b = 0.35).ConclusionProviding more time in the practice, and more time per patient and experiencing less job stress are all associated with perceptions by patients of better care and better practice performance. Workload and job stress should be assessed by using list size adjusted data in order to realise better quality of care. Organisational development using this kind of data feedback could benefit both patients and GP.",2009,48,92,1,0,4,4,7,7,6,8,9,9,10
5a9847dc44598142d4552f6f4e4d6b30b31c43c0,"Primary care spirometry services can be provided by trained primary care staff, peripatetic specialist services, or through referral to hospital-based or laboratory spirometry. The first of these options is the focus of this Standards Document. It aims to provide detailed information for clinicians, managers and healthcare commissioners on the key areas of quality required for diagnostic spirometry in primary care--including training requirements and quality assurance. These proposals and recommendations are designed to raise the standard of spirometry and respiratory diagnosis in primary care and to provide the impetus for debate, improvement and maintenance of quality for diagnostic (rather than screening) spirometry performed in primary care. This document should therefore challenge current performance and should constitute an aspirational guide for delivery of this service.",2009,170,185,4,5,17,21,24,19,14,15,19,11,9
c2f2e32233712d0fc3ca57a401464600ca6e28e0,"BACKGROUND AND AIMS
Clinical inertia is considered a major barrier to better care. We assessed its prevalence, predictors and associations with the intermediate outcomes of diabetes care.


MATERIALS AND METHODS
Baseline and follow-up data of a Dutch randomized controlled trial on the implementation of a locally adapted guideline were used. The study involved 30 general practices and 1283 patients. Treatment targets differed between study groups [HbA1c <or= 8.0% and blood pressure (BP) < 140/85% versus HbA1c <or= 8.5% and BP < 150/85]. Clinical inertia was defined as the failure to intensify therapy when indicated. A complete medication profile of all participating patients was obtained.


RESULTS
In the intervention and control group, the percentages of patients with poor diabetes or lipid control who did not receive treatment intensification were 45% and 90%, approximately. More control group patients with BP levels above target were confronted with inertia (72.7% versus 63.3%, P < 0.05). In poorly controlled hypertensive patients, inertia was associated with the height of systolic BP at baseline [adjusted odds ratio (OR) 0.98, 95% confidence interval (CI) 0.98-0.99] and the frequency of BP control (adjusted OR 0.89, 95% CI 0.81-0.99). If a practice nurse managed these patients, clinical inertia was less common (adjusted OR 0.12, 95% CI 0.02-0.91). In both study groups, cholesterol decreased significantly more in patients who received proper treatment intensification.


CONCLUSION
GPs were more inclined to control blood glucose levels than BP or cholesterol levels. Inertia in response to poorly controlled high BP was less common if nurses assisted GPs.",2009,68,91,3,1,2,13,4,22,7,9,4,9,4
3641e66e6e0bc275ccf1ad277880a9611df92fdc,"‘There are few things we should keenly desire if we really knew what we wanted.’ Francois de la Rochefoucauld (French writer 1613–1680) Social prescribing is about expanding the range of options available to GP and patient as they grapple with a problem. Where that problem has its origins in socioeconomic deprivation or long-term psychosocial issues, it is easy for both patient and GP to feel overwhelmed and reluctant to open what could turn out to be a can of worms. Settling for a short-term medical fix may be pragmatic but can easily become a conspiracy of silence which confirms the underlying sense of defeat. Can or should we try to do more during the precious minutes of a GP consultation?

Where there are psychosocial issues GPs do suggest social avenues, such as visiting a Citizens Advice Bureau for financial problems, or a dance class for exercise and loneliness, but without a supportive framework this tends to be a token action. The big picture difficulty with leaving underlying psychosocial problems largely hidden in the consulting room is the medicalisation of society's ills. This ranges from using antidepressants for the misery of a difficult life, to the complex pharmaceutical regimes prescribed to patients with obesity and type 2 diabetes. This sort of medicalisation may help immediate problems (including driving the economy through jobs in the healthcare industries) but it is not enough if our society is to have a sustainable future.

Another way of looking at this is in terms of choice. The consumerist type of choice of provider beloved of the government, is what Canadian philosopher Charles Taylor calls ‘weak evaluation’.1 By this he means a utilitarian ‘weighing-up’ of generally short-term consequences of a choice. These choices represent ‘second-order desires’, such as to feel more cheerful, or to relieve a …",2009,31,118,1,0,0,1,2,6,3,12,8,8,19
e26b2804e4d12e0391893c496d46fdbd832dd1a2,"1. Introduction Designing PCR and sequencing primers are essential activities for molecular biologists around the world. This chapter assumes acquaintance with the principles and practice of PCR, as outlined in, for example, refs. 1–4. Primer3 is a computer program that suggests PCR primers for a variety of applications, for example to create STSs (sequence tagged sites) for radiation hybrid mapping (5), or to amplify sequences for single nucleotide polymor-phism discovery (6). Primer3 can also select single primers for sequencing reactions and can design oligonucleotide hybridization probes. In selecting oligos for primers or hybridization probes, Primer3 can consider many factors. These include oligo melting temperature, length, GC content , 3′ stability, estimated secondary structure, the likelihood of annealing to or amplifying undesirable sequences (for example interspersed repeats), the likelihood of primer–dimer formation between two copies of the same primer, and the accuracy of the source sequence. In the design of primer pairs Primer3 can consider product size and melting temperature, the likelihood of primer– dimer formation between the two primers in the pair, the difference between primer melting temperatures, and primer location relative to particular regions of interest or to be avoided.",2000,21,16090,1332,0,0,1,0,0,0,2,1,2,8
e0ed44b682c1ec1684909eb9c7c06fd659aa7c74,"Objective: To investigate and compare the prevalence, comorbidities and management of gout in practice in the UK and Germany. Methods: A retrospective analysis of patients with gout, identified through the records of 2.5 million patients in UK general practices and 2.4 million patients attending GPs or internists in Germany, using the IMS Disease Analyzer. Results: The prevalence of gout was 1.4% in the UK and Germany. Obesity was the most common comorbidity in the UK (27.7%), but in Germany the most common comorbidity was diabetes (25.9%). The prevalence of comorbidities tended to increase with serum uric acid (sUA) levels. There was a positive correlation between sUA level and the frequency of gout flares. Compared with those in whom sUA was <360 μmol/l (<6 mg/dl), odds ratios for a gout flare were 1.33 and 1.37 at sUA 360–420 μmol/l (6–7 mg/dl), and 2.15 and 2.48 at sUA >530 μmol/l ( >9 mg/dl) in the UK and Germany, respectively (p<0.01). Conclusions: The prevalence of gout in practice in the UK and Germany in the years 2000–5 was 1.4%, consistent with previous UK data for 1990–9. Chronic comorbidities were common among patients with gout and included conditions associated with an increased risk for cardiovascular disease, such as obesity, diabetes and hypertension. The importance of regular monitoring of sUA in order to tailor gout treatment was highlighted by data from this study showing that patients with sUA levels ⩾360 μmol/l (⩾6 mg/dl) had an increased risk of gout flares.",2007,47,453,26,2,4,21,23,26,43,50,49,46,47
036d7b6b85f93a2b071c0354cbc0f811a1919829,"CONTEXT
People with severe mental illness (SMI) appear to have an elevated risk of death from cardiovascular disease, but results regarding cancer mortality are conflicting.


OBJECTIVE
To estimate this excess mortality and the contribution of antipsychotic medication, smoking, and social deprivation.


DESIGN
Retrospective cohort study.


SETTING
United Kingdom's General Practice Research Database. Patients Two cohorts were compared: people with SMI diagnoses and people without such diagnoses. Main Outcome Measure Mortality rates for coronary heart disease (CHD), stroke, and the 7 most common cancers in the United Kingdom.


RESULTS
A total of 46 136 people with SMI and 300 426 without SMI were selected for the study. Hazard ratios (HRs) for CHD mortality in people with SMI compared with controls were 3.22 (95% confidence interval [CI], 1.99-5.21) for people 18 through 49 years old, 1.86 (95% CI, 1.63-2.12) for those 50 through 75 years old, and 1.05 (95% CI, 0.92-1.19) for those older than 75 years. For stroke deaths, the HRs were 2.53 (95% CI, 0.99-6.47) for those younger than 50 years, 1.89 (95% CI, 1.50-2.38) for those 50 through 75 years old, and 1.34 (95% CI, 1.17-1.54) for those older than 75 years. The only significant result for cancer deaths was an unadjusted HR for respiratory tumors of 1.32 (95% CI, 1.04-1.68) for those 50 to 75 years old, which lost statistical significance after controlling for smoking and social deprivation. Increased HRs for CHD mortality occurred irrespective of sex, SMI diagnosis, or prescription of antipsychotic medication during follow-up. However, a higher prescribed dose of antipsychotics predicted greater risk of mortality from CHD and stroke.


CONCLUSIONS
This large community sample demonstrates that people with SMI have an increased risk of death from CHD and stroke that is not wholly explained by antipsychotic medication, smoking, or social deprivation scores. Rates of nonrespiratory cancer mortality were not raised. Further research is required concerning prevention of this mortality, including cardiovascular risk assessment, monitoring of antipsychotic medication, and attention to diet and exercise.",2007,36,566,31,12,32,36,45,63,46,46,37,53,36
0c2d5f55d7b9e566f3c2e2784e191b49d59fdcff,"Background There is evidence that the prevalence of common mental disorders varies across Europe. Aims To compare prevalence of common mental disorders in general practice attendees in six European countries. Method Unselected attendees to general practices in the UK, Spain, Portugal, Slovenia, Estonia and The Netherlands were assessed for major depression, panic syndrome and other anxiety syndrome. Prevalence of DSM–IV major depression, other anxiety syndrome and panic syndrome was compared between the UK and other countries after taking account of differences in demographic factors and practice consultation rates. Results Prevalence was estimated in 2344 men and 4865 women. The highest prevalence for all disorders occurred in the UK and Spain, and lowest in Slovenia and The Netherlands. Men aged 30–50 and women aged 18–30 had the highest prevalence of major depression; men aged 40–60 had the highest prevalence of anxiety, and men and women aged 40–50 had the highest prevalence of panic syndrome. Demographic factors accounted for the variance between the UK and Spain but otherwise had little impact on the significance of observed country differences. Conclusions These results add to the evidence for real differences between European countries in prevalence of psychological disorders and show that the burden of care on general practitioners varies markedly between countries.",2008,33,261,11,7,13,14,27,21,20,22,22,28,32
b0a15b2a986f058ec01e7b43eea7b1b5b35a9441,"CONTEXT
Strategies for prevention of depression are hindered by lack of evidence about the combined predictive effect of known risk factors.


OBJECTIVES
To develop a risk algorithm for onset of major depression.


DESIGN
Cohort of adult general practice attendees followed up at 6 and 12 months. We measured 39 known risk factors to construct a risk model for onset of major depression using stepwise logistic regression. We corrected the model for overfitting and tested it in an external population.


SETTING
General practices in 6 European countries and in Chile.


PARTICIPANTS
In Europe and Chile, 10 045 attendees were recruited April 2003 to February 2005. The algorithm was developed in 5216 European attendees who were not depressed at recruitment and had follow-up data on depression status. It was tested in 1732 patients in Chile who were not depressed at recruitment. Main Outcome Measure DSM-IV major depression.


RESULTS
Sixty-six percent of people approached participated, of whom 89.5% participated again at 6 months and 85.9%, at 12 months. Nine of the 10 factors in the risk algorithm were age, sex, educational level achieved, results of lifetime screen for depression, family history of psychological difficulties, physical health and mental health subscale scores on the Short Form 12, unsupported difficulties in paid or unpaid work, and experiences of discrimination. Country was the tenth factor. The algorithm's average C index across countries was 0.790 (95% confidence interval [CI], 0.767-0.813). Effect size for difference in predicted log odds of depression between European attendees who became depressed and those who did not was 1.28 (95% CI, 1.17-1.40). Application of the algorithm in Chilean attendees resulted in a C index of 0.710 (95% CI, 0.670-0.749).


CONCLUSION
This first risk algorithm for onset of major depression functions as well as similar risk algorithms for cardiovascular events and may be useful in prevention of depression.",2008,71,147,5,1,6,8,16,9,15,12,12,11,7
8cdc8ed40279a869624c0796e9319ea1850af3b6,"AIMS
The purpose of this study was to describe the demographic and employment characteristics of Australian practice nurses and explore the relationship between these characteristics and the nurses' role.


BACKGROUND
Nursing in general practice is an integral component of primary care and chronic disease management in the United Kingdom and New Zealand, but in Australia it is an emerging specialty and there is limited data on the workforce and role.


DESIGN
National postal survey embedded in a sequential mixed method design.


METHODS
284 practice nurses completed a postal survey during 2003-2004. Descriptive statistics and factor analysis were utilized to analyse the data.


RESULTS
Most participants were female (99%), Registered Nurses (86%), employed part-time in a group practice, with a mean age of 45.8 years, and had a hospital nursing certificate as their highest qualification (63%). The tasks currently undertaken by participants and those requiring further education were inversely related (R2 = -0.779). Conversely, tasks perceived to be appropriate for a practice nurse and those currently undertaken by participants were positively related (R2 = 0.8996). There was a mismatch between the number of participants who perceived that a particular task was appropriate and those who undertook the task. This disparity was not completely explained by demographic or employment characteristics. Extrinsic factors such as legal and funding issues, lack of space and general practitioner attitudes were identified as barriers to role expansion.


CONCLUSION
Practice nurses are a clinically experienced workforce whose skills are not optimally harnessed to improve the care of the growing number of people with chronic and complex conditions. Relevance to clinical practice. Study data reveal a need to overcome the funding, regulatory and interprofessional barriers that currently constrain the practice nurse role. Expansion of the practice nurse role is clearly a useful adjunct to specialist management of chronic and complex disease, particularly within the context of contemporary policy initiatives.",2008,28,133,7,2,12,4,3,8,10,11,9,4,14
3be75b6ed1658b431ccd7d95ebcb4a3321942449,"BackgroundThis study was carried out to compare the HRQoL of patients in general practice with differing chronic diseases with the HRQoL of patients without chronic conditions, to evaluate the HRQoL of general practice patients in Germany compared with the HRQoL of the general population, and to explore the influence of different chronic diseases on patients' HRQoL, independently of the effects of multiple confounding variables.MethodsA cross-sectional questionnaire survey including the SF-36, the EQ-5D and demographic questions was conducted in 20 general practices in Germany. 1009 consecutive patients aged 15–89 participated. The SF-36 scale scores of general practice patients with differing chronic diseases were compared with those of patients without chronic conditions. Differences in the SF-36 scale/summary scores and proportions in the EQ-5D dimensions between patients and the general population were analyzed. Independent effects of chronic conditions and demographic variables on the HRQoL were analyzed using multivariable linear regression and polynomial regression models.ResultsThe HRQoL for general practice patients with differing chronic diseases tended to show more physical than mental health impairments compared with the reference group of patients without. Patients in general practice in Germany had considerably lower SF-36 scores than the general population (P < 0.001 for all) and showed significantly higher proportions of problems in all EQ-5D dimensions except for the self-care dimension (P < 0.001 for all). The mean EQ VAS for general practice patients was lower than that for the general population (69.2 versus 77.4, P < 0.001). The HRQoL for general practice patients in Germany seemed to be more strongly affected by diseases like depression, back pain, OA of the knee, and cancer than by hypertension and diabetes.ConclusionGeneral practice patients with differing chronic diseases in Germany had impaired quality of life, especially in terms of physical health. The independent impacts on the HRQoL were different depending on the type of chronic disease. Findings from this study might help health professionals to concern more influential diseases in primary care from the patient's perspective.",2008,55,122,7,0,3,9,17,7,11,14,12,12,4
fbff227595f056558eca8c081f11838ece184113,"BackgroundContinual quality improvement in primary care is an international priority. In the United Kingdom, the major initiative for improving quality of care is the Quality and Outcomes Framework (QoF) of the 2004 GP contract. Although the primary focus of the QoF is on clinical care, it is acknowledged that a comprehensive assessment of quality also requires valid and reliable measurement of the patient perspective, so financial incentives are included in the contract for general practices to survey patients' views. One questionnaire specified for use in the QoF is the General Practice Assessment Questionnaire (GPAQ). This paper describes the development of the GPAQ (with post-consultation and postal versions) and presents a preliminary examination of the psychometric properties of the questionnaire.MethodsDescription of scale development and preliminary analysis of psychometric characteristics (internal reliability, factor structure), based on a large dataset of routinely collected GPAQ surveys (n = 190,038 responses to the consultation version of GPAQ and 20,309 responses to the postal version) from practices in the United Kingdom during the 2005–6 contract year.ResultsRespondents tend to report generally favourable ratings. Responses were particularly skewed on the GP communication scale, though no more so than for other questionnaires in current use in the UK for which data were available. Factor analysis identified 2 factors that clearly relate to core concepts in primary care quality ('access' and 'interpersonal care') that were common to both version of the GPAQ. The other factors related to 'enablement' in the post-consultation version and 'nursing care' in the postal version.ConclusionThis preliminary evaluation indicates that the scales of the GPAQ are internally reliable and that the items demonstrate an interpretable factor structure. Issues concerning the distributions of GPAQ responses are discussed. Potential further developments of the item content for the GPAQ are also outlined.",2008,53,97,1,2,10,5,4,12,8,7,11,7,10
44655b06c43cd1a05cec8c15dd83c3a7b4f08f30,"The emergence of healthcare assistants (HCAs) in general practice raises questions about roles and responsibilities, patients' acceptance, cost-effectiveness, patient safety and delegation, training and competence, workforce development, and professional identity. There has been minimal research into the role of HCAs and their experiences, as well as those of other staff working with HCAs in general practice. Lessons may be learned from their role and evidence of their effectiveness in hospital settings. Such research highlights blurred and contested role boundaries and threats to professional identity, which have implications for teamwork, quality of patient care, and patient safety. In this paper it is argued that transferability of evidence from hospital settings to the context of general practice cannot be assumed. Drawing on the limited research in general practice, the challenges and benefits of developing the HCA role in general practice are discussed. It is suggested that in the context of changing skill-mix models, viewing roles as fluid and dynamic is more helpful and reflective of individuals' experiences than endeavouring to impose fixed role boundaries. It is concluded that HCAs can make an increasingly useful contribution to the skill mix in general practice, but that more research and evaluation are needed to inform their training and development within the general practice team.",2008,52,85,4,2,4,4,9,8,7,6,12,6,6
424f859cb27e3c10d1c5dd17d5da6196668e2327,"Abstract Objective: To assess the effectiveness of a home exercise programme of strength and balance retraining exercises in reducing falls and injuries in elderly women. Design: Randomised controlled trial of an individually tailored programme of physical therapy in the home (exercise group, n=116) compared with the usual care and an equal number of social visits (control group, n=117). Setting: 17 general practices in Dunedin, New Zealand. Subjects: Women aged 80 years and older living in the community and registered with a general practice in Dunedin. Main outcome measures: Number of falls and injuries related to falls and time between falls during one year of follow up; changes in muscle strength and balance measures after six months. Results: After one year there were 152 falls in the control group and 88 falls in the exercise group. The mean (SD) rate of falls was lower in the exercise than the control group (0.87 (1.29) v 1.34 (1.93) falls per year respectively; difference 0.47; 95% confidence interval 0.04 to 0.90). The relative hazard for the first four falls in the exercise group compared with the control group was 0.68 (0.52 to 0.90). The relative hazard for a first fall with injury in the exercise group compared with the control group was 0.61 (0.39 to 0.97). After six months, balance had improved in the exercise group (difference between groups in change in balance score 0.43 (0.21 to 0.65). Conclusions: An individual programme of strength and balance retraining exercises improved physical function and was effective in reducing falls and injuries in women 80 years and older. Key messages Modifiable risk factors for falls in elderly people have been well defined; they include loss of muscle strength and impaired balance A programme to improve strength and balance in women aged 80 years and older can be set up safely with four home visits from a physiotherapist This programme reduced falls and moderate injuries appreciably over the subsequent year in Dunedin, New Zealand The benefit was most noticeable in elderly people who fell often",1997,23,1076,54,2,5,20,29,44,50,59,58,60,49
80e12d69f24cc8e6944182da199e5d04617abe97,"BackgroundGeneral practitioners (GPs) or researchers sometimes need to identify frequent attenders (FAs) in order to screen them for unidentified problems and to test specific interventions. We wanted to assess different methods for selecting FAs to identify the most feasible and effective one for use in a general (group) practice.MethodsIn the second Dutch National Survey of General Practice, data were collected on 375 899 persons registered with 104 practices. Frequent attendance is defined as the top 3% and 10% of enlisted patients in each one-year age-sex group measured during the study year. We used these two selections as our reference standard. We also selected the top 3% and 10% FAs (90 and 97 percentile) based on four selection methods of diminishing preciseness. We compared the test characteristics of these four methods.ResultsOf all enlisted patients, 24 % did not consult the practice during the study year. The mean number of contacts in the top 10% FAs increased in men from 5.8 (age 15–24 years) to 17.5 (age 64–75 years) and in women from 9.7 to 19.8. In the top 3% of FAs, contacts increased in men from 9.2 to 24.5 and in women from 14 to 27.8.The selection of FAs becomes more precise when smaller age classes are used. All selection methods show acceptable results (kappa 0.849 – 0.942) except the three group method.ConclusionTo correctly identify frequent attenders in general practice, we recommend dividing patients into at least three age groups per sex.",2008,31,47,2,0,3,2,2,1,9,8,4,7,2
96ffb5cfa0cb6fd6ccb22ddb7ee98dd90b86df49,"Objective: To develop a taxonomy describing patient safety events in general practice from reports submitted by a random representative sample of general practitioners (GPs), and to determine proportions of reported event types. Design: 433 reports received by the Threats to Australian Patient Safety (TAPS) study were analysed by three investigating GPs, classifying event types contained. Agreement between investigators was recorded as the taxonomy developed. Setting and participants: 84 volunteers from a random sample of 320 GPs, previously shown to be representative of 4666 GPs in New South Wales, Australia. Main outcome measures: Taxonomy, agreement of investigators coding, proportions of error types. Results: A three-level taxonomy resulted. At the first level, errors relating to the processes of healthcare (type 1; n = 365 (69.5%)) were more common than those relating to deficiencies in the knowledge and skills of health professionals (type 2; n = 160 (30.5%)). At the second level, five type 1 themes were identified: healthcare systems (n = 112 (21.3%)); investigations (n = 65 (12.4%)); medications (n = 107 (20.4%)); other treatments (n = 13 (2.5%)); and communication (n = 68 (12.9%)). Two type 2 themes were identified: diagnosis (n = 62 (11.8%)) and management (n = 98 (18.7%)). The third level comprised 35 descriptors of the themes. Good inter-coder agreement was demonstrated with an overall κ score of 0.66. A least two out of three investigators independently agreed on event classification in 92% of cases. Conclusions: The proposed taxonomy for reported events in general practice provides a comprehensible tool for clinicians describing threats to patient safety, and could be built into reporting systems to remove difficulties arising from coder interpretation of events.",2008,37,79,4,3,4,6,7,5,6,3,16,4,10
abf37c9298831051761ae9c23d300aa185d725cc,"BACKGROUND
There has been little research into poetry-based medical education. Few studies consider learners' perceptions in depth.


OBJECTIVE
To explore general practice registrars' (GPRs) perceptions of two poetry-based sessions.


METHODS
GPRs in one general practice vocational training scheme experienced two poetry sessions. In one, the facilitator selected poems; in the other, poems were chosen by registrars. Poems were read and discussed, with emphasis on personal response. Data were obtained through in-depth semi-structured interviews with six registrars. Interviews were audiotaped, transcribed and analysed using interpretative phenomenological analysis. Identification of individual ideas and shared themes enabled exploration of the registrars' experiences.


RESULTS
Registrars described how poetry helped them explore emotional territory. They recognized a broadening of education, describing how poems helped them consider different points of view, increasing their understanding of others. Vicarious experience, development of empathy and self-discovery were also reported. Participants speculated on how this might impact on patient care and professional practice. Facilitator-selected poems provided variety and ambiguity, provoking discussions with clinical relevance. Learner-selected poems enabled involvement, self-revelation and understanding of peers and developed emotional expression.


CONCLUSIONS
These registrars reported difficulties expressing feelings in the culture of science-based medical training. Poetry sessions may provide an environment for emotional exploration, which could broaden understanding of self and others. Poetry-based education may develop emotional competence. The participants recognized development of key skills including close reading, attentive listening and interpretation of meaning. These skills may help doctors to understand individual patient's unique experience of illness, encouraging personalized care that respects patients' perspectives.",2008,52,41,1,1,1,4,3,1,7,0,4,2,4
b8b96b418ca0302bbc1a50f7e1bd8f8ca5ca37b4,"BackgroundObesity has become a global pandemic, considered the sixth leading cause of mortality by the WHO. As gatekeepers to the health system, General Practitioners are placed in an ideal position to manage obesity. Yet, very few consultations address weight management. This study aims to explore reasons why patients attending General Practice appointments are not engaging with their General Practitioner (GP) for weight management and their perception of the role of the GP in managing their weight.MethodsIn February 2006, 367 participants aged between 17 and 64 were recruited from three General Practices in Melbourne to complete a waiting room self – administered questionnaire. Questions included basic demographics, the role of the GP in weight management, the likelihood of bringing up weight management with their GP and reasons why they would not, and their nominated ideal person to consult for weight management. Physical measurements to determine weight status were then completed. The statistical methods included means and standard deviations to summarise continuous variables such as weight and height. Sub groups of weight and questionnaire answers were analysed using the χ2 test of significant differences taking p as < 0.05.ResultsThe population sample had similar obesity co-morbidity rates to the National Heart Foundation data. 74% of patients were not likely to bring up weight management when they visit their GP. Negative reasons were time limitation on both the patient's and doctor's part and the doctor lacking experience. The GP was the least likely person to tell a patient to lose weight after partner, family and friends. Of the 14% that had been told by their GP to lose weight, 90% had cardiovascular obesity related co-morbidities. GPs (15%) were 4th in the list of ideal persons to manage weight after personal trainerConclusionPatients do not have confidence in their GPs for weight management, preferring other health professionals who may lack evidence based training. Concurrently, GPs target only those with obesity related co-morbidities. Further studies evaluating GPs' opinions about weight management, effective strategies that can be implemented in primary care and the co-ordination of the team approach need to be done.",2008,28,67,5,0,0,1,11,4,11,9,8,2,4
625f20bafb88207c9b7ab998c44b3e000003e720,"BACKGROUND
In general practice, depression is often not recognized. As treatment of depression is effective, screening has been proposed as one solution to combat this 'hidden morbidity'. The results of screening programmes for depression, however, are inconsistent and most studies do not show a positive effect on patient outcomes. Patients do not always accept this diagnosis and hence do not receive proper treatment. Nothing is known about the tendency of those patients who screen positive for depression to accept treatment for their 'disclosed' disorder.


OBJECTIVE
In this study, we aimed to better understand the views of patients who screened positive in a screening programme for depression.


METHODS
We performed a qualitative study with semi-structured in-depth interviews with 17 patients. These adult patients (nine females), all suffering from major depressive disorder, were disclosed by a screening programme for depression performed within 11 Dutch general practices. The transcripts were independently analysed by two researchers using MAXqda2.


RESULTS
All patients appreciated the active way in which they were approached for screening. Fifteen of the 17 patients recognized the depressive symptoms but nine of them did not accept the diagnosis. The first explanation for resistance to the diagnosis of depression is fear of stigmatization and scepticism about the usefulness of labelling. Secondly, patients experienced their depressive symptoms as a normal and transitory reaction to adversity. Thirdly, patients had doubts about the necessity and effectiveness of treatment. Depressive symptoms, such as feelings of guilt, self-depreciation and fatigue, hamper help-seeking behaviour.


CONCLUSIONS
We conclude that some patients with undisclosed depression, who took the trouble of going through a complete screening programme, felt aversion to being diagnosed as having depression. In the context of screening for depression, we recommend that the patients' view on depression be elicited before diagnosing and offering treatment.",2008,43,46,2,1,2,2,8,4,4,1,7,2,5
075fc2c5a90f1a6029d18672552859144484436f,"Textbook of medical physiology , Textbook of medical physiology , کتابخانه دیجیتال جندی شاپور اهواز",1961,0,10469,559,1,0,0,0,1,0,2,0,0,0
092832c2443dbe966744a26b783f6e7b2d0c5473,,1991,0,15703,410,0,6,18,25,54,40,31,24,31,745
edd1d68c1288765c3c51a34915e0517fb1680167,"For a long time, superoxide generation by an NADPH oxidase was considered as an oddity only found in professional phagocytes. Over the last years, six homologs of the cytochrome subunit of the phagocyte NADPH oxidase were found: NOX1, NOX3, NOX4, NOX5, DUOX1, and DUOX2. Together with the phagocyte NADPH oxidase itself (NOX2/gp91(phox)), the homologs are now referred to as the NOX family of NADPH oxidases. These enzymes share the capacity to transport electrons across the plasma membrane and to generate superoxide and other downstream reactive oxygen species (ROS). Activation mechanisms and tissue distribution of the different members of the family are markedly different. The physiological functions of NOX family enzymes include host defense, posttranlational processing of proteins, cellular signaling, regulation of gene expression, and cell differentiation. NOX enzymes also contribute to a wide range of pathological processes. NOX deficiency may lead to immunosuppresion, lack of otoconogenesis, or hypothyroidism. Increased NOX activity also contributes to a large number or pathologies, in particular cardiovascular diseases and neurodegeneration. This review summarizes the current state of knowledge of the functions of NOX enzymes in physiology and pathology.",2007,1066,5234,404,64,184,269,248,329,413,456,452,429,439
5ee048a433b632c11ebf8de88916eca490742c05,"OBJECTIVE
To develop and validate a new Simplified Acute Physiology Score, the SAPS II, from a large sample of surgical and medical patients, and to provide a method to convert the score to a probability of hospital mortality.


DESIGN AND SETTING
The SAPS II and the probability of hospital mortality were developed and validated using data from consecutive admissions to 137 adult medical and/or surgical intensive care units in 12 countries.


PATIENTS
The 13,152 patients were randomly divided into developmental (65%) and validation (35%) samples. Patients younger than 18 years, burn patients, coronary care patients, and cardiac surgery patients were excluded.


OUTCOME MEASURE
Vital status at hospital discharge.


RESULTS
The SAPS II includes only 17 variables: 12 physiology variables, age, type of admission (scheduled surgical, unscheduled surgical, or medical), and three underlying disease variables (acquired immunodeficiency syndrome, metastatic cancer, and hematologic malignancy). Goodness-of-fit tests indicated that the model performed well in the developmental sample and validated well in an independent sample of patients (P = .883 and P = .104 in the developmental and validation samples, respectively). The area under the receiver operating characteristic curve was 0.88 in the developmental sample and 0.86 in the validation sample.


CONCLUSION
The SAPS II, based on a large international sample of patients, provides an estimate of the risk of death without having to specify a primary diagnosis. This is a starting point for future evaluation of the efficiency of intensive care units.",1993,0,5149,287,0,3,9,19,27,52,56,97,86,139
a68cdd29e4304621d793db7b735a5ade85fc6547,"Plant responses to salt and water stress have much in common. Salinity reduces the ability of plants to take up water, and this quickly causes reductions in growth rate, along with a suite of metabolic changes identical to those caused by water stress. The initial reduction in shoot growth is probably due to hormonal signals generated by the roots. There may be salt-specific effects that later have an impact on growth; if excessive amounts of salt enter the plant, salt will eventually rise to toxic levels in the older transpiring leaves, causing premature senescence, and reduce the photosynthetic leaf area of the plant to a level that cannot sustain growth. These effects take time to develop. Salt-tolerant plants differ from salt-sensitive ones in having a low rate of Na+ and Cl-- transport to leaves, and the ability to compartmentalize these ions in vacuoles to prevent their build-up in cytoplasm or cell walls and thus avoid salt toxicity. In order to understand the processes that give rise to tolerance of salt, as distinct from tolerance of osmotic stress, and to identify genes that control the transport of salt across membranes, it is important to avoid treatments that induce cell plasmolysis, and to design experiments that distinguish between tolerance of salt and tolerance of water stress.",2002,69,5640,418,4,31,57,86,122,155,187,233,297,356
bbbe8f992ca7c982ea52b54663eaeaa93adf65ba,"Seeds: Germination, Structure, and Composition. Seed Development and Maturation. Development-Regulation and Maturation. Cellular Events during Germination and Seedling Growth. Dormancy and the Control of Germination. Some Ecophysiological Aspects of Germination. Mobilization of Stored Seed Reserves. Control of the Mobilization of Stored Reserves. Seeds and Germination: Some Agricultural and Industrial Aspects. Index.",1986,138,3943,294,10,11,13,12,20,31,30,21,31,37
a7e2f71ea0db557c2b0f025a01ad65ff7eacf6c7,"Using a process model of emotion, a distinction between antecedent-focused and response-focused emotion regulation is proposed. To test this distinction, 120 participants were shown a disgusting film while their experiential, behavioral, and physiological responses were recorded. Participants were told to either (a) think about the film in such a way that they would feel nothing (reappraisal, a form of antecedent-focused emotion regulation), (b) behave in such a way that someone watching them would not know they were feeling anything (suppression, a form of response-focused emotion regulation), or (c) watch the film (a control condition). Compared with the control condition, both reappraisal and suppression were effective in reducing emotion-expressive behavior. However, reappraisal decreased disgust experience, whereas suppression increased sympathetic activation. These results suggest that these 2 emotion regulatory processes may have different adaptive consequences.",1998,143,3504,605,4,10,14,17,24,27,35,46,57,74
9438aa83eb8218b7e6e3891ad7bc2b388e35bc33,"In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic “virtual organizations” formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, “The Anatomy of the Grid,” by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration—within and across organizational domains. This is a DRAFT document and continues to be revised. The latest version can be found at http://www.globus.org/research/papers/ogsa.pdf. Please send comments to foster@mcs.anl.gov, carl@isi.edu, jnick@us.ibm.com, tuecke@mcs.anl.gov Physiology of the Grid 2",2002,84,3620,238,141,427,550,551,436,326,231,230,166,131
62b5ac92ebd1c7340684614426836ee66152cce8,"This is the first volume of the proposed many-sectioned ""Handbook"" in which the American Physiological Society intends to present comprehensively the entire field of physiology. The scope and depth of the work may be estimated by realizing that the Section on Neurophysiology alone will comprise three volumes. The present work covers the following principal topics, all but the first of which are divided into several chapters: History of Neurophysiology (M. A. B. Brazier), Neuron Physiology (Introduction, J. C. Eccles), Brain Potentials and Rhythms (Introduction, A. Fessard), Sensory Mechanisms (Introduction, Lord Adrian), and Vision (Introduction, H. K. Hartline). To a large degree, the individual contributors have overcome the difficult task of presenting both the fundamentals and the most recent advances pertaining to their subjects. The historical development of knowledge and hypotheses about most of the particular topics is given detailed consideration, and this aspect alone should help to prevent the material",1960,0,10321,0,0,0,0,1,1,0,1,2,4,46
05c2af79a61c24400f25c00027dd660575de03c1,"Find loads of the textbook of work physiology book catalogues in this site as the choice of you visiting this page. You can also join to the website book library that will show you numerous books from any types. Literature, science, politics, and many more catalogues are presented to offer you the best book to find. The book that really makes you feels satisfied. Or that's the book that will save you from your job deadline.",1970,0,4118,188,0,13,17,41,34,31,49,68,53,60
b1ff12e211ec2eee611203e849787d1977e0f22c,"Molecular chaperones, including the heat-shock proteins (Hsps), are a ubiquitous feature of cells in which these proteins cope with stress-induced denaturation of other proteins. Hsps have received the most attention in model organisms undergoing experimental stress in the laboratory, and the function of Hsps at the molecular and cellular level is becoming well understood in this context. A complementary focus is now emerging on the Hsps of both model and nonmodel organisms undergoing stress in nature, on the roles of Hsps in the stress physiology of whole multicellular eukaryotes and the tissues and organs they comprise, and on the ecological and evolutionary correlates of variation in Hsps and the genes that encode them. This focus discloses that (a) expression of Hsps can occur in nature, (b) all species have hsp genes but they vary in the patterns of their expression, (c) Hsp expression can be correlated with resistance to stress, and (d) species' thresholds for Hsp expression are correlated with levels of stress that they naturally undergo. These conclusions are now well established and may require little additional confirmation; many significant questions remain unanswered concerning both the mechanisms of Hsp-mediated stress tolerance at the organismal level and the evolutionary mechanisms that have diversified the hsp genes.",1999,389,3563,164,10,38,76,84,81,93,100,105,127,148
176cfe9afceb31dd6e768413e9ead95b5d34489b,"P2X receptors are membrane ion channels that open in response to the binding of extracellular ATP. Seven genes in vertebrates encode P2X receptor subunits, which are 40-50% identical in amino acid sequence. Each subunit has two transmembrane domains, separated by an extracellular domain (approximately 280 amino acids). Channels form as multimers of several subunits. Homomeric P2X1, P2X2, P2X3, P2X4, P2X5, and P2X7 channels and heteromeric P2X2/3 and P2X1/5 channels have been most fully characterized following heterologous expression. Some agonists (e.g., alphabeta-methylene ATP) and antagonists [e.g., 2',3'-O-(2,4,6-trinitrophenyl)-ATP] are strongly selective for receptors containing P2X1 and P2X3 subunits. All P2X receptors are permeable to small monovalent cations; some have significant calcium or anion permeability. In many cells, activation of homomeric P2X7 receptors induces a permeability increase to larger organic cations including some fluorescent dyes and also signals to the cytoskeleton; these changes probably involve additional interacting proteins. P2X receptors are abundantly distributed, and functional responses are seen in neurons, glia, epithelia, endothelia, bone, muscle, and hemopoietic tissues. The molecular composition of native receptors is becoming understood, and some cells express more than one type of P2X receptor. On smooth muscles, P2X receptors respond to ATP released from sympathetic motor nerves (e.g., in ejaculation). On sensory nerves, they are involved in the initiation of afferent signals in several viscera (e.g., bladder, intestine) and play a key role in sensing tissue-damaging and inflammatory stimuli. Paracrine roles for ATP signaling through P2X receptors are likely in neurohypophysis, ducted glands, airway epithelia, kidney, bone, and hemopoietic tissues. In the last case, P2X7 receptor activation stimulates cytokine release by engaging intracellular signaling pathways.",2002,735,2697,360,0,76,141,175,164,189,185,184,160,168
956c168f8c0fc9712d79540f90400dd0c0917f6a,"Volume 1: The gametes, fertilization and early embryogenesis the reproductive systems - the female, the male the pituitary and the hypothalmus. Volume 2: Reproductive behaviour and its control reproductive processes and their control.",1988,0,7438,11,11,48,78,97,137,105,131,281,327,342
32b4b3cba70730997053958a9b1f960217bb5132,"Physiology of the Gastrointestinal Tract, Fifth Edition - winner of a 2013 Highly Commended BMA Medical Book Award for Internal Medicine - covers the study of the mechanical, physical, and biochemical functions of the GI Tract while linking the clinical disease or disorder, bridging the gap between clinical and laboratory medicine. The gastrointestinal system is responsible for the breakdown and absorption of various foods and liquids needed to sustain life. Other diseases and disorders treated by clinicians in this area include: food allergies, constipation, chronic liver disease and cirrhosis, gallstones, gastritis, GERD, hemorrhoids, IBS, lactose intolerance, pancreatic, appendicitis, celiac disease, Crohn's disease, peptic ulcer, stomach ulcer, viral hepatitis, colorectal cancer and liver transplants. The new edition is a highly referenced and useful resource for gastroenterologists, physiologists, internists, professional researchers, and instructors teaching courses for clinical and research students. *2013 Highly Commended BMA Medical Book Award for Internal Medicine* Discusses the multiple processes governing gastrointestinal function* Each section edited by preeminent scientist in the field* Updated, four-color illustrations",1994,0,3458,1,104,186,188,195,181,208,209,175,162,175
129169f853b1e71e05d97fe6ff733d7cd57b22d8,"This paper summarizes the current views on coping styles as a useful concept in understanding individual adaptive capacity and vulnerability to stress-related disease. Studies in feral populations indicate the existence of a proactive and a reactive coping style. These coping styles seem to play a role in the population ecology of the species. Despite domestication, genetic selection and inbreeding, the same coping styles can, to some extent, also be observed in laboratory and farm animals. Coping styles are characterized by consistent behavioral and neuroendocrine characteristics, some of which seem to be causally linked to each other. Evidence is accumulating that the two coping styles might explain a differential vulnerability to stress mediated disease due to the differential adaptive value of the two coping styles and the accompanying neuroendocrine differentiation.",1999,102,2410,262,0,5,24,25,51,46,62,51,91,72
b37d0d9fe0bc9a1015032d56882e72d246e8a8c2,,1985,0,4838,14,48,92,174,204,192,204,193,191,170,152
cc5c86cf7ea41efa31afba93b67602c0d4982baf,,1975,0,6862,0,155,136,167,216,277,219,256,197,257,216
cce9ff2a9df6d171e35c3532cae28f7e230e95bd,"Anatomical and physiological observations in monkeys indicate that the primate visual system consists of several separate and independent subdivisions that analyze different aspects of the same retinal image: cells in cortical visual areas 1 and 2 and higher visual areas are segregated into three interdigitating subdivisions that differ in their selectivity for color, stereopsis, movement, and orientation. The pathways selective for form and color seem to be derived mainly from the parvocellular geniculate subdivisions, the depth- and movement-selective components from the magnocellular. At lower levels, in the retina and in the geniculate, cells in these two subdivisions differ in their color selectivity, contrast sensitivity, temporal properties, and spatial resolution. These major differences in the properties of cells at lower levels in each of the subdivisions led to the prediction that different visual functions, such as color, depth, movement, and form perception, should exhibit corresponding differences. Human perceptual experiments are remarkably consistent with these predictions. Moreover, perceptual experiments can be designed to ask which subdivisions of the system are responsible for particular visual abilities, such as figure/ground discrimination or perception of depth from perspective or relative movement--functions that might be difficult to deduce from single-cell response properties.",1988,93,3141,114,4,38,85,61,170,114,114,129,106,91
3e7219417f000d4005f8bb1014d93725c63b7479,"Introducing a new hobby for other people may inspire them to join with you. Reading, as one of mutual hobby, is considered as the very easy hobby to do. But, many people are not interested in this hobby. Why? Boring is the reason of why. However, this feel actually can deal with the book and time of you reading. Yeah, one that we will refer to break the boredom in reading is choosing textbook of medical physiology as the reading material.",2005,0,2339,184,78,77,92,88,82,86,116,139,158,160
24f619f07d1bc3b4bcf88822b38b4b5c28ab307c,"Review of medical physiology , Review of medical physiology , کتابخانه الکترونیک و دیجیتال - آذرسا",1969,0,3092,155,10,7,17,15,12,11,14,16,11,13
e6f57af88b6fb9dab61ba63e81840f77e07991c7,"[PDF]  [Full Text]  [Abstract] , January 15, 2008; 105 (2): 751-756. PNAS J. W. Koo and R. S. Duman IL-1 is an essential mediator of the antineurogenic and anhedonic effects of stress [PDF]  [Full Text]  [Abstract] , February 1, 2008; 294 (2): R501-R509. Am J Physiol Regulatory Integrative Comp Physiol and A. Boveris A. Navarro, J. M. Lopez-Cepero, M. J. Bandez, M.-J. Sanchez-Pino, C. Gomez, E. Cadenas Hippocampal mitochondrial dysfunction in rat aging",2007,407,2072,203,4,50,119,142,161,193,181,181,200,216
344c7523b0b051bf4c6fed4e7bcd69a8e19a4c23,"Glucagon-like peptide 1 (GLP-1) is a 30-amino acid peptide hormone produced in the intestinal epithelial endocrine L-cells by differential processing of proglucagon, the gene which is expressed in these cells. The current knowledge regarding regulation of proglucagon gene expression in the gut and in the brain and mechanisms responsible for the posttranslational processing are reviewed. GLP-1 is released in response to meal intake, and the stimuli and molecular mechanisms involved are discussed. GLP-1 is extremely rapidly metabolized and inactivated by the enzyme dipeptidyl peptidase IV even before the hormone has left the gut, raising the possibility that the actions of GLP-1 are transmitted via sensory neurons in the intestine and the liver expressing the GLP-1 receptor. Because of this, it is important to distinguish between measurements of the intact hormone (responsible for endocrine actions) or the sum of the intact hormone and its metabolites, reflecting the total L-cell secretion and therefore also the possible neural actions. The main actions of GLP-1 are to stimulate insulin secretion (i.e., to act as an incretin hormone) and to inhibit glucagon secretion, thereby contributing to limit postprandial glucose excursions. It also inhibits gastrointestinal motility and secretion and thus acts as an enterogastrone and part of the ""ileal brake"" mechanism. GLP-1 also appears to be a physiological regulator of appetite and food intake. Because of these actions, GLP-1 or GLP-1 receptor agonists are currently being evaluated for the therapy of type 2 diabetes. Decreased secretion of GLP-1 may contribute to the development of obesity, and exaggerated secretion may be responsible for postprandial reactive hypoglycemia.",2007,385,2394,166,1,45,107,109,159,186,198,216,213,237
05d89da56bf7547950604d164dfd24ccf5ac69d9,"This is the fourth edition of an established and successful reference for plant scientists. The author has taken into consideration extensive reviews performed by colleagues and students who have touted this book as the ultimate reference for research and learning. The original structure and philosophy of the book continue in this new edition, providing a genuine synthesis of modern physicochemical and physiological thinking, while entirely updating the detailed content. Key concepts in plant physiology are developed with the use of chemistry, physics, and mathematics fundamentals. The figures and illustrations have been improved and the list of references has been expanded to reflect the author's continuing commitment to providing the most valuable learning tool in the field. This revision will ensure the reputation of Park Nobel's work as a leader in the field.It includes more than 40 per cent new coverage. It incorporates student-recommended changes from the previous edition. It also includes: five brand new equations and four new tables, with updates to 24 equations and six tables; and, 30 new figures added with more than three-quarters of figures and legends improved . It is organized so that a student has easy access to locate any biophysical phenomenon in which he or she is interested. It features: per-chapter key equation tables; problems with solutions presented in the back of the book; appendices with conversion factors, constants/coefficients, abbreviations and symbols.",1991,4,2144,229,6,10,19,38,36,37,42,43,48,61
efa4d50f41f3cfab43330427f363699e20c44dc5,"THE first edition of this valuable text-book was reviewed in NATURE of June 20, 1936, p. 1012; we are pleased to know that the book was so well received as to make a reprinting necessary in 1937 and now a second edition appears.Plant PhysiologyBy Meirion Thomas. Second edition. Pp. xii + 596. (London: J. and A. Churchill, Ltd., 1940.) 21s.",1967,0,3119,253,0,2,0,2,0,4,1,2,3,3
3ad3d79cf80bb7978c476b8f9761fb33f319764c,Majors topics addressed in this review on zinc physiology are ; 1) chemistry and biochemistry; 2) interface of biochemistry and physiology of zinc; 3) physiology and cell and molecular biology; 4) pathology,1993,97,2447,71,2,15,24,23,34,40,74,65,69,59
78b94a18aef1f97264c1938e3df6c38fe1013327,,1998,0,2134,128,0,8,26,29,32,82,71,111,127,124
93b6093a26bac3a14837bb0c77b77cbff8e1c1d6,"The inorganic anions nitrate (NO3−) and nitrite (NO2−) were previously thought to be inert end products of endogenous nitric oxide (NO) metabolism. However, recent studies show that these supposedly inert anions can be recycled in vivo to form NO, representing an important alternative source of NO to the classical l-arginine–NO-synthase pathway, in particular in hypoxic states. This Review discusses the emerging important biological functions of the nitrate–nitrite–NO pathway, and highlights studies that implicate the therapeutic potential of nitrate and nitrite in conditions such as myocardial infarction, stroke, systemic and pulmonary hypertension, and gastric ulceration.",2008,144,1959,95,38,88,88,102,132,137,157,180,166,183
4872938b71fea99eba48d8e5775b1e9f6f6282cc,"THE ENDOTHELIUM has long been viewed as an inert cellophane-like membrane that lines the circulatory system with its primary essential function being the maintenance of vessel wall permeability. Shortly after the first description of circulating blood by William Harvey in 1628, the existence of a",1998,503,2352,76,3,52,90,103,118,112,111,126,127,131
2cd3f49cdb58a0d5f9a14ad224b8e254cfc971b7,"Platelet-derived growth factors (PDGFs) and their receptors (PDGFRs) have served as prototypes for growth factor and receptor tyrosine kinase function for more than 25 years. Studies of PDGFs and PDGFRs in animal development have revealed roles for PDGFR-alpha signaling in gastrulation and in the development of the cranial and cardiac neural crest, gonads, lung, intestine, skin, CNS, and skeleton. Similarly, roles for PDGFR-beta signaling have been established in blood vessel formation and early hematopoiesis. PDGF signaling is implicated in a range of diseases. Autocrine activation of PDGF signaling pathways is involved in certain gliomas, sarcomas, and leukemias. Paracrine PDGF signaling is commonly observed in epithelial cancers, where it triggers stromal recruitment and may be involved in epithelial-mesenchymal transition, thereby affecting tumor growth, angiogenesis, invasion, and metastasis. PDGFs drive pathological mesenchymal responses in vascular disorders such as atherosclerosis, restenosis, pulmonary hypertension, and retinal diseases, as well as in fibrotic diseases, including pulmonary fibrosis, liver cirrhosis, scleroderma, glomerulosclerosis, and cardiac fibrosis. We review basic aspects of the PDGF ligands and receptors, their developmental and pathological functions, principles of their pharmacological inhibition, and results using PDGF pathway-inhibitory or stimulatory drugs in preclinical and clinical contexts.",2008,374,1852,127,13,92,132,123,151,140,187,145,147,157
310278f19db789cdf662cd5a5ccad60d054156a7,"Microglia, the macrophages of the central nervous system parenchyma, have in the normal healthy brain a distinct phenotype induced by molecules expressed on or secreted by adjacent neurons and astrocytes, and this phenotype is maintained in part by virtue of the blood-brain barrier's exclusion of serum components. Microglia are continually active, their processes palpating and surveying their local microenvironment. The microglia rapidly change their phenotype in response to any disturbance of nervous system homeostasis and are commonly referred to as activated on the basis of the changes in their morphology or expression of cell surface antigens. A wealth of data now demonstrate that the microglia have very diverse effector functions, in line with macrophage populations in other organs. The term activated microglia needs to be qualified to reflect the distinct and very different states of activation-associated effector functions in different disease states. Manipulating the effector functions of microglia has the potential to modify the outcome of diverse neurological diseases.",2009,144,1530,90,18,96,123,149,178,159,166,118,131,112
a4ded2846b5e20d9228f8e676a6b0539cb2d0fdb,"This review describes normal bone anatomy and physiology as an introduction to the subsequent articles in this section that discuss clinical applications of iliac crest bone biopsy. The normal anatomy and functions of the skeleton are reviewed first, followed by a general description of the processes of bone modeling and remodeling. The bone remodeling process regulates the gain and loss of bone mineral density in the adult skeleton and directly influences bone strength. Thorough understanding of the bone remodeling process is critical to appreciation of the value of and interpretation of the results of iliac crest bone histomorphometry. Osteoclast recruitment, activation, and bone resorption is discussed in some detail, followed by a review of osteoblast recruitment and the process of new bone formation. Next, the collagenous and noncollagenous protein components and function of bone extracellular matrix are summarized, followed by a description of the process of mineralization of newly formed bone matrix. The actions of biomechanical forces on bone are sensed by the osteocyte syncytium within bone via the canalicular network and intercellular gap junctions. Finally, concepts regarding bone remodeling, osteoclast and osteoblast function, extracellular matrix, matrix mineralization, and osteocyte function are synthesized in a summary of the currently understood functional determinants of bone strength. This information lays the groundwork for understanding the utility and clinical applications of iliac crest bone biopsy.",2008,58,1495,142,1,7,14,37,50,103,99,136,173,193
f7ab2c8324b0aaa08bf53f16b7bd68c716185f60,"Microorganisms have a variety of evolutionary adaptations and physiological acclimation mechanisms that allow them to survive and remain active in the face of environmental stress. Physiological responses to stress have costs at the organismal level that can result in altered ecosystem-level C, energy, and nutrient flows. These large-scale impacts result from direct effects on active microbes' physiology and by controlling the composition of the active microbial community. We first consider some general aspects of how microbes experience environmental stresses and how they respond to them. We then discuss the impacts of two important ecosystem-level stressors, drought and freezing, on microbial physiology and community composition. Even when microbial community response to stress is limited, the physiological costs imposed on soil microbes are large enough that they may cause large shifts in the allocation and fate of C and N. For example, for microbes to synthesize the osmolytes they need to survive a single drought episode they may consume up to 5% of total annual net primary production in grassland ecosystems, while acclimating to freezing conditions switches Arctic tundra soils from immobilizing N during the growing season to mineralizing it during the winter. We suggest that more effectively integrating microbial ecology into ecosystem ecology will require a more complete integration of microbial physiological ecology, population biology, and process ecology.",2007,80,1610,118,2,16,31,55,72,69,116,110,115,144
271fb86a07b21eb8060c3272d6c298bfb1d59189,"Woody plants such as trees have a significant economic and climatic influence on global economies and ecologies. This completely revised classic book is an up-to-date synthesis of the intensive research devoted to woody plants published in the second edition, with additional important aspects from the authors' previous book, ""Growth Control in Woody Plants"". Intended primarily as a reference for researchers, the interdisciplinary nature of the book makes it useful to a broad range of scientists and researchers from agroforesters, agronomists, and arborists to plant pathologists and soil scientists. This third edition provides crutial updates to many chapters, including: responses of plants to elevated CO2; the process and regulation of cambial growth; photoinhibition and photoprotection of photosynthesis; nitrogen metabolism and internal recycling, and more.Revised chapters focus on emerging discoveries of the patterns and processes of woody plant physiology. This is the only book to provide recommendations for the use of specific management practices and experimental procedures and equipment. Interdisciplinary approach will appeal to a broad range of scientists, researchers, and growers. It is thoroughly updated with the latest research devoted to woody plants.",1983,0,2528,55,17,33,30,36,36,38,29,37,38,49
64f814ad0aeac192b62ba73440505b91b0fde6aa,"Cholesterol, fatty acids, fat-soluble vitamins, and other lipids present in our diets are not only nutritionally important but serve as precursors for ligands that bind to receptors in the nucleus. To become biologically active, these lipids must first be absorbed by the intestine and transformed by metabolic enzymes before they are delivered to their sites of action in the body. Ultimately, the lipids must be eliminated to maintain a normal physiological state. The need to coordinate this entire lipid-based metabolic signaling cascade raises important questions regarding the mechanisms that govern these pathways. Specifically, what is the nature of communication between these bioactive lipids and their receptors, binding proteins, transporters, and metabolizing enzymes that links them physiologically and speaks to a higher level of metabolic control? Some general principles that govern the actions of this class of bioactive lipids and their nuclear receptors are considered here, and the scheme that emerges reveals a complex molecular script at work.",2001,29,1984,78,0,68,148,132,112,118,115,117,127,133
deda41530c48862aaae37a6aaabefb5c2e379181,"A model to predict global patterns in vegetation physiognomy was developed from physiological considera- tions influencing the distributions of different functional types of plant. Primary driving variables are mean coldest- month temperature, annual accumulated temeprature over 5""C, and a drought index incorporating the seasonality of precipitation and the available water capacity of the soil. The model predicts which plant types can occur in a given environment, and selects the potentially dominant types from among them. Biomes arise as combinations of domi- nant types. Global environmental data were supplied as monthly means of temperature, precipitation and sunshine (interpolated to a global 0.5"" grid, with a lapse-rate correc-",1992,42,2005,132,2,35,32,63,60,55,75,73,69,64
1c7f88d8161cb1117fb72204fc3a2fbc0cd96afb,"The emergence of tissue engineering raises new possibilities for the study of complex physiological and pathophysiological processes in vitro. Many tools are now available to create 3D tissue models in vitro, but the blueprints for what to make have been slower to arrive. We discuss here some of the 'design principles' for recreating the interwoven set of biochemical and mechanical cues in the cellular microenvironment, and the methods for implementing them. We emphasize applications that involve epithelial tissues for which 3D models could explain mechanisms of disease or aid in drug development.",2006,154,1994,33,15,63,59,106,126,167,137,174,166,160
da414f44d02f96eb45fc77b47d7fe7584d8823e3,"This review is focused on purinergic neurotransmission, i.e., ATP released from nerves as a transmitter or cotransmitter to act as an extracellular signaling molecule on both pre- and postjunctional membranes at neuroeffector junctions and synapses, as well as acting as a trophic factor during development and regeneration. Emphasis is placed on the physiology and pathophysiology of ATP, but extracellular roles of its breakdown product, adenosine, are also considered because of their intimate interactions. The early history of the involvement of ATP in autonomic and skeletal neuromuscular transmission and in activities in the central nervous system and ganglia is reviewed. Brief background information is given about the identification of receptor subtypes for purines and pyrimidines and about ATP storage, release, and ectoenzymatic breakdown. Evidence that ATP is a cotransmitter in most, if not all, peripheral and central neurons is presented, as well as full accounts of neurotransmission and neuromodulation in autonomic and sensory ganglia and in the brain and spinal cord. There is coverage of neuron-glia interactions and of purinergic neuroeffector transmission to nonmuscular cells. To establish the primitive and widespread nature of purinergic neurotransmission, both the ontogeny and phylogeny of purinergic signaling are considered. Finally, the pathophysiology of purinergic neurotransmission in both peripheral and central nervous systems is reviewed, and speculations are made about future developments.",2007,2060,1422,155,24,128,136,135,121,121,127,100,106,94
2cdbdcd441e363eddfac792715193722474bc07b,Chapter 1. Structure and Function of Exercising Muscle Chapter 2. Fuel for Exercise: Bioenergetics and Muscle Metabolism Substrates Chapter 3. Neural Control of Exercising Muscle Chapter 4. Hormonal Control During Exercise Chapter 5. Energy Expenditure and Fatigue Chapter 6. The Cardiovascular System and Its Control Chapter 7. The Respiratory System and Its Regulation Chapter 8. Cardiorespiratory Responses to Acute Exercise Chapter 9. Principles of Exercise Training.,1995,0,2053,125,6,2,10,16,27,31,30,47,37,47
4953f7dc730ac40491fae52a59ce01ddba618c1c,Studies of physiological mechanisms are needed to predict climate effects on ecosystems at species and community levels.,2008,18,1834,84,6,43,68,93,139,149,167,144,183,166
c47c3a150ba7934154f86ee8883321e6b2a57a4d,"Preface Part I. Oxygen: 1. Respiration 2. Blood 3. Circulation Part II. Food and Energy: 4. Food and fuel 5. Energy metabolism Part III. Temperature: 6. Temperature effects 7. Temperature regulation Part IV. Water: 8. Water and osmotic regulation 9. Excretion Part V. Movement, Information, Integration: 10. Movement, muscle, biomechanics 11. Control and integration 12. Hormonal control 13. Information and senses Appendices Index.",1985,0,2360,208,29,17,25,27,35,46,31,29,42,29
7ee44d0355b1e3c67f9a7bba4c8e9c131b8e8870,"During recent years there has been remarkable progress in the understanding and practical use of chlorophyll fluorescence in plant science. This 'renaissance' of chlorophyll fluorescence was induced by the urgent need of applied research (like plant stress physiology, ecophysiology, phytopathology etc.) for quantitative, non-invasive, rapid methods to assess photosynthesis in intact leaves. Recent developments of suitable instrumentation and methodology have substantially increased these possibilities. Actually, a vast amount of knowledge on chlorophyll fluorescence had already accumulated over more than 50 years, since the discovery of the Kautsky effect in 1931 (Kautsky and Hirsch 1931) (for reviews, see e.g., Lavorel and Etienne 1977, Briantais et al. 1986, Renger and Schreiber 1986). On the one hand this knowledge was mechanistic, resulting from biophysically oriented basic research. On the other hand it was phenomenological, originating from applied plant physiological research. Until recently the phenomenology of whole leaf chlorophyll fluorescence appeared far too complex to find serious attention of biophysicists. Thus, for a long time, there was a gap between applied and basic research in chlorophyll fluorescence. Developments in instrumentation (Ogren and Baker 1985, Schreiber 1986, Schreiber et al. 1986) and methodology (Bradbury and Baker 1981, Krause et al. 1982, Quick and Horton 1984, Dietz et al. 1985, Demmig et al. 1987, Weis and Berry 1987, Bilger et al. 1989, Genty et al. 1989) has succeeded in closing this gap and bringing these two disciplines into sufficiently close contact and in mutually stimulating interaction. Consequently the present ""renaissance"" of chlorophyll fluorescence may be the product of a fruitful dynamic interaction between three different research disciplines, i.e., basic and applied research linked to new developments in instrumentation and methodology (see scheme in Fig. 1). As a result, measuring chlorophyll fluorescence has become a very attractive means of obtaining rapid, semiquantitative information on photosynthesis, used by an increasing number of researchers not only in the laboratory but also in the field. The wide range of possible applications is reflected by the broad spectrum of contributions to this issue of Photosynthesis Research. The progress made in chlorophyll fluorescence instrumentation and methodology has also induced new developments in the adjacent fields of absorbance spectroscopy (e.g., Klughammer et al. or Harbinson et al. in this issue), photoacoustic spectroscopy (e.g., Canaani, Dau and Hansen, Kolbowski et al. or Snel et al. in this issue) and chlorophyll luminescence (delayed fluorescence) (Bilger and Schreiber in this issue). These new developments are expected to play a role in",1990,25,1830,53,1,0,5,7,9,26,18,16,20,17
6622cf6e6950276e5da4063a9dbd81e161e990f9,"According to classical concepts of physiologic control, healthy systems are self-regulated to reduce variability and maintain physiologic constancy. Contrary to the predictions of homeostasis, however, the output of a wide variety of systems, such as the normal human heartbeat, fluctuates in a complex manner, even under resting conditions. Scaling techniques adapted from statistical physics reveal the presence of long-range, power-law correlations, as part of multifractal cascades operating over a wide range of time scales. These scaling properties suggest that the nonlinear regulatory systems are operating far from equilibrium, and that maintaining constancy is not the goal of physiologic control. In contrast, for subjects at high risk of sudden death (including those with heart failure), fractal organization, along with certain nonlinear interactions, breaks down. Application of fractal analysis may provide new approaches to assessing cardiac risk and forecasting sudden cardiac death, as well as to monitoring the aging process. Similar approaches show promise in assessing other regulatory systems, such as human gait control in health and disease. Elucidating the fractal and nonlinear mechanisms involved in physiologic control and complex signaling networks is emerging as a major challenge in the postgenomic era.",2002,89,1803,103,9,20,35,61,70,85,71,94,116,110
8f1643efa9f59e74f6137196b94d6d69b7f42d7f,"Biologists usually refer to mammals and birds as homeotherms, but these animals universally experience regional and temporal variations in body temperature. These variations could represent adaptive strategies of heterothermy, which in turn would favor genotypes that function over a wide range of temperatures. This coadaptation of thermoregulation and thermosensitivity has been studied extensively among ectotherms, but remains unexplored among endotherms. In this review, we apply classical models of thermal adaptation to predict variation in body temperature within and among populations of mammals and birds. We then relate these predictions to observations generated by comparative and experimental studies. In general, optimality models can explain the qualitative effects of abiotic and biotic factors on thermoregulation. Similar insights should emerge when using models to predict variation in the thermosensitivity of endotherms, but the dearth of empirical data on this subject precludes a rigorous analysis at this time. Future research should focus on the selective pressures imposed by regional and temporal heterothermy in endotherms.",2002,364,981,57,0,0,0,0,0,2,32,40,49,48
b0bfbe817f7f626518a04fd74dad34ea410c5468,"The neurotrophins are a small group of dimeric proteins that profoundly affect the development of the nervous system of vertebrates. Recent studies have established clear correlations between the survival requirements for different neurotrophins of functionally distinct subsets of sensory neurons. The biological role of the neurotrophins is not limited to the prevention of programmed cell death of specific groups of neurons during development. Neurotrophin-3 in particular seems to act on neurons well before the period of target innervation and of normally occurring cell death. In animals lacking functional neurotrophin or receptor genes, neuronal numbers do not seem to be massively reduced in the CNS, unlike in the PNS. Finally, rapid actions of neurotrophins on synaptic efficacy, as well as the regulation of their mRNAs by electrical activity, suggest that neurotrophins might play important roles in regulating neuronal connectivity in the developing and in the adult central nervous system.",1996,135,1953,48,21,72,115,160,129,127,86,86,78,83
06224cd1d420e9566157bbfca0750bf3ef64868a,"Heparan sulphate proteoglycans reside on the plasma membrane of all animal cells studied so far and are a major component of extracellular matrices. Studies of model organisms and human diseases have demonstrated their importance in development and normal physiology. A recurrent theme is the electrostatic interaction of the heparan sulphate chains with protein ligands, which affects metabolism, transport, information transfer, support and regulation in all organ systems. The importance of these interactions is exemplified by phenotypic studies of mice and humans bearing mutations in the core proteins or the biosynthetic enzymes responsible for assembling the heparan sulphate chains.",2007,102,1374,65,16,56,83,115,98,119,120,131,121,95
b6f6e845207c85102ac7e36eab8a70098c5f24a4,"Since the first identification of renin by Tigerstedt and Bergmann in 1898, the renin-angiotensin system (RAS) has been extensively studied. The current view of the system is characterized by an increased complexity, as evidenced by the discovery of new functional components and pathways of the RAS. In recent years, the pathophysiological implications of the system have been the main focus of attention, and inhibitors of the RAS such as angiotensin-converting enzyme (ACE) inhibitors and angiotensin (ANG) II receptor blockers have become important clinical tools in the treatment of cardiovascular and renal diseases such as hypertension, heart failure, and diabetic nephropathy. Nevertheless, the tissue RAS also plays an important role in mediating diverse physiological functions. These focus not only on the classical actions of ANG on the cardiovascular system, namely, the maintenance of cardiovascular homeostasis, but also on other functions. Recently, the research efforts studying these noncardiovascular effects of the RAS have intensified, and a large body of data are now available to support the existence of numerous organ-based RAS exerting diverse physiological effects. ANG II has direct effects at the cellular level and can influence, for example, cell growth and differentiation, but also may play a role as a mediator of apoptosis. These universal paracrine and autocrine actions may be important in many organ systems and can mediate important physiological stimuli. Transgenic overexpression and knock-out strategies of RAS genes in animals have also shown a central functional role of the RAS in prenatal development. Taken together, these findings may become increasingly important in the study of organ physiology but also for a fresh look at the implications of these findings for organ pathophysiology.",2006,825,1433,96,2,51,81,79,114,104,125,94,93,123
67eefe022de61f0371da08207a5169db34029239,"MARK provides parameter estimates from marked animals when they are re-encountered at a later time as dead recoveries, or live recaptures or re-sightings. The time intervals between re-encounters do not have to be equal. More than one attribute group of animals can be modelled. The basic input to MARK is the encounter history for each animal. MARK can also estimate the size of closed populations. Parameters can be constrained to be the same across re-encounter occasions, or by age, or group, using the parameter index matrix. A set of common models for initial screening of data are provided. Time effects, group effects, time x group effects and a null model of none of the above, are provided for each parameter. Besides the logit function to link the design matrix to the parameters of the model, other link functions include the log—log, complimentary log—log, sine, log, and identity. The estimates of model parameters are computed via numerical maximum likelihood techniques. The number of parameters that are...",1999,52,6813,1642,20,35,63,151,157,217,215,282,313,371
a4760abdebd2c2d10fbb8cc3b16bd6061fb5d69f,The Committee for Research and Ethical Issues of the International Association for the Study of Pain (IASP®) is concerned with the ethical aspects of studies producing experimental pain and any suffering it may cause in animals. Such studies are essential if new and clinically relevant knowledge about the mechanisms of pain is to be acquired. Investigations in conscious animals intended to stimulate chronic pain in man are being performed. Such experiments require careful planning to avoid or at least minimize pain in the animals.,1983,4,7300,625,0,3,3,5,4,6,17,15,28,39
0c7f2f51c76aa75dec1807332fd133f73f77d43b,"The influence of diet on the distribution of nitrogen isotopes in animals was investigated by analyzing animals grown in the laboratory on diets of constant nitrogen isotopic composition. 
The isotopic composition of the nitrogen in an animal reflects the nitrogen isotopic composition of its diet. The δ^(15)N values of the whole bodies of animals are usually more positive than those of their diets. Different individuals of a species raised on the same diet can have significantly different δ^(15)N values. The variability of the relationship between the δ^(15)N values of animals and their diets is greater for different species raised on the same diet than for the same species raised on different diets. Different tissues of mice are also enriched in ^(15)N relative to the diet, with the difference between the δ^(15)N values of a tissue and the diet depending on both the kind of tissue and the diet involved. The δ^(15)N values of collagen and chitin, biochemical components that are often preserved in fossil animal remains, are also related to the δ^(15)N value of the diet. 
The dependence of the δ^(15)N values of whole animals and their tissues and biochemical components on the δ^(15)N value of diet indicates that the isotopic composition of animal nitrogen can be used to obtain information about an animal's diet if its potential food sources had different δ^(15)N values. The nitrogen isotopic method of dietary analysis probably can be used to estimate the relative use of legumes vs non-legumes or of aquatic vs terrestrial organisms as food sources for extant and fossil animals. However, the method probably will not be applicable in those modern ecosystems in which the use of chemical fertilizers has influenced the distribution of nitrogen isotopes in food sources. 
The isotopic method of dietary analysis was used to reconstruct changes in the diet of the human population that occupied the Tehuacan Valley of Mexico over a 7000 yr span. Variations in the δ^(15)C and δ^(15)N values of bone collagen suggest that C_4 and/or CAM plants (presumably mostly corn) and legumes (presumably mostly beans) were introduced into the diet much earlier than suggested by conventional archaeological analysis.",1978,56,5896,510,4,6,7,13,12,22,17,21,28,16
8e055c46c78ca7df9af63a32816ea1eef067064a,"The understanding of the dynamics of animal populations and of related ecological and evolutionary issues frequently depends on a direct analysis of life history parameters. For instance, examination of trade-offs between reproduction and survival usually rely on individually marked animals, for which the exact time of death is most often unknown, because marked individuals cannot be followed closely through time. Thus, the quantitative analysis of survival studies and experiments must be based on capture- recapture (or resighting) models which consider, besides the parameters of primary interest, recapture or resighting rates that are nuisance parameters. Capture-recapture models oriented to estimation of survival rates are the result of a recent change in emphasis from earlier approaches in which population size was the most important parameter, survival rates having been first introduced as nuisance parameters. This emphasis on survival rates in capture-recapture models developed rapidly in the 1980s and used as a basic structure the Cormack-Jolly-Seber survival model applied to an homogeneous group of animals, with various kinds of constraints on the model parameters. These approaches are conditional on first captures; hence they do not attempt to model the initial capture of unmarked animals as functions of population abundance in addition to survival and capture probabilities. This paper synthesizes, using a common framework, these recent developments together with new ones, with an emphasis on flexibility in modeling, model selection, and the analysis of multiple data sets. The effects on survival and capture rates of time, age, and categorical variables characterizing the individuals (e.g., sex) can be considered, as well as interactions between such effects. This ""analysis of variance"" philosophy emphasizes the structure of the survival and capture process rather than the technical characteristics of any particular model. The flexible array of models encompassed in this synthesis uses a common notation. As a result of the great level of flexibility and relevance achieved, the focus is changed from fitting a particular model to model building and model selection. The following procedure is recommended: (1) start from a global model compatible with the biology of the species studied and with the design of the study, and assess its fit; (2) select a more parsimonious model using Akaike's Information Criterion to limit the number of formal tests; (3) test for the most important biological questions by comparing this model with neighboring ones using likelihood ratio tests; and (4) obtain maximum likelihood estimates of model parameters with estimates of precision. Computer software is critical, as few of the models now available have parameter estimators that are in closed form. A comprehensive table of existing computer software is provided. We used RELEASE for data summary and goodness-of-fit tests and SURGE for iterative model fitting and the computation of likelihood ratio tests. Five increasingly complex examples are given to illustrate the theory. The first, using two data sets on the European Dipper (Cinclus cinclus), tests for sex-specific parameters,",1992,112,4145,462,12,16,18,53,44,65,73,95,83,104
695c252520246d8a76a8b785ef57e95ad838d565,"""Animals and Why They Matter"" examines the barriers that our philosophical traditions have erected between human beings and animals and reveals that the too-often ridiculed subject of animal rights is an issue crucially related to such problems within the human community as racism, sexism, and age discrimination. Mary Midgley's profound and clearly written narrative is a thought-provoking study of the way in which the opposition between reason and emotion has shaped our moral and political ideas and the problems it has raised. Whether considering vegetarianism, women's rights, or the ""humanity"" of pets, this book goes to the heart of the question of why all animals matter.",1984,0,478,15,1,1,2,3,2,5,31,6,3,4
a32728cf2a8baf408e3270da5c6ced7aba9e4068,"Abstract The practical analysis of space use and habitat selection by animals is often a problem due to the lack of well-designed programs. I present here the “adehabitat” package for the R software, which offers basic GIS (Geographic Information System) functions, methods to analyze radio-tracking data and habitat selection by wildlife, and interfaces with other R packages. These tools can be downloaded freely on the internet. Because the functions of this package can be combined with other functions of R, “adehabitat” provides a powerful environment for the analysis of the space and habitat use.",2006,22,2743,465,1,10,29,32,49,79,112,130,188,240
c375287977ae3f84f456ca93b8dee25375bf1a0c,"With a standard set of primers directed toward conserved regions, we have used the polymerase chain reaction to amplify homologous segments of mtDNA from more than 100 animal species, including mammals, birds, amphibians, fishes, and some invertebrates. Amplification and direct sequencing were possible using unpurified mtDNA from nanogram samples of fresh specimens and microgram amounts of tissues preserved for months in alcohol or decades in the dry state. The bird and fish sequences evolve with the same strong bias toward transitions that holds for mammals. However, because the light strand of birds is deficient in thymine, thymine to cytosine transitions are less common than in other taxa. Amino acid replacement in a segment of the cytochrome b gene is faster in mammals and birds than in fishes and the pattern of replacements fits the structural hypothesis for cytochrome b. The unexpectedly wide taxonomic utility of these primers offers opportunities for phylogenetic and population research.",1989,22,4670,214,2,15,41,43,60,97,90,96,110,126
9739f7c963191891f04d56d1138b93a6c815d141,"Over the past 100 years, the global average temperature has increased by approximately 0.6 °C and is projected to continue to rise at a rapid rate. Although species have responded to climatic changes throughout their evolutionary history, a primary concern for wild species and their ecosystems is this rapid rate of change. We gathered information on species and global warming from 143 studies for our meta-analyses. These analyses reveal a consistent temperature-related shift, or ‘fingerprint’, in species ranging from molluscs to mammals and from grasses to trees. Indeed, more than 80% of the species that show changes are shifting in the direction expected on the basis of known physiological constraints of species. Consequently, the balance of evidence from these studies strongly suggests that a significant impact of global warming is already discernible in animal and plant populations. The synergism of rapid temperature rise and other stresses, in particular habitat destruction, could easily disrupt the connectedness among species and lead to a reformulation of species communities, reflecting differential changes in species, and to numerous extirpations and possibly extinctions.",2003,46,4262,204,56,124,172,185,236,231,234,273,269,303
fab4e2bc033cb8bc1a5587b235184c66e4faea2b,"By Matthew James Keelingand Pejman RohaniPrinceton, NJ: Princeton University Press,2008.408 pp., Illustrated. $65.00 (hardcover).Mathematical modeling of infectious dis-eases has progressed dramatically over thepast 3 decades and continues to ﬂourishat the nexus of mathematics, epidemiol-ogy, and infectious diseases research. Nowrecognized as a valuable tool, mathemat-ical models are being integrated into thepublic health decision-making processmore than ever before. However, despiterapid advancements in this area, a formaltraining program for mathematical mod-eling is lacking, and there are very fewbooks suitable for a broad readership. Tosupport this bridging science, a commonlanguage that is understood in all con-tributing disciplines is required.",2007,39,3012,262,2,18,57,93,159,163,195,201,215,267
18d1dada692a5414b044d9570b6178639de5141d,,1996,597,11367,22,4,6,4,6,8,3,6,311,370,404
f96efe5d624b6357ec956e33481cabf84c93d446,"Toxoplasmosis is one of the more common parasitic zoonoses world-wide. Its causative agent, Toxoplasma gondii, is a facultatively heteroxenous, polyxenous protozoon that has developed several potential routes of transmission within and between different host species. If first contracted during pregnancy, T. gondii may be transmitted vertically by tachyzoites that are passed to the foetus via the placenta. Horizontal transmission of T. gondii may involve three life-cycle stages, i.e. ingesting infectious oocysts from the environment or ingesting tissue cysts or tachyzoites which are contained in meat or primary offal (viscera) of many different animals. Transmission may also occur via tachyzoites contained in blood products, tissue transplants, or unpasteurised milk. However, it is not known which of these routes is more important epidemiologically. In the past, the consumption of raw or undercooked meat, in particular of pigs and sheep, has been regarded as a major route of transmission to humans. However, recent studies showed that the prevalence of T. gondii in meat-producing animals decreased considerably over the past 20 years in areas with intensive farm management. For example, in several countries of the European Union prevalences of T. gondii in fattening pigs are now <1%. Considering these data it is unlikely that pork is still a major source of infection for humans in these countries. However, it is likely that the major routes of transmission are different in human populations with differences in culture and eating habits. In the Americas, recent outbreaks of acute toxoplasmosis in humans have been associated with oocyst contamination of the environment. Therefore, future epidemiological studies on T. gondii infections should consider the role of oocysts as potential sources of infection for humans, and methods to monitor these are currently being developed. This review presents recent epidemiological data on T. gondii, hypotheses on the major routes of transmission to humans in different populations, and preventive measures that may reduce the risk of contracting a primary infection during pregnancy.",2000,815,2953,243,2,10,32,40,43,55,81,86,107,152
f168482f78426e3baf262352fafd973e4f765b90,Introduction to resource selection studies. Examples of the use of resource selectory studies. Examples of the use of resource selection functions. Statistical modelling procedures. Studies with resources defined by several categories. Estimating a resource selection probability function from a census of resource units using logistic regression. Estimating a resource selection probability function from a census of resource units at several points in time using the proportional hazards model. Estimating a resource selection function from samples of resource units using proportional hazards and log-linear models. Estimating a resource selection function from two samples of resource units using logistic regression and discriminant function methods. General log-linear modelling. Analysis of the amount of use. The comparison of selection for different types of resource unit. References. Index.,1994,0,2940,295,6,4,3,15,15,26,35,30,41,53
c03e8924fe6e1e9da5b4a7dadee8d5d01ad15f6b,"There is good evidence that the complex microbial flora present in the gastrointestinal tract of all warm-blooded animals is effective in providing resistance to disease. However, the composition of this protective flora can be altered by dietary and environmental influences, making the host animal susceptible to disease and/or reducing its efficiency of food utilization. What we are doing with the probiotic treatments is re-establishing the natural condition which exists in the wild animal but which has been disrupted by modern trends in conditions used for rearing young animals, including human babies, and in modern approaches to nutrition and disease therapy. These are all areas where the gut flora can be altered for the worse and where, by the administration of probiotics, the natural balance of the gut microflora can be restored and the animal returned to its normal nutrition, growth and health status.",1989,84,3584,121,0,5,12,18,12,12,12,13,37,32
bf7defd7918575b38a6dffa77f71d09ac4270cab,"Small RNAs of 20–30 nucleotides can target both chromatin and transcripts, and thereby keep both the genome and the transcriptome under extensive surveillance. Recent progress in high-throughput sequencing has uncovered an astounding landscape of small RNAs in eukaryotic cells. Various small RNAs of distinctive characteristics have been found and can be classified into three classes based on their biogenesis mechanism and the type of Argonaute protein that they are associated with: microRNAs (miRNAs), endogenous small interfering RNAs (endo-siRNAs or esiRNAs) and Piwi-interacting RNAs (piRNAs). This Review summarizes our current knowledge of how these intriguing molecules are generated in animal cells.",2009,224,2985,178,94,228,276,313,368,300,278,223,256,202
c760865d5250effb18e3b4a761482413cc96426e,,1993,0,2299,400,0,2,2,4,5,6,9,8,7,50
d91dcb167ec9c097d1ff87ba1c2beaf3973a3869,"T.B. Farver, Concepts of Normality in Clinical Biochemistry. J.G. Hauge, DNA Technology in Diagnosis, Breeding, and Therapy. J.J. Kaneko, Carbohydrate Metabolism and Its Diseases. M.L. Bruss, Lipids and Ketones. J.J. Kaneko, Serum Proteins and the Dysproteinemias. L.J. Gershwin, Clinical Immunology. J.W. Harvey, The Erythrocyte: Physiology, Metabolism, and Biochemical Disorders. J.J. Kaneko, Porphyrins and the Porhyrias. J.E. Smith, Iron Metabolism and Its Disorders. W.J. Dodds, Hemostasis. J.G. Zinkl and M.B. Kabbur, Neutrophil Function. J.W. Kramer and W.E. Hoffmann, Clinical Enzymology. B.C. Tennant, Hepatic Function. D.F. Brobst, Pancreatic Function. W.E. Hornbuckle and B.C. Tennant, Gastrointestinal Function. G.H. Cardinet III, Skeletal Muscle Function. D.R. Finco, Kidney Function. G.P. Carlson, Fluid, Electrolyte, and Acid-Base Balance. J.A. Mol and A. Rijnberk, Pituitary Function. A. Rijnberk and J.A. Mol, Adrenocortical Function. J.J. Kaneko, Thyroid Function. L-E. Edqvist and M. Forsberg, Clinical Reproductive Endocrinology. T.J. Rosol and C.C. Capen, Calcium-Regulating Hormones and Diseases of Abnormal Mineral (Calcium, Phosphorus, Magnesium) Metabolism. R.B. Rucker and J.G. Morris, The Vitamins. M. Haskins and U. Giger, Lysosomal Storage Diseases. B.R. Madewell, Tumor Markers. C.S. Bailey and W. Vernau, Cerebrospinal Fluid. J.R. Turk and S.W. Casteel, Clinical Biochemistry in Toxicology. W.F. Loeb, Clinical Biochemistry of Laboratory Rodents and Rabbits. J.T. Lumeij, Avian Clinical Biochemistry. Appendixes. Index.",1963,0,3627,199,1,2,0,2,2,3,2,2,6,7
129169f853b1e71e05d97fe6ff733d7cd57b22d8,"This paper summarizes the current views on coping styles as a useful concept in understanding individual adaptive capacity and vulnerability to stress-related disease. Studies in feral populations indicate the existence of a proactive and a reactive coping style. These coping styles seem to play a role in the population ecology of the species. Despite domestication, genetic selection and inbreeding, the same coping styles can, to some extent, also be observed in laboratory and farm animals. Coping styles are characterized by consistent behavioral and neuroendocrine characteristics, some of which seem to be causally linked to each other. Evidence is accumulating that the two coping styles might explain a differential vulnerability to stress mediated disease due to the differential adaptive value of the two coping styles and the accompanying neuroendocrine differentiation.",1999,102,2410,262,0,5,24,25,51,46,62,51,91,72
5ec88f2510deece23ceaaffaa0ef515abc3d67ec,"When you did me the honor of asking me to fill your presidential chair, I accepted perhaps without duly considering the duties of the president of a society, founded largely to further the study of evolution, at the close of the year that marks the centenary of Darwin and Wallace's initial presentation of the theory of natural selection. It seemed to me that most of the significant aspects of modern evolutionary theory have come either from geneticists, or from those heroic museum workers who suffering through years of neglect, were able to establish about 20 years ago what has come to be called the ""new systematics."" You had, however, chosen an ecologist as your president and one of that school at times supposed to study the environment without any relation to the organism. A few months later I happened to be in Sicily. An early interest in zoogeography and in aquatic insects led me to attempt to collect near Palermo, certain species of water-bugs, of the genus Cprixa, described a century ago by Fieber and supposed to occur in the region, but never fully reinvestigated. It is hard to find suitable localities in so highly cultivated a landscape as the Concha d'Oro. Fortunately, I was driven up Monte Pellegrino, the hill that rises to the west of the city, to admire the view. A little below the summit, a church with a simple baroque facade stands in front of a cave in the limestone of the hill. Here in the 16th century a stalactite encrusted skeleton associated with a cross and twelve beads was discovered. Of this skeleton nothing is certainly known save that it is that of Santa Rosalia, a saint of whom little is reliably reported save that she seems to have lived in the 12th century, that her skeleton was found in this cave, and that she has been the chief patroness of Palermo ever since. Other limestone caverns on Monte Pellegrino had yielded bones of extinct Pleistocene Equus, and on the walls of one of the rock shelters at the bottom of the hill there are beautiful Gravettian engravings. Moreover, a small relic of the saint that I saw in the treasury of the Cathedral of Monreale has a venerable and *Address of the President, American Society of Naturalists, delivered at the annual meeting, Washington, D. C., December 30, 1958.",1959,33,3684,152,1,2,2,7,1,7,4,10,8,11
f22304fc493c3057d99df4fe35d8cc549a99e6b1,"SUMMARY Various gram-negative animal and plant pathogens use a novel, sec-independent protein secretion system as a basic virulence mechanism. It is becoming increasingly clear that these so-called type III secretion systems inject (translocate) proteins into the cytosol of eukaryotic cells, where the translocated proteins facilitate bacterial pathogenesis by specifically interfering with host cell signal transduction and other cellular processes. Accordingly, some type III secretion systems are activated by bacterial contact with host cell surfaces. Individual type III secretion systems direct the secretion and translocation of a variety of unrelated proteins, which account for species-specific pathogenesis phenotypes. In contrast to the secreted virulence factors, most of the 15 to 20 membrane-associated proteins which constitute the type III secretion apparatus are conserved among different pathogens. Most of the inner membrane components of the type III secretion apparatus show additional homologies to flagellar biosynthetic proteins, while a conserved outer membrane factor is similar to secretins from type II and other secretion pathways. Structurally conserved chaperones which specifically bind to individual secreted proteins play an important role in type III protein secretion, apparently by preventing premature interactions of the secreted factors with other proteins. The genes encoding type III secretion systems are clustered, and various pieces of evidence suggest that these systems have been acquired by horizontal genetic transfer during evolution. Expression of type III secretion systems is coordinately regulated in response to host environmental stimuli by networks of transcription factors. This review comprises a comparison of the structure, function, regulation, and impact on host cells of the type III secretion systems in the animal pathogens Yersinia spp., Pseudomonas aeruginosa, Shigella flexneri, Salmonella typhimurium, enteropathogenic Escherichia coli, and Chlamydia spp. and the plant pathogens Pseudomonas syringae, Erwinia spp., Ralstonia solanacearum, Xanthomonas campestris, and Rhizobium spp.",1998,545,2427,189,12,119,176,162,140,184,191,167,123,97
303e936f367602ee3556fefafb7a41ee65ce1738,,1993,325,2873,72,4,11,21,19,29,36,44,40,51,63
561d6920bfe33e087fd9d411d873e83a5e71872b,,1959,0,3656,9,1,3,5,3,3,4,4,8,5,5
62c1699c60cd032cbbec64d9125beff2788e8465,"As I have pointed out earlier, when I met Oliver Zangwill in 1961 at a meeting on dyslexia in Baltimore, he listened patiently to the exposition of my ideas on the significance of the cortico-cortical connections for the higher functions. A short time later, while on a trip to Boston, he suggested to me that I should prepare an extended account of these ideas. This paper would never have been written without Professor Zangwill’s urging, and I am grateful to him for having brought me to a more careful review of the older literature and a more precise statement of my own ideas. Although Russell Brain, who was then the editor of Brain, had some misgivings about the section on philosophical implications he agreed to take the manuscript unchanged.",1965,127,3000,60,1,5,11,11,21,20,19,19,17,27
b19d5c577dfd6383e5a4a2be98e1ef631c6ec622,"The Guide for the Care and Use of Laboratory Animals by the Institute for Laboratory Animal Research (ILAR) of the National Research Council in the USA, is well known among most individuals involved in laboratory animal care and use. Most of the time insiders refer to it as the ‘Guide’. In the year 2011, ILAR published its eighth edition. This new edition gathered the latest facts, incorporated up-to-date knowledge and reorganized the contents to provide better guidance to laboratory animal care and use programmes. The history of the Guide began in 1946, when Dr Nathan R Brewer and his colleagues in the Chicago area started to improve the care and wellbeing of laboratory animals by exchanging ideas at monthly meetings. The activities of this group led, in 1950, to the foundation of the Animal Care Panel (ACP), which became a growing non-profit organization and was later renamed as American Association for Laboratory Animal Science (AALAS). In 1963, the Animal Facilities Standards Committee of the ACP prepared the first edition of the Guide for the Care and Use of Laboratory Animals. Gradually, the Guide has become the primary reference in many research organizations in the USA, and compliance with it is obligatory for the Public Health Service (PHS)-assured institutions. When the Association for the Assessment and Accreditation for Laboratory Animal Care (AAALAC International) expanded its activities beyond the USA and went international, the Guide became a resource for animal care and use programmes around the world. Today the Guide is one of three primary standards AAALAC uses to evaluate an institution’s animal care and use programme. The other two documents are the Guide for the Care and Use of Agricultural Animals in Research and Teaching (Ag Guide), FASS 2010; and the European Convention for the Protection of Vertebrate Animals Used for Experimental and Other Scientific Purposes, Council of Europe (ETS 123). Over time, the Guide was updated several times culminating in the seventh edition being published in 1996. In the last decade, laboratory animal science advanced so significantly that another update was considered necessary to promote the best animal care and use practices. In 2006, a committee was appointed by the National Research Council in the USA, and started the process of updating the Guide. This process was accompanied by extensive public hearings and solicited comments from a wide range of scientific communities and the public. Fifteen years after its seventh edition, the new eighth edition of the Guide was finally completed and published. The Guide is not a handbook; it is an extensive collection of detailed descriptions of standards for all components of a good laboratory animal care and use programme. The frame of the book not only focuses on the wellbeing of lab animals, but also on the health and safety of people working with animals. Compared with the previous edition, the eighth edition is organized differently. After a brief overview, the Guide is divided into five chapters covering details of recommended standards for the care and use of laboratory animals. In addition, extra information related to the Guide can be found in the appendices at the end of the book. The first chapter introduces and defines the key concepts and terms used in the Guide. It describes the goals of the Guide as well as the intended audience and how to use the Guide. The overall intention is to support the readers to build a programme which creates a system of selfregulation and regulatory oversight, a concept that has been proven of value in many research situations. The concept of the 3Rs (Replacement, Reduction, Refinement) was always part of the philosophy of the Guide. In this eighth version now, this concept is mentioned expressis verbis and the individual definitions of each ‘R’ are outlined. Chapter 2 highlights the components of a state of the art animal care and use programme. After a short summary of the programme management, the chapter defines in details the respective roles and responsibilities of programme oversight. In the past, the primary responsibility of programme oversight fell primarily on the Animal Care and Use Committee (IACUC); now it is shared with the institutional official (IO) and the attending veterinarian (AV). The chapter further defines the roles and responsibilities of the key management for all elements of the programme and supplies definitions for regulations and policies. Many recommendations on occupational health and safety are provided here. The need for a disaster plan is now changed from the ‘should’ to the ‘must’ requirement. Environment, housing and management of laboratory animals are the topics of the third chapter. It should be noted that, in this chapter, a new section was added for addressing the care and use of aquatic animals. With this, the authors acknowledge the increased importance of these species, especially zebrafishes, in the laboratory environment. As in the previous editions, this chapter provides well-organized tables for quick references for housing space for species typically used in research. It is important to mention that the Guide stresses in particular for this topic the uses of a performance-based approach to decide on the space requirements for each particular case. Therefore, the recommended space is now defined as ‘recommended minimum space’. The next chapter discusses issues related to veterinary care. It covers regular aspects of veterinary care in laboratory animals, such as acquisition and clinical care of animals, surgery, pain management and anaesthesia, and preventive medicine. The Guide gives a lot of importance",1979,0,2725,40,0,0,0,3,1,0,2,5,2,1
1e6a25cb1d49fefd81cf2af3e7e2771558d73cba,"A novel coronavirus (SCoV) is the etiological agent of severe acute respiratory syndrome (SARS). SCoV-like viruses were isolated from Himalayan palm civets found in a live-animal market in Guangdong, China. Evidence of virus infection was also detected in other animals (including a raccoon dog, Nyctereutes procyonoides) and in humans working at the same market. All the animal isolates retain a 29-nucleotide sequence that is not found in most human isolates. The detection of SCoV-like viruses in small, live wild mammals in a retail market indicates a route of interspecies transmission, although the natural reservoir is not known.",2003,22,1971,75,28,177,143,130,86,82,60,50,47,45
ef216b547c246c1c9b6c842c51274fe0dd54cbd2,"Many studies have described the effects of urbanization on species richness. These studies indicate that urbanization can increase or decrease species richness, depending on several variables. Some of these variables include: taxonomic group, spatial scale of analysis, and intensity of urbanization. Recent reviews of birds (the most-studied group) indicate that species richness decreases with increasing urbanization in most cases but produces no change or even increases richness in some studies. Here I expand beyond the bird studies by reviewing 105 studies on the effects of urbanization on the species richness of non-avian species: mammals, reptiles, amphibians, invertebrates and plants. For all groups, species richness tends to be reduced in areas with extreme urbanization (i.e., central urban core areas). However, the effects of moderate levels of urbanization (i.e., suburban areas) vary significantly among groups. Most of the plant studies (about 65%) indicate increasing species richness with moderate urbanization whereas only a minority of invertebrate studies (about 30%) and a very small minority of non-avian vertebrate studies (about 12%) show increasing species richness. Possible explanations for these results are discussed, including the importance of nonnative species importation, spatial heterogeneity, intermediate disturbance and scale as major factors influencing species richness.",2008,153,1582,100,1,17,30,53,64,71,110,141,161,185
798657e6e98fc7958a4b9dee079c96a8d5c23e1d,"Helminths, arthropods, & protozoa of domesticated animals , Helminths, arthropods, & protozoa of domesticated animals , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1969,0,2522,102,1,0,1,2,2,4,2,2,4,3
f07f33137788f9351fe8ee6d47e70d9674b6896c,"The most commonly recognized behavioral patterns of animals and people at the onset of febrile infectious diseases are lethargy, depression, anorexia, and reduction in grooming. Findings from recent lines of research are reviewed to formulate the perspective that the behavior of sick animals and people is not a maladaptive response or the effect of debilitation, but rather an organized, evolved behavioral strategy to facilitate the role of fever in combating viral and bacterial infections. The sick individual is viewed as being at a life or death juncture and its behavior is an all-out effort to overcome the disease.",1988,206,1786,118,0,1,3,6,7,7,14,20,17,38
e4b58ccd4385a6f4d9b97237d68a409b10b1c22a,"The arthropods constitute the most diverse animal group, but, despite their rich fossil record and a century of study, their phylogenetic relationships remain unclear1. Taxa previously proposed to be sister groups to the arthropods include Annelida, Onychophora, Tardigrada and others, but hypotheses of phylogenetic relationships have been conflicting2,3. For example, onychophorans, like arthropods, moult periodically, have an arthropod arrangement of haemocoel1,4, and have been related to arthropods in morphological and mitochondrial DNA sequence analyses4,5. Like annelids, they possess segmental nephridia and muscles that are a combination of smooth and obliquely striated fibres6. Our phylogenetic analysis of 18S ribosomal DNA sequences indicates a close relationship between arthropods, nematodes and all other moulting phyla. The results suggest that ecdysis (moulting) arose once and support the idea of a new clade, Ecdysozoa, containing moulting animals: arthropods, tardigrades, onychophorans, nematodes, nematomorphs, kinor-hynchs and priapulids. No support is found for a clade of segmented animals, the Articulata, uniting annelids with arthropods. The hypothesis that nematodes are related to arthropods has important implications for developmental genetic studies using as model systems the nematode Caenorhabditis elegans and the arthropod Drosophila melanogaster, which are generally held to be phylogenetically distant from each other.",1997,25,1464,113,4,59,74,100,94,95,72,96,91,80
f18b42ff20acf234ca57b36ad9acc97b655ba3ea,"Development of methods that allow an efficient expression of exogenous genes in animals would provide tools for gene function studies, treatment of diseases and for obtaining gene products. Therefore, we have developed a hydrodynamics-based procedure for expressing transgenes in mice by systemic administration of plasmid DNA. Using cDNA of luciferase and β-galactosidase as a reporter gene, we demonstrated that an efficient gene transfer and expression can be achieved by a rapid injection of a large volume of DNA solution into animals via the tail vein. Among the organs expressing the transgene, the liver showed the highest level of gene expression. As high as 45 μg of luciferase protein per gram of liver can be achi- eved by a single tail vein injection of 5 μg of plasmid DNA into a mouse. Histochemical analysis using β-galactosidase gene as a reporter reveals that approximately 40% of hepatocytes express the transgene. The time–response curve shows that the level of transgene expression in the liver reaches the peak level in approximately 8 h after injection and decreases thereafter. The peak level of gene expression can be regained by repeated injection of plasmid DNA. These results suggest that a simple, convenient and efficient method has been developed and which can be used as an effective means for studying gene function, gene regulation and molecular pathophysiology through gene transfer, as well as for expressing proteins in animals.",1999,28,1608,72,0,14,39,48,80,70,90,94,98,77
57baceddb3aa4c1744d11c468c47160796c373a8,VOLUME 1 Chapter 1 Bones and joints Chapter 2 Muscle and tendon Chapter 3 The nervous system Chapter 4 The eye and ear Chapter 5 The skin and appendages VOLUME 2 Chapter 1 Alimentary and peritoneum Chapter 2 The liver and biliary system Chapter 3 The pancreas Chapter 4 The urinary system Chapter 5 The respiratory system VOLUME 3 Chapter 1 The cardiovascular system Chapter 2 The hematopoietic system Chapter 3 The endocrine glands Chapter 4 The female genital system Chapter 5 The male genital system,1970,0,2313,42,19,16,25,16,21,22,24,15,23,23
5d9dfa842ee30f3a308737d1a422551c94e8c709,"This report summarizes the results of a multinational pharmaceutical company survey and the outcome of an International Life Sciences Institute (ILSI) Workshop (April 1999), which served to better understand concordance of the toxicity of pharmaceuticals observed in humans with that observed in experimental animals. The Workshop included representatives from academia, the multinational pharmaceutical industry, and international regulatory scientists. The main aim of this project was to examine the strengths and weaknesses of animal studies to predict human toxicity (HT). The database was developed from a survey which covered only those compounds where HTs were identified during clinical development of new pharmaceuticals, determining whether animal toxicity studies identified concordant target organ toxicities in humans. Data collected included codified compounds, therapeutic category, the HT organ system affected, and the species and duration of studies in which the corresponding HT was either first identified or not observed. This survey includes input from 12 pharmaceutical companies with data compiled from 150 compounds with 221 HT events reported. Multiple HTs were reported in 47 cases. The results showed the true positive HT concordance rate of 71% for rodent and nonrodent species, with nonrodents alone being predictive for 63% of HTs and rodents alone for 43%. The highest incidence of overall concordance was seen in hematological, gastrointestinal, and cardiovascular HTs, and the least was seen in cutaneous HT. Where animal models, in one or more species, identified concordant HT, 94% were first observed in studies of 1 month or less in duration. These survey results support the value of in vivo toxicology studies to predict for many significant HTs associated with pharmaceuticals and have helped to identify HT categories that may benefit from improved methods.",2000,37,1596,43,7,5,18,9,20,28,25,36,44,62
20dddc153678a64917edca0b0091b3f31839763e,"It is difficult to review reprints of the classical titles. Even more so, to review a book written by somebody about whom the foreword says in the first sentence: ""Charles Elton was a founder of ecology ..."". In addition, this new edition of a fundamental ecological work, first published in 1958, already contains a very competent review of itself this is represented by the foreword written by Daniel Simberloff. Simberloff's text is therefore, ironically, more interesting to review than the book itself everything seems to have been said about the book, and you could hardly find a monograph in the field that does not cite it. Everybody claims it as inspirational and still timely. Let me therefore comment rather on the foreword instead (or better say, on the book through the viewpoint of the foreword). By showing what Elton got right (most of the issue) and what he did not (much less), it provides us with a bright analysis of the four decades of development in the field of biological invasions. Elton's interest in the issue started with an article in the Times in 1933 and developed through a series of broadcasts. The examples he used in the book remain timely this is actually an indication of how serious a problem it is if almost all noxious invaders of the 1950s are still with us. To mention that introduced species are believed to be the second greatest cause of species extinctions and endangerment, following habitat destruction, is like carrying coals to Newcastle. Elton himself lamented on the low level of predictiveness generally associated with biological invasions. However, Simbedoff points out that ""he was prophetic"". Is this not an argument against definite scepticism and, at the same time, an indication that, to some extent, predictions is possible? Provided that it is made by a bright enough mind and brilliant observer, then maybe. Elton clearly foresaw some pattems and generalities which have been confirmed and statistically proven since then. The only widely criticized opinion is his belief in the importance of native species richness and its effect on invasibility. There are other issues which he ignored or did not recognize their importance such as propagule pressure, hybridization of native species with aliens and associated genetical effects, and evolutionary consequences. This does not make his work less valuable or influential it just means that he left us with a lot of problems to study. The book, a foundation stone of invasion biology, put a strong emphasis on animals; plants are obviously the second here. It is a credit to plant ecologists worldwide that they were able to cope with this handicap. At present, the generalizing theories attempting to explain the mechanisms underlying invasions are probably more developed for plants than for animals. ""Anyone wishing to understand the problem, from ordinary citizens through specialized researchers can profit from reading (or rereading) The Ecology of invasions,"" concludes Daniel Simberloff. I would like to add that to produce this new edition was an excellent idea. I strongly suspect that this was one of those books that everybody cites but quite a few of those who do that have never read it. Its the classics, you know everything is supposed to be in there so why read it? Now the new generation of invasion biologists can have it on their library shelves. Just go and get it.",1958,213,2095,103,0,0,1,0,2,1,3,1,5,1
881d8db3d0c1671f6dfc0fa7180747af7350fd1b,"AbstractIt is well documented that animals take risk of predation into account when making decisions about how to behave in particular situations, often trading-off risk against opportunities for mating or acquiring energy. Such an ability implies that animals have reliable information about the risk of predation at a given place and time. Chemosensory cues are an important source of such information. They reliably reveal the presence of predators (or their presence in the immediate past) and may also provide information on predator activity level and diet. In certain circumstances (e.g., in the dark, for animals in hiding) they may be the only cues available. Although a vast literature exists on the responses of prey to predator chemosensory cues (or odours), these studies are widely scattered, from marine biology to biological control, and not well known or appreciated by behavioural ecologists. In this paper, we provide an exhaustive review of this literature, primarily in tabular form. We highlight so...",1998,273,1321,81,6,9,30,51,43,50,65,59,63,68
029ea9d26e90ddf18a3e99caf24c021189363e34,"The worldwide contamination of foods and feeds with mycotoxins is a significant problem. Mycotoxins are secondary metabolites of molds that have adverse effects on humans, animals, and crops that result in illnesses and economic losses. Aflatoxins, ochratoxins, trichothecenes, zearelenone, fumonisins, tremorgenic toxins, and ergot alkaloids are the mycotoxins of greatest agro-economic importance. Some molds are capable of producing more than one mycotoxin and some mycotoxins are produced by more than one fungal species. Often more than one mycotoxin is found on a contaminated substrate. Factors influencing the presence of mycotoxins in foods or feeds include environmental conditions related to storage that can be controlled. Other extrinsic factors such as climate or intrinsic factors such as fungal strain specificity, strain variation, and instability of toxigenic properties are more difficult to control. Mycotoxins have various acute and chronic effects on humans and animals (especially monogastrics) depending on species and susceptibility of an animal within a species. Ruminants have, however, generally been more resistant to the adverse effects of mycotoxins. This is because the rumen microbiota is capable of degrading mycotoxins. The economic impact of mycotoxins include loss of human and animal life, increased health care and veterinary care costs, reduced livestock production, disposal of contaminated foods and feeds, and investment in research and applications to reduce severity of the mycotoxin problem. Although efforts have continued internationally to set guidelines to control mycotoxins, practical measures have not been adequately implemented.",2001,220,1446,57,0,9,22,29,28,45,48,65,72,54
fe58fc4efe9ab80a86037e0878663f011718e89b,"The stable nitrogen and carbon isotope ratios of bone collagen prepared from more than 100 animals representing 66 species of birds, fish, and mammals are presented. The δ15N values of bone collagen from animals that fed exclusively in the marine environment are, on average, 9%. more positive than those from animals that fed exclusively in the terrestrial environment; ranges for the two groups overlap by less than 1%. Bone collagen δ15N values also serve to separate marine fish from the small number of freshwater fish we analyzed. The bone collagen δ15N values of birds and fish that spent part of their life cycles feeding in the marine environment and part in the freshwater environment are intermediate between those of animals that fed exclusively in one or the other system. Further, animals that fed at successive trophic levels in the marine and terrestrial environment are separated, on average, by a 3%. difference in the δ15N values of their bone collagen. Specifically, carnivorous and herbivorous terrestrial animals have mean δ15N values for bone collagen of + 8.0 and + 5.3%., respectively. Among marine animals, those that fed on fish have a mean δ15N value for bone collagen of + 16.5%., whereas those that fed on invertebrates have a mean δ15N value of + 13.3%. These results support previous suggestions of a 3%. enrichment in δ15N values at each successively higher trophic level. In contrast to the results for δ15N values, the ranges of bone collagen δ13C values from marine and terrestrial feeders overlap to a great extent. Additionally, bone collagen δ13C values do not reflect the trophic levels at which the animals fed. These results indicate that bone collagen δ15N values will be useful in determining relative dependence on marine and terrestrial food sources and in investigating trophic level relationships among different animal species within an ecosystem. This approach should be applicable to animals represented by prehistoric or fossilized bone in which collagen is preserved.",1984,76,1691,95,0,7,11,9,12,12,10,11,9,10
6eef363c38186ba6a52cc7134446a5d34eff6ae6,,1954,0,2424,26,0,3,7,8,11,11,8,16,15,11
e47f5da58f0276bddd54b9575938e6cbad65a31d,"The medicinal properties of curcumin obtained from Curcuma longa L. cannot be utilised because of poor bioavailability due to its rapid metabolism in the liver and intestinal wall. In this study, the effect of combining piperine, a known inhibitor of hepatic and intestinal glucuronidation, was evaluated on the bioavailability of curcumin in rats and healthy human volunteers. When curcumin was given alone, in the dose 2 g/kg to rats, moderate serum concentrations were achieved over a period of 4 h. Concomitant administration of piperine 20 mg/kg increased the serum concentration of curcumin for a short period of 1-2 h post drug. Time to maximum was significantly increased (P < 0.02) while elimination half life and clearance significantly decreased (P < 0.02), and the bioavailability was increased by 154%. On the other hand in humans after a dose of 2 g curcumin alone, serum levels were either undetectable or very low. Concomitant administration of piperine 20 mg produced much higher concentrations from 0.25 to 1 h post drug (P < 0.01 at 0.25 and 0.5 h; P < 0.001 at 1 h), the increase in bioavailability was 2000%. The study shows that in the dosages used, piperine enhances the serum concentration, extent of absorption and bioavailability of curcumin in both rats and humans with no adverse effects.",1998,19,1471,42,1,3,3,7,3,13,13,19,26,28
3d96555276b099a61d0040004a40188fee05d419,"Increasing concern about the impacts of global warming on biodiversity has stimulated extensive discussion, but methods to translate broad-scale shifts in climate into direct impacts on living animals remain simplistic. A key missing element from models of climatic change impacts on animals is the buffering influence of behavioral thermoregulation. Here, we show how behavioral and mass/energy balance models can be combined with spatial data on climate, topography, and vegetation to predict impacts of increased air temperature on thermoregulating ectotherms such as reptiles and insects (a large portion of global biodiversity). We show that for most “cold-blooded” terrestrial animals, the primary thermal challenge is not to attain high body temperatures (although this is important in temperate environments) but to stay cool (particularly in tropical and desert areas, where ectotherm biodiversity is greatest). The impact of climate warming on thermoregulating ectotherms will depend critically on how changes in vegetation cover alter the availability of shade as well as the animals' capacities to alter their seasonal timing of activity and reproduction. Warmer environments also may increase maintenance energy costs while simultaneously constraining activity time, putting pressure on mass and energy budgets. Energy- and mass-balance models provide a general method to integrate the complexity of these direct interactions between organisms and climate into spatial predictions of the impact of climate change on biodiversity. This methodology allows quantitative organism- and habitat-specific assessments of climate change impacts.",2009,41,795,55,10,33,33,45,73,87,62,68,88,88
9e9af5b393fad67bb98a3e11674f75d3238e2aef,,1969,0,1606,247,2,2,0,1,1,0,0,1,1,1
419e738bba85a8061553b7ae7a691062fba69065,"▪ Abstract The functional causes of life history trade-offs have been a topic of interest to evolutionary biologists for over six decades. Our review of life history trade-offs discusses conceptual issues associated with physiological aspects of trade-offs, and it describes recent advances on this topic. We focus on studies of four model systems: wing polymorphic insects, Drosophila, lizards, and birds. The most significant recent advances have been: (a) incorporation of genetics in physiological studies of trade-offs, (b) integration of investigations of nutrient input with nutrient allocation, (c) development of more sophisticated models of resource acquisition and allocation, (d) a shift to more integrated, multidisciplinary studies of intraspecific trade-offs, and (e) the first detailed investigations of the endocrine regulation of life history trade-offs.",2001,209,1283,57,7,10,22,37,34,38,57,53,49,64
9cc17e349f73c6fe769ec8eb14eeb100857d70c0,"Summary:  Innate immunity constitutes the first line of defense against attempted microbial invasion, and it is a well‐described phenomenon in vertebrates and insects. Recent pioneering work has revealed striking similarities between the molecular organization of animal and plant systems for nonself recognition and anti‐microbial defense. Like animals, plants have acquired the ability to recognize invariant pathogen‐associated molecular patterns (PAMPs) that are characteristic of microbial organisms but which are not found in potential host plants. Such structures, also termed general elicitors of plant defense, are often indispensable for the microbial lifestyle and, upon receptor‐mediated perception, inevitably betray the invader to the plant's surveillance system. Remarkable similarities have been uncovered in the molecular mode of PAMP perception in animals and plants, including the discovery of plant receptors resembling mammalian Toll‐like receptors or cytoplasmic nucleotide‐binding oligomerization domain leucine‐rich repeat proteins. Moreover, molecular building blocks of PAMP‐induced signaling cascades leading to the transcriptional activation of immune response genes are shared among the two kingdoms. In particular, nitric oxide as well as mitogen‐activated protein kinase cascades have been implicated in triggering innate immune responses, part of which is the production of anti‐microbial compounds. In addition to PAMP‐mediated pathogen defense, disease resistance programs are often initiated upon plant‐cultivar‐specific recognition of microbial race‐specific virulence factors, a recognition specificity that is not known from animals.",2004,167,1174,84,8,41,87,110,103,88,92,75,74,70
42bbd01ef11a27598bfca4d491bb39026d2e6b20,"A clear picture of animal relationships is a prerequisite to understand how the morphological and ecological diversity of animals evolved over time. Among others, the placement of the acoelomorph flatworms, Acoela and Nemertodermatida, has fundamental implications for the origin and evolution of various animal organ systems. Their position, however, has been inconsistent in phylogenetic studies using one or several genes. Furthermore, Acoela has been among the least stable taxa in recent animal phylogenomic analyses, which simultaneously examine many genes from many species, while Nemertodermatida has not been sampled in any phylogenomic study. New sequence data are presented here from organisms targeted for their instability or lack of representation in prior analyses, and are analysed in combination with other publicly available data. We also designed new automated explicit methods for identifying and selecting common genes across different species, and developed highly optimized supercomputing tools to reconstruct relationships from gene sequences. The results of the work corroborate several recently established findings about animal relationships and provide new support for the placement of other groups. These new data and methods strongly uphold previous suggestions that Acoelomorpha is sister clade to all other bilaterian animals, find diminishing evidence for the placement of the enigmatic Xenoturbella within Deuterostomia, and place Cycliophora with Entoprocta and Ectoprocta. The work highlights the implications that these arrangements have for metazoan evolution and permits a clearer picture of ancestral morphologies and life histories in the deep past.",2009,74,682,65,4,62,68,67,70,70,86,71,56,40
eaf811e6e91c687c4026d7f0f096c3409af089d5,"Recent advances in integrative studies of locomotion have revealed several general principles. Energy storage and exchange mechanisms discovered in walking and running bipeds apply to multilegged locomotion and even to flying and swimming. Nonpropulsive lateral forces can be sizable, but they may benefit stability, maneuverability, or other criteria that become apparent in natural environments. Locomotor control systems combine rapid mechanical preflexes with multimodal sensory feedback and feedforward commands. Muscles have a surprising variety of functions in locomotion, serving as motors, brakes, springs, and struts. Integrative approaches reveal not only how each component within a locomotor system operates but how they function as a collective whole.",2000,197,1294,37,6,27,23,29,31,39,52,56,45,66
d31dfbb2f9a210e41ade9112fd3ba75aed3227e2,"I . Introduction, 365 2. Definition, 365 3. The gut microflora and its contribution to resistance, 366 4. Causes of induced changes in gut flora, 366 5. Composition of probiotics, 367 6. Mode of action of probiotics, 368 7. Practical results with probiotics. 370 7.1. Growth promotion of farm animals. 370 7.2. Effects on intestinal infections, 371 7.3. Alleviation of lactose intolerance, 372 7.4. Relief of constipation. 372 7.5. Antitumour activities, 372 7.6. Anticholesterolaemic effects, 372 8. Characteristics of a good probiotic, 373 9. Future developments, 374 10. Summary, 374 11. References, 374",2008,77,1000,62,36,44,46,59,64,84,71,87,76,88
4f76d400566e256852da82a2935e7483a053aff0,"IntroductionPleasure and reward are generated by brain circuits that are largely shared between humans and other animals.DiscussionHere, we survey some fundamental topics regarding pleasure mechanisms and explicitly compare humans and animals.ConclusionTopics surveyed include liking, wanting, and learning components of reward; brain coding versus brain causing of reward; subjective pleasure versus objective hedonic reactions; roles of orbitofrontal cortex and related cortex regions; subcortical hedonic hotspots for pleasure generation; reappraisals of dopamine and pleasure-electrode controversies; and the relation of pleasure to happiness.",2008,268,995,46,9,32,63,69,68,74,79,88,87,70
fc7020ed1de0034afb37298a6cafb99490dd26fc,,1989,0,1592,110,1,19,15,22,18,16,17,26,30,37
cfb2e9e2841c3c19b3938f4a69b75fa1edd4fb5d,"The worldwide best-selling intermediate microeconomics textbook is distinguished by its remarkably up-to-date and rigorous yet accessible analytical approach. The seventh edition has been carefully updated and revised, adding a wealth of new applications and examples that analyse the important lessons offered by eBay, drug companies, the Yellow Pages and even Maine Lobstermen.",1987,0,2028,93,0,1,1,1,7,4,6,7,5,7
8eccb702db8fa03b35ad999c168ba41037723172,"This paper reviews some of the most popular policy evaluation methods in empirical microeconomics: social experiments, natural experiments, matching, instrumental variables, discontinuity design, and control functions. It discusses identification of traditionally used average parameters and more complex distributional parameters. The adequacy, assumptions, and data requirements of each approach are discussed drawing on empirical evidence from the education and employment policy evaluation literature. A workhorse simulation model of education returns is used throughout the paper to discuss and illustrate each approach. The full set of STATA datasets and do-files are available free online and can be used to reproduce all estimation and simulation results.",2009,134,1059,66,44,78,82,84,88,91,105,87,75,86
7bb70a28be826f93541e1f1fca4e8aee52a5dce6,"Twenty years ago, most banking courses focused on either management or monetary aspects of banking, with no connecting. Since then, a microeconomic theory of banking has developed, mainly through a switch of emphasis from the modeling of risk to the modeling of imperfect information. This asymmetric information model is based on the assumption that different economic agents possess different pieces of information on relevant economic variables, and that they will use the information for their own profit. The model has been extremely useful in explaining the role of banks in the economy. It has also been useful in pointing out structural weaknesses of the banking sector that may justify government intervention--for example, exposure to runs and panics, the persistence of rationing in the credit market, and solvency problems. Microeconomics of Banking provides a guide to the new theory. Topics include why financial intermediaries exist, the industrial organization approach to banking, optimal contracting between lenders and borrowers, the equilibrium of the credit market, macroeconomic consequences of financial imperfections, individual bank runs and systemic risk, risk management inside the banking firm, and bank regulation. Each chapter ends with a detailed problem set and solutions.",1997,166,1191,15,0,13,22,37,44,52,90,69,55,48
e7d688587abe1c7cde5be5bd52b16e81860d69cb,"The article reviews the book ""The Microeconomics of Banking,"" 2nd edn, by Xavier Freixas and Jean-Charles Rochet.",2009,6,555,36,48,42,31,40,41,36,52,36,34,39
2149d1f44b534a62f7f11ac6166ed485e1a9d8a7,"Preface ix Prologue: Economics and the Wealth of Nations and People 1 Part I: Coordination and Conflict: Generic Social Interactions 21 Chapter One: Social Interactions and Institutional Design 23 Chapter Two: Spontaneous Order: The Self-organization of Economic Life 56 Chapter Three: Preferences and Behavior 93 Chapter Four: Coordination Failures and Institutional Responses 127 Chapter Five: Dividing the Gains to Cooperation: Bargaining and Rent Seeking 167 Part II : Competition and Cooperation: The Institutions of Capitalism 203 Chapter Six: Utopian Capitalism: Decentralized Coordination 205 Chapter Seven: Exchange: Contracts, Norms, and Power 233 Chapter Eight: Employment, Unemployment, and Wages 267 Chapter Nine: Credit Markets, Wealth Constraints, and Allocative Inefficiency 299 Chapter Ten: The Institutions of a Capitalist Economy 331 Part III: Change: The Coevolution of Institutions and Preferences 363 Chapter Eleven: Institutional and Individual Evolution 365 Chapter Twelve: Chance, Collective Action, and Institutional Innovation 402 Chapter Thirteen: The Coevolution of Institutions and Preferences 437 Part IV: Conclusion 471 Chapter Fourteen: Economic Governance: Markets, States, and Communities 473 Problem Sets 502 Additional Readings 529 Works Cited 537 Index 571",2003,1,953,48,5,9,32,53,64,59,58,62,60,74
0ee9179b57c6f9f4a8ae0ad70c5f507df7d731f1,"'Personalized medicine' promises to increase the quality of clinical care and, in some cases, decrease health-care costs. Despite this, only a handful of diagnostic tests have made it to market, with mixed success. Historically, the challenges in this field were scientific. However, as discussed in this article, with the maturation of the '-omics' sciences, it now seems that the major barriers are increasingly related to economics. Overcoming the poor microeconomic alignment of incentives among key stakeholders is therefore crucial to catalysing the further development and adoption of personalized medicine, and we propose several actions that could help achieve this goal.",2009,5,170,4,4,17,23,22,20,22,15,12,9,9
5de57633474d065cabde23e6b1def5bb3a6a3d37,"This paper reviews some of the most popular policy evaluation methods in empirical microeconomics: social experiments, natural experiments, matching, instrumental variables, discontinuity design, and control functions. It discusses identification of traditionally used average parameters and more complex distributional parameters. The adequacy, assumptions, and data requirements of each approach are discussed, drawing on empirical evidence from the education and employment policy evaluation literature. A workhorse simulation model of education and earnings is used throughout the paper to discuss and illustrate each approach. The full set of STATA data sets and do-files are available free online and can be used to reproduce all estimation results.",2009,167,60,1,4,11,10,6,2,4,0,4,0,5
def040a2e825a1ecd635e5dc195c1e8e5e465e64,"How do students enrolled in online courses perform relative to those who choose a more traditional classroom environment? What student characteristics help explain differences in student academic achievement in the two modes of instruction? What factors affect the students' choice of instruction mode? The authors address these questions in relation to the teaching of introductory economics courses. They find that the two groups of students are significantly different in age, gender composition, marital status and number of children, GPA, previous economics exposure, planned major, and other important characteristics. The raw data suggested a higher mean score for the online class sections. But after considering course selection bias, the findings indicated that age and GPA positively affect students' performance in the course, whereas the online teaching mode has a narrowly insignificant, or even negative, effect. Semester effects are most important for the online subsample, and male students enjoy a premium in the traditional classroom setting.",2009,42,107,7,1,4,7,15,11,7,16,7,11,8
1644ff8ea98e753133f713c9ae39ccaf00d1e68c,Introduction Part I. The Theory of the Firm: 1. The consumer 2. The firm 3. Separation of consumer objectives and firm objectives Part II. The Entrepreneur in Equilibrium: 4. The entrepreneur 5. Competition between entrepreneurs Part III. Human Capital and Financial Capital: 6. Human capital and the organization of the firm 7. Financial capital and the organization of the firm Part IV. Intermediation by the Firm: 8. The firm as intermediary in the pure exchange economy 9. The firm versus free riding Part V. Market Making by the Firm: 10. The firm creates markets 11. The firm in the market for contracts 12. Conclusion.,2009,1,110,4,0,6,5,12,9,14,19,6,7,14
6bbc9c84d4671d15e32ded5e98cc2d69329a7afc,"Consider a group consisting of S members facing a common budget constraint p'xi=1: any demand vector belonging to the budget set can be (privately or publicly) consumed by the members. Although the intragroup decision process is not known, it is assumed to generate Pareto-efficient outcomes; neither individual consumptions nor intragroup transfers are observable. The paper analyzes when, to what extent, and under which conditions it is possible to recover the underlying structure-individual preferences and the decision process-from the group's aggregate behavior. We show that the general version of the model is not identified. However, a simple exclusion assumption (whereby each member does not consume at least one good) is sufficient to guarantee generic identifiability of the welfare-relevant structural concepts. Copyright 2009 The Econometric Society.",2009,40,118,5,6,10,13,13,4,8,8,9,11,8
5eb7a931be5d30bb3207233a6b6fc1e5d8d35dfd,,2009,0,78,9,3,6,7,9,3,5,8,12,11,9
32ef38c321142ea1975e5a2c59acd0bb4b516a6b,"We develop a framework based on microeconomic theory from which the ideal gas like market models can be addressed. A kinetic exchange model based on that framework is proposed and its distributional features have been studied by considering its moments. Next, we derive the moments of the CC model (Eur. Phys. J. B 17 (2000) 167) as well. Some precise solutions are obtained which conform with the solutions obtained earlier. Finally, an output market is introduced with global price determination in the model with some necessary modifications.",2009,19,58,5,1,4,7,8,7,4,2,4,9,3
a8e598a00d6c91fe41e8db9dd40c19171933f8be,"Over the last thirty years, a new paradigm in banking theory has overturned economists' traditional vision of the banking sector. The asymmetric information model, extremely powerful in many areas of economic theory, has proven useful in banking theory both for explaining the role of banks in the economy and for pointing out structural weaknesses in the banking sector that may justify government intervention. In the past, banking courses in most doctoral programs in economics, business, or finance focused either on management or monetary issues and their macroeconomic consequences; a microeconomic theory of banking did not exist because the Arrow-Debreu general equilibrium model of complete contingent markets (the standard reference at the time) was unable to explain the role of banks in the economy. This text provides students with a guide to the microeconomic theory of banking that has emerged since then, examining the main issues and offering the necessary tools for understanding how they have been modeled. This second edition covers the recent dramatic developments in academic research on the microeconomics of banking, with a focus on four important topics: the theory of two-sided markets and its implications for the payment card industry; ""non-price competition"" and its effect on the competition-stability tradeoff and the entry of new banks; the transmission of monetary policy and the effect on the functioning of the credit market of capital requirements for banks; and the theoretical foundations of banking regulation, which have been clarified, although recent developments in risk modeling have not yet led to a significant parallel development of economic modeling.",2008,0,228,12,2,10,25,21,23,47,35,16,15,19
a079086b15e10e3fe1b3b1aaadec897590b5cc6c,"In this paper we focus on education as a private decision to invest in ""human capital"" and the estimation of the rate of return to that private investment. While the literature is replete with studies that estimate the rate of return using regression methods where the estimated return is obtained as the coefficient on a years of education variable in a log wage equation that contains controls for work experience and other individual characteristics, the issue is surrounded with difficulties. We outline the theoretical arguments underpinning the empirical developments and show that the evidence on private returns to the individual is compelling. Despite some of these issues surrounding the estimation of the return to schooling, our evidence, based on estimates from a variety of datasets and specifications, is that there is an unambiguously positive effect on the earnings of an individual from participation in education. Moreover, the size of the effect seems large relative to the returns on other investments. Copyright Blackwell Publishing Ltd, 2003.",2003,81,541,29,5,16,20,30,36,22,26,38,27,31
703954375c1c66c27297540214cfc855f7af2788,"Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making.",2004,78,493,20,7,39,43,48,40,49,24,44,25,30
f51e9221eceeee0b77c9692a6951d45dfcc1b3df,"Abstract Introductory Microeconomics as offered by the University of South Africa (Unisa) is a compulsory module for a Bachelor of Commerce, a Bachelor of Accountancy or a Bachelor of Administration degree. Success or failure in Introductory Microeconomics directly impacts on the number of years students take to complete their degrees, and eventually also on the throughput subsidy to Unisa. A number of exceptional institutional rules and regulations impact on the teaching of Introductory Microeconomics at Unisa, as an open and distance learning (ODL) institution. Unlike many residential institutions, Unisa does not require Mathematics at school level for registration for Introductory Microeconomics. This article reports on research done at Unisa to determine how student success in Introductory Microeconomics is influenced by variables such as race, home language, whether the students passed mathematics at matriculation level, matriculation exemption1, gender and the passing of assignments. Although this research confirms previous research that home language and age do impact on student success, it finds that the successful passing of assignments has the greatest impact on student success. 1. Matriculation (“matric”): In South Africa, the final school exit certificate, which is awarded with or without university exemption (“endorsement”).",2009,22,13,1,0,2,0,2,0,1,0,1,5,0
63b8967bb550e9e57f6e644aba73fb5c5f5fd2b4,"Purpose – The past decade has witnessed a trend toward unbundling of enterprises that were once highly integrated in vertical forms or horizontally as conglomerates. The economic forces behind these changes simultaneously enabled collaborative relationships that replaced command‐control coordination. While such change has been widespread, the food industry serves as an example of an industry where such strategies have been incompletely pursued. This paper aims to provide a microeconomic explanation of three bases for the emergence of collaboration and network formation: transaction costs, interdependence in value creation processes, and shared resources.Design/methodology/approach – Within a setting of the food industry, a microeconomic theory of firm level choice of transactions is presented and extended to consider market level equilibrium in where persistent relationships are defined to compose an integrated economic network.Findings – The paper presents a framework for identification of the optimal pa...",2009,25,18,0,1,6,1,4,1,1,2,1,0,0
d10b699b81a62170458734ee252de720ea7c9d0b,"Section 1: INTRODUCTION. 1. Economics and Institutions: A Shift of Emphasis. Section 2: PREFERENCES, UTILITIES, DEMAND, AND UNCERTAINTY. 2. Consumers and Their Preferences. 3. Utilities--Indifference Curves. 4. Demand and Behavior in Markets. 5. Some Applications of Consumer, Demand, and Welfare Analysis. 6. Uncertainty and the Emergence of Insurance. 7. Uncertainty--Applications and Criticisms. Section 3: PRODUCTION AND COST. 8. The Discovery of Production and Its Technology. 9. Cost and Choice. 10. Cost Curves. Section 4: DECISIONS AND GAMES. 11. Game Theory and the Tools of Strategic Business Analysis. 12. Decision Making Over Time. 13. The Internal Organization of the Firm. Section 5: MARKETS. 14. Perfectly Competitive Markets: Short Run Analysis. 15. Competitive Markets in the Long Run. 16. Market Institutions and Auctions. 17. The Age of Entrepreneurship: Monopoly. 18. Natural Monopoly and the Economics of Regulation. 19. The World of Oligopoly: Preliminaries to Successful Entry. 20. Market Entry and the Emergency of Perfect Competition. Section 6: EXCHANGE AND GENERAL EQUILIBRIUM. 21. The Problem of Exchange. 22. General Equilibrium and the Origins of the Free-Market and Interventionist Ideologies. Section 7: BREAKDOWNS AND MARKET FAILURE. 23. Moral Hazard and Adverse Selection: Informational Market Failures. 24. Externalities: The Free Market--Interventionist Battle Continues. Section 8: INPUT MARKETS AND THE ORIGINS OF CLASS STRUGGLE. 25. Public Goods, the Consequences of Strategic Voting Behavior, and the Role of Government. 26. Input Markets and the Origins of Class Conflict.",2008,0,115,5,3,9,5,11,7,10,6,6,7,5
baa3fbdd1c56e68c1c9ea52dae7a36bd30d97d47,"Preface. I. AN INTRODUCTION TO MICROECONOMICS. 1. Microeconomics: A Working Methodology. Appendix A1: Model Building. II. INDIVIDUAL CHOICE. 2. A Theory of Preferences. 3. Demand Theory. Appendix 3A: Composite Commodities. 4. More Demand Theory. 5. Intertemporal Decision Making and Capital Values. III. PRODUCTION AND COST. 6. Production and Cost: One Variable Input. 7. Production and Cost: Many Variable Inputs. IV. MARKETS FOR GOODS. 8. The Theory of Perfect Competition. 9. Applications of the Competitive Model. 10. Monopoly. V. RESOURCE MARKETS AND GENERAL EQUILIBRIUM. 11. Input Markets and the Allocation of Resources. 12. The Distribution of Income. 13. Competitive General Equilibrium. Appendix 13A: Efficiency. VI. IMPERFECT COMPETITION. 14. Price Discrimination and Monopoly Practices. 15. Game Theory 16. Oligopoly. VII. UNCERTAINTY AND ASYMMETRIC INFORMATION. 17. Choice Making Under Uncertainty. 18. Asymmetric Information, the Rules of the Game, and Externalities. 19. The Theory of the Firm. 20. Asymmetric Information and Market Behaviour. Answers to Problems. Glossary. Index.",2009,0,8,0,0,0,2,1,0,1,0,0,1,2
191b7c32f89ab82d3b7011f7d724e0ec911874b0,,2008,14,66,4,0,4,2,4,5,8,7,4,9,7
fe156eff70ae341eba64589a1d9311c7b9debd53,"In this relatively short survey, we present the core elements of the microeconomic analysis of insurance markets at a level suitable for senior undergraduate and graduate economics students. The aim of this analysis is to understand how insurance markets work, what their fundamental economic functions are, and how efficiently they may be expected to carry these out.",2008,110,59,2,1,1,8,3,6,6,5,5,6,3
1ac2f395f072d5fc4a23ff89637c77cab7d4df36,"The proliferation of economics courses offered partly or completely online (Arnold Katz and William E. Becker, 1999) raises important questions about the effects of the new technologies on student learning. Do students enrolled in online courses learn more or less than students taught face-to-face? Can we identify any student characteristics, such as gender, race, ACT scores, or grade averages, that are associated with better outcomes in one technology or another? How would the online (or face-to-face) students fare if they had taken the course using the alternative technology? This paper addresses these questions using student data from our Principles of Microeconomics courses at Michigan State University.",2002,4,372,27,4,7,13,15,18,19,24,21,26,25
8939fd3f868aa844bc99d37e03cf50a8690f7327,"Economic geography during an era of global competition involves a paradox. It is widely recognized that changes in technology and competition have diminished many of the traditional roles of location. Yet clusters, or geographic concentrations of interconnected companies, are a striking feature of virtually every national, regional, state, and even metropolitan economy, especially in more advanced nations. The prevalence of clusters reveals important insights about the microeconomics of competition and the role of location in competitive advantage. Even as old reasons for clustering have diminished in importance with globalization, new influences of clusters on competition have taken on growing importance in an increasingly complex, knowledge-based, and dynamic economy. Clusters represent a new way of thinking about national, state, and local economies, and they necessitate new roles for companies, government, and other institutions in enhancing competitiveness.",2000,38,4058,281,10,19,39,69,81,84,89,136,166,184
d02d2eb57454778faba3c4d7eebaa95592aade84,"This book brings together in one place the work of one of our most respected economic theorists, on a field which he has played a large part in originating: the New Institutional Economics. Transaction cost economics, which studies the governance of contractual relations, is the branch of the New Institutional Economics with which Oliver Williamson is especially associated. Transaction cost economics takes issue with one of the fundamental building blocks in microeconomics: the theory of the firm. Whereas orthodox economics describes the firm in technological terms, as a production function, transaction cost economics describes the firm in organizational terms, as a governance structure. Alternative feasible forms of organization--firms, markets, hybrids, bureaus--are examined comparatively. The analytical action resides in the details of transactions and the mechanisms of governance. Transaction cost economics has had a pervasive influence on current economic thought about how and why institutions function as they do, and it has become a practical framework for research in organizations by representatives of a variety of disciplines. Through a transaction cost analysis, The Mechanisms of Governance shows how and why simple contracts give way to complex contracts and internal organization as the hazards of contracting build up. That complicates the study of economic organization, but a richer and more relevant theory of organization is the result. Many testable implications and lessons for public policy accrue to this framework. Applications of both kinds are numerous and growing. Written by one of the leading economic theorists of our time, The Mechanisms of Governance is sure to be an important work for years to come. It will be of interest to scholars and students of economics, organization, management, and law.",1997,52,4612,368,27,54,92,93,120,158,202,180,223,219
eb4b8a5afb3ea7022fdae1c4e553fe19654fb0af,"Microeconomics develops core microeconomic principles to a high level using a clear and carefully constructed learning framework. The book will give readers a solid foundation in microeconomic analysis, using mathematical techniques where appropriate, and will enable them to apply these analytical techniques to a range of economic problems. It compounds the student's understanding of principles and techniques by re-using them throughout the text after each has been covered. The book is designed to assist the student's learning in every way possible and contains comprehensive sets of problems and exercises at all stages. ONLINE RESOURCE CENTRE For lecturers: worked solutions to selected exercises in the book, figures from the book, PowerPoint presentations, solutions manual and figures to accompany the solutions manual",2007,177,49,2,1,1,1,1,7,8,6,4,5,5
648efe361ab23991f315dd95b5387514569baeb2,,2008,0,23,2,4,3,1,1,0,3,1,1,2,1
caaaf486a8f7c2618627f206e3ea8e6651cad068,"The 2008 crash has left all the established economic doctrines - equilibrium models, real business cycles, disequilibria models - in disarray. Part of the problem is due to Smith’s ""veil of ignorance"": individuals unknowingly pursue society’s interest and, as a result, have no clue as to the macroeconomic effects of their actions: witness the Keynes and Leontief multipliers, the concept of value added, fiat money, Engel’s law and technical progress, to name but a few of the macrofoundations of microeconomics. A good viewpoint to take bearings anew lies in comparing the post-Great Depression institutions with those emerging from Thatcher and Reagan’s economic policies: deregulation, exogenous vs. endoge- nous money, shadow banking vs. Volcker’s Rule. Very simply, the banks, whose lending determined deposits after Roosevelt, and were a public service became private enterprises whose deposits determine lending. These underlay the great moderation preceding 2006, and the subsequent crash.",1986,1,3059,422,0,1,0,0,0,0,0,2,0,0
4e77fd7c955f8fd70d9e1c6cb670fa1e38ad7157,"Part 1 Rise of science-related technology: introduction evolutionary theory in economics and technical change process innovations materials innovations product and system innovation paradigm change. Part 2 Innovations and the firms: the microeconomics of innovation success and failure, the role of marketing and user-producer networks innovation, size of firm, economies of scale and scope uncertainty, project evaluation and finance of innovation, management strategy and theory of the firm. Part 3 Macroeconomics of innovation - science, technology and economic growth globalization and multinational corporations underdevelopment and catching up. Part 4 Innovation and public policies: market failure and aspects of public support for innovation technical change, employment and skills environmental issues technological assessment. Appendix: measurement and definitions.",1975,0,4611,103,1,2,5,7,4,18,10,13,4,21
d205b8690cc46bcb930a51cb64cffa0ecead7bc0,"With the beginning of the new millennium it has become more and more apparent that education and human capital constitute a key element of modern economies. Despite the important role of human capital in modern societies, there are still many unknowns about the process of educational production as well as individual and collective decisions concerning how much and what kind of education to obtain. This literature review aims at providing a better understanding of the process of human capital formation and educational attainment. Although human capital plays an important role in both microeconomics and macroeconomics, we focus on the former branch of literature in order to analyze the individual incentives to acquire skills. This review is divided into six parts each of them representing an important stream of human capital literature. First, we introduce the basic concept of human capital that models individuals as investing in skills in response to the expected returns to education. After this, we investigate the different implications of investments in general and specific human capital and then provide an overview of various empirical studies measuring the rate of return to education. Because educational attainment may also be affected by other factors such as school characteristics or family background, we review the literature on educational production functions and discuss the significance of potential inputs into the process of educational production. Subsequently, we refer to models of human capital accumulation over the life-cycle that manage to replicate the empirical life-cycle patterns with respect to the age-earnings profile of individuals. Finally, we analyze the effects of taxation and education subsidies on the formation of human capital.",2007,171,37,2,0,0,0,1,3,2,3,6,5,4
315a7b0d6c6080fbbf179f6dccc99a6af2447180,"When should government intervene in market activity and when is it best to let market forces take their natural course? How does the existing empirical evidence about government performance guide our answers to these questions? In this clear, concise book, Clifford Winston offers his innovative analysis --shaped by thirty years of evidence --to assess the efficacy of government interventions. Markets fail when it is possible to make one person better off without making someone else worse off, thus indicating inefficiency. Governments fail when an intervention is unwarranted because markets are performing well or when the intervention fails to correct a market problem efficiently. Winston concludes from existing research that the cost of government failure may actually be considerably greater than the cost of market failure: ""My search of the evidence is not limited to policy failures. I will report success stories, but few of them emerged from my search."" The prevalence of market failure is due to a lack of conviction in favor of markets, the inflexibility of intervening government agencies, and political forces that enable certain interest groups to benefit at the expense of society as a whole. Winston suggests that government policy can be improved by making greater use of market-oriented solutions that have already produced benefits in certain situations.",2007,118,120,10,1,14,4,6,10,13,8,10,14,7
464c48f2dda4942e8f998707c372aba90db30033,"1. Introduction 2. Supply and Demand 3. A Consumer's Constrained Choice 4. Demand 5. Consumer Welfare and Policy Analysis 6. Firms and Production 7. Costs 8. Competitive Firms and Markets 9. Properties and Applications of the Competitive Model 10. General Equilibrium and Economic Welfare 11. Monopoly 12. Pricing and Advertising 13. Oligopoly and Monopolistic Competition 14. Game Theory 15. Factor Markets 16. Uncertainty 17. Externalities, Open Access, and Public Goods 18. Asymmetric Information 19. Contracts and Moral Hazard",2007,0,114,13,0,3,4,6,11,13,8,15,7,10
b5f18ffb038093c199cf2ccc7ae542bb00fa53ab,"Tropical forests may contribute to the well-being of local people by providing a form of “natural insurance.” We draw on microeconomic theory to conceptualize a model relating agricultural risks to collection of non-timber forest products. Forest collection trips are positively correlated with both agricultural shocks and expected agricultural risks in an event-count model of survey data from the Brazilian Amazon. This suggests that households rely on forests to mitigate agricultural risk. Forest product collection may be less important to households with other consumption-smoothing options, but its importance is not restricted to the poorest households. (JEL Q23)",2001,65,355,15,0,1,7,16,23,13,8,10,17,32
deafb4a1abac1d762cf7e6d18d979d3271852bfd,"The resource-based view can be positioned relative to at least three theoretical traditions: SCP-based theories of industry determinants of firm performance, neo-classical microeconomics, and evolutionary economics. In the 1991 article, only the first of these ways of positioning the resourcebased view is explored. This article briefly discusses some of the implications of positioning the resource-based view relative to these other two literatures; it also discusses some of the empirical implications of each of these different resource-based theories.",2001,31,2473,176,2,10,28,39,36,56,63,85,94,109
20ce660fa8ef6f027c17a2d0fda0b92e4c5bfa75,"Abstract: Interest in using classroom experiments to teach economics is increasing whereas empirical evidence on how experiments affect learning is limited and mixed. The author used a pretest-posttest control-group design to test whether classroom experiments and grade incentives that reward performance in experiments affect learning of introductory microeconomics. The author measured the partial effects of experiments independently of instructor quality and teaching methods using Test of Understanding in College Economics scores. Experiments without incentives are associated with higher posttest scores and greater improvement over pretest scores, but grade incentives may offset benefits of experiments. Controlling for student aptitude and other characteristics, limiting influence of potential outliers, or adjusting for potential selection bias from incomplete observation of test scores does not alter the conclusion that experiments increase learning whereas grade incentives do not.",2006,37,149,12,1,5,5,6,9,18,9,8,11,17
aace52e1ecc601b14b117e1a17fd081c1d468df0,,2007,0,45,2,1,4,2,5,0,2,2,2,4,2
b9f3670d36d851e3d3083c70cdd49f0b18570850,"In this article it is claimed that in order to obtain the right composition, structure, and functionality for a new product, one needs to anticipate what the demand of the product will be and take into account all costs involved (associated manufacturing and supply chain). To obtain the demand, one needs a pricing model, which in turn relies on consumer preferences that are connected to the product composition, structure, and functionality. Thus, a model, including the varying characteristics of the product, the manufacturing capacity and site, the supply chain and ultimately, the markets, is proposed. I use a very simple case of insect repellent to illustrate how the best repellent, identified as the one that consumers will prefer the most, is not the most profitable one and how one can obtain the insect repellent composition that maximizes profit. © 2007 American Institute of Chemical Engineers AIChE J, 2007",2007,69,56,1,0,5,4,3,2,4,5,4,2,5
3aeeca365fa1f59fa7a491fdd1f10eadfa535eca,"Microeconomics' notions of “market supply” and “market demand” do not exist in real-world markets. Its models give a central place to equilibria, implying that they are predictions. It distracts from more essential aspects of economic behavior and exchange and encourages inventing absurd tales, especially concerning production. We should consider society as it is organized, with different social groups, norms, and customs, and then concentrate on decision making and choice.",2008,10,11,0,0,0,1,0,1,1,1,2,0,2
ad7fa90b2b1994f4bd7ced630638656fdf95fbab,"This paper uses OLS regression analysis to examine the effect of student characteristics on performance in Introductory Microeconomics at five South African universities. No consistent race-effects were found, but Indian students performed significantly worse than Whites at historically-White universities. Male students outperformed females in general. Older students did better at the historically-White institutions only. At one university, Black students who speak English as their home language outperformed those who are non-English speakers. Students who devoted more time to study outside formal classes did better in general. Greater verbal and mathematical ability had large and significant positive effects on student achievement. Copyright (c) 2006 The Author. Journal compilation (c) 2006 Economic Society of South Africa.",2006,36,65,11,1,2,2,6,6,7,10,2,5,5
2ac66da70aa7b99760fc221f018dff28d6ba3952,"Trying to achieve higher usage efficiency for spectrum has been on the research agenda for some time now. More efficient transmission technologies are being developed, but they alone will not solve the problem of spatially and temporally underused spectrum and radio resources. Mechanisms to optimize spectrum access over space and time are required. This paper describes schemes, based on multiple agents that collaborate to find more efficient allocation patterns in a defined coverage area. Leveraging on microeconomics inspired mechanisms, the paper describes and analyses schemes based on collaborating 'agents' (hat can be either whole operator, or BS or end user terminal) that negotiate with each other to find the most optimized allocation pattern for a given area and allocation duration. The optimization strategies investigated include both bargaining as well as auction based mechanisms. Both allow the negotiation of spectrum and radio resources, based on market driven incentives. The auction types investigated support dynamic allocations on different timescales, ranging from short to medium and long term allocation scenarios. While auctions are discussed to be used for the longer term allocations, a MAC based rental protocol is evaluated for shorter term allocations when operated either at the BS or end user terminal level. Finally, the paper discusses how the MAC based rental protocol between BSs can be implemented.",2007,24,25,0,0,3,4,5,1,3,4,2,1,2
3ce5e7926aa01087d16937752f5e68d829e64333,"The Econometrics of Panel DataSpringer Handbook of Science and Technology IndicatorsPanel Data EconometricsThe Econometrics of Panel DataA Practitioner's Guide to Stochastic Frontier Analysis Using StataBenchmarking for Performance EvaluationEssays on Microeconomics and Industrial OrganisationHealth System EfficiencyInternational Journal of Production EconomicsEconometric Analysis of Model Selection and Model TestingInternational Applications of Productivity and Efficiency AnalysisAdvanced Robust and Nonparametric Methods in Efficiency AnalysisEconometrics and the Philosophy of EconomicsThe Measurement of Productive EfficiencyMeasuring Efficiency in Health CareFinancial, Macro and Micro Econometrics Using REconometric Analysis of Cross Section and Panel DataApplied EconometricsProductivity and Efficiency AnalysisEconometric Model SelectionProductivity and Efficiency AnalysisStochastic Frontier AnalysisThe Oxford Handbook of Health EconomicsThe Measurement of Productive Efficiency and Productivity GrowthNew Directions in Productivity Measurement and Efficiency AnalysisA Primer on Efficiency Measurement for Utilities and Transport RegulatorsPanel Data EconometricsProduction and Efficiency Analysis with RApplications of Modern Production TheoryThe Measurement of Productive EfficiencyNonparametric Econometric Methods and ApplicationAn Introduction to Efficiency and Productivity AnalysisHealth, the Medical Profession, and RegulationThe Analysis of Household SurveysData Envelopment AnalysisProgramming Collective IntelligenceEfficiency AnalysisProductivity and Efficiency AnalysisMeasurement of Productivity and EfficiencyProduction Frontiers",2008,241,1097,88,34,55,56,65,77,62,84,91,90,108
b8074c1ebfffe3fc1ef166497a28846f63d993e7,"This volume presents a collection of studies on the dynamics of income inequality based on micro data. Using a simple but powerful empirical methodology, the authors analyze the roles of prices, occupational choice, and educational choice in accounting for household income and its contribution to inequality. It casts doubt on the grand theories of growth and income inequality that have dominated discussions in development economics. It paves the way for a full-blown, micro-based general equilibrium theory of income determination and income inequality.",2004,118,270,10,4,11,18,14,17,25,18,21,20,14
8217443b8b4c075e60b322e39e4cd77b010c6c09,This volume proposes evolutionary microeconomics as a synthesis of the collective schools of heterodox economic thought with complex systems theory and graph theory. The text charts a research programme for evolutionary economics that encompasses various theories.,2001,0,323,15,5,6,26,26,19,22,25,20,19,21
7c646037d51fc101aba12e74c02cc1cded26c0d1,,1997,0,411,35,0,4,5,8,14,24,24,24,17,18
3c80d6d5eb6fc6ce5203900d0efff719fce95f0d,Microeconomic principles courses focus on perfectly competitive markets far more than other market structures. The authors examine five possible reasons for this but find none of them sufficiently compelling. They conclude that textbook authors should place more emphasis on how economists select appropriate models and test models' predictions against the empirical evidence. This requires a more even treatment of market structures and less emphasis on perfect competition. Greater emphasis on imperfect competition would also allow textbook authors to address the neglected topic of dynamic efficiency.,2007,41,29,1,2,0,3,3,5,0,2,2,2,2
af69f1afbdceae76ba2dc5ccf828fbbcd32ec2ad,Contents: Preface Preface to Walrasian Microeconomics 1. Introduction 2. The Theory of Demand: Utility Maximization 3. Topics in Demand Theory 4. Production and Cost 5. Models of the Firm 6. Markets in Isolation 7. Interacting Markets 8. The Fixed-Factor-Supply Economy 9. Dynamics and Equilibrium 10. Methodological Individualism and the Theory of Price Determination 11. Imperfect Competition 12. Economic Welfare 13. Capital 14. The Grand View 15. Some Alternative Assumptions and Methods of Analysis Appendices Index,2006,0,29,1,0,1,3,0,2,1,2,4,4,5
03c66b9affed9acdb378ea9bf675f4ee0dcf7769,"Despite considerable research on customer retention and word-of-mouth referrals, it has always been difficult quantifying their contributions to the bottom line. Using a metric known as ?net promoter score,? the author believes firms can now measure the dollar value of customers based on satisfaction levels.

The author administered a survey designed to assess customer relationships to thousands of customers in six industries. He determined that customers tend to cluster into one of three categories: promoters, passives and detractors. Promoters represent more than 80% of the positive referrals a company receives, while detractors represent more than 80% of the negative word-of-mouth. NPS is determined by subtracting the percentage of detractors from the percentage of promoters. Using this data, a firm can quantify the value of a customer by tracking five categories: retention rate, profit margins, spending, cost efficiencies and word-of-mouth.

The firm can then use NPS to make strategic decisions by targeting its efforts to leverage the most value for its customer service dollar. For instance, American Express targets its promoters with premium credit cards in an effort to increase profitability, and GE sends cross-functional teams to its detractors in order to prevent the spread of negative word-of-mouth.",2006,3,104,6,0,6,11,6,8,9,7,7,6,5
a8f495264b07f8d7f701ca659c5c4a643d78bf5c,"from them. The Adam Smith Address: Location, Clusters, and the New Microeconomics",1998,8,316,28,0,1,6,6,9,9,8,14,11,6
c5d706479b8b5f88bc4dfb884ae781fbfe73df85,"Previous studies have documented a gender gap in the study of economics in Canada, the UK, and the US. One important factor may be women's low expectations about their ability to succeed in economics courses. Women in our sample expect to do less well than men in an introductory microeconomics course, even after controlling for variables relating to family background, academic experience, and mathematics experience. These expectations are partly self-fulfilling, since expected grades have an important and positive effect on class performance. We also find that having taken an economics course in secondary school actually has a negative effect on performance. We observe this negative effect for women and men, but it is more pronounced for women. When we control for both expectations and secondary-school experience with economics, the independent effect of gender is small and insignificant.",2005,54,65,9,0,1,3,8,2,4,7,4,6,4
acdb9071cb24bce944909b4334ac82ccf16e6548,"Using Microsoft Excel, the market leading spreadsheet package, this book combines theory with modelling aspects and spreadsheet analysis. Microeconomics Using Excel provides students with the tools with which to better understand microeconomic analysis.A new textbook, it focuses on solving microeconomic problems by integrating economic theory, policy analysis and spreadsheet modelling. This unique approach facilitates a more comprehensive understanding of the link between theory and problem solving.It is divided into four core parts:analysis of price policiesanalysis of structural policiesmulti-market modelsbudget policy and priority settings. The theory behind each problem is explained and each model is solved using excel. Each model is also available on the accompanying CD and can be used as a prototype for analysis and specific needs.Microeconomics using Excel will be of great interest to students studying economics as well as to professionals in economic and policy analysis.",2007,39,16,1,1,0,1,1,4,1,3,1,0,0
f570fe56548b16fb69518e0dc0da6b493d7c7c59,"Few other economists have been read and cited as often as R.H. Coase has been, even though, as he admits, ""most economists have a different way of looking at economic problems and do not share my conception of the nature of our subject."" Coase's particular interest has been that part of economic theory that deals with firms, industries, and markets—what is known as price theory or microeconomics. He has always urged his fellow economists to examine the foundations on which their theory exists, and this volume collects some of his classic articles probing those very foundations. ""The Nature of the Firm"" (1937) introduced the then-revolutionary concept of transaction costs into economic theory. ""The Problem of Social Cost"" (1960) further developed this concept, emphasizing the effect of the law on the working of the economic system. The remaining papers and new introductory essay clarify and extend Coarse's arguments and address his critics. ""These essays bear rereading. Coase's careful attention to actual institutions not only offers deep insight into economics but also provides the best argument for Coase's methodological position. The clarity of the exposition and the elegance of the style also make them a pleasure to read and a model worthy of emulation.""—Lewis A. Kornhauser, Journal of Economic Literature Ronald H. Coase was awarded the Nobel Prize in Economic Science in 1991.",1990,0,1984,90,10,6,10,13,27,31,44,26,44,53
ae0eb7b44ac3cbca7dd1f8175b44fb2ce62c974a,"Abstract The relation between Thermodynamics and Economics is a paramount issue in Ecological Economics. Two different levels can be distinguished when discussing it: formal and substantive. At the formal level, a mathematical framework is used to describe both thermodynamic and economic systems. At the substantive level, thermodynamic laws are applied to economic processes. In Ecological Economics, there is a widespread claim that neoclassical economics has the same mathematical formulation as classical mechanics and is therefore fundamentally flawed because: 1) utility does not obey a conservation law as energy does; 2) an equilibrium theory cannot be used to study irreversible processes. Here, we show that neoclassical economics is based on a wrong formulation of classical mechanics, being in fact formally analogous to equilibrium thermodynamics. The similarity between both formalisms, namely that they are both cases of constrained optimisation, is easily perceived when thermodynamics is looked upon using the Tisza–Callen axiomatisation. In this paper, we take the formal analogy between equilibrium thermodynamics and economic systems far enough to answer the formal criticisms, proving that the formalism of neoclassical economics has irreversibility embedded in it. However, the formal similarity between equilibrium thermodynamics and neoclassical microeconomics does not mean that economic models are in accordance with mass, energy and entropy balance equations. In fact, neoclassical theory suffers from flaws in the substantive integration with thermodynamic laws as has already been fully demonstrated by valuable work done by ecological economists in this field.",2006,44,46,1,2,3,2,10,2,0,2,1,0,2
7a4c5e838aef0a82de5113ebe96ea531b566237d,"Part 1 Introduction: thinking like an economist supply and demand. Part 2 The theory of consumer behaviour - rational consumer choices individual and market demand applications of rational choice and demand theories the economics of information and choice under uncertainty (supplementary) explaining tastes - the importance of altruism and other non-egoistic behaviour (supplementary) cognitive limitations and consumer behaviour (supplementary). Part 3 The theory of the firm and market structure: production costs perfect competition mono[poly oligopoly and monopolistic competition. Part 4 Factor markets: labour capital (supplementary). Part 5 General equilibrium and welfare: general equilibrium and market efficiency externalities, property rights, and the Case theorem government (supplementary). Appendices: the utility function approach to the consumer budgeting problem additional topics in demand theory additional topics in supply theory search theory and the winner's curse mathematical extensions of production theory additional extensions of the theory of costs additional models of monopolistic competition a more detailed look at exhaustible resource allocation.",1991,0,430,22,0,1,1,2,6,8,4,6,8,10
a50268a11b5d6e1c21a41a0b4ec3548b5a3627dd,"Abstract: The author presents new evidence on the effects of attendance on academic performance. He used a large panel data set for introductory microeconomics students to explicitly take into account the effect of unobservable factors correlated with attendance, such as ability, effort, and motivation. He found that neither proxy variables nor instrumental variables provide a solution to the omitted variable bias. Panel estimators indicate that attendance has a smaller but significant impact on performance. Lecture and classes have a similar effect on performance individually, although their impact cannot be identified separately. Overall, the results indicate that, after controlling for unobservable student characteristics, attendance has a statistically significant and quantitatively relevant effect on student learning.",2006,45,178,15,2,2,8,6,14,14,14,10,14,9
d9ca4a5d05a398de65d60fbde9cfb3121e2e37ff,"The conventional utility-based approach to microeconomics is now nearly a century old and although frequently criticised, it has yet to be replaced. On the Reappraisal of Microeconomics offers an alternative approach that overcomes most of the objections to orthodox theory, whilst offering some unique additional advantages. 

The authors present a new approach to non-equilibrium microeconomics that applies equally to production, trade and consumption, and that is also consistent with the laws of thermodynamics. This new theory is not limited to equilibrium or near-equilibrium conditions. The core of the theory is proof that, for each agent (firm or individual), there exists an unique function of goods and money (denoted Z) that can be interpreted as subjective wealth for an individual or the owners of a firm. Exchanges may occur only when both parties enjoy an increase in subjective wealth as a consequence. On average, this Z-function will increase over time if, and only if, the agent obeys a simple decision rule in all economic transactions: namely to 'avoid avoidable losses'understood, or AAL, it being understood that some losses are unavoidable. Dynamic equations describing growth (or decline) can be derived simply by calculating time derivatives of a wealth function, without the need for constrained maximization of an integral of utility (or some surrogate) BM_1_over time. The Z-function also has a number of other interesting properties that can be used for multi-agent and multi-sectoral simulation models to explore a variety of economic situations that cannot be addressed so easily using conventional methods.",2005,0,27,0,1,0,2,0,0,4,4,3,6,0
5455fe83185e45030345dfcc6b6c7f32772e5d6b,"This paper presents new evidence on the effects of attendance on academic performance. We exploit a large panel data set for Introductory Microeconomics students to explicitly take into account the effect of unobservable factors correlated with attendance, such as ability, effort and motivation. We find that neither proxy variables nor instrumental variables provide a viable solution to the omitted variable bias. Panel estimators indicate that attendance has a positive and significant impact on performance. Lecture and classes have a similar effect on performance individually, although their impact cannot be identified separately. Overall, the results indicate that, after controlling for unobservable student characteristics, teaching has an important independent effect on learning.",2004,31,78,4,0,0,0,2,4,6,7,7,3,8
18fb9a20de81ac2fe240cfaf852a3220e5e34b70,"The Alcohol Use Disorders Identification Test (AUDIT) has been developed from a six-country WHO collaborative project as a screening instrument for hazardous and harmful alcohol consumption. It is a 10-item questionnaire which covers the domains of alcohol consumption, drinking behaviour, and alcohol-related problems. Questions were selected from a 150-item assessment schedule (which was administered to 1888 persons attending representative primary health care facilities) on the basis of their representativeness for these conceptual domains and their perceived usefulness for intervention. Responses to each question are scored from 0 to 4, giving a maximum possible score of 40. Among those diagnosed as having hazardous or harmful alcohol use, 92% had an AUDIT score of 8 or more, and 94% of those with non-hazardous consumption had a score of less than 8. AUDIT provides a simple method of early detection of hazardous and harmful alcohol use in primary health care settings and is the first instrument of its type to be derived on the basis of a cross-national study.",1993,57,9714,701,6,13,20,24,33,35,45,91,85,94
625402497e36290fb27d9462cd8510466fa23409,"We present a consumption‐based model that explains a wide variety of dynamic asset pricing phenomena, including the procyclical variation of stock prices, the long‐horizon predictability of excess stock returns, and the countercyclical variation of stock market volatility. The model captures much of the history of stock prices from consumption data. It explains the short‐and long‐run equity premium puzzles despite a low and constant risk‐free rate. The results are essentially the same whether we model stocks as a claim to the consumption stream or as a claim to volatile dividends poorly corelated with consumption. The model is driven by an independently and identically distributed consumption growth process and adds a slow ‐moving external habit to the standard power utility function. These features generate slow countercyclical variation in risk premia. The model posits a fundamentally novel description of risk premia. Investors fear stocks primarily because they do poorly in recessions unrelated to the risks of long‐run average consumption growth.",1995,64,5106,602,0,1,18,30,45,57,88,119,135,156
ead204ecf890ed4ebcb6e7a2babe781733c8fef3,"This paper develops a class of recursive, but not necessarily expected utility, preferences over intertemporal consumption lotteries. An important feature of these general preferences is that they permit risk attitudes to be disentangled from the degree of intertemporal substitutability. Moreover, in an infinite horizon, representative-agent context, these preference specifications lead to a model of asset returns in which appropriate versions of both the atemporal CAPM and the intertemporal consumption CAPM are nested as special cases. In the authors' general model, systematic risk of an asset is determined by covariance with both the return to the market portfolio and consumption growth. Copyright 1989 by The Econometric Society.",1989,44,4362,586,14,11,19,16,20,18,12,29,31,35
af9b5cebe8134185523b0db52a521f8b979602c7,"This paper argues for the recognition of important experiential aspects of consumption. Specifically, a general framework is constructed to represent typical consumer behavior variables. Based on this paradigm, the prevailing information processing model is contrasted with an experiential view that focuses on the symbolic, hedonic, and esthetic nature of consumption. This view regards the consumption experience as a phenomenon directed toward the pursuit of fantasies, feelings, and fun.",1982,47,6457,309,1,3,3,10,10,14,9,16,21,20
40b197a53970708b71b622e43153e7ad694a41ae,"Publisher Summary 
A common hypothesis about the behavior of limited liability asset prices in perfect markets is the random walk of returns or in its continuous-time form the geometric Brownian motion hypothesis, which implies that asset prices are stationary and log-normally distributed. A number of investigators of the behavior of stock and commodity prices have questioned the accuracy of the hypothesis. In an earlier study described in the chapter, it was examined that the continuous-time consumption-portfolio problem for an individual whose income is generated by capital gains on investments in assets with prices assumed to satisfy the “geometric Brownian motion” hypothesis. Under the additional assumption of a constant relative or constant absolute risk-aversion utility function, explicit solutions for the optimal consumption and portfolio rules were derived. The changes in these optimal rules with respect to shifts in various parameters such as expected return, interest rates, and risk were examined by the technique of comparative statics. This chapter presents an extension of these results for more general utility functions, price behavior assumptions, and income generated also from noncapital gains sources. If the geometric Brownian motion hypothesis is accepted, then a general separation or mutual fund theorem can be proved such that, in this model, the classical Tobin mean-variance rules hold without the objectionable assumptions of quadratic utility or of normality of distributions for prices. Hence, when asset prices are generated by a geometric Brownian motion, the two-asset case can be worked on without loss of generality.",1975,5,3382,546,3,4,3,2,2,5,5,5,1,6
fb61c1d54386ae6edf49d083b20018d3043ce56d,"This paper defines hedonic consumption as those facets of consumer behavior that relate to the multisensory, fantasy and emotive aspects of product usage experience. After delineating these concepts, their theoretical antecedents are traced, followed by a discussion of differences between the traditional and hedonic views, methodological implications of the latter approach, and behavioral propositions in four substantive areas relevant to hedonic consumption—mental constructs, product classes, product usage and individual differences. Conclusions concern the usefulness of the hedonic perspective in supplementing and extending marketing research on consumer behavior.",1982,59,4497,206,1,2,3,4,2,8,3,6,7,7
0c218d8102b671b8d28d70034be445b107c3dfa4,"The rapidly growing world energy use has already raised concerns over supply difficulties, exhaustion of energy resources and heavy environmental impacts (ozone layer depletion, global warming, climate change, etc.). The global contribution from buildings towards energy consumption, both residential and commercial, has steadily increased reaching figures between 20% and 40% in developed countries, and has exceeded the other major sectors: industrial and transportation. Growth in population, increasing demand for building services and comfort levels, together with the rise in time spent inside buildings, assure the upward trend in energy demand will continue in the future. For this reason, energy efficiency in buildings is today a prime objective for energy policy at regional, national and international levels. Among building services, the growth in HVAC systems energy use is particularly significant (50% of building consumption and 20% of total consumption in the USA). This paper analyses available information concerning energy consumption in buildings, and particularly related to HVAC systems. Many questions arise: Is the necessary information available? Which are the main building types? What end uses should be considered in the breakdown? Comparisons between different countries are presented specially for commercial buildings. The case of offices is analysed in deeper detail.",2008,3,4640,89,8,26,47,114,162,260,381,428,524,578
d31bcc9e36b98c5350b916ee89074e31aed5a637,"What is the exact nature of the consumption function? Can this term be defined so that it will be consistent with empirical evidence and a valid instrument in the hands of future economic researchers and policy makers? In this volume a distinguished American economist presents a new theory of the consumption function, tests it against extensive statistical J material and suggests some of its significant implications.Central to the new theory is its sharp distinction between two concepts of income, measured income, or that which is recorded for a particular period, and permanent income, a longer-period concept in terms of which consumers decide how much to spend and how much to save. Milton Friedman suggests that the total amount spent on consumption is on the average the same fraction of permanent income, regardless of the size of permanent income. The magnitude of the fraction depends on variables such as interest rate, degree of uncertainty relating to occupation, ratio of wealth to income, family size, and so on.The hypothesis is shown to be consistent with budget studies and time series data, and some of its far-reaching implications are explored in the final chapter.",1957,10,4129,260,1,0,3,6,5,8,2,6,7,9
262e58a8a7f60f999fdb49747685279c476bdb82,"Abstract This article presents a theory developed to explain why consumers make the choices they do. The theory identifies five consumption values influencing consumer choice behavior. Three representative applications of the theory are illustrated pertaining to choices involving cigarette smoking. The illustrations examined include the choice to buy or not buy (or to use or not use) cigarettes, the choice of one type of cigarette over another, and the choice of one cigarette brand over another. Results of the operationalization of the theory suggest that it may be used to predict consumption behavior, as well as to describe and explain it.",1991,29,2989,277,0,0,1,3,2,1,4,5,7,14
a2dccf7f5a50526cc5ad093251c6b9921542989a,"Although the measurement of drinking is necessary for assessing and evaluating the treatment of alcohol problems, this key dependent variable has not always been reported in outcome studies.1, 2, 3 Today, the issue is not whether to measure drinking, but how to measure drinking. Concerns about how best to measure drinking patterns and problems date back to at least 1926, when Pearl stressed the importance of separating steady daily drinkers from occasional heavy drinkers.4",1992,74,3593,169,1,3,9,10,14,24,23,20,50,27
c57d7bc48a47f9e56ce0383fe7ceb0f596247807,"M Y FIRST published paper' has come of age, and at a time when the subjects it dealt with have come back into fashion. It developed the equilibrium conditions for a rational consumer's lifetime consumption-saving pattern, a problem more recently given by Harrod the useful name of ""hump saving"" but which Landry, Bbhm-Bawerk, Fisher, and others had touched on long before my time.2 It dealt only with a single individual and did not discuss the mutual determination by all individuals of the",1958,0,3397,140,0,1,0,0,0,0,1,0,2,1
a3403729f632100547f88e68e18680cd8f63df3c,"This paper derives a single-beta asset pricing model in a multi-good, continuous-time model with uncertain consumption-goods prices and uncertain investment opportunities. When no riskless asset exists, a zero-beta pricing model is derived. Asset betas are measured relative to changes in the aggregate real consumption rate, rather than relative to the market. In a singlegood model, an individual’s asset portfolio results in an optimal consumption rate that has the maximum possible correlation with changes in aggregate consumption. If the capital markets are unconstrained Pareto-optimal, then changes in all individuals’ optimal consumption rates are shown to be perfectly correlated. The capital asset pricing model (CAPM) of Sharpe (1964) and Lintner (1965) is an important theory of the structure of equilibrium expected returns on securities in the capital markets. Empirical tests of the model have had mixed results, in that security returns do appear to be positively related to their respective measured market ‘betas’, but not in the precise manner implied by the CAPM.’ By relaxing the assumptions involved in the derivation of the CAPM, the model has been extended to more general economies, usually at the expense of simplicity in the structure of equilibrium expected returns. This paper further develops the intertemporal extension of the CAPM that was initiated by Merton (1973) in a continuous-time model. Merton’s intertemporal CAPM with stochastic investment opportunities states that the expected excess return on any asset is given by a ‘multi-beta’ version of the CAPM with the number of betas being equal to one plus the number of state variables needed to describe the relevant characteristics of *I am grateful for the helpful comments of Sudipto Bhattacharya, George Constantanides, Eugene Fama, Nils Hakansson, Jon Ingersoll, John Long (the referee), Merton Miller, Stephen Ross, Myron Scholes, and especially Robert Litzenberger. Of course, they are not responsible for any remaining errors. ‘See Jensen (1972) for a survey of many of these results.",1979,51,2694,275,1,10,10,17,22,30,21,29,28,33
618096956487f87cecd47fa0e1762ca5bf0a0722,"This article considers the potential of a revival of interest in theories of practice for the study of consumption. It presents an abridged account of the basic precepts of a theory of practice and extracts some broad principles for its application to the analysis of final consumption. The basic assumption is that consumption occurs as items are appropriated in the course of engaging in particular practices and that being a competent practitioner requires appropriation of the requisite services, possession of appropriate tools, and devotion of a suitable level of attention to the conduct of the practice. Such a view stresses the routine, collective and conventional nature of much consumption but also emphasizes that practices are internally differentiated and dynamic. Distinctive features of the account include its understanding of the way wants emanate from practices, of the processes whereby practices emerge, develop and change, of the consequences of extensive personal involvements in many practices, and of the manner of recruitment to practices. The article concludes with discussion of some theoretical, substantive and methodological implications.",2005,62,2119,236,5,7,33,29,44,73,119,139,161,174
493cf3b988780efde51a84e87af276d0becc2103,"This paper studies the role of detrended wealth in predicting stock returns. We call a transitory movement in wealth one that produces a deviation from its shared trend with consumption and labor income. Using U.S. quarterly stock market data, we find that these trend deviations in wealth are strong predictors of both real stock returns and excess returns over a Treasury bill rate. We also find that this variable is a better forecaster of future returns at short and intermediate horizons than is the dividend yield, the earnings yield, the dividend payout ratio and several other popular forecasting variables. ; Why should wealth, detrended in this way, forecast asset returns? We show that a wide class of optimal models of consumer behavior imply that the log consumption-aggregate (human and nonhuman) wealth ratio forecasts the expected return on aggregate wealth, or the market portfolio. Although this ratio is not observable, we demonstrate that its important predictive components may be expressed in terms of observable variables, namely in terms of consumption, nonhuman wealth and labor income. The framework implies that these variables are cointegrated, and that deviations from this shared trend summarize agents' expectations of future returns on the market portfolio.",2001,77,1950,238,29,48,33,62,91,74,73,102,105,106
dad7109d6973f91ef1db561f88543906848bcdc4,"This paper investigates the testable restrictions on the time-series behavior of consumption and asset returns implied by a representative agent model in which intertemporal preferences are represented by utility functions that generalize conventional, time-additive, expected utility. The model based on these preferences allows a clearer separation of observable behavior attributable to risk aversion and to intertemporal substitution. Further, it nests the predictions of both the consumption CAPM and the static CAPM, and it allows direct tests of the expected utility hypothesis. We find that the performance of the non-expected utility model and tests of the expected utility hypothesis are sensitive to the choice of both consumption measure and instrumental variables.",1991,52,2147,276,10,9,14,15,19,24,20,29,38,38
95c6db422148b9b6ce9b233a934134534a055e31,"fatty acids affect cardiac function (including antiarrhythmic effects), hemodynamics (cardiac mechanics), and arterial endothelial function have helped clarify potential mechanisms of action. The present Statement will address distinctions between plant-derived (-linolenic acid, C18:3n-3) and marine-derived (eicosapentaenoic acid, C20:5n-3 [EPA] and docosahexaenoic acid, C22:6n-3 [DHA]) omega-3 fatty acids. (Unless otherwise noted, the term omega-3 fatty acids will refer to the latter.) Evidence from epidemiological studies and RCTs will be reviewed, and recommendations reflecting the current state of knowledge will be made with regard to both fish consumption and omega-3 fatty acid (plant- and marine-derived) supplementation. This will be done in the context of recent guidance issued by the US Environmental Protection Agency and the Food and Drug Administration (FDA) about the presence of environmental contaminants in certain species of fish.",2002,153,2691,76,1,34,72,96,139,145,164,158,140,186
0c2030b54ffb098b206773f420e84438e0d04f64,"Cultural meaning in a consumer society moves ceaselessly from one location to another. In the usual trajectory, cultural meaning moves first from the culturally constituted world to consumer goods and then from these goods to the individual consumer. Several instruments are responsible for this movement: advertising, the fashion system, and four consumption rituals. This article analyzes the movement of cultural meaning theoretically, showing both where cultural meaning is resident in the contemporary North American consumer system and the means by which this meaning is transferred from one location in this system to another.",1986,64,2612,171,0,6,8,9,10,7,8,16,10,12
f64c312608ee46de4488b741097886bf81d3f2cd,Recent evidence on the effect of government spending shocks on consumption cannot be easily reconciled with existing optimizing business cycle models. We extend the standard New Keynesian model to allow for the presence of rule-of-thumb (non-Ricardian) consumers. We show how the interaction of the latter with sticky prices and deficit financing can account for the existing evidence on the effects of government spending,2004,278,1790,246,60,36,35,59,38,122,113,117,128,139
cdb417feb40da9c9207f0d5879c406e593711df7,"BACKGROUND
The rising prevalence of obesity in children has been linked in part to the consumption of sugar-sweetened drinks. Our aim was to examine this relation.


METHODS
We enrolled 548 ethnically diverse schoolchildren (age 11.7 years, SD 0.8) from public schools in four Massachusetts communities, and studied them prospectively for 19 months from October, 1995, to May, 1997. We examined the association between baseline and change in consumption of sugar-sweetened drinks (the independent variables), and difference in measures of obesity, with linear and logistic regression analyses adjusted for potentially confounding variables and clustering of results within schools.


FINDINGS
For each additional serving of sugar-sweetened drink consumed, both body mass index (BMI) (mean 0.24 kg/m2; 95% CI 0.10-0.39; p=0.03) and frequency of obesity (odds ratio 1.60; 95% CI 1.14-2.24; p=0.02) increased after adjustment for anthropometric, demographic, dietary, and lifestyle variables. Baseline consumption of sugar-sweetened drinks was also independently associated with change in BMI (mean 0.18 kg/m2 for each daily serving; 95% CI 0.09-0.27; p=0.02).


INTERPRETATION
Consumption of sugar-sweetened drinks is associated with obesity in children.",2001,30,2477,87,20,46,90,135,136,158,155,135,161,161
b0bcdd43c9dfee951c5d1b33e996926bfc895693,"Energy-aware design and evaluation of network protocols requires knowledge of the energy consumption behavior of actual wireless interfaces. But little practical information is available about the energy consumption behavior of well-known wireless network interfaces and device specifications do not provide information in a form that is helpful to protocol developers. This paper describes a series of experiments which obtained detailed measurements of the energy consumption of an IEEE 802.11 wireless network interface operating in an ad hoc networking environment. The data is presented as a collection of linear equations for calculating the energy consumed in sending, receiving and discarding broadcast and point-to-point data packets of various sizes. Some implications for protocol design and evaluation in ad hoc networks are discussed.",2001,18,1934,128,11,45,90,113,144,159,154,137,134,119
b26e0fa7b363f42e31bbde61356f5fcf24d509fd,"There is a growing interest in reducing energy consumption and the associated greenhouse gas emissions in every sector of the economy. The residential sector is a substantial consumer of energy in every country, and therefore a focus for energy consumption efforts. Since the energy consumption characteristics of the residential sector are complex and inter-related, comprehensive models are needed to assess the technoeconomic impacts of adopting energy efficiency and renewable energy technologies suitable for residential applications. The aim of this paper is to provide an up-to-date review of the various modeling techniques used for modeling residential sector energy consumption. Two distinct approaches are identified: top-down and bottom-up. The top-down approach treats the residential sector as an energy sink and is not concerned with individual end-uses. It utilizes historic aggregate energy values and regresses the energy consumption of the housing stock as a function of top-level variables such as macroeconomic indicators (e.g. gross domestic product, unemployment, and inflation), energy price, and general climate. The bottom-up approach extrapolates the estimated energy consumption of a representative set of individual houses to regional and national levels, and consists of two distinct methodologies: the statistical method and the engineering method. Each technique relies on different levels of input information, different calculation or simulation techniques, and provides results with different applicability. A critical review of each technique, focusing on the strengths, shortcomings and purposes, is provided along with a review of models reported in the literature.",2009,86,1570,84,5,26,53,88,90,115,155,167,200,192
e431ee4d945632c54dc30691becf7352afb9a8ca,"The author examines consumer affective responses to product/consumption experiences and their relationship to selected aspects of postpurchase processes. In separate field studies of automobile owners and CATV subscribers, subjects reported the nature and frequency of emotional experiences in connection with product ownership and usage. Analysis confirms hypotheses about the existence of independent dimensions of positive and negative affect. Both dimensions of affective response are found directly related to the favorability of consumer satisfaction judgments, extent of seller-directed complaint behavior, and extent of word-of-mouth transmission.",1987,67,2245,95,1,1,0,0,6,6,11,11,13,22
80598bb0e18bceec032cbab23dce6d9f5fec998d,"BACKGROUND
The oxidative modification of low-density lipoproteins increases their incorporation into the arterial intima, an essential step in atherogenesis. Although dietary antioxidants, such as vitamin C, carotene, and vitamin E, have been hypothesized to prevent coronary heart disease, prospective epidemiologic data are sparse.


METHODS
In 1986, 39,910 U.S. male health professionals 40 to 75 years of age who were free of diagnosed coronary heart disease, diabetes, and hypercholesterolemia completed detailed dietary questionnaires that assessed their usual intake of vitamin C, carotene, and vitamin E in addition to other nutrients. During four years of follow-up, we documented 667 cases of coronary disease.


RESULTS
After controlling for age and several coronary risk factors, we observed a lower risk of coronary disease among men with higher intakes of vitamin E (P for trend = 0.003). For men consuming more than 60 IU per day of vitamin E, the multivariate relative risk was 0.64 (95 percent confidence interval, 0.49 to 0.83) as compared with those consuming less than 7.5 IU per day. As compared with men who did not take vitamin E supplements, men who took at least 100 IU per day for at least two years had a multivariate relative risk of coronary disease of 0.63 (95 percent confidence interval, 0.47 to 0.84). Carotene intake was not associated with a lower risk of coronary disease among those who had never smoked, but it was inversely associated with the risk among current smokers (relative risk, 0.30; 95 percent confidence interval, 0.11 to 0.82) and former smokers (relative risk, 0.60; 95 percent confidence interval, 0.38 to 0.94). In contrast, a high intake of vitamin C was not associated with a lower risk of coronary disease.


CONCLUSIONS
These data do not prove a causal relation, but they provide evidence of an association between a high intake of vitamin E and a lower risk of coronary heart disease in men. Public policy recommendations with regard to the use of vitamin E supplements should await the results of additional studies.",1993,50,2375,41,20,97,128,139,142,136,157,176,151,116
a4ee2c918b2655394ab2aff8a6695c580cba76d4,"This paper proposes that the time-series data on consumption, income, and interest rates are best viewed as generated not by a single representative consumer but by two groups of consumers. Half the consumers are forward-looking and consume their permanent income, but are extremely reluctant to substitute consumption intertemporally. Half the consumers follow the ""rule of thumb"" of consuming their current income. The paper documents three empirical regularities that, it argues, are best explained by this model. First, expected changes in income are associated with expected changes in consumption. Second, expected real interest rates are not associated with expected changes in consumption. Third, periods in which consumption is high relative to income are typically followed by high growth in income. The paper concludes by briefly discussing the implications of these findings for economic policy and economic research.",1989,66,1875,207,4,9,13,16,16,17,26,25,32,27
d65e4d8d832d51628a648638e96950102817a934,"This article introduces the subculture of consumption as an analytic category through which to better understand consumers and the manner in which they organize their lives and identities. Recognizing that consumption activities, product categories, or even brands may serve as the basis for interaction and social cohesion, the concept of the subculture of consumption solves many problems inherent in the use of ascribed social categories as devices for understanding consumer behavior. This article is based on three years of ethnographic fieldwork with Harley-Davidson motorcycle owners. A key feature of the fieldwork was a process of progressive contextualization of the researchers from outsiders to insiders situated within the subculture. Analysis of the social structure, dominant values, and revealing symbolic behaviors of this distinct, consumption-oriented subculture have led to the advancement of a theoretical framework that situates subcultures of consumption in the context of modern consumer culture and discusses, among other implications, a symbiosis between such subcultures and marketing institutions. Transferability of the principal findings of this research to other subcultures of consumption is established through comparisons with ethnographies of other self-selecting, consumption-oriented subcultures.",1995,55,2033,113,0,5,4,15,6,10,20,32,29,44
fc762a0d5a2b764dd3de22ba3cd1e1bff06659c9,"The HIF-1 transcription factor drives hypoxic gene expression changes that are thought to be adaptive for cells exposed to a reduced-oxygen environment. For example, HIF-1 induces the expression of glycolytic genes. It is presumed that increased glycolysis is necessary to produce energy when low oxygen will not support oxidative phosphorylation at the mitochondria. However, we find that while HIF-1 stimulates glycolysis, it also actively represses mitochondrial function and oxygen consumption by inducing pyruvate dehydrogenase kinase 1 (PDK1). PDK1 phosphorylates and inhibits pyruvate dehydrogenase from using pyruvate to fuel the mitochondrial TCA cycle. This causes a drop in mitochondrial oxygen consumption and results in a relative increase in intracellular oxygen tension. We show by genetic means that HIF-1-dependent block to oxygen utilization results in increased oxygen availability, decreased cell death when total oxygen is limiting, and reduced cell death in response to the hypoxic cytotoxin tirapazamine.",2006,50,1773,89,21,62,74,89,108,106,111,121,150,150
d968e0687a8f2557505aad3988a4de921dcf45fd,"Improved feedback on electricity consumption may provide a tool for customers to better control their consumption and ultimately save energy. This paper asks which kind of feedback is most successful. For this purpose, a psychological model is presented that illustrates how and why feedback works. Relevant features of feedback are identified that may determine its effectiveness: frequency, duration, content, breakdown, medium and way of presentation, comparisons, and combination with other instruments. The paper continues with an analysis of international experience in order to find empirical evidence for which kinds of feedback work best. In spite of considerable data restraints and research gaps, there is some indication that the most successful feedback combines the following features: it is given frequently and over a long time, provides an appliance-specific breakdown, is presented in a clear and appealing way, and uses computerized and interactive tools.",2008,47,1315,118,2,24,56,77,91,141,146,122,138,138
8cb76820ff3b22f223239f1982c64d528b1433c3,"In a meta-analysis of 88 studies, we examined the association between soft drink consumption and nutrition and health outcomes. We found clear associations of soft drink intake with increased energy intake and body weight. Soft drink intake also was associated with lower intakes of milk, calcium, and other nutrients and with an increased risk of several medical problems (e.g., diabetes). Study design significantly influenced results: larger effect sizes were observed in studies with stronger methods (longitudinal and experimental vs cross-sectional studies). Several other factors also moderated effect sizes (e.g., gender, age, beverage type). Finally, studies funded by the food industry reported significantly smaller effects than did non-industry-funded studies. Recommendations to reduce population soft drink consumption are strongly supported by the available science.",2007,141,1703,45,20,56,82,113,125,131,157,173,154,124
47e420af2ec38b50dc27deff847022fc7f15c4cc,"This article solves a realistically calibrated life cycle model of consumption and portfolio choice with non-tradable labor income and borrowing constraints. Since labor income substitutes for riskless asset holdings, the optimal share invested in equities is roughly decreasing over life. We compute a measure of the importance of human capital for investment behavior. We find that ignoring labor income generates large utility costs, while the cost of ignoring only its risk is an order of magnitude smaller, except when we allow for a disastrous labor income shock. Moreover, we study the implications of introducing endogenous borrowing constraints in this incomplete-markets setting. Copyright 2005, Oxford University Press.",2005,100,1354,216,25,40,53,68,87,81,83,75,105,75
2ec854b0488ebdadcf877137ccff8db621fd2f38,"Obesity is a major epidemic, but its causes are still unclear. In this article, we investigate the relation between the intake of high-fructose corn syrup (HFCS) and the development of obesity. We analyzed food consumption patterns by using US Department of Agriculture food consumption tables from 1967 to 2000. The consumption of HFCS increased > 1000% between 1970 and 1990, far exceeding the changes in intake of any other food or food group. HFCS now represents > 40% of caloric sweeteners added to foods and beverages and is the sole caloric sweetener in soft drinks in the United States. Our most conservative estimate of the consumption of HFCS indicates a daily average of 132 kcal for all Americans aged > or = 2 y, and the top 20% of consumers of caloric sweeteners ingest 316 kcal from HFCS/d. The increased use of HFCS in the United States mirrors the rapid increase in obesity. The digestion, absorption, and metabolism of fructose differ from those of glucose. Hepatic metabolism of fructose favors de novo lipogenesis. In addition, unlike glucose, fructose does not stimulate insulin secretion or enhance leptin production. Because insulin and leptin act as key afferent signals in the regulation of food intake and body weight, this suggests that dietary fructose may contribute to increased energy intake and weight gain. Furthermore, calorically sweetened beverages may enhance caloric overconsumption. Thus, the increase in consumption of HFCS has a temporal relation to the epidemic of obesity, and the overconsumption of HFCS in calorically sweetened beverages may play a role in the epidemic of obesity.",2004,65,1815,93,22,63,69,92,118,109,104,120,136,141
928da9164cf5240094de1ceab2ee321276aa7edb,This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles.,1975,14,10031,461,15,17,16,13,31,37,32,31,53,60
42607bb3d65c74eb44364a379d5496e69567e323,"В статье производится анализ агрегированной производственной функции, вводится аппарат, позволяющий различать движение вдоль такой функции от ее сдвигов. На основании сделанных в статье предположений делаются выводы о характере технического прогресса и технологических изменений. Существенное внимание уделяется вариантам применения концепции агрегированной производственной функции.",1957,0,10414,459,0,0,0,0,0,0,0,0,0,0
3d6ca6a699e579463e0b01526d9842d41f26b314,"Previous studies of the so-called frontier production function have not utilized an adequate characterization of the disturbance term for such a model. In this paper we provide an appropriate specification, by defining the disturbance term as the sum of symmetric normal and (negative) half-normal random variables. Various aspects of maximum-likelihood estimation for the coefficients of a production function with an additive disturbance term of this sort are then considered.",2001,17,4806,749,87,93,118,143,142,166,174,168,222,246
4c17fd88f8f4eb15005416edac67875e8be6dfb1,"Evidence gleaned from the instrumental record of climate data identifies a robust, recurring pattern of ocean–atmosphere climate variability centered over the midlatitude North Pacific basin. Over the past century, the amplitude of this climate pattern has varied irregularly at interannual-to-interdecadal timescales. There is evidence of reversals in the prevailing polarity of the oscillation occurring around 1925, 1947, and 1977; the last two reversals correspond to dramatic shifts in salmon production regimes in the North Pacific Ocean. This climate pattern also affects coastal sea and continental surface air temperatures, as well as streamflow in major west coast river systems, from Alaska to California.",1997,66,6350,845,8,23,66,92,121,179,232,231,275,237
6df5ec442559d953bd95e5f20ee1685b33976764,"‘The Production of Space’, in: Frans Jacobi, Imagine, Space Poetry, Copenhagen, 1996, unpaginated.",1996,10,7342,731,11,11,18,22,55,69,111,104,136,151
071f211ee4799ea191743cc893496ce4e098cdcc,"A stochastic frontier production function is defined for panel data on firms, in which the non-negative technical inefficiency effects are assumed to be a function of firm-specific variables and time. The inefficiency effects are assumed to be independently distributed as truncations of normal distributions with constant variance, but with means which are a linear function of observable variables. This panel data model is an extension of recently proposed models for inefficiency effects in stochastic frontiers for cross-sectional data. An empirical application of the model is obtained using up to ten years of data on paddy farmers from an Indian village. The null hypotheses, that the inefficiency effects are not stochastic or do not depend on the farmer-specific variables and time of observation, are rejected for these data.",1995,19,5471,757,0,7,9,22,38,53,61,95,118,138
5544b476e318fc12320dfcc979456864983e36c0,"In this provocative and broad-ranging work, a distinguished team of authors argues that the ways in which knowledge  scientific, social and cultural  is produced are undergoing fundamental changes at the end of the twentieth century. They claim that these changes mark a distinct shift into a new mode of knowledge production which is replacing or reforming established institutions, disciplines, practices and policies. Identifying a range of features of the new moder of knowledge production  reflexivity, transdisciplinarity, heterogeneity  the authors show the connections between these features and the changing role of knowledge in social relations. While the knowledge produced by research and development in science and technology (both public and industrial) is accorded central concern, the authors also outline the changing dimensions of social scientific and humanities knowledge and the relations between the production of knowledge and its dissemination through education. Placing science policy and scientific knowledge in its broader context within contemporary societies, this book will be essential reading for all those concerned with the changing nature of knowledge, with the social study of science, with educational systems, and with the relations between R&D and social, economic and technological development.",1994,0,6452,315,2,8,29,19,39,65,96,111,157,183
3ea3eac3443404048aaec9bde79e2384c5f45f7b,"Inducible expression systems in which T7 RNA polymerase transcribes coding sequences cloned under control of a T7lac promoter efficiently produce a wide variety of proteins in Escherichia coli. Investigation of factors that affect stability, growth, and induction of T7 expression strains in shaking vessels led to the recognition that sporadic, unintended induction of expression in complex media, previously reported by others, is almost certainly caused by small amounts of lactose. Glucose prevents induction by lactose by well-studied mechanisms. Amino acids also inhibit induction by lactose during log-phase growth, and high rates of aeration inhibit induction at low lactose concentrations. These observations, and metabolic balancing of pH, allowed development of reliable non-inducing and auto-inducing media in which batch cultures grow to high densities. Expression strains grown to saturation in non-inducing media retain plasmid and remain fully viable for weeks in the refrigerator, making it easy to prepare many freezer stocks in parallel and use working stocks for an extended period. Auto-induction allows efficient screening of many clones in parallel for expression and solubility, as cultures have only to be inoculated and grown to saturation, and yields of target protein are typically several-fold higher than obtained by conventional IPTG induction. Auto-inducing media have been developed for labeling proteins with selenomethionine, 15N or 13C, and for production of target proteins by arabinose induction of T7 RNA polymerase from the pBAD promoter in BL21-AI. Selenomethionine labeling was equally efficient in the commonly used methionine auxotroph B834(DE3) (found to be metE) or the prototroph BL21(DE3).",2005,59,4740,337,15,71,88,155,198,258,292,292,364,393
3dc5c82b090dcd8e6dbf110200ce334f797b3f37,"Lignocellulosic biomass can be utilized to produce ethanol, a promising alternative energy source for the limited crude oil. There are mainly two processes involved in the conversion: hydrolysis of cellulose in the lignocellulosic biomass to produce reducing sugars, and fermentation of the sugars to ethanol. The cost of ethanol production from lignocellulosic materials is relatively high based on current technologies, and the main challenges are the low yield and high cost of the hydrolysis process. Considerable research efforts have been made to improve the hydrolysis of lignocellulosic materials. Pretreatment of lignocellulosic materials to remove lignin and hemicellulose can significantly enhance the hydrolysis of cellulose. Optimization of the cellulase enzymes and the enzyme loading can also improve the hydrolysis. Simultaneous saccharification and fermentation effectively removes glucose, which is an inhibitor to cellulase activity, thus increasing the yield and rate of cellulose hydrolysis.",2002,92,5457,274,4,13,28,38,50,83,130,165,271,392
f35c1c3bc573c9e096de33e3d83eb5be34f6ab57,"A doubling in global food demand projected for the next 50 years poses huge challenges for the sustainability both of food production and of terrestrial and aquatic ecosystems and the services they provide to society. Agriculturalists are the principal managers of global useable lands and will shape, perhaps irreversibly, the surface of the Earth in the coming decades. New incentives and policies for ensuring the sustainability of agriculture and ecosystem services will be crucial if we are to meet the demands of improving yields without compromising environmental integrity or public health.",2002,92,5989,237,4,27,40,61,78,103,134,150,230,273
81bc203a4e66f6440c02ce443c28f5a67f0a37db,"Using a unique international data set from a 1989–90 survey of 62 automotive assembly plants, the author tests two hypotheses: that innovative HR practices affect performance not individually but as interrelated elements in an internally consistent HR “bundle” or system; and that these HR bundles contribute most to assembly plant productivity and quality when they are integrated with manufacturing policies under the “organizational logic” of a flexible production system. Analysis of the survey data, which tests three indices representing distinct bundles of human resource and manufacturing practices, supports both hypotheses. Flexible production plants with team-based work systems, “high-commitment” HR practices (such as contingent compensation and extensive training), and low inventory and repair buffers consistently outperformed mass production plants. Variables capturing two-way and three-way interactions among the bundles of practices are even better predictors of performance, supporting the integration hypothesis.",1995,90,3904,322,14,26,40,57,57,87,92,106,118,140
62f5939746e3c0a3132b93c0221f3dc247ca3020,"This article reviews some of the criticisms directed towards the eclectic paradigm of international production over the past decade, and restates its main tenets. The second part of the article considers a number of possible extensions of the paradigm and concludes by asserting that it remains “a robust general framework for explaining and analysing not only the economic rationale of economic production but many organisational and impact issues in relation to MNE activity as well.”",1988,66,3941,251,4,4,11,16,19,25,21,22,33,44
814c4bdf69bccb4253c28f8337c6a072b0257ceb,,1990,11,4445,319,4,4,7,9,15,28,19,32,35,23
2fbcdd66c7ddcc5ca9ee04aeead8efe98eb3ad0e,"Integrating conceptually similar models of the growth of marine and terrestrial primary producers yielded an estimated global net primary production (NPP) of 104.9 petagrams of carbon per year, with roughly equal contributions from land and oceans. Approaches based on satellite indices of absorbed solar radiation indicate marked heterogeneity in NPP for both land and oceans, reflecting the influence of physical and ecological processes. The spatial and temporal distributions of ocean NPP are consistent with primary limitation by light, nutrients, and temperature. On land, water limitation imposes additional constraints. On land and ocean, progressive changes in NPP can result in altered carbon storage, although contrasts in mechanisms of carbon storage and rates of organic matter turnover result in a range of relations between carbon storage and changes in NPP.",1998,40,4160,227,5,18,20,26,37,41,63,67,72,93
5144695b43464a1eacccd73768ac6d31aa4c35c0,"The invention disclosed herein is a liquid fuel composition having reduced soot and smoking characterized comprising a major proportion of a liquid hydrocarbon fuel and a minor proportion of Group IIA and Group IIB metal salts of carboxylic acids. A preferred fuel composition is from 0.1 to 0.6 percent by weight of barium- and zinc 2-ethylhexanoates admixed in diesel fuel, wherein the weight ratio of barium to zinc is about 10 to 1. Further improvement in smoke and soot reduction is obtained in hydrocarbon fuels when an ether is additionally incorporated into the salt and fuel mixture. A mixture of from about 0.1 to 0.6 percent by weight of barium 2-ethylhexanoate and zinc 2-ethylhexanoate, from between 0.2 to 0.5 percent by weight of the monomethyl ether of ethylene glycol and the balance, a diesel fuel, has substantially reduced smoke and soot forming characteristics.",1996,18,4896,223,9,16,37,58,71,92,136,138,171,221
b3816479d1e3b33c6960062d9b34ee08b80bfe98,,1986,0,3533,262,0,1,0,2,4,9,7,9,20,12
091a00beaa12470fa819d6a3c7b6fe06da8f7b41,"Biodiesel has become more attractive recently because of its environmental benefits and the fact that it is made from renewable resources. The cost of biodiesel, however, is the main hurdle to commercialization of the product. The used cooking oils are used as raw material, adaption of continuous transesterification process and recovery of high quality glycerol from biodiesel by-product (glycerol) are primary options to be considered to lower the cost of biodiesel. There are four primary ways to make biodiesel, direct use and blending, microemulsions, thermal cracking (pyrolysis) and transesterification. The most commonly used method is transesterification of vegetable oils and animal fats. The transesterification reaction is aAected by molar ratio of glycerides to alcohol, catalysts, reaction temperature, reaction time and free fatty acids and water content of oils or fats. The mechanism and kinetics of the transesterification show how the reaction occurs and progresses. The processes of transesterification and its downstream operations are also addressed. ” 1999 Published by Elsevier Science B.V. All rights reserved.",1999,57,5207,184,1,3,3,10,13,35,60,81,134,232
dd45d5d76bf12745ee3ea805437ff7a60a4ae890,"Preparing words in speech production is normally a fast and accurate process. We generate them two or three per second in fluent conversation; and overtly naming a clear picture of an object can easily be initiated within 600 msec after picture onset. The underlying process, however, is exceedingly complex. The theory reviewed in this target article analyzes this process as staged and feed-forward. After a first stage of conceptual preparation, word generation proceeds through lexical selection, morphological and phonological encoding, phonetic encoding, and articulation itself. In addition, the speaker exerts some degree of output control, by monitoring of self-produced internal and overt speech. The core of the theory, ranging from lexical selection to the initiation of phonetic encoding, is captured in a computational model, called WEAVER++. Both the theory and the computational model have been developed in interaction with reaction time experiments, particularly in picture naming or related word production paradigms, with the aim of accounting for the real-time processing in normal word production. A comprehensive review of theory, model, and experiments is presented. The model can handle some of the main observations in the domain of speech errors (the major empirical domain for most other theories of lexical access), and the theory opens new ways of approaching the cerebral organization of speech production by way of high-temporal-resolution imaging.",1996,352,2847,440,0,0,13,21,40,59,100,100,94,91
59eede676a521227f5c24850567b9b8dc654b4dd,"Interest in graphene centres on its excellent mechanical, electrical, thermal and optical properties, its very high specific surface area, and our ability to influence these properties through chemical functionalization. There are a number of methods for generating graphene and chemically modified graphene from graphite and derivatives of graphite, each with different advantages and disadvantages. Here we review the use of colloidal suspensions to produce new materials composed of graphene and chemically modified graphene. This approach is both versatile and scalable, and is adaptable to a wide variety of applications.",2009,67,5668,44,26,214,452,558,670,653,607,540,507,456
08ba6c50d0b0d0509c9e51c55e532ce3270db930,"The use of renewable energy sources is becoming increasingly necessary, if we are to achieve the changes required to address the impacts of global warming. Biomass is the most common form of renewable energy, widely used in the third world but until recently, less so in the Western world. Latterly much attention has been focused on identifying suitable biomass species, which can provide high-energy outputs, to replace conventional fossil fuel energy sources. The type of biomass required is largely determined by the energy conversion process and the form in which the energy is required. In the first of three papers, the background to biomass production (in a European climate) and plant properties is examined. In the second paper, energy conversion technologies are reviewed, with emphasis on the production of a gaseous fuel to supplement the gas derived from the landfilling of organic wastes (landfill gas) and used in gas engines to generate electricity. The potential of a restored landfill site to act as a biomass source, providing fuel to supplement landfill gas-fuelled power stations, is examined, together with a comparison of the economics of power production from purpose-grown biomass versus waste-biomass. The third paper considers particular gasification technologies and their potential for biomass gasification.",2002,11,3727,208,3,6,15,20,26,43,55,75,119,170
44466565fdfd765ba5d3eb711795ad0d87d4c254,"With the radical changes in information production that the Internet has introduced, we stand at an important moment of transition, says Yochai Benkler in this thought-provoking book. The phenomenon he describes as social production is reshaping markets, while at the same time offering new opportunities to enhance individual freedom, cultural diversity, political discourse, and justice. But these results are by no means inevitable: a systematic campaign to protect the entrenched industrial information economy of the last century threatens the promise of today's emerging networked information environment. In this comprehensive social theory of the Internet and the networked information economy, Benkler describes how patterns of information, knowledge, and cultural production are changing--and shows that the way information and knowledge are made available can either limit or enlarge the ways people can create and express themselves. He describes the range of legal and policy choices that confront us and maintains that there is much to be gained--or lost--by the decisions we make today.",2006,170,3064,272,8,74,162,228,237,274,276,273,296,265
f8e208c67545cb9f66185ecfbbbe484721e58a04,"Fully exploiting the properties of graphene will require a method for the mass production of this remarkable material. Two main routes are possible: large-scale growth or large-scale exfoliation. Here, we demonstrate graphene dispersions with concentrations up to approximately 0.01 mg ml(-1), produced by dispersion and exfoliation of graphite in organic solvents such as N-methyl-pyrrolidone. This is possible because the energy required to exfoliate graphene is balanced by the solvent-graphene interaction for solvents whose surface energies match that of graphene. We confirm the presence of individual graphene sheets by Raman spectroscopy, transmission electron microscopy and electron diffraction. Our method results in a monolayer yield of approximately 1 wt%, which could potentially be improved to 7-12 wt% with further processing. The absence of defects or oxides is confirmed by X-ray photoelectron, infrared and Raman spectroscopies. We are able to produce semi-transparent conducting films and conducting composites. Solution processing of graphene opens up a range of potential large-area applications, from device and sensor fabrication to liquid-phase chemistry.",2008,91,4810,61,6,60,174,259,341,400,428,483,463,454
01f1c4f6ad67a44eca04adf672bb2608a93d4f04,"Many papers have regressed non-parametric estimates of productive efficiency on environmental variables in two-stage procedures to account for exogenous factors that might affect firms’ performance. None of these have described a coherent data-generating process (DGP). Moreover, conventional approaches to inference employed in these papers are invalid due to complicated, unknown serial correlation among the estimated efficiencies. We first describe a sensible DGP for such models. We propose single and double bootstrap procedures; both permit valid inference, and the double bootstrap procedure improves statistical efficiency in the second-stage regression. We examine the statistical performance of our estimators using Monte Carlo experiments.",2007,116,2632,386,45,80,123,145,148,156,171,182,193,184
8a895d50e51b5469c4acbc22aaea2e0511a07ead,,1986,99,2879,380,4,26,15,21,31,74,42,41,35,47
8e05a1ce6103477137f939b939a6a2b52c12d005,"Diabetic hyperglycaemia causes a variety of pathological changes in small vessels, arteries and peripheral nerves. Vascular endothelial cells are an important target of hyperglycaemic damage, but the mechanisms underlying this damage are not fully understood. Three seemingly independent biochemical pathways are involved in the pathogenesis: glucose-induced activation of protein kinase C isoforms; increased formation of glucose-derived advanced glycation end-products; and increased glucose flux through the aldose reductase pathway. The relevance of each of these pathways is supported by animal studies in which pathway-specific inhibitors prevent various hyperglycaemia-induced abnormalities. Hyperglycaemia increases the production of reactive oxygen species inside cultured bovine aortic endothelial cells. Here we show that this increase in reactive oxygen species is prevented by an inhibitor of electron transport chain complex II, by an uncoupler of oxidative phosphorylation, by uncoupling protein-1 and by manganese superoxide dismutase. Normalizing levels of mitochondrial reactive oxygen species with each of these agents prevents glucose-induced activation of protein kinase C, formation of advanced glycation end-products, sorbitol accumulation and NFκB activation.",2000,23,4006,158,21,75,144,188,161,218,210,226,187,194
862b60c9d8d5c522db074d0d7bf2a60d78968645,"Frontier production functions are important for the prediction of technical efficiencies of individual firms in an industry. A stochastic frontier production function model for panel data is presented, for which the firm effects are an exponential function of time. The best predictor for the technical efficiency of an individual firm at a particular time period is presented for this time-varying model. An empirical example is presented using agricultural data for paddy farmers in a village in India.",1992,43,2735,319,4,7,5,10,14,10,18,34,41,33
74fbaf9f0f2d9f132a9f5382501ffc9e340162ef,"Microalgae represent an exceptionally diverse but highly specialized group of micro-organisms adapted to various ecological habitats. Many microalgae have the ability to produce substantial amounts (e.g. 20-50% dry cell weight) of triacylglycerols (TAG) as a storage lipid under photo-oxidative stress or other adverse environmental conditions. Fatty acids, the building blocks for TAGs and all other cellular lipids, are synthesized in the chloroplast using a single set of enzymes, of which acetyl CoA carboxylase (ACCase) is key in regulating fatty acid synthesis rates. However, the expression of genes involved in fatty acid synthesis is poorly understood in microalgae. Synthesis and sequestration of TAG into cytosolic lipid bodies appear to be a protective mechanism by which algal cells cope with stress conditions, but little is known about regulation of TAG formation at the molecular and cellular level. While the concept of using microalgae as an alternative and renewable source of lipid-rich biomass feedstock for biofuels has been explored over the past few decades, a scalable, commercially viable system has yet to emerge. Today, the production of algal oil is primarily confined to high-value specialty oils with nutritional value, rather than commodity oils for biofuel. This review provides a brief summary of the current knowledge on oleaginous algae and their fatty acid and TAG biosynthesis, algal model systems and genomic approaches to a better understanding of TAG production, and a historical perspective and path forward for microalgae-based biofuel research and commercialization.",2008,179,3167,215,9,51,122,187,245,356,341,347,297,300
96f35d6c1ce2417bdb6c1dfc60fe735bf937821d,Are you looking to uncover the wealth of networks how social production transforms markets and freedom Digitalbook. Correct here it is possible to locate as well as download the wealth of networks how social production transforms markets and freedom Book. We've got ebooks for every single topic the wealth of networks how social production transforms markets and freedom accessible for download cost-free. Search the site also as find Jean Campbell eBook in layout. We also have a fantastic collection of information connected to this Digitalbook for you. As well because the best part is you could assessment as well as download for the wealth of networks how social production transforms markets and freedom eBook,2007,1,2306,294,73,101,131,157,168,184,226,206,167,163
e21b66810bc19b7145affd127591671dc924817d,"Previous studies of the so-called frontier production function have not utilized an adequate characterization of the disturbance term for such a model. In this paper we provide an appropriate specification, by defining the disturbance term as the sum of symmetric normal and (negative) half-normal random variables. Various aspects of maximum-likelihood estimation for the coefficients of a production function with an additive disturbance term of this sort are then considered.",1977,17,4901,94,2,5,7,12,6,7,10,8,9,14
5f06f0f5202c32631c81fdda2a419b0fc113bceb,"Lignocellulosic biomass has long been recognized as a potential sustainable source of mixed sugars for fermentation to biofuels and other biomaterials. Several technologies have been developed during the past 80 years that allow this conversion process to occur, and the clear objective now is to make this process cost-competitive in today's markets. Here, we consider the natural resistance of plant cell walls to microbial and enzymatic deconstruction, collectively known as “biomass recalcitrance.” It is this property of plants that is largely responsible for the high cost of lignocellulose conversion. To achieve sustainable energy production, it will be necessary to overcome the chemical and structural properties that have evolved in biomass to prevent its disassembly.",2007,34,3694,104,18,83,102,180,249,258,339,373,324,367
dbaf5a3e49a493d0bdfe295a530917461ef2fec2,"The efficiency of crop production is defined in thermodynamic terms as the ratio of energy output (carbohydrate) to energy input (solar radiation). Temperature and water supply are the main climatic constraints on efficiency. Over most of Britain, the radiation and thermal climates are uniform and rainfall is the main discriminant of yield between regions. Total production of dry matter by barley, potatoes, sugar beet, and apples is strongly correlated with intercepted radiation and these crops form carbohydrate at about 1.4 g per MJ solar energy, equivalent to 2.4% efficiency. Crop growth in Britain may therefore be analysed in terms of ( a ) the amount of light intercepted during the growing season and ( b ) the efficiency with which intercepted light is used. The amount intercepted depends on the seasonal distribution of leaf area which, in turn, depends on temperature and soil water supply. These variables are discussed in terms of the rate and duration of development phases. A factorial analysis of efficiency shows that the major arable crops in Britain intercept only about 40 % of annual solar radiation and their efficiency for supplying energy through economic yield is only about 0.3%. Some of the factors responsible for this figure are well understood and some are immutable. More work is needed to identify the factors responsible for the large differences between average commercial and record yields.",1977,18,3201,179,0,1,6,6,6,8,11,13,29,20
ba274095c25b1917367acb0d22d6516e96ea4581,"Abstract The error term in the stochastic frontier model is of the form ( v – u ), where v is a normal error term representing pure randomness, and u is a non-negative error term representing technical inefficiency. The entire ( v – u ) is easily estimated for each observation, but a previously unsolved problem is how to separate it into its two components, v and u . This paper suggests a solution to this problem, by considering the expected value of u , conditional on ( v – u ). An explicit formula is given for the half-normal and exponential cases.",1982,6,3197,181,2,1,8,6,9,7,10,23,15,19
ddbbf87a2a0ec74f9c62239ddf3abdb45afd9c7d,"N RECENT YEARS, public and professional interest in schools has been heightened by a spate of reports, many of them critical of current school policy.' These policy documents have added to persistent and long-standing concerns about the cost, effectiveness, and fairness of the current school structure, and have made schooling once again a serious public issue. As in the past, however, any renewed interest in education is likely to be short-lived, doomed to dissipate as frustration over the inability of policy to improve school practice sets in. This frustration about school policy relates directly to knowledge about the educational production process and in turn to underlying research on schools. Although the educational process has been extensively researched, clear policy prescriptions flowing from this research have been difficult to derive.2 There exists, however, a consistency to the research findings that does have an immediate application to school policy: Schools differ dramatically in ""quality,""",1986,109,3076,204,2,7,15,20,23,40,44,36,49,49
5d79d86d4501b4a5381a7a8cc3562bf66cf975e6,,1960,1,3194,290,3,1,3,2,3,3,6,4,4,8
3a902940adf403f18ce778e394953142906f9cf3,,1993,0,3097,188,0,3,15,18,22,23,28,38,41,57
11339a042ddff3670b07dec625c9d7ac91d92dd0,"The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These operations are under the control of a schema, which is a theoretical formulation of the comprehender's goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is concerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psychological research report, and methods for the empirical testing of the model are developed.",1978,73,4653,58,2,16,68,57,81,108,93,105,131,71
3a932920c44c06b43fc24393c8710dfd2238eb37,"Biofuels produced from various lignocellulosic materials, such as wood, agricultural, or forest residues, have the potential to be a valuable substitute for, or complement to, gasoline. Many physicochemical structural and compositional factors hinder the hydrolysis of cellulose present in biomass to sugars and other organic compounds that can later be converted to fuels. The goal of pretreatment is to make the cellulose accessible to hydrolysis for conversion to fuels. Various pretreatment techniques change the physical and chemical structure of the lignocellulosic biomass and improve hydrolysis rates. During the past few years a large number of pretreatment methods have been developed, including alkali treatment, ammonia explosion, and others. Many methods have been shown to result in high sugar yields, above 90% of the theoretical yield for lignocellulosic biomasses such as woods, grasses, corn, and so on. In this review, we discuss the various pretreatment process methods and the recent literature that...",2009,136,3107,171,6,62,155,224,267,278,295,365,365,297
8ca7cd110597890534158e13e2e46519408afab2,"A number of in situ cosmogenic radionuclides and stable nuclides have been measured in natural exposed rock surfaces with a view to study their in situ production and rock erosion rates [1]. The in situ radionuclides can be used for a high-resolution tomography of the erosional history of an exposed surface; two stable nuclides (3He, 21Ne) and five radionuclides (10Be, 26Al, 36Cl, 14C, 39Ar) having half-lives in the range of ∼ 300-1.5 × 106 yr half-life are measurable in many rock types. 
 
A prerequisite for the application of the in situ nuclides for the study of erosional histories of surfaces is a knowledge of their production rates under different irradiation conditions; altitude, latitude, irradiation geometry and shielding. Relative nuclide production rates can be determined fairly accurately using the extensive available data on cosmic ray neutrons [2]. Absolute nuclide production rates cannot generally be predicted with any accuracy because of lack of data on excitation functions of nuclides unless some normalization is possible, as was done in the case of several cosmic ray produced isotopes in the atmosphere [3]. Based on a recent natural calibration experiment in which erosion free surfaces exposed to cosmic radiation for ∼ 11,000 yrs were sampled, the absolute production rates of 10Be and 26Al in quartz have been accurately estimated for mountain altitudes in Sierra Nevada [4]. The absolute production rates of 10Be and 26Al in quartz can therefore be estimated fairly accurately for any given latitude and altitude. Some measurements of 14C in rocks of low erosion rate [5] similarly allow an estimate of its production rate. Attempts made to measure the in situ production rates of 3He in rocks have not yet led to a convergent production rate. In view of the importance of knowing the production rates of isotopes of He, Ne and Ar, I present here theoretical estimates of their production rates based on available cross-section data. 
 
I discuss the information that can be extracted from the study of the in situ nuclides in rocks. Useful parameters characterizing the exposure history of a rock surface are: (1) the effective surface exposure age; and (2) the time-averaged erosion rate. The implications of these parameters for single and multiple nuclide studies are discussed in terms of the erosion models considered.",1991,27,2216,400,4,5,9,18,15,15,12,19,20,37
ad06686fc56233f4e8dcaf38403a8c3bf9770a26,,1970,9,3129,212,1,0,6,13,16,5,9,16,26,13
3eef25fd2af656581206fb7fb7aa1b632fea9e3e,,1977,3,5861,62,1,4,6,6,3,7,5,6,2,7
6593e6dad83e8ab574da70a29d638ff654ce5151,"We constructed an infectious molecular clone of acquired immunodeficiency syndrome-associated retrovirus. Upon transfection, this clone directed the production of infectious virus particles in a wide variety of cells in addition to human T4 cells. The progeny, infectious virions, were synthesized in mouse, mink, monkey, and several human non-T cell lines, indicating the absence of any intracellular obstacle to viral RNA or protein production or assembly. During the course of these studies, a human colon carcinoma cell line, exquisitely sensitive to DNA transfection, was identified.",1986,55,2795,200,0,12,17,24,50,45,84,59,78,81
943ea1b0f125611fadd677c2b70763403ecd0cb9,"Anaerobic digestion of energy crops, residues, and wastes is of increasing interest in order to reduce the greenhouse gas emissions and to facilitate a sustainable development of energy supply. Production of biogas provides a versatile carrier of renewable energy, as methane can be used for replacement of fossil fuels in both heat and power generation and as a vehicle fuel. For biogas production, various process types are applied which can be classified in wet and dry fermentation systems. Most often applied are wet digester systems using vertical stirred tank digester with different stirrer types dependent on the origin of the feedstock. Biogas is mainly utilized in engine-based combined heat and power plants, whereas microgas turbines and fuel cells are expensive alternatives which need further development work for reducing the costs and increasing their reliability. Gas upgrading and utilization as renewable vehicle fuel or injection into the natural gas grid is of increasing interest because the gas can be used in a more efficient way. The digestate from anaerobic fermentation is a valuable fertilizer due to the increased availability of nitrogen and the better short-term fertilization effect. Anaerobic treatment minimizes the survival of pathogens which is important for using the digested residue as fertilizer. This paper reviews the current state and perspectives of biogas production, including the biochemical parameters and feedstocks which influence the efficiency and reliability of the microbial conversion and gas yield.",2009,103,2207,175,2,28,67,103,146,194,238,239,263,242
0816dab8cfac8b622afc557e3d995916a488dccb,"Nano-sized TiO2 photocatalytic water-splitting technology has great potential for low-cost, environmentally friendly solar-hydrogen production to support the future hydrogen economy. Presently, the solar-to-hydrogen energy conversion efficiency is too low for the technology to be economically sound. The main barriers are the rapid recombination of photo-generated electron/hole pairs as well as backward reaction and the poor activation of TiO2 by visible light. In response to these deficiencies, many investigators have been conducting research with an emphasis on effective remediation methods. Some investigators studied the effects of addition of sacrificial reagents and carbonate salts to prohibit rapid recombination of electron/hole pairs and backward reactions. Other research focused on the enhancement of photocatalysis by modification of TiO2 by means of metal loading, metal ion doping, dye sensitization, composite semiconductor, anion doping and metal ion-implantation. This paper aims to review the up-to-date development of the above-mentioned technologies applied to TiO2 photocatalytic hydrogen production. Based on the studies reported in the literature, metal ion-implantation and dye sensitization are very effective methods to extend the activating spectrum to the visible range. Therefore, they play an important role in the development of efficient photocatalytic hydrogen production.",2007,124,3343,23,19,46,67,87,152,210,234,299,321,330
3e88d8a52759097d481b92ec0d06606afbadafab,"Apoptosis in vivo is followed almost inevitably by rapid uptake into adjacent phagocytic cells, a critical process in tissue remodeling, regulation of the immune response, or resolution of inflammation. Phagocytosis of apoptotic cells by macrophages has been suggested to be a quiet process that does not lead to production of inflammatory mediators. Here we show that phagocytosis of apoptotic neutrophils (in contrast to immunoglobulin G-opsonized apoptotic cells) actively inhibited the production of interleukin (IL)-1beta, IL-8, IL-10, granulocyte macrophage colony-stimulating factor, and tumor necrosis factor-alpha, as well as leukotriene C4 and thromboxane B2, by human monocyte-derived macrophages. In contrast, production of transforming growth factor (TGF)-beta1, prostaglandin E2, and platelet-activating factor (PAF) was increased. The latter appeared to be involved in the inhibition of proinflammatory cytokine production because addition of exogenous TGF-beta1, prostaglandin E2, or PAF resulted in inhibition of lipopolysaccharide-stimulated cytokine production. Furthermore, anti-TGF-beta antibody, indomethacin, or PAF receptor antagonists restored cytokine production in lipopolysaccharide-stimulated macrophages that had phagocytosed apoptotic cells. These results suggest that binding and/or phagocytosis of apoptotic cells induces active antiinflammatory or suppressive properties in human macrophages. Therefore, it is likely that resolution of inflammation depends not only on the removal of apoptotic cells but on active suppression of inflammatory mediator production. Disorders in either could result in chronic inflammatory diseases.",1998,52,2972,148,14,56,76,101,106,145,123,126,152,145
12a1245fef2f0fa02260ee1514c924acfc04bb4b,THE CONTEXT AND IMPORTANCE OF INVENTORY MANAGEMENT AND PRODUCTION PLANNING AND SCHEDULING. The Importance of Inventory Management and Production Planning and Scheduling. Strategic Issues. Frameworks for Inventory Management and Production Planning and Scheduling. Forecasting. TRADITIONAL REPLENISHMENT SYSTEMS FOR MANAGING INDIVIDUAL--ITEM INVENTORIES. Order Quantities When Demand is Approximately Level. Lot Sizing for Individual Items with Time--Varying Demand. Individual Items with Probabilistic Demand. SPECIAL CLASSES OF ITEMS. Managing the Most Important (Class A) Inventories. Managing Routine (Class C) Inventories. Style Goods and Perishable Items. THE COMPLEXITIES OF MULTIPLE ITEMS AND MULTIPLE LOCATIONS. Coorinated Replenishments at a Single Stocking Point. Supply Chain Management and Multiechelon Inventories. PRODUCTION PLANNING AND SCHEDULING. An Overall Framework for Production Planning and Scheduling. Medium--Range Aggregate Production Planning. Material Requirements Planning and its Extensions. Just--in--Time and Optimized Production Technology. Short--Range Production Scheduling. Summary. Appendices. Indexes.,1998,1,2628,186,3,19,43,65,80,94,100,128,136,156
653019d2f88c9102b5a01a530b18e4a9ff61d82f,"Notes on contributors Acknowledgements 1. The Idiom of Co-production Sheila Jasanoff 2. Ordering Knowledge, Ordering Society Sheila Jasanoff 3. Climate Science and the Making of a Global Political Order Clark A. Miller 4. Co-producing CITES and the African Elephant Charis Thompson 5. Knowledge and Political Order in the European Environment Agency Claire Waterton and Brian Wynne 6. Plants, Power and Development: Founding the Imperial Department of Agriculture for the West Indies, 1880-1914 William K. Storey 7. Mapping Systems and Moral Order: Constituting property in genome laboratories Stephen Hilgartner 8. Patients and Scientists in French Muscular Dystrophy Research Vololona Rabeharisoa and Michel Callon 9. Circumscribing Expertise: Membership categories in courtroom testimony Michael Lynch 10. The Science of Merit and the Merit of Science: Mental order and social order in early twentieth-century France and America John Carson 11. Mysteries of State, Mysteries of Nature: Authority, knowledge and expertise in the seventeenth century Peter Dear 12. Reconstructing Sociotechnical Order: Vannevar Bush and US science policy Michael Aaron Dennis 13. Science and the Political Imagination in Contemporary Democracies Yaron Ezrah 14. Afterword Sheila Jasanoff References Index",2004,0,2822,151,5,12,31,52,73,97,116,136,154,170
784b1ecff1021f33344fae1bb2e43cb26fa187c8,"Identifying and building a sustainable energy system are perhaps two of the most critical issues that today's society must address. Replacing our current energy carrier mix with a sustainable fuel is one of the key pieces in that system. Hydrogen as an energy carrier, primarily derived from water, can address issues of sustainability, environmental emissions, and energy security. Issues relating to hydrogen production pathways are addressed here. Future energy systems require money and energy to build. Given that the United States has a finite supply of both, hard decisions must be made about the path forward, and this path must be followed with a sustained and focused effort.",2004,8,3755,12,2,11,27,27,43,23,44,64,79,117
cc44a5f5387b5bfe375f566cc396ad20ff0c36a4,,1997,0,2390,251,6,6,12,29,35,50,76,68,79,109
28ef2fb8d1c03235801489d68244fa8184381a19,"Biodiesel is gaining more and more importance as an attractive fuel due to the depleting fossil fuel resources. Chemically biodiesel is monoalkyl esters of long chain fatty acids derived from renewable feed stock like vegetable oils and animal fats. It is produced by transesterification in which, oil or fat is reacted with a monohydric alcohol in presence of a catalyst. The process of transesterification is affected by the mode of reaction condition, molar ratio of alcohol to oil, type of alcohol, type and amount of catalysts, reaction time and temperature and purity of reactants. In the present paper various methods of preparation of biodiesel with different combination of oil and catalysts have been described. The technical tools and processes for monitoring the transesterification reactions like TLC, GC, HPLC, GPC, 1H NMR and NIR have also been summarized. In addition, fuel properties and specifications provided by different countries are discussed.",2006,57,3117,101,15,56,98,170,209,231,244,312,287,259
3d1b77f644961a31a5508d6423575c702049a7ef,Translatora s Acknowledgements. 1. Plan of the Present Work. 2. Social Space. 3. Spatial Architectonics. 4. From Absolute Space to Abstract Space. 5. Contradictory Space. 6. From the Contradictions of Space to Differential Space. 7. Openings and Conclusions. Afterword by David Harvey. Index.,1992,0,3858,12,2,16,18,13,18,29,38,54,24,48
066a12b69280f53cc074d13ac47ab1d369fbcde8,"Solid lipid nanoparticles (SLN) have attracted increasing attention during recent years. This paper presents an overview about the selection of the ingredients, different ways of SLN production and SLN applications. Aspects of SLN stability and possibilities of SLN stabilization by lyophilization and spray drying are discussed. Special attention is paid to the relation between drug incorporation and the complexity of SLN dispersions, which includes the presence of alternative colloidal structures (liposomes, micelles, drug nanosuspensions, mixed micelles, liquid crystals) and the physical state of the lipid (supercooled melts, different lipid modifications). Appropriate analytical methods are needed for the characterization of SLN. The use of several analytical techniques is a necessity. Alternative structures and dynamic phenomena on the molecular level have to be considered. Aspects of SLN administration and the in vivo fate of the carrier are discussed.",2001,124,2565,130,2,14,24,30,38,43,62,65,101,106
70b5a5f938996e66f1f4851b4bcb726dce4acec4,* Starting from Need* Evolution of the Toyota Production System* Further Development* Genealogy of the Toyota Production System* The True Intention of the Ford System* Surviving the Low-Growth Period,1988,0,1964,290,0,0,0,1,0,2,0,3,4,4
3c9dc71914a48574811458ce0056831a0fa31f52,"Part 1 The field of cultural production: the field of cultural production, or - the economic world reversed the production of belief - contribution to an economy of symbolic goods the market of symbolic goods. Part 2 Flaubert and the French literary field: is the structure of ""sentimental education"" an instance of social self-analysis? field of power, literary field and habitus principles for a sociology of cultural works Flaubert's point of view. Part 3 The pure gaze - essays on art: outline of a sociological theory of art perception the institutionalization of Anomie the historical genesis of a pure aesthetic.",1993,0,2598,162,0,2,5,4,12,10,10,26,34,47
fd671723ea64a0fdfc216d09197e887972104744,"IL-10 inhibits the ability of macrophage but not B cell APC to stimulate cytokine synthesis by Th1 T cell clones. In this study we have examined the direct effects of IL-10 on both macrophage cell lines and normal peritoneal macrophages. LPS (or LPS and IFN-gamma)-induced production of IL-1, IL-6, and TNF-alpha proteins was significantly inhibited by IL-10 in two macrophage cell lines. Furthermore, IL-10 appears to be a more potent inhibitor of monokine synthesis than IL-4 when added at similar concentrations. LPS or LPS- and IFN-gamma-induced expression of IL-1 alpha, IL-6, or TNF-alpha mRNA was also inhibited by IL-10 as shown by semiquantitative polymerase chain reaction or Northern blot analysis. Inhibition of LPS-induced IL-6 secretion by IL-10 was less marked in FACS-purified peritoneal macrophages than in the macrophage cell lines. However, IL-6 production by peritoneal macrophages was enhanced by addition of anti-IL-10 antibodies, implying the presence in these cultures of endogenous IL-10, which results in an intrinsic reduction of monokine synthesis after LPS activation. Consistent with this proposal, LPS-stimulated peritoneal macrophages were shown to directly produce IL-10 detectable by ELISA. Furthermore, IFN-gamma was found to enhance IL-6 production by LPS-stimulated peritoneal macrophages, and this could be explained by its suppression of IL-10 production by this same population of cells. In addition to its effects on monokine synthesis, IL-10 also induces a significant change in morphology in IFN-gamma-stimulated peritoneal macrophages. The potent action of IL-10 on the macrophage, particularly at the level of monokine production, supports an important role for this cytokine not only in the regulation of T cell responses but also in acute inflammatory responses.",1991,0,2934,92,0,26,54,73,90,114,111,158,169,151
3354c8ce02811f46e500e7787b3d4acd567ee453,"Abstract Our research addresses the confusion and inconsistency associated with “lean production.” We attempt to clarify the semantic confusion surrounding lean production by conducting an extensive literature review using a historical evolutionary perspective in tracing its main components. We identify a key set of measurement items by charting the linkages between measurement instruments that have been used to measure its various components from the past literature, and using a rigorous, two-stage empirical method and data from a large set of manufacturing firms, we narrow the list of items selected to represent lean production to 48 items, empirically identifying 10 underlying components. In doing so, we map the operational space corresponding to conceptual space surrounding lean production. Configuration theory provides the theoretical underpinnings and helps to explain the synergistic relationships among its underlying components.",2007,84,1950,185,1,15,35,69,91,128,152,175,159,153
bc3a9a223aa89a85232fab3d5b3eda3525af0be7,"The reaction centers of PSI and PSII in chloroplast thylakoids are the major generation site of reactive oxygen species (ROS). Photoreduction of oxygen to hydrogen peroxide (H2O2) in PSI was discovered over 50 years ago by [Mehler (1951)][1]. Subsequently, the primary reduced product was identified",2006,52,2114,189,2,38,62,78,98,92,125,135,178,187
eda66985466dac06ea756d7a5e0336f95c8c81de,"The cosmic ray flux increases at higher altitude as air pressure and the shielding effect of the atmosphere decrease. Altitude-dependent scaling factors are required to compensate for this effect in calculating cosmic ray exposure ages. Scaling factors in current use assume a uniform relationship between altitude and atmospheric pressure over the Earth's surface. This masks regional differences in mean annual pressure and spatial variation in cosmogenic isotope production rates. Outside Antarctica, air pressures over land depart from the standard atmosphere by ±4.4 hPa (1σ) near sea level, corresponding to offsets of ±3–4% in isotope production rates. Greater offsets occur in regions of persistent high and low pressure such as Siberia and Iceland, where conventional scaling factors predict production rates in error by ±10%. The largest deviations occur over Antarctica where ground level pressures are 20–40 hPa lower than the standard atmosphere at all altitudes. Isotope production rates in Antarctica are therefore 25–30% higher than values calculated by scaling Northern Hemisphere production rates with conventional scaling factors. Exposure ages of old Antarctic surfaces, especially those based on cosmogenic radionuclides at levels close to saturation, may be millions of years younger than published estimates.",2000,27,1908,397,1,10,16,23,37,45,78,83,72,97
7af58e0d3f3f6b7d3c498849914d30b2cca8b995,"Although the concept of justification has played a significant role in many social psychological theories, its presence in recent examinations of stereotyping has been minimal. We describe and evaluate previous notions of stereotyping as ego-justification and group-justification and propose an additional account, that of system-justification, which refers to psychological processes contributing to the preservation of existing social arrangements even at the expense of personal and group interest. It is argued that the notion of system-justification is necessary to account for previously unexplained phenomena, most notably the participation by disadvantaged individuals and groups in negative stereotypes of themselves, and the consensual nature of stereotypic beliefs despite differences in social relations within and between social groups. We offer a selective review of existing research that demonstrates the role of stereotypes in the production of false consciousness and develop the implications of a system-justification approach. 
 
[T]he rationalizing and justifying function of a stereotype exceeds its function as a reflector of group attributes—G. W. Allport (1958, p. 192).",1994,211,2486,133,1,5,9,8,17,28,28,37,53,45
70cf53a7e288df25c39c9d212eae3cf4452a3a26,This paper first sets out the main features of the eclectic theory of international production and then seeks to evaluate its significance of ownership- and location- specific variables in explaining the industrial pattern and geographical distribution of the sales of U.S. affiliates in fourteen manufacturing industries in seven countries in 1970.,1980,29,2711,170,0,4,2,2,3,2,4,1,5,5
9af8e429d9c65fb0b2ca18f5bc721426b1f432e2,Each of a collection of items are to be produced on two machines (or stages). Each machine can handle only one item at a time and each item must be processed through machine one and then through machine two. The setup time plus work time for each item for each machine is known. A simple decision rule is obtained in this paper for the optimal scheduling of the production so that the total elapsed time is a minimum. A three‐machine problem is also discussed and solved for a restricted case.,1954,0,3052,262,3,0,4,2,2,4,4,3,1,0
5fbe31a3c6c44df33eb245f591f43a3c33b1fb66,"The concept sectoral system of innovation and production provides a multidimensional, integrated and dynamic view of sectors. It is proposed that a sectoral system is a set of products and the set of agents carrying out market and non-market interactions for the creation, production and sale of those products. A sectoral systems has a specific knowledge base, technologies, inputs and demand. Agents are individuals and organizations at various levels of aggregation. They interact through processes of communication, exchange, co-operation, competition and command, and these interactions are shaped by institutions. A sectoral system undergoes change and transformation through the co-evolution of its various elements. © 2002 Elsevier Science B.V. All rights reserved.",2002,143,2378,161,20,26,46,43,63,78,110,116,131,156
ec2ca0daeeae2a4a1023369ac6b5df8a4e6fd4c9,"The capacity of 12 cytokines to induce NO2- or H2O2 release from murine peritoneal macrophages was tested by using resident macrophages, or macrophages elicited with periodate, casein, or thioglycollate broth. Elevated H2O2 release in response to PMA was observed in resident macrophages after a 48-h incubation with IFN-gamma, TNF-alpha, TNF-beta, or CSF-GM. Of these, only IFN-gamma induced substantial NO2- secretion during the culture period. The cytokines inactive in both assays under the conditions tested were IL-1 beta, IL-2, IL-3, IL-4, IFN-alpha, IFN-beta, CSF-M, and transforming growth factor-beta 1. Incubation of macrophages with IFN-gamma for 48 h in the presence of LPS inhibited H2O2 production but augmented NO2- release, whereas incubation in the presence of the arginine analog NG-monomethylarginine inhibited NO2- release but not H2O2 production. Although neither TNF-alpha nor TNF-beta induced NO2- synthesis on its own, addition of either cytokine together with IFN-gamma increased macrophage NO2- production up to six-fold over that in macrophages treated with IFN-gamma alone. Moreover, IFN-alpha or IFN-beta in combination with LPS could also induce NO2- production in macrophages, as was previously reported for IFN-gamma plus LPS. These data suggest that: 1) tested as a sole agent, IFN-gamma was the only one of the 12 cytokines capable of inducing both NO2- and H2O2 release; 2) the pathways leading to secretion of H2O2 and NO2- are independent; 3) either IFN-gamma and TNF-alpha/beta or IFN-alpha/beta/gamma and LPS can interact synergistically to induce NO2- release.",1988,0,2800,57,1,9,19,40,55,83,109,143,162,137
4fed748c8cd0adfd353c5ba79ab666e33242125b,"Global industrialization is the result of an integrated system of production and trade. Open international trade has encouraged nations to specialize in different branches of manufacturing and even in different stages of production within a specific industry. This process, fueled by the explosion of new products and new technologies since World War II, has led to the emergence of a global manufacturing system in which production capacity is dispersed to an unprecedented number of developing as well as industrialized countries (Harris, 1987; Gereffi, 1989b). The revolution in transportation and communications technology has permitted manufacturers and retailers alike to establish international production and trade networks that cover vast geographical distances. While considerable attention has been given to the involvement of industrial capital in international contracting, the key role played by commercial capital (i.e., large retailers and brand-named companies that buy but don't make the goods they sell) in the expansion of manufactured exports from developing countries has been relatively ignored. This chapter will show how these ‘big buyers’ have shaped the production networks established in the world's most dynamic exporting countries, especially the newly industrialized countries (NICs) of East Asia. The argument proceeds in several stages. First, a distinction is made between producer-driven and buyer-driven commodity chains, which represent alternative modes of organizing international industries. These commodity chains, though primarily controlled by private economic agents, are also influenced by state policies in both the producing (exporting) and consuming (importing) countries. Second, the main organizational features of buyer-driven commodity chains are identified, using the apparel industry as a case study. The apparel commodity chain contains two very different segments. The companies that make and sell standardized clothing have production patterns and sourcing strategies that contrast with firms in the fashion segment of the industry, which has been the most actively committed to global sourcing. Recent changes within the retail sector of the United States are analyzed in this chapter to identify the emergence of new types of big buyers and to show why they have distinct strategies of global sourcing. Third, the locational patterns of global sourcing in apparel are charted, with an emphasis on the production frontiers favored by different kinds of US buyers. Several of the primary mechanisms used by big buyers to source products from overseas are outlined in order to demonstrate how transnational production systems are sustained and altered by American retailers and branded apparel companies.",1994,0,2111,226,1,2,9,8,10,16,33,39,55,59
b76c7f7247cf206ad373e30a94bc1679e3b0fab4,"Abstract Currently, hydrogen is primarily used in the chemical industry, but in the near future it will become a significant fuel. There are many processes for hydrogen production. This paper reviews the technologies related to hydrogen production from both fossil and renewable biomass resources including reforming (steam, partial oxidation, autothermal, plasma, and aqueous phase) and pyrolysis. In addition, electrolysis and other methods for generating hydrogen from water, hydrogen storage related approaches, and hydrogen purification methods such as desulfurization and water-gas-shift are discussed.",2009,238,2221,58,24,91,122,124,152,179,190,212,210,208
711dc5324ab6d95cdd1e4a7ed3f4acd64ce2abec,"Lignocelluloses are often a major or sometimes the sole components of different waste streams from various industries, forestry, agriculture and municipalities. Hydrolysis of these materials is the first step for either digestion to biogas (methane) or fermentation to ethanol. However, enzymatic hydrolysis of lignocelluloses with no pretreatment is usually not so effective because of high stability of the materials to enzymatic or bacterial attacks. The present work is dedicated to reviewing the methods that have been studied for pretreatment of lignocellulosic wastes for conversion to ethanol or biogas. Effective parameters in pretreatment of lignocelluloses, such as crystallinity, accessible surface area, and protection by lignin and hemicellulose are described first. Then, several pretreatment methods are discussed and their effects on improvement in ethanol and/or biogas production are described. They include milling, irradiation, microwave, steam explosion, ammonia fiber explosion (AFEX), supercritical CO2 and its explosion, alkaline hydrolysis, liquid hot-water pretreatment, organosolv processes, wet oxidation, ozonolysis, dilute-and concentrated-acid hydrolyses, and biological pretreatments.",2008,194,2245,102,2,21,51,113,179,247,221,212,244,211
6773dc7c6565385873961d8f45a22d40f12ff78f,"Regression methods were used to select and score 12 items from the Medical Outcomes Study 36-Item Short-Form Health Survey (SF-36) to reproduce the Physical Component Summary and Mental Component Summary scales in the general US population (n=2,333). The resulting 12-item short-form (SF-12) achieved multiple R squares of 0.911 and 0.918 in predictions of the SF-36 Physical Component Summary and SF-36 Mental Component Summary scores, respectively. Scoring algorithms from the general population used to score 12-item versions of the two components (Physical Components Summary and Mental Component Summary) achieved R squares of 0.905 with the SF-36 Physical Component Summary and 0.938 with SF-36 Mental Component Summary when cross-validated in the Medical Outcomes Study. Test-retest (2-week)correlations of 0.89 and 0.76 were observed for the 12-item Physical Component Summary and the 12-item Mental Component Summary, respectively, in the general US population (n=232). Twenty cross-sectional and longitudinal tests of empirical validity previously published for the 36-item short-form scales and summary measures were replicated for the 12-item Physical Component Summary and the 12-item Mental Component Summary, including comparisons between patient groups known to differ or to change in terms of the presence and seriousness of physical and mental conditions, acute symptoms, age and aging, self-reported 1-year changes in health, and recovery for depression. In 14 validity tests involving physical criteria, relative validity estimates for the 12-item Physical Component Summary ranged from 0.43 to 0.93 (median=0.67) in comparison with the best 36-item short-form scale. Relative validity estimates for the 12-item Mental Component Summary in 6 tests involving mental criteria ranged from 0.60 to 107 (median=0.97) in relation to the best 36-item short-form scale. Average scores for the 2 summary measures, and those for most scales in the 8-scale profile based on the 12-item short-form, closely mirrored those for the 36-item short-form, although standard errors were nearly always larger for the 12-item short-form.",1996,22,13313,1015,0,0,0,1,0,0,1,0,0,0
96131e374caf8b0b4cf7e205246d9feeb69d09a4,"A general and systematic account of the role of knowledge in society aimed to stimulate both critical discussion and empirical investigations. This book is concerned with the sociology of 'everything that passes for knowledge in society'. It focuses particularly on that 'common-sense knowledge' which constitutes the reality of everyday life for the ordinary member of society. The authors are concerned to present an analysis of knowledge in everyday life in the context of a theory of society as a dialectical process between objective and subjective reality. Their development of a theory of institutions, legitimations and socializations has implications beyond the discipline of sociology, and their 'humanistic' approach has considerable relevance for other social scientists, historians, philosophers and anthropologists.",1967,3,16016,699,0,0,0,1,0,0,0,0,0,0
d92f735b0773b4e697e7e72798eccae2f647acd6,"We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.",1987,36,11953,518,0,0,2,3,2,2,7,4,3,4
3e7856e631a849861a997d0e011dde348ebd1b90,"We describe a new basis for the construction of a genetic linkage map of the human genome. The basic principle of the mapping scheme is to develop, by recombinant DNA techniques, random single-copy DNA probes capable of detecting DNA sequence polymorphisms, when hybridized to restriction digests of an individual's DNA. Each of these probes will define a locus. Loci can be expanded or contracted to include more or less polymorphism by further application of recombinant DNA technology. Suitably polymorphic loci can be tested for linkage relationships in human pedigrees by established methods; and loci can be arranged into linkage groups to form a true genetic map of ""DNA marker loci."" Pedigrees in which inherited traits are known to be segregating can then be analyzed, making possible the mapping of the gene(s) responsible for the trait with respect to the DNA marker loci, without requiring direct access to a specified gene's DNA. For inherited diseases mapped in this way, linked DNA marker loci can be used predictively for genetic counseling.",1980,81,7266,779,2,11,25,36,55,76,95,111,101,95
bf0d895386ca9ab0b0a3d803a0358e548f7be8ab,"At the heart of emotion, mood, and any other emotionally charged event are states experienced as simply feeling good or bad, energized or enervated. These states--called core affect--influence reflexes, perception, cognition, and behavior and are influenced by many causes internal and external, but people have no direct access to these causal connections. Core affect can therefore be experienced as free-floating (mood) or can be attributed to some cause (and thereby begin an emotional episode). These basic processes spawn a broad framework that includes perception of the core-affect-altering properties of stimuli, motives, empathy, emotional meta-experience, and affect versus emotion regulation; it accounts for prototypical emotional episodes, such as fear and anger, as core affect attributed to something plus various nonemotional processes.",2003,303,4174,424,6,39,62,64,95,118,170,190,210,248
d74f541b21645166d2c6ca0c22185b50a1091cd7,"Although the methodological literature is replete with advice regarding the development and validation of multi-item scales based on reflective measures, the issue of index construction using formative measures has received little attention. The authors seek to address this gap by (1) examining the nature of formative indicators, (2) discussing ways in which the quality of formative measures can be assessed, and (3) illustrating the proposed procedures with empirical data. The aim is to enhance researchers' understanding of formative measures and assist them in their index construction efforts.",2001,66,3982,283,4,11,22,46,69,110,159,185,196,290
6ef3da9d9c5a7e2f96a1470bad413c857f64aa95,"You can use this book to design a house for yourself with your family; you can use it to work with your neighbors to improve your town and neighborhood; you can use it to design an office, or a workshop, or a public building. And you can use it to guide you in the actual process of construction. After a ten-year silence, Christopher Alexander and his colleagues at the Center for Environmental Structure are now publishing a major statement in the form of three books which will, in their words, ""lay the basis for an entirely new approach to architecture, building and planning, which will we hope replace existing ideas and practices entirely."" The three books are The Timeless Way of Building, The Oregon Experiment, and this book, A Pattern Language. At the core of these books is the idea that people should design for themselves their own houses, streets, and communities. This idea may be radical (it implies a radical transformation of the architectural profession) but it comes simply from the observation that most of the wonderful places of the world were not made by architects but by the people. At the core of the books, too, is the point that in designing their environments people always rely on certain ""languages,"" which, like the languages we speak, allow them to articulate and communicate an infinite variety of designs within a forma system which gives them coherence. This book provides a language of this kind. It will enable a person to make a design for almost any kind of building, or any part of the built environment. ""Patterns,"" the units of this language, are answers to design problems (How high should a window sill be? How many stories should a building have? How much space in a neighborhood should be devoted to grass and trees?). More than 250 of the patterns in this pattern language are given: each consists of a problem statement, a discussion of the problem with an illustration, and a solution. As the authors say in their introduction, many of the patterns are archetypal, so deeply rooted in the nature of things that it seemly likely that they will be a part of human nature, and human action, as much in five hundred years as they are today.",1981,0,4407,308,1,2,0,1,4,0,2,7,3,4
1ba843ad1c318113a78f6e66af22c4c99eb52937,"It is widely known that when there are negative moving average errors, a high order augmented autoregression is necessary for unit root tests to have good size, but that information criteria such as the AIC and BIC tend to select a truncation lag that is very small. Furthermore, size distortions increase with the number of deterministic terms in the regression. We trace these problems to the fact that information criteria omit important biases induced by a low order augmented autoregression. We consider a class of Modified Information Criteria (MIC) which account for the fact that the bias in the sum of the autoregressive coefficients is highly dependent on the lag order k. Using a local asymptotic framework in which the root of an MA(1) process is local to -1, we show that the MIC allows for added dependence between k and the number of deterministic terms in the regression. Most importantly, the k selected by the recommended MAIC is such that both its level and rate of increase with the sample size are desirable for unit root tests in the local asymptotic framework, whereas the AIC, MBIC and especially the BIC are less attractive in at least one dimension. In monte-carlo experiments, the MAIC is found to yield huge size improvements to the DF(GLS) and the feasible point optimal P(t) test developed in Elliot, Rothenberg and Stock (1996). We also extend the M tests developed in Perron and Ng (1996) to allow for GLS detrending of the data. The M(GLS) tests are shown to have power functions that lie very close to the power envelope. In addition, we recommend using GLS detrended data to estimate the required autoregressive spectral density at frequency zero. This provides more efficient estimates on the one hand, and ensures that the estimate of the spectral density is invariant to the parameters of the deterministic trend function, a property not respected by the estimation procedure currently employed by several studies. The MAIC along with GLS detrended data yield a set of Mbar(GLS) tests with desirable size and power properties.",2001,32,3764,442,14,19,37,68,115,169,155,169,231,239
ba16facb32fec367cc0f4c9f1e04e80943f2c89a,"class does not exist either, precisely since the relation between the two shadows compared is not a relation of simple comparison and common appurtenance to the same totality, but of substantial participation. The shadow perceived on the table is therefore no more an isolable object than is, on the sensorimotor plane, the watch which disappears under one cushion and which the child expects to see appear under another. But if there is thus an apparent return to the past it is for an opposite reason to that which obstructs objectification in sensorimotor intelligence; in the latter case the object is difficult to form in proportion as the child has difficulty in intercoordinating perceptual images, whereas on the plane of conceptual thought the object, already elaborated, again loses its identity to the extent that it is coordinated with other objects to construct a class or a relation. In conclusion, in the case of the object as in that of space, from the very beginnings of verbal reflection there is a return of the difficulties already overcome on the plane of action, and there is repetition, with temporal displacements, of the stages and process of adaptation defined by the transition from egocentrism to objectivity. And in both cases the phenomenon is due to the difficulties experienced by the child, after he has reached the social plane, in inserting his sensorimotor acquisitions in a framework of relationships of logical classes and deductive structures admitting of true generalisation, that is, taking into account the point of view of others and all possible points of view as well as his own. § 4. From Sensori-Motor Universe to Representation of the Child’s World II. Causality and Time The development of causality from the first months of life to the eleventh or twelfth year reveals the same graphic curve as that of space or object. The acquisition of causality seems to be completed with the formation of sensorimotor intelligence; in the measure that objectification and spatialisation of relations of cause and effect succeed the magico-phenomenalistic egocentrism of the primitive connections, a whole evolution resumes with the advent of speech and representative thought which seems to reproduce the preceding evolution before really extending it. But among the displacements to which this history of the concept of cause gives rise, distinction must again be made between the simple temporal displacements in extension due to the repetition of primitive processes on the occasion of new problems analogous to old ones, and the temporal displacements in comprehension due to the transition from one plane of activity to another; that is, from the plane of action to that of representation. It seems useless to us to emphasise the former. Nothing is more natural than the fact that belief in the efficacy of personal activity, a belief encouraged by chance comparisons through immediate or phenomenalistic experience, is again found throughout childhood in those moments of anxiety or of desire which characterise infantile magic. The second type of temporal displacements, however, raises questions which it is useful to mention here. During the first months of life the child does not dissociate the external world from his own activity. Perceptual images, not yet consolidated into objects or coordinated in a coherent space, seem to him to be governed by his desires and efforts, though these are not attributed to a self which is separate from the universe. Then gradually, as progress is made in the intelligence which elaborates objects and space by spinning a tight web of relations among these images, the child attributes an autonomous causality to things and persons and conceives of the existence of causal relations independent of himself, his own body becoming a source among other sources of effects integrated in this total system. What will happen when, through speech and representative thought, the subject succeeds not only in foreseeing the development of phenomena and in acting upon them but in evoking them apart from any action in order to try to explain them? It is here that the paradox of displacement in comprehension appears. By virtue of the ""why"" obsessing the child’s mind, as soon as his representation of the world can be detached without too much risk of error, one perceives that this universe, centred on the self, which seemed abolished because it was eliminated from practical action relating to the immediate environment, reappears on the plane of thought and impresses itself on the little child as the sole understandable conception of totality. Undoubtedly the child no longer behaves, as did the baby, as though he commanded everything and everybody. He knows that adults have their own will, that the rain, wind, clouds, stars, and all things are characterised by movements and effects he undergoes but cannot control. In short, on the practical plane, the objectification and spatialisation of causality remain acquired. But this does not at all prevent the child from representing the universe to himself as a large machine, organised exactly by whom he does not know, but organised with the help of adults and for the sake of the well-being of men and particularly of children. Just as in a house everything is arranged according to a plan, despite imperfections and partial failures, so also the raison d’être for everything in the physical universe is the function of a sort of order in the world, an order both material and moral, of which the child is the center. Adults are there ""to take care of us,"" animals to do us service, the stars to warm us and give us light, plants to nourish us, rain to make the gardens grow, clouds to ""make night,"" mountains to climb on, and lakes for boats, etc. Furthermore, to this more or less explicit and coherent artificialism there corresponds a latent animism which endows everything with the will to play its role and with just the force and awareness needed to act with regularity. Thus the causal egocentrism, which on the sensorimotor plane disappears gradually under the influence of spatialisation and objectification, reappears from the time of the beginnings of thought in almost as radical a form. Doubtless the child no longer attributes personal causality to others or to things, but while endowing objects with specific activities he centers all these activities on man and above all on himself. It seems clear that in this sense we may speak of temporal displacement from one plane to another and that the phenomenon is thus comparable to the phenomena which characterise the evolution of space and object. But it is in a still deeper sense that the primitive schemata of causality are again transposed in the child’s first reflective representations. If it is true that from the second year of life the child attributes causality to others and to objects instead of reserving a monopoly on them for his own activity, we have still to discover how he represents to himself the mechanism of these causal relations. We have just recalled that corresponding to the egocentric artificialism which makes the universe gravitate around man and child is an animism capable of explaining the activity of creatures and things in this sort of world. This example is precisely of a kind to help us understand the second kind of temporal displacement of which we now speak: if the child renounces considering his actions as the cause of every event, he nevertheless is unable to represent to himself the action of bodies except by means of schemata drawn from his own activity. An object animated by a ""natural"" movement like the wind which pushes clouds, or the moon which advances, thus seems endowed with purposefulness and finality, for the child is unable to conceive of an action without a conscious goal. Through lack of awareness, every process involving a relation of energies, such as the rising of the water level in a glass in which a pebble has been dropped, seems due to forces copied from the model of personal activity; the pebble ""weighs"" on the bottom of the water, it ""forces"" the water to rise, and if one held the pebble on a string midway of the column of the water the level would not change. In short, even though there is objectivity on the practical plane, causality may remain egocentric from the representative point of view to the extent that the first causal conceptions are drawn from the completely subjective consciousness of the activity of the self. With regard to spatialisation of the causal connection the same temporal displacement between representation and action is observable. Thus the child can acknowledge in practice the necessity for a spatial contact between cause and effect, but that does not make causality geometric or mechanical. For example, the parts of a bicycle all seem necessary to the child long before he thinks of establishing irreversible causal series among them. However, subsequent to these primitive stages of representation during which one sees reappear on the plane of thought forms of causality relative to those of the first sensorimotor stages and which seem surpassed by the causal structures of the final stages of sensorimotor intelligence, one witnesses a truly reflective objectification and spatialisation, whose progress is parallel to that which we have described on the plane of action. Thus it is that subsequent to the animism and dynamism we have just mentioned, we see a gradual ""mechanism"" taking form, correlative to the principles of conservation described in § 3 and to the elaboration of a relative space. Causality, like the other categories, therefore evolves on the plane of thought from an initial egocentrism to a combined objectivity and relativity, thus reproducing, in surpassing, its earlier sensorimotor evolution. With regard to time, concerning which we have tried to describe on the purely practical plane of the first two years of life the transformation from ",1954,0,5793,316,0,1,3,2,5,2,9,8,5,9
8ad0cdfe7bcca856bc2e12a26fbd8e736bb58164,"Surely since the Enlightenment, if not before, the study of mind has centered principally on how man achieves a ""true"" knowledge of the world. Emphasis in this pursuit has varied, of course: empiricists have concentrated on the mind's interplay with an external world of nature, hoping to find the key in the association of sensations and ideas, while rationalists have looked inward to the powers of mind itself for the principles of right reason. The objective, in either case, has been to discover how we achieve ""reality,"" that is to say, how we get a reliable fix on the world, a world that is, as it were, assumed to be immutable and, as it were, ""there to be observed."" This quest has, of course, had a profound effect on the development of psychology, and the empiricist and rationalist traditions have dominated our conceptions of how the mind grows and how it gets its grasp on the ""real world."" Indeed, at midcentury Gestalt theory represented the rationalist wing of this enterprise and American learning theory the empiricist. Both gave accounts of mental development as proceeding in some more or less linear and uniform fashion from an initial incompetence in grasping reality to a final competence, in one case attributing it to the working out of internal processes or mental organization, and in the other to some unspecified principle of reflection by which—whether through reinforcement, association, or conditioning—we came to respond to the world ""as it is."" There have always been dissidents who",1991,5,3805,260,0,3,7,15,13,11,15,26,32,19
3f1611bc35d9260251922b6d0774744c5f623da2,,1999,5,3291,306,19,44,61,82,90,113,136,139,142,162
549c188319903b825ba415466c0bb7b76833826d,"Self-efficacy theory asserts that personal mastery expectations are the primary determinants of behavioral change. Further, it is suggested that individual differences in past experiences and attribution of success to skill or chance result in different levels of generalized self-efficacy expectations. To measure these generalized expectancies, a Self-efficacy Scale was developed. A factor analysis yielded two subscales: a General Self-efficacy subscale (17 items) and a Social Self-efficacy subscale (6 items). Confirmation of several predicted conceptual relationships between the Self-efficacy subscales and other personality measures (i.e., Locus of Control, Personal Control, Social Desirability, Ego Strength, Interpersonal Competence, and Self-esteem) provided evidence of construct validity. Positive relationships between the Self-efficacy Scale and vocational, educational, and military success established criterion validity. Future research and clinical uses of the scale were discussed.",1982,16,3454,300,1,2,3,4,7,5,11,10,10,19
0a897f0e7adc1dfd39ff5fdb2cab46ecf08e5015,"It has been proposed that gene-regulatory circuits with virtually any desired property can be constructed from networks of simple regulatory elements. These properties, which include multistability and oscillations, have been found in specialized gene circuits such as the bacteriophage λ switch and the Cyanobacteria circadian oscillator. However, these behaviours have not been demonstrated in networks of non-specialized regulatory components. Here we present the construction of a genetic toggle switch—a synthetic, bistable gene-regulatory network—in Escherichia coli and provide a simple theory that predicts the conditions necessary for bistability. The toggle is constructed from any two repressible promoters arranged in a mutually inhibitory network. It is flipped between stable states using transient chemical or thermal induction and exhibits a nearly ideal switching threshold. As a practical device, the toggle switch forms a synthetic, addressable cellular memory unit and has implications for biotechnology, biocomputing and gene therapy.",2000,41,3839,179,20,36,44,57,91,105,139,163,159,196
6886b2f45b1b26f8e96fe09b8ccb6728333bc9ea,"CHAPTER 1: SOCIAL CONSTRUCTION - FROM ""WHAT IS"" TO ""WHAT COULD BE"" Together We Construct our Worlds The Social Origins of the Real and the Good Grounding Dialogues on Social Construction From Deconstruction to Reconstruction Reflective Pragmatism: The Working Vocabularies of the World CHAPTER 2: CONSTRUCTING THE REAL AND THE GOOD The Language We Live By Everyday Conversation: The Power of the Unremarkable Institutional Realities: Foucault on Power Identity Politics: To Be or Not To Be CHAPTER 3: HORIZONS OF HUMAN ENQUIRY From Empiricism to Constructionism Research Traditions in Transformation Discourse Study: Exploring Constructed Worlds Imagination in Action: Qualitative Enquiry CHAPTER 4: THE RELATIONAL SELF Generative Theory Individualism: Separation and its Discontents Self as Relationship: First Steps Self as Relationship: The Emerging Vision Mind as Relational Action Multi-Being: What Shall We Become Together? CHAPTER 5: DIALOGUE - CONFLICT AND TRANSFORMATION Exploring Dialogue: Key Concepts Dialogue and Difference Toward Transformative Dialogue CHAPTER 6: EDUCATION AS RELATIONAL PROCESS Knowledge as Socially Constructed Education as Relational Process The Challenge of Student Evaluation CHAPTER 7: THE HELPING PROFESSIONS - CO-CONSTRUCTION IN ACTION Therapy as Social Construction Collaborative Means to Human Well-being Meaning and Medicine CHAPTER 8: MAKING MEANING IN ORGANIZATIONS Organizing Through Language Relational Organizing: Key to the Future The Organization as System CHAPTER 9: SOCIAL CONSTRUCTION IN QUESTION Realism: ""But there is a World Out There!"" The Challenge of Moral Relativism Missing Ingredients: The Body and Power The Challenge of Moral Relativism",1999,0,3186,348,0,19,60,70,82,119,145,143,160,165
e8c14469c1910d35be3e3d7576d9ccc344c78be7,The age-classified matrix model stage-classified life cycles stage-classified matrix models analysis of the life-cycle graph sensitivity analysis and evolutionary demography statistical inference time-varying and stochastic models density-dependent models two-sex models.,2001,0,3104,692,46,74,115,118,135,176,165,184,186,187
28028e186ff5f566e69bf62cbe5f7346c0d7690d,"This highly original work presents laboratory science in a deliberately skeptical way: as an anthropological approach to the culture of the scientist. Drawing on recent work in literary criticism, the authors study how the social world of the laboratory produces papers and other ""texts,""' and how the scientific vision of reality becomes that set of statements considered, for the time being, too expensive to change. The book is based on field work done by Bruno Latour in Roger Guillemin's laboratory at the Salk Institute and provides an important link between the sociology of modern sciences and laboratory studies in the history of science.",1979,0,5312,190,0,0,0,1,0,0,0,1,1,7
11d6ee80673f4cf867d0efd432f5f0eb44603b15,"Often lost in the debate over the validity of social construction is the question of what is being constructed. Particularly troublesome in this area is the status of the natural sciences, where there is conflict between biological and social approaches to mental illness, and in other areas. Ian Hacking looks at the issue of child abuse, and examines the ways in which advanced research on new weapons influences not the content but the form of science. In conclusion, Hacking comments on the ""culture wars"" in anthropology, in particular the spat between leading enthnographers over Hawaii and Captain Cook.",1999,0,3441,179,8,32,52,69,136,153,125,140,154,166
a068df4c5f829bd0b85bf72fd919006c563c6345,SummaryAn optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.,2006,3,3985,254,147,181,158,168,171,174,198,204,191,212
13241a844c714549c173e239714ae020386172e3,"The authors describe a model of autobiographical memory in which memories are transitory mental constructions within a self-memory system (SMS). The SMS contains an autobiographical knowledge base and current goals of the working self. Within the SMS, control processes modulate access to the knowledge base by successively shaping cues used to activate autobiographical memory knowledge structures and, in this way, form specific memories. The relation of the knowledge base to active goals is reciprocal, and the knowledge base ""grounds"" the goals of the working self. It is shown how this model can be used to draw together a wide range of diverse data from cognitive, social, developmental, personality, clinical, and neuropsychological autobiographical memory research.",2000,307,3151,301,7,26,41,51,76,64,96,114,142,161
8f31845b80b0fb9d19a22f7558197c266e0875d4,,1978,0,3473,235,1,2,7,3,6,7,8,14,12,13
8ea4b7b71afb190a8df4726578734c6c7769ee1b,"Publikacijoje apžvelgiama žinojimo sociologijos disciplinos raida, pateikiamos svarbiausios jos nagrinėjamos sąvokos ir tyrimo tikslai. Teigiama, kad tikrovė yra socialiskai konstruojama ir kad žinojimo sociologija turi analizuoti sio konstravimo procesus. Ji turi aiskinti ne tik empirine žinojimo įvairove visuomenėse, bet taip pat ir procesus, dėl kurių bet kuris žinojimas tampa socialiskai pripažinta tikrove. K. Marxo tezė, kad žmogaus sąmone apsprendžia jo socialinė būtis, tapo bazine žinojimo sociologijos teze. Terminą „žinojimo sociologija“įvedė M. Scheleris. Jis teigė, kad visuomenė lemia idėjų būtį, bet ne jų prigimtį ir pabrėžė individualaus žmogiskojo žinojimo aprioriskumą, kuris prasmės sistemą įgyja visuomenėje. K. Mannheimas teigė, kad visuomenė sąlygoja ne tik žmogiskosios idealizacijos formą, bet ir turinį. Jam svarbiausias buvo ideologijos reiskinys. Skyrė partikuliarinės, totalinės ir bendrosios ideologijos sąvokas. R. Mertonas siekė sujungti žinojimo sociologijos ir struktūrinės funkcinės teorijos pozicijas. Autoriai isplecia sios sociolgijos tyrimo objektą teigdami, kad ji turi tirti ne tik idėjų istoriją, bet viską, kas visuomenėje laikoma žinojimu.",1967,26,10013,12,0,0,0,8,19,26,46,26,46,35
6a997f8c84d8031bc101ebd40497ad27a31a900c,"Between the 1960s and 1980s, most life scientists focused their attention on studies of nucleic acids and the translation of the coded information. Protein degradation was a neglected area, considered to be a nonspecific, dead-end process. Although it was known that proteins do turn over, the large extent and high specificity of the process, whereby distinct proteins have half-lives that range from a few minutes to several days, was not appreciated. The discovery of the lysosome by Christian de Duve did not significantly change this view, because it became clear that this organelle is involved mostly in the degradation of extracellular proteins, and their proteases cannot be substrate specific. The discovery of the complex cascade of the ubiquitin pathway revolutionized the field. It is clear now that degradation of cellular proteins is a highly complex, temporally controlled, and tightly regulated process that plays major roles in a variety of basic pathways during cell life and death as well as in health and disease. With the multitude of substrates targeted and the myriad processes involved, it is not surprising that aberrations in the pathway are implicated in the pathogenesis of many diseases, certain malignancies, and neurodegeneration among them. Degradation of a protein via the ubiquitin/proteasome pathway involves two successive steps: 1) conjugation of multiple ubiquitin moieties to the substrate and 2) degradation of the tagged protein by the downstream 26S proteasome complex. Despite intensive research, the unknown still exceeds what we currently know on intracellular protein degradation, and major key questions have remained unsolved. Among these are the modes of specific and timed recognition for the degradation of the many substrates and the mechanisms that underlie aberrations in the system that lead to pathogenesis of diseases.",2002,622,3801,170,18,141,186,215,227,231,228,242,194,212
2b80a43a758d78ff339d68b8d48b8533a2bb066d,"The need for an integrated social constructivist approach towards the study of science and technology is outlined. Within such a programme both scientific facts and technological artefacts are to be understood as social constructs. Literature on the sociology of science, the science-technology relationship, and technology studies is reviewed. The empirical programme of relativism within the sociology of scientific knowledge and a recent study of the social construction of technological artefacts are combined to produce the new approach. The concepts of `interpretative flexibility' and `closure mechanism', and the notion of `social group' are developed and illustrated by reference to a study of solar physics and a study of the development of the bicycle. The paper concludes by setting out some of the terrain to be explored in future studies.",1984,62,3453,152,2,2,6,7,13,6,8,14,19,24
289d3a9562f57d0182d1aae9376b0e3793d80272,"Publisher Summary This chapter discusses data concerning the time course of word identification in a discourse context. A simulation of arithmetic word-problem understanding provides a plausible account for some well-known phenomena. The current theories use representations with several mutually constraining layers. There is typically a linguistic level of representation, conceptual levels to represent both the local and global meaning and structure of a text, and a level at which the text itself has lost its individuality and its information content. Knowledge provides part of the context within which a discourse interpreted. The integration phase is the price the model pays for the necessary flexibility in the construction process.",1988,104,3029,210,5,43,33,45,43,71,31,51,55,59
d7b86ad1e746746156e7ffa33ed65f81e748da05,"Contemporary Issues and Historical Perspectives. The Normative Development of Self-representations during Childhood. The Normative Development of Self-representations during Adolescence. The Developmental Emergence of Self-conscious Emotions. The Content, Valence, and Organization of Self-evaluative Judgments. Discrepancies between Real and Ideal Self-concepts. Social Sources of Individual Differences in Self-evaluation. A Model of the Causes, Correlates, and Consequences of Global Self-worth. The Authenticity of the Self. The Effects of Child Abuse on I-self and Me-self Processes. Autonomy and Connectedness as Dimensions of the Self. Interventions to Promote Adaptive Self-evaluations.",2001,0,2552,333,37,58,78,67,100,117,123,152,134,162
4fa67c6925cdf6954e53d2b6f0fd455829080157,"A field study of 29 resource-constrained firms that varied dramatically in their responses to similar objective environments is used to examine the process by which entrepreneurs in resource-poor environments were able to render unique services by recombining elements at hand for new purposes that challenged institutional definitions and limits. We found that Lévi-Strauss's concept of bricolage—making do with what is at hand—explained many of the behaviors we observed in small firms that were able to create something from nothing by exploiting physical, social, or institutional inputs that other firms rejected or ignored. We demonstrate the socially constructed nature of resource environments and the role of bricolage in this construction. Using our field data and the existing literature on bricolage, we advance a formal definition of entrepreneurial bricolage and induce the beginnings of a process model of bricolage and firm growth. Central to our contribution is the notion that companies engaging in bricolage refuse to enact the limitations imposed by dominant definitions of resource environments, suggesting that, for understanding entrepreneurial behavior, a constructivist approach to resource environments is more fruitful than objectivist views.",2005,128,2486,234,1,10,41,39,77,91,90,130,177,195
463c58c96dc4b9c69d82ff764241ca2efa94b957,"This short treatise looks at how we construct a social reality from our sense impressions; at how, for example, we construct a five-pound note with all that implies in terms of value and social meaning, from the printed piece of paper we see and touch. In ""The Construction of Social Reality,"" eminent philosopher John Searle examines the structure of social reality (or those portions of the world that are facts only by human agreement, such as money, marriage, property, and government), and contrasts it to a brute reality that is independent of human agreement. Searle shows that brute reality provides the indisputable foundation for all social reality, and that social reality, while very real, is maintained by nothing more than custom and habit.""",1997,0,3250,86,37,52,47,59,66,107,126,133,98,133
d2ac1de069931abed0dd047e26374a02107c123d,"In vitro recombination techniques were used to construct a new cloning vehicle, pBR322. This plasmid, derived from pBR313, is a relaxed replicating plasmid, does not produce and is sensitive to colicin E1, and carries resistance genes to the antibiotics ampicillin (Ap) and tetracycline (Tc). The antibiotic-resistant genes on pBR322 are not transposable. The vector pBR322 was constructed in order to have a plasmid with a single PstI site, located in the ampicillin-resistant gene (Apr), in addition to four unique restriction sites, EcoRI, HindIII, BamHI and SalI. Survival of Escherichia coli strain X1776 containing pBR313 and pBR322 as a function of thymine and diaminopimelic acid (DAP) starvation and sensitivity to bile salts was found to be equivalent to the non-plasmid containing strain. Conjugal transfer of these plasmids in bi- and triparental matings were significantly reduced or undetectable relative to the plasmid ColE1.",1977,1,4609,26,3,40,118,185,188,188,222,231,223,226
2f8b034ac986d4007fa4801312ffbb2dbcb556c2,"We examine the issue of determining an acceptable minimum embedding dimension by looking at the behavior of near neighbors under changes in the embedding dimension from d\ensuremath{\rightarrow}d+1. When the number of nearest neighbors arising through projection is zero in dimension ${\mathit{d}}_{\mathit{E}}$, the attractor has been unfolded in this dimension. The precise determination of ${\mathit{d}}_{\mathit{E}}$ is clouded by ``noise,'' and we examine the manner in which noise changes the determination of ${\mathit{d}}_{\mathit{E}}$. Our criterion also indicates the error one makes by choosing an embedding dimension smaller than ${\mathit{d}}_{\mathit{E}}$. This knowledge may be useful in the practical analysis of observed time series.",1992,4,3078,136,1,7,13,13,22,35,45,34,49,67
98f39777985726e110037f6633d5b7581da72850,"Strategies for theory construction in nursing , Strategies for theory construction in nursing , کتابخانه دیجیتال جندی شاپور اهواز",1983,0,2090,434,0,2,0,3,5,2,4,8,5,7
613460a5ca7de5867cf068a2f7f9f5c15e960b36,,1967,27,3209,91,7,15,15,17,29,33,23,25,19,30
0c8ec6db3c5b78de4ea6d607529bb6015dd4df28,"Vision is critical for the functional and structural maturation of connections in the mammalian visual system. Visual experience, however, is a subset of a more general requirement for neural activity in transforming immature circuits into the organized connections that subserve adult brain function. Early in development, internally generated spontaneous activity sculpts circuits on the basis of the brain's “best guess” at the initial configuration of connections necessary for function and survival. With maturation of the sense organs, the developing brain relies less on spontaneous activity and increasingly on sensory experience. The sequential combination of spontaneously generated and experience-dependent neural activity endows the brain with an ongoing ability to accommodate to dynamically changing inputs during development and throughout life.",1996,117,2817,104,1,31,94,126,111,136,112,143,127,150
29c180990c1d744170fd25011c95f5b48990bfca,,1968,13,3015,156,0,0,2,3,2,4,6,2,1,2
be0f68b0ff1420844ceec6a3790eb6959d5c5bf1,"Abstract The purpose of this study was to develop a scale for measuring celebrity endorsers' perceived expertise, trustworthiness, and attractiveness. Accepted psychometric scale-development procedures were followed which rigorously tested a large pool of items for their reliability and validity. Using two exploratory and two confirmatory samples, the current research developed a 15-item semantic differential scale to measure perceived expertise, trustworthiness, and attractiveness. The scale was validated using respondents' self-reported measures of intention to purchase and perception of quality for the products being tested. The resulting scale demonstrated high reliability and validity.",1990,58,2070,336,0,1,0,2,4,5,7,7,4,4
49ba8e1045460dcd811900e2eb56cb5264dfbc85,"The concept of consumer ethnocentrism is introduced and a corresponding measure, the CETSCALE, is formulated and validated. Four separate studies provide support for the CETSCALE's reliability and ...",1987,28,2058,309,0,1,5,0,3,3,9,9,4,18
fd632793b0c514bdbccd73d174d7a35ba4f23e8d,"We present the lifting scheme, a simple construction of second generation wavelets; these are wavelets that are not necessarily translates and dilates of one fixed function. Such wavelets can be adapted to intervals, domains, surfaces, weights, and irregular samples. We show how the lifting scheme leads to a faster, in-place calculation of the wavelet transform. Several examples are included.",1998,179,2376,140,30,34,53,69,76,81,89,119,115,128
ed6d04529554135a479dedb017586d484a133ab8,"We present an automatic method for docking organic ligands into protein binding sites. The method can be used in the design process of specific protein ligands. It combines an appropriate model of the physico-chemical properties of the docked molecules with efficient methods for sampling the conformational space of the ligand. If the ligand is flexible, it can adopt a large variety of different conformations. Each such minimum in conformational space presents a potential candidate for the conformation of the ligand in the complexed state. Our docking method samples the conformation space of the ligand on the basis of a discrete model and uses a tree-search technique for placing the ligand incrementally into the active site. For placing the first fragment of the ligand into the protein, we use hashing techniques adapted from computer vision. The incremental construction algorithm is based on a greedy strategy combined with efficient methods for overlap detection and for the search of new interactions. We present results on 19 complexes of which the binding geometry has been crystallographically determined. All considered ligands are docked in at most three minutes on a current workstation. The experimentally observed binding mode of the ligand is reproduced with 0.5 to 1.2 A rms deviation. It is almost always found among the highest-ranking conformations computed.",1996,89,2317,122,0,15,23,29,39,43,46,75,86,98
50e75209f684b0407d0eae8044f51d38e439e6d9,"This paper reports on the key features of the “Guide for the Design and Construction of Externally Bonded FRP Systems for Strengthening Concrete Structures” issued by the American Concrete Institute (ACI). Fiber reinforced polymers (FRP) systems have emerged as an alternative to traditional materials and techniques for the strengthening of existing concrete structures to resist higher design loads, correct deterioration-related damage, design or construction error, or increase ductility. Structural elements strengthened with externally bonded FRP systems include beams, slabs, columns, walls, joints/connections, chimneys and smokestacks, vaults, domes, tunnels, silos, pipes, and trusses. Externally bonded FRP systems have also been used to strengthen masonry, timber, steel, and cast-iron structures.",2005,115,2786,13,67,69,108,95,112,114,164,172,240,215
b0a2f40fb592f1c6ad47f4f92ab0c1f50eb96dd6,"A control circuit for a fuel ignition system of the direct ignition type for providing an interlock on start-up to prevent the energization of a fuel supply valve of the system under certain failure conditions includes an interlock relay which is energized, after a predetermined delay, over normally closed contacts of a flame relay to control a timing network to cause a flame sensing circuit to enable the flame relay to energize the valve during a trial for ignition interval defined by the timing network, the flame sensing circuit maintaining the flame relay, and thus the valve, energized when a flame is provided during the trial for ignition interval, the energization of the valve and the interlock relay being prevented if the normally closed contacts of the flame relay are open on start-up, and the delayed operation of the interlock relay preventing lockout of the system following momentary loss of power.",1996,82,2378,142,17,29,64,67,99,88,104,81,112,125
71355b129cc754f142cfb7f1f22e8fc40287cd4c,"Between 1973 and 1978, the Indonesian Government constructed over 61,000 primary schools throughout the country. This is one of the largest school construction programs on record. I evaluate the effect of this program on education and wages by combining differences across regions in the number of schools constructed with differences across cohorts induced by the timing of the program. The estimates suggest that the construction of primary schools led to an increase in education and earnings. Children ages 2 to 6 in 1974 received 0.12 to 0.19 more years of education for each school constructed per 1,000 children in their region of birth. Using the variations in schooling generated by this policy as instrumental variables for the impact of education on wages generates estimates of economic returns to education ranging from 6.8 percent to 10.6 percent.",2000,43,1898,217,7,15,21,37,32,40,51,80,82,75
3d61a1f799e002f83dae783bebbf9a1d0c020ec9,"The seemingly innocent observation that the activities of organisms bring about changes in environments is so obvious that it seems an unlikely focus for a new line of thinking about evolution. Yet niche construction - as this process of organism-driven environmental modification is known - has hidden complexities. By transforming biotic and abiotic sources of natural selection in external environments, niche construction generates feedback in evolution on a scale hitherto underestimated - and in a manner that transforms the evolutionary dynamic. It also plays a critical role in ecology, supporting ecosystem engineering and influencing the flow of energy and nutrients through ecosystems. Despite this, niche construction has been given short shrift in theoretical biology, in part because it cannot be fully understood within the framework of standard evolutionary theory. Wedding evolution and ecology, this book extends evolutionary theory by formally including niche construction and ecological inheritance as additional evolutionary processes. The authors support their historic move with empirical data, theoretical population genetics, and conceptual models. They also describe new research methods capable of testing the theory. They demonstrate how their theory can resolve long-standing problems in ecology, particularly by advancing the sorely needed synthesis of ecology and evolution, and how it offers an evolutionary basis for the human sciences. Already hailed as a pioneering work by some of the world's most influential biologists, this is a rare, potentially field-changing contribution to the biological sciences.",2003,0,2155,119,11,43,50,76,107,99,112,127,160,115
dbef6e605e236df3fdfb8ac0cf14eb442a6ec73c,"This pioneering book, first published in 1987, launched the new field of social studies of technology. It introduced a method of inquiry--social construction of technology, or SCOT--that became a key part of the wider discipline of science and technology studies. The book helped the MIT Press shape its STS list and inspired the Inside Technology series. The thirteen essays in the book tell stories about such varied technologies as thirteenth-century galleys, eighteenth-century cooking stoves, and twentieth-century missile systems. Taken together, they affirm the fruitfulness of an approach to the study of technology that gives equal weight to technical, social, economic, and political questions, and they demonstrate the illuminating effects of the integration of empirics and theory. The approaches in this volume--collectively called SCOT (after the volume’s title) have since broadened their scope, and twenty-five years after the publication of this book, it is difficult to think of a technology that has not been studied from a SCOT perspective and impossible to think of a technology that cannot be studied that way.",1989,8,2640,53,3,6,12,15,12,20,22,23,25,38
da040c1ba2ccded11d6d4f8879b7c690f5b1b955,Preface From Individual Knowledge to Communal Construction The Impasse of Individual Knowledge Crisis in Representation and the Emergence of Social Construction Constructionism in Question Social Construction and Moral Orders Criticism and Consequence Social Psychology and the Wrong Revolution The Cultural Consequences of Deficit Discourse Objectivity as Rhetorical Achievement From Self to Relationship Self-Narration in Social Life Emotion as Relationship Transcending Narrative in the Therapeutic Context The Communal Origins of Meaning Deceit: From Conscience to Community Notes References Index,1994,0,1939,173,2,2,21,24,33,39,40,81,62,82
78e6c78ebb00d7626557caff7f7544f6157716bc,Acknowledgments 1: Introduction 2: The Interaction between Verbs and Constructions 3: Relations among Constructions 4: On Linking 5: Partial Productivity 6: The English Ditransitive Construction 7: The English Caused-Motion Construction 8: The English Resultative Construction 9: The Way Construction 10: Conclusion Notes Bibliography Index,1995,0,2063,160,3,3,17,21,29,28,49,51,74,59
3521a3d3438c11cc545f6a9c122957f019546872,"ing WS1S Systems to Verify Parameterized Networks . . . . . . . . . . . . 188 Kai Baukus, Saddek Bensalem, Yassine Lakhnech and Karsten Stahl FMona: A Tool for Expressing Validation Techniques over Infinite State Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 J.-P. Bodeveix and M. Filali Transitive Closures of Regular Relations for Verifying Infinite-State Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 Bengt Jonsson and Marcus Nilsson Diagnostic and Test Generation Using Static Analysis to Improve Automatic Test Generation . . . . . . . . . . . . . 235 Marius Bozga, Jean-Claude Fernandez and Lucian Ghirvu Efficient Diagnostic Generation for Boolean Equation Systems . . . . . . . . . . . . 251 Radu Mateescu Efficient Model-Checking Compositional State Space Generation with Partial Order Reductions for Asynchronous Communicating Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266 Jean-Pierre Krimm and Laurent Mounier Checking for CFFD-Preorder with Tester Processes . . . . . . . . . . . . . . . . . . . . . . . 283 Juhana Helovuo and Antti Valmari Fair Bisimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 Thomas A. Henzinger and Sriram K. Rajamani Integrating Low Level Symmetries into Reachability Analysis . . . . . . . . . . . . . 315 Karsten Schmidt Model-Checking Tools Model Checking Support for the ASM High-Level Language . . . . . . . . . . . . . . 331 Giuseppe Del Castillo and Kirsten Winter Table of",2001,0,1990,97,30,44,47,60,66,63,88,77,79,92
f645238c6b9414d21c0211efab4ca4ae4a15b220,,2005,0,1400,261,8,11,29,33,42,54,66,98,82,121
5ce7ed3be7a06e1d91d799b9dbb866756283c05e,"Genetic manipulation of Candida albicans is constrained by its diploid genome and asexual life cycle. Recessive mutations are not expressed when heterozygous and undesired mutations introduced in the course of random mutagenesis cannot be removed by genetic back-crossing. To circumvent these problems, we developed a genotypic screen that permitted identification of a heterozygous recessive mutation at the URA3 locus. The mutation was introduced by targeted mutagenesis, homologous integration of transforming DNA, to avoid introduction of extraneous mutations. The ura3 mutation was rendered homozygous by a second round of transformation resulting in a Ura- strain otherwise isogenic with the parental clinical isolate. Subsequent mutation of the Ura- strain was achieved by targeted mutagenesis using the URA3 gene as a selectable marker. URA3 selection was used repeatedly for the sequential introduction of mutations by flanking the URA3 gene with direct repeats of the Salmonella typhimurium hisG gene. Spontaneous intrachromosomal recombination between the flanking repeats excised the URA3 gene restoring a Ura- phenotype. These Ura- segregants were selected on 5-fluoroorotic acid-containing medium and used in the next round of mutagenesis. To permit the physical mapping of disrupted genes, the 18-bp recognition sequence of the endonuclease I-SceI was incorporated into the hisG repeats. Site-specific cleavage of the chromosome with I-SceI revealed the position of the integrated sequences.",1993,5,1699,228,2,6,23,19,38,49,60,58,68,94
b978f372ba65014d60cee1f30a6fd9931a25a849,"People can consciously re-experience past events and pre-experience possible future events. This fMRI study examined the neural regions mediating the construction and elaboration of past and future events. Participants were cued with a noun for 20s and instructed to construct a past or future event within a specified time period (week, year, 5-20 years). Once participants had the event in mind, they made a button press and for the remainder of the 20s elaborated on the event. Importantly, all events generated were episodic and did not differ on a number of phenomenological qualities (detail, emotionality, personal significance, field/observer perspective). Conjunction analyses indicated the left hippocampus was commonly engaged by past and future event construction, along with posterior visuospatial regions, but considerable neural differentiation was also observed during the construction phase. Future events recruited regions involved in prospective thinking and generation processes, specifically right frontopolar cortex and left ventrolateral prefrontal cortex, respectively. Furthermore, future event construction uniquely engaged the right hippocampus, possibly as a response to the novelty of these events. In contrast to the construction phase, elaboration was characterized by remarkable overlap in regions comprising the autobiographical memory retrieval network, attributable to the common processes engaged during elaboration, including self-referential processing, contextual and episodic imagery. This striking neural overlap is consistent with findings that amnesic patients exhibit deficits in both past and future thinking, and confirms that the episodic system contributes importantly to imagining the future.",2007,92,1547,116,31,49,90,79,104,123,129,118,135,126
a98a2bac5241ed13d3592a8e618f6316b58190a8,"How do we construct national identities in discourse? Which topics, which discursive strategies and which linguistic devices are employed to construct national sameness and uniqueness on the one hand, and differences to other national collectives on the other hand? The Discursive Construction of National Identity analyses discourses of national identity in Europe with particular attention to Austria. In the tradition of critical discourse analysis, the authors analyse current and on-going transformations in the self-and other definition of national identities using an innovative interdisciplinary approach which combines discourse-historical theory and methodology and political science perspectives. Thus, the rhetorical promotion of national identification and the discursive construction and reproduction of national difference on public, semi-public and semi-private levels within a nation state are analysed in much detail and illustrated with a huge amount of examples taken from many genres (speeches, focus-groups, interviews, media, and so forth). In addition to the critical discourse analysis of multiple genres accompanying various commemorative and celebratory events in 1995, this extended and revised edition is able to draw comparisons with similar events in 2005. The impact of socio-political changes in Austria and in the European Union is also made transparent in the attempts of constructing hegemonic national identities. Key Features: *Discourse-historical approach. *Interdisciplinarity (cultural studies, discourse analysis, history, political science). *Multi-method, multi-genre. *Qualitative case studies.",2009,7,1196,139,53,61,68,74,97,95,100,94,124,124
313b7af851fc0283df23122320bfc77f45cad5f7,"We present Neighbor-Net, a distance based method for constructing phylogenetic networks that is based on the Neighbor-Joining (NJ) algorithm of Saitou and Nei. Neighbor-Net provides a snapshot of the data that can guide more detailed analysis. Unlike split decomposition, Neighbor-Net scales well and can quickly produce detailed and informative networks for several hundred taxa. We illustrate the method by reanalyzing three published data sets: a collection of 110 highly recombinant Salmonella multi-locus sequence typing sequences, the 135 ""African Eve"" human mitochondrial sequences published by Vigilant et al., and a collection of 12 Archeal chaperonin sequences demonstrating strong evidence for gene conversion. Neighbor-Net is available as part of the SplitsTree4 software package.",2002,67,1596,188,0,2,16,32,46,48,61,65,84,90
57c2cdd004ba193e2a8307dbb821eb036776583e,"Early research on online self-presentation mostly focused on identity constructions in anonymous online environments. Such studies found that individuals tended to engage in role-play games and anti-normative behaviors in the online world. More recent studies have examined identity performance in less anonymous online settings such as Internet dating sites and reported different findings. The present study investigates identity construction on Facebook, a newly emerged nonymous online environment. Based on content analysis of 63 Facebook accounts, we find that the identities produced in this nonymous environment differ from those constructed in the anonymous online environments previously reported. Facebook users predominantly claim their identities implicitly rather than explicitly; they ""show rather than tell"" and stress group and consumer identities over personally narrated ones. The characteristics of such identities are described and the implications of this finding are discussed.",2008,37,1557,98,1,17,50,75,132,140,179,187,157,174
90af586c8c237cbecfd20a6c5a68e3f6952c0673,"The web of human relations which originates, nurtures, and transforms technologies has long deserved attention. Computers, bicycles, natural gas pipelines, and condoms live and have their being in the midst of enormously complicated human networks of production, distribution, and evaluation. We need to know about the kinds of social institutions that do this work. This book, a report on a conference, goes far in opening this vast area for public inspection.",1989,1,2382,62,6,9,23,26,32,30,40,39,57,47
910701a8f21a0afa9a1de6b0a8c4b4ad44423793,"Dowe really knowwhatwewant?Ormustwe sometimes construct our preferences on the spot, using whatever cues are available – even when these cues lead us astray? One of the main themes that has emerged from behavioral decision research during the past three decades is the view that people’s preferences are often constructed in the process of elicitation. This idea is derived from studies demonstrating that normatively equivalent methods of elicitation (e.g., choice and pricing) give rise to systematically different responses. These preference reversals violate the principle of procedure invariance that is fundamental to all theories of rational choice. If different elicitation procedures produce different orderings of options, how can preferences be defined and in what sense do they exist? This book shows not only the historical roots of preference construction but also the blossoming of the conceptwithin psychology, law,marketing, philosophy, environmental policy, and economics. Decision making is now understood to be a highly contingent form of information processing, sensitive to task complexity, time pressure, response mode, framing, reference points, and other contextual factors.",1995,49,2028,62,3,6,14,15,21,32,31,72,55,44
0b3c644209d2baf2fd45cbddd79760343e230120,"Abstract A survey on time performance of different types of construction projects in Saudi Arabia was conducted to determine the causes of delay and their importance according to each of the project participants, i.e., the owner, consultant and the contractor. The field survey conducted included 23 contractors, 19 consultants, and 15 owners. Seventy-three causes of delay were identified during the research. 76% of the contractors and 56% of the consultants indicated that average of time overrun is between 10% and 30% of the original duration. The most common cause of delay identified by all the three parties is “change order”. Surveys concluded that 70% of projects experienced time overrun and found that 45 out of 76 projects considered were delayed.",2006,7,1394,129,0,6,20,17,23,39,72,101,106,163
761ab1a352eb93adb742e62076890189fc8bbeb2,"We argue that the social construction of target populations is an important, albeit overlooked, political phenomenon that should take its place in the study of public policy by political scientists. The theory contends that social constructions influence the policy agenda and the selection of policy tools, as well as the rationales that legitimate policy choices. Constructions become embedded in policy as messages that are absorbed by citizens and affect their orientations and participation. The theory is important because it helps explain why some groups are advantaged more than others independently of traditional notions of political power and how policy designs reinforce or alter such advantages. An understanding of social constructions of target populations augments conventional hypotheses about the dynamics of policy change, the determination of beneficiaries and losers, the reasons for differing levels and types of participation among target groups, and the role of policy in democracy.",1993,51,1769,147,5,1,13,4,9,13,10,17,16,23
b01526fe142da20a714c2d3d4148cdb2d5497a41,"a b s t r a c t At the beginning of the 21st century, a new social arrangement of work poses a series of questions and challenges to scholars who aim to help people develop their working lives. Given the globalization of career counseling, we decided to address these issues and then to formulate potentially innovative responses in an international forum. We used this approach to avoid the difficulties of creating models and methods in one country and then trying to export them to other countries where they would be adapted for use. This article presents the initial outcome of this collaboration, a counseling model and methods. The life-designing model for career intervention endorses five presuppositions about people and their work lives: contextual possibilities, dynamic processes, non-linear progression, multiple perspectives, and personal patterns. Thinking from these five presuppositions, we have crafted a contextualized model based on the epistemology of social constructionism, particularly recognizing that an individual’s knowledge and identity are the product of social interaction and that meaning is co-constructed through discourse. The life-design",2009,72,1260,109,4,22,45,99,91,113,124,117,138,159
f5276ee10f01035b8358d2fd650e53df6c7c85a2,"This article focuses on the construction, directly in physical space, of simply parametrized covariance functions for data-assimilation applications. A self-contained, rigorous mathematical summary of relevant topics from correlation theory is provided as a foundation for this construction. Covariance and correlation functions are defined, and common notions of homogeneity and isotropy are clarified. Classical results are stated, and proven where instructive. Included are smoothness properties relevant to multivariate statistical-analysis algorithms where wind/wind and wind/mass correlation models are obtained by differentiating the correlation model of a mass variable. the Convolution Theorem is introduced as the primary tool used to construct classes of covariance and cross-covariance functions on three-dimensional Euclidean space R3. Among these are classes of compactly supported functions that restrict to covariance and cross-covariance functions on the unit sphere S2, and that vanish identically on subsets of positive measure on S2. It is shown that these covariance and cross-covariance functions on S2, referred to as being space-limited, cannot be obtained using truncated spectral expansions. Compactly supported and space-limited covariance functions determine sparse covariance matrices when evaluated on a grid, thereby easing computational burdens in atmospheric data-analysis algorithms. 
 
Convolution integrals leading to practical examples of compactly supported covariance and cross-covariance functions on R3 are reduced and evaluated. More specifically, suppose that gi and gj are radially symmetric functions defined on R3 such that gi(x) = 0 for |x| > di and gj(x) = 0 for |xv > dj, O di + dj and |x - y| > 2di, respectively, Additional covariance functions on R3 are constructed using convolutions over the real numbers R, rather than R3. Families of compactly supported approximants to standard second- and third-order autoregressive functions are constructed as illustrative examples. Compactly supported covariance functions of the form C(x,y) := Co(|x - y|), x,y ∈ R3, where the functions Co(r) for r ∈ R are 5th-order piecewise rational functions, are also constructed. These functions are used to develop space-limited product covariance functions B(x, y) C(x, y), x, y ∈ S2, approximating given covariance functions B(x, y) supported on all of S2 × S2.",1999,48,1568,183,5,5,4,14,17,19,37,34,52,53
63af0e90729f983539a0eaba62eaf04b61926a70,"Several methods have been previously used to approximate free boundaries in finite-difference numerical simulations. A simple, but powerful, method is described that is based on the concept of a fractional volume of fluid (VOF). This method is shown to be more flexible and efficient than other methods for treating complicated free boundary configurations. To illustrate the method, a description is given for an incompressible hydrodynamics code, SOLA-VOF, that uses the VOF technique to track free fluid surfaces.",1981,35,11265,505,0,0,0,0,0,0,0,0,0,0
b904e9b7cd539936a2058f623a06c8fea10651c8,"This brief paper derives Euler’s equations for an inviscid fluid, summarizes the Cauchy momentum equation, derives the Navier-Stokes equation from that, and then talks about finite difference method approaches to solutions. Typical texts for this material are apparently Acheson, Elementary Fluid Dynamics and Landau and Lifschitz, Fluid Mechanics. 1. Basic Definitions We describe a fluid flow in three-dimensional space R as a vector field representing the velocity at all locations in the fluid. Concretely, then, a fluid flow is a function ~v : R× R → R that assigns to each point (t, ~x) in spacetime a velocity ~v(t, ~x) in space. In the special situation where ~v does not depend on t we say that the flow is steady. A trajectory or particle path is a curve ~x : R→ R such that for all t ∈ R, d dt ~x(t) = ~v(t, ~x(t)). Fix a t0 ∈ R; a streamline at time t0 is a curve ~x : R→ R such that for all t ∈ R, d dt ~x(t) = ~v(t0, ~x(t)). In the special case of steady flow the streamlines are constant across times t0 and any trajectory is a streamline. In non-steady flows, particle paths need not be streamlines. Consider the 2-dimensional example ~v = [− sin t cos t]>. At t0 = 0 the velocities all point up and the streamlines are vertical straight lines. At t0 = π/2 the velocities all point left and the streamlines are horizontal straight lines. Any trajectory is of the form ~x = [cos t + C1 sin t + C2] >; this traces out a radius-1 circle centered at [C1 C2] >. Indeed, all radius-1 circles in the plane arise as trajectories. These circles cross each other at many (in fact, all) points. If you find it counterintuitive that distinct trajectories can pass through a single point, remember that they do so at different times. 2. Acceleration Let f : R × R → R be some scalar field (such as temperature). Then ∂f/∂t is the rate of change of f at some fixed point in space. If we precompose f with a 1 Fluid Dynamics Math 211, Fall 2014, Carleton College trajectory ~x, then the chain rule gives us the rate of change of f with respect to time along that curve: D Dt f := d dt f(t, x(t), y(t), z(t)) = ∂f ∂t + ∂f ∂x dx dt + ∂f ∂y dy dt + ∂f ∂z dz dt = ( ∂ ∂t + dx dt ∂ ∂x + dy dt ∂ ∂y + dz dt ∂ ∂z ) f = ( ∂ ∂t + ~v · ∇ ) f. Intuitively, if ~x describes the trajectory of a small sensor for the quantity f (such as a thermometer), then Df/Dt gives the rate of change of the output of the sensor with respect to time. The ∂f/∂t term arises because f varies with time. The ~v ·∇f term arises because f is being measured at varying points in space. If we apply this idea to each component function of ~v, then we obtain an acceleration (or force per unit mass) vector field ~a(t, x) := D~v Dt = ∂~v ∂t + (~v · ∇)~v. That is, for any spacetime point (t, ~x), the vector ~a(t, ~x) is the acceleration of the particle whose trajectory happens to pass through ~x at time t. Let’s check that it agrees with our usual notion of acceleration. Suppose that a curve ~x describes the trajectory of a particle. The acceleration should be d dt d dt~x. By the definition of trajectory, d dt d dt ~x = d dt ~v(t, ~x(t)). The right-hand side is precisely D~v/Dt. Returning to our 2-dimensional example ~v = [− sin t cos t]>, we have ~a = [− cos t − sin t]>. Notice that ~v · ~a = 0. This is the well-known fact that in constant-speed circular motion the centripetal acceleration is perpendicular to the velocity. (In fact, the acceleration of any constant-speed trajectory is perpendicular to its velocity.) 3. Ideal Fluids An ideal fluid is one of constant density ρ, such that for any surface within the fluid the only stresses on the surface are normal. That is, there exists a scalar field p : R × R → R, called the pressure, such that for any surface element ∆S with outward-pointing unit normal vector ~n, the force exerted by the fluid inside ∆S on the fluid outside ∆S is p~n ∆S. The constant density condition implies that the fluid is incompressible, meaning ∇ · ~v = 0, as follows. For any region of space R, the rate of flow of mass out of the region is ∫∫ ∂R ρ~v · ~n dS = ∫∫∫",1968,0,10113,994,0,0,0,0,0,0,1,21,29,25
2d4f6af07a0bb851ecd0f3e2243676518dc555b8,"Ludwig Krinner (Dated: November 5th 2012) Abstract This is a script made with the help of Landau Lifshitz, Book VI [1] on fluid mechanics, that gives a short introduction to basic fluid mechanics. This short script includes, various equations of continuity, Eulers equation for motion of nonviscous fluid, gravitational waves in nonviscous fluids, Navier-Stokes Equation for viscous fluids, viscous flow within a pipe, some turbulence and laminar wake (can be used to calculate the lift of a wing), all along with some basic thermodynamics and vector calculus. Plagiarism: For creation of this script we have closely oriented ourselves towards Laundau Lifschitz Course in theoretical physics Vol 6 Fluid Mechanics. This is no original work! Even though we have tried to write everyting in our own words (in order to have to understand everything), most of the formulas are similar or equal to the book, while some of the steps which seemed too fast are supplemented by own “fill in” calculations. I will not cite anything properly, since this is not an official paper or homework or term paper, and since I basically cite every line of calculation and from the content also every line of text.",1980,4,7139,953,6,4,4,11,6,8,7,6,4,11
08236a76e2355ee36c154f51b6b2159e054a3191,*Introduction. *Conservation Laws of Fluid Motion and Boundary Conditions. *Turbulence and its Modelling. *The Finite Volume Method for Diffusion Problems. *The Finite Volume Method for Convection-Diffusion Problems. *Solution Algorithms for Pressure-Velocity Coupling in Steady Flows. *Solution of Discretised Equations. *The Finite Volume Method for Unsteady Flows. *Implementation of Boundary Conditions. *Advanced topics and applications. Appendices. References. Index.,2007,0,7120,527,194,239,276,330,368,429,451,568,552,526
83f1b137f2528e8a5c8a26771dd91cf970a0789c,"This text develops and applies the techniques used to solve problems in fluid mechanics on computers and describes in detail those most often used in practice. It includes advanced techniques in computational fluid dynamics, such as direct and large-eddy simulation of turbulence, multigrid methods, parallel computing, moving grids, structured, block-structured and unstructured boundary-fitted grids, and free surface flows.",1996,0,6178,362,4,17,31,36,71,94,137,160,187,212
ba2712feae036b228a3b142629b8a6593187a87a,"Applications of second-moment turbulent closure hypotheses to geophysical fluid problems have developed rapidly since 1973, when genuine predictive skill in coping with the effects of stratification was demonstrated. The purpose here is to synthesize and organize material that has appeared in a number of articles and add new useful material so that a complete (and improved) description of a turbulence model from conception to application is condensed in a single article. It is hoped that this will be a useful reference to users of the model for application to either atmospheric or oceanic boundary layers.",1982,103,6208,973,1,5,6,12,14,14,14,27,32,19
8705e8fe0632bd11f200455a5125692a2547a018,"Note: Includes references and index Reference Record created on 2004-09-07, modified on 2016-08-08",1997,20,6000,610,1,11,29,43,88,102,97,114,169,180
8fafdfc080df616e449cbda5fd9dc86d70f4ce7c,Preliminaries * Fundamentals * Inviscid Shallow-Water Theory * Friction and Viscous Flow * Homogeneous Models of the Wind-Driven Oceanic Circulation * Quasigeostrophic Motion of a Stratified Fluid on a Sphere * Instability Theory * Ageostrophic Motion,1979,0,4580,311,5,13,16,23,32,35,37,54,60,56
c04a2060f080cc8dec616929bff1ea8f9dd19e60,A theory is developed for the propagation of stress waves in a porous elastic solid containing compressible viscous fluid. The emphasis of the present treatment is on materials where fluid and solid are of comparable densities as for instance in the case of water‐saturated rock. The paper denoted here as Part I is restricted to the lower frequency range where the assumption of Poiseuille flow is valid. The extension to the higher frequencies will be treated in Part II. It is found that the material may be described by four nondimensional parameters and a characteristic frequency. There are two dilatational waves and one rotational wave. The physical interpretation of the result is clarified by treating first the case where the fluid is frictionless. The case of a material containing viscous fluid is then developed and discussed numerically. Phase velocity dispersion curves and attenuation coefficients for the three types of waves are plotted as a function of the frequency for various combinations of the characteristic parameters.,1956,0,6390,319,1,0,1,1,2,6,5,4,6,1
202870134de1f771f678cb540d2ea082b1ab9c5d,"§1. We shall denote by uα(P) = uα (x1, x2, x3, t), α = 1, 2, 3, the components of velocity at the moment t at the point with rectangular cartesian coordinates x1, x2, x3. In considering the turbulence it is natural to assume the components of the velocity uα (P) at every point P = (x1, x2, x3, t) of the considered domain G of the four-dimensional space (x1, x2, x3, t) are random variables in the sense of the theory of probabilities (cf. for this approach to the problem Millionshtchikov (1939) Denoting by Ᾱ the mathematical expectation of the random variable A we suppose that ῡ2α and (duα /dxβ)2― are finite and bounded in every bounded subdomain of the domain G.",1991,2,4611,453,26,30,32,30,46,54,51,65,78,63
fb98d93832bf3e2c825e08371c4680e9500750d6,,1981,0,3996,526,1,0,1,2,1,4,4,5,7,8
65afba3daff41d488f11e017bc02ba99854e52b7,"We present an overview of the lattice Boltzmann method (LBM), a parallel and efficient algorithm for simulating single-phase and multiphase fluid flows and for incorporating additional physical complexities. The LBM is especially useful for modeling complicated boundary conditions and multiphase interfaces. Recent extensions of this method are described, including simulations of fluid turbulence, suspension flows, and reaction diffusion systems.",2001,184,6057,217,62,109,118,114,142,211,241,226,290,295
f791e34262a8909299027036132288d68eb07a99,,1952,0,6154,404,0,0,1,0,1,1,0,1,2,1
425cb3106a15e476a2d338e32b896e704cd38a81,"This overview of diffusion and separation processes brings unsurpassed, engaging clarity to this complex topic. Diffusion is a key part of the undergraduate chemical engineering curriculum and at the core of understanding chemical purification and reaction engineering. This spontaneous mixing process is also central to our daily lives, with importance in phenomena as diverse as the dispersal of pollutants to digestion in the small intestine. For students, Diffusion goes from the basics of mass transfer and diffusion itself, with strong support through worked examples and a range of student questions. It also takes the reader right through to the cutting edge of our understanding, and the new examples in this third edition will appeal to professional scientists and engineers. Retaining the trademark enthusiastic style, the broad coverage now extends to biology and medicine.",1984,25,4862,302,0,3,7,6,14,20,18,26,24,35
b691bea88b6cd666499b4cc30141f24a46a37037,1 Preliminary Concepts 2 Fundamental Equations of Compressible Viscous Flow 3 Solutions of the Newtonian Viscous-Flow Equations 4 Laminar Boundary Layers 5 The Stability of Laminar Flows 6 Incompressible Turbulent Mean Flow 7 Compressible Boundary Layer Flow Appendices A Transport Properties of Various Newtonian Fluids B Equations of Motion of Incompressible Newtonian Fluids in Cylindrical and Spherical Coordinates C A Runge-Kutta Subroutine for N Simultaneous Differential Equations Bibliography Index,1974,0,5912,283,1,3,1,6,5,12,18,14,11,20
1350fb3231b36d942b6ff1dacb8df68a6df9c9b0,"An important achievement of modern experimental fluid mechanics is the invention and development of techniques for the measurement of whole, instantaneous fields of scalars and vectors. These techniques include tomographic interferometry (Hesselink 1988) and planar laser-induced fluorescence for scalars (Hassa et al 1987), and nuclear-magnetic-resonance imaging (Lee et al 1987), planar laser-induced fluorescence, laser-speckle velocimetry, particle-tracking velocimetry, molecular-tracking velocimetry (Miles et al 1989), and particle-image velocimetry for velocity fields. Reviews of these methods can be found in articles by Lauterborn & Vogel (1984), Adrian (1986a), Hesselink (1988), and Dudderar et al (1988), in books written by Merzkirch (1987) and edited by Chiang & Reid (1988) and Gad-el-Hak (1989).",1991,0,3309,258,3,24,48,39,70,65,85,71,73,104
00021f14a02740a3ada3b9e090dce979f5003e43,"A new technique is described for the numerical investigation of the time‐dependent flow of an incompressible fluid, the boundary of which is partially confined and partially free. The full Navier‐Stokes equations are written in finite‐difference form, and the solution is accomplished by finite‐time‐step advancement. The primary dependent variables are the pressure and the velocity components. Also used is a set of marker particles which move with the fluid. The technique is called the marker and cell method. Some examples of the application of this method are presented. All non‐linear effects are completely included, and the transient aspects can be computed for as much elapsed time as desired.",1965,7,5594,193,0,1,5,7,4,16,17,9,13,9
e0069fc257328c8f362a5e4c37c59f19eb719523,,2003,0,3649,223,53,79,107,171,156,168,222,241,267,223
d9a5c17a17c29bc5b50f55c1fe4f168034ae0279,"1. Introduction.- 1.1. Historical Background.- 1.2. Some Examples of Spectral Methods.- 1.2.1. A Fourier Galerkin Method for the Wave Equation.- 1.2.2. A Chebyshev Collocation Method for the Heat Equation.- 1.2.3. A Legendre Tau Method for the Poisson Equation.- 1.2.4. Basic Aspects of Galerkin, Tau and Collocation Methods.- 1.3. The Equations of Fluid Dynamics.- 1.3.1. Compressible Navier-Stokes.- 1.3.2. Compressible Euler.- 1.3.3. Compressible Potential.- 1.3.4. Incompressible Flow.- 1.3.5. Boundary Layer.- 1.4. Spectral Accuracy for a Two-Dimensional Fluid Calculation.- 1.5. Three-Dimensional Applications in Fluids.- 2. Spectral Approximation.- 2.1. The Fourier System.- 2.1.1. The Continuous Fourier Expansion.- 2.1.2. The Discrete Fourier Expansion.- 2.1.3. Differentiation.- 2.1.4. The Gibbs Phenomenon.- 2.2. Orthogonal Polynomials in ( - 1, 1).- 2.2.1. Sturm-Liouville Problems.- 2.2.2. Orthogonal Systems of Polynomials.- 2.2.3. Gauss-Type Quadratures and Discrete Polynomial Transforms.- 2.3. Legendre Polynomials.- 2.3.1. Basic Formulas.- 2.3.2. Differentiation.- 2.4. Chebyshev Polynomials.- 2.4.1. Basic Formulas.- 2.4.2. Differentiation.- 2.5. Generalizations.- 2.5.1. Jacobi Polynomials.- 2.5.2. Mapping.- 2.5.3. Semi-Infinite Intervals.- 2.5.4. Infinite Intervals.- 3. Fundamentals of Spectral Methods for PDEs.- 3.1. Spectral Projection of the Burgers Equation.- 3.1.1. Fourier Galerkin.- 3.1.2. Fourier Collocation.- 3.1.3. Chebyshev Tau.- 3.1.4. Chebyshev Collocation.- 3.2. Convolution Sums.- 3.2.1. Pseudospectral Transform Methods.- 3 2 2 Aliasing Removal by Padding or Truncation.- 3.2.3. Aliasing Removal by Phase Shifts.- 3.2.4. Convolution Sums in Chebyshev Methods.- 3.2.5. Relation Between Collocation and Pseudospectral Methods.- 3.3. Boundary Conditions.- 3.4. Coordinate Singularities.- 3.4.1. Polar Coordinates.- 3.4.2. Spherical Polar Coordinates.- 3.5. Two-Dimensional Mapping.- 4. Temporal Discretization.- 4.1. Introduction.- 4.2. The Eigenvalues of Basic Spectral Operators.- 4.2.1. The First-Derivative Operator.- 4.2.2. The Second-Derivative Operator.- 4.3. Some Standard Schemes.- 4.3.1. Multistep Schemes.- 4.3.2. Runge-Kutta Methods.- 4.4. Special Purpose Schemes.- 4.4.1. High Resolution Temporal Schemes.- 4.4.2. Special Integration Techniques.- 4.4.3. Lerat Schemes.- 4.5. Conservation Forms.- 4.6. Aliasing.- 5. Solution Techniques for Implicit Spectral Equations.- 5.1. Direct Methods.- 5.1.1. Fourier Approximations.- 5.1.2. Chebyshev Tau Approximations.- 5.1.3. Schur-Decomposition and Matrix-Diagonalization.- 5.2. Fundamentals of Iterative Methods.- 5.2.1. Richardson Iteration.- 5.2.2. Preconditioning.- 5.2.3. Non-Periodic Problems.- 5.2.4. Finite-Element Preconditioning.- 5.3. Conventional Iterative Methods.- 5.3.1. Descent Methods for Symmetric, Positive-Definite Systems.- 5.3.2. Descent Methods for Non-Symmetric Problems.- 5.3.3. Chebyshev Acceleration.- 5.4. Multidimensional Preconditioning.- 5.4.1. Finite-Difference Solvers.- 5.4.2. Modified Finite-Difference Preconditioners.- 5.5. Spectral Multigrid Methods.- 5.5.1. Model Problem Discussion.- 5.5.2. Two-Dimensional Problems.- 5.5.3. Interpolation Operators.- 5.5.4. Coarse-Grid Operators.- 5.5.5. Relaxation Schemes.- 5.6. A Semi-Implicit Method for the Navier-Stokes Equations.- 6. Simple Incompressible Flows.- 6.1. Burgers Equation.- 6.2. Shear Flow Past a Circle.- 6.3. Boundary-Layer Flows.- 6.4. Linear Stability.- 7. Some Algorithms for Unsteady Navier-Stokes Equations.- 7.1. Introduction.- 7.2. Homogeneous Flows.- 7.2.1. A Spectral Galerkin Solution Technique.- 7.2.2. Treatment of the Nonlinear Terms.- 7.2.3. Refinements.- 7.2.4. Pseudospectral and Collocation Methods.- 7.3. Inhomogeneous Flows.- 7.3.1. Coupled Methods.- 7.3.2. Splitting Methods.- 7.3.3. Galerkin Methods.- 7.3.4. Other Confined Flows.- 7.3.5. Unbounded Flows.- 7.3.6. Aliasing in Transition Calculations.- 7.4. Flows with Multiple Inhomogeneous Directions.- 7.4.1. Choice of Mesh.- 7.4.2. Coupled Methods.- 7.4.3. Splitting Methods.- 7.4.4. Other Methods.- 7.5. Mixed Spectral/Finite-Difference Methods.- 8. Compressible Flow.- 8.1. Introduction.- 8.2. Boundary Conditions for Hyperbolic Problems.- 8.3. Basic Results for Scalar Nonsmooth Problems.- 8.4. Homogeneous Turbulence.- 8.5. Shock-Capturing.- 8.5.1. Potential Flow.- 8.5.2. Ringleb Flow.- 8.5.3. Astrophysical Nozzle.- 8.6. Shock-Fitting.- 8.7. Reacting Flows.- 9. Global Approximation Results.- 9.1. Fourier Approximation.- 9.1.1. Inverse Inequalities for Trigonometric Polynomials.- 9.1.2. Estimates for the Truncation and Best Approximation Errors.- 9.1.3. Estimates for the Interpolation Error.- 9.2. Sturm-Liouville Expansions.- 9.2.1. Regular Sturm-Liouville Problems.- 9.2.2. Singular Sturm-Liouville Problems.- 9.3. Discrete Norms.- 9.4. Legendre Approximations.- 9.4.1. Inverse Inequalities for Algebraic Polynomials.- 9.4.2. Estimates for the Truncation and Best Approximation Errors.- 9.4.3. Estimates for the Interpolation Error.- 9.5. Chebyshev Approximations.- 9.5.1. Inverse Inequalities for Polynomials.- 9.5.2. Estimates for the Truncation and Best Approximation Errors.- 9.5.3. Estimates for the Interpolation Error.- 9.5.4. Proofs of Some Approximation Results.- 9.6. Other Polynomial Approximations.- 9.6.1. Jacobi Polynomials.- 9.6.2. Laguerre and Hermite Polynomials.- 9.7. Approximation Results in Several Dimensions.- 9.7.1. Fourier Approximations.- 9.7.2. Legendre Approximations.- 9.7.3. Chebyshev Approximations.- 9.7.4. Blended Fourier and Chebyshev Approximations.- 10. Theory of Stability and Convergence for Spectral Methods.- 10.1. The Three Examples Revisited.- 10.1.1. A Fourier Galerkin Method for the Wave Equation.- 10.1.2. A Chebyshev Collocation Method for the Heat Equation.- 10.1.3. A Legendre Tau Method for the Poisson Equation.- 10.2. Towards a General Theory.- 10.3. General Formulation of Spectral Approximations to Linear Steady Problems.- 10.4. Galerkin, Collocation and Tau Methods.- 10.4.1. Galerkin Methods.- 10.4.2. Tau Methods.- 10.4.3. Collocation Methods.- 10.5. General Formulation of Spectral Approximations to Linear Evolution Equations.- 10.5.1. Conditions for Stability and Convergence: The Parabolic Case.- 10.5.2. Conditions for Stability and Convergence: The Hyperbolic Case.- 10.6. The Error Equation.- 11. Steady, Smooth Problems.- 11.1. The Poisson Equation.- 11.1.1. Legendre Methods.- 11.1.2. Chebyshev Methods.- 11.1.3. Other Boundary Value Problems.- 11.2. Advection-Diffusion Equation.- 11.2.1. Linear Advection-Diffusion Equation.- 11.2.2. Steady Burgers Equation.- 11.3. Navier-Stokes Equations.- 11.3.1. Compatibility Conditions Between Velocity and Pressure.- 11.3.2. Direct Discretization of the Continuity Equation: The ""inf-sup"" Condition.- 11.3.3. Discretizations of the Continuity Equation by an Influence-Matrix Technique: The Kleiser-Schumann Method.- 11.3.4. Navier-Stokes Equations in Streamfunction Formulation.- 11.4. The Eigenvalues of Some Spectral Operators.- 11.4.1. The Discrete Eigenvalues for Lu = ? uxx.- 11.4.2. The Discrete Eigenvalues for Lu = ? vuxx + bux.- 11.4.3. The Discrete Eigenvalues for Lu = ux.- 12. Transient, Smooth Problems.- 12.1. Linear Hyperbolic Equations.- 12.1.1. Periodic Boundary Conditions.- 12.1.2. Non-Periodic Boundary Conditions.- 12.1.3. Hyperbolic Systems.- 12.1.4. Spectral Accuracy for Non-Smooth Solutions.- 12.2. Heat Equation.- 12.2.1. Semi-Discrete Approximation.- 12.2.2. Fully Discrete Approximation.- 12.3. Advection-Diffusion Equation.- 12.3.1. Semi-Discrete Approximation.- 12.3.2. Fully Discrete Approximation.- 13. Domain Decomposition Methods.- 13.1. Introduction.- 13.2. Patching Methods.- 13.2.1. Notation.- 13.2.2. Discretization.- 13.2.3. Solution Techniques.- 13.2.4. Examples.- 13.3. Variational Methods.- 13.3.1. Formulation.- 13.3.2. The Spectral-Element Method.- 13.4. The Alternating Schwarz Method.- 13.5. Mathematical Aspects of Domain Decomposition Methods.- 13.5.1. Patching Methods.- 13.5.2. Equivalence Between Patching and Variational Methods.- 13.6. Some Stability and Convergence Results.- 13.6.1. Patching Methods.- 13.6.2. Variational Methods.- Appendices.- A. Basic Mathematical Concepts.- B. Fast Fourier Transforms.- C. Jacobi-Gauss-Lobatto Roots.- References.",1987,0,4103,147,6,15,45,72,78,69,87,95,93,129
58cb1e26e323ae41fad6f4638110e58116e9923a,"1. The Phase Equilibrium Problem. 2. Classical Thermodynamics of Phase Equilibria. 3. Thermodynamic Properties from Volumetric Data. 4. Intermolecular Forces, Corresponding States and Osmotic Systems. 5. Fugacities in Gas Mixtures. 6. Fugacities in Liquid Mixtures: Excess Functions. 7. Fugacities in Liquid Mixtures: Models and Theories of Solutions. 8. Polymers: Solutions, Blends, Membranes, and Gels. 9. Electrolyte Solutions. 10. Solubilities of Gases in Liquids. 11. Solubilities of Solids in Liquids. 12. High-Pressure Phase Equilibria. Appendix A. Uniformity of Intensive Potentials as a Criterion of Phase Equilibrium. Appendix B. A Brief Introduction to Statistical Thermodynamics. Appendix C. Virial Coefficients for Quantum Gases. Appendix D. The Gibbs-Duhem Equation. Appendix E. Liquid-Liquid Equilibria in Binary and Multicomponent Systems. Appendix F. Estimation of Activity Coefficients. Appendix G. A General Theorem for Mixtures with Associating or Solvating Molecules. Appendix H. Brief Introduction to Perturbation Theory of Dense Fluids. Appendix I. The Ion-Interaction Model of Pitzer for Multielectrolyte Solutions. Appendix J. Conversion Factors and Constants. Index.",1969,0,4434,200,0,7,6,10,4,5,16,8,12,16
ed587e9f225d4e264962e07d64ba652fc8a2b6d9,"BACKGROUND
Retinal ischemia induces intraocular neovascularization, which often leads to glaucoma, vitreous hemorrhage, and retinal detachment, presumably by stimulating the release of angiogenic molecules. Vascular endothelial growth factor (VEGF) is an endothelial-cell-specific angiogenic factor whose production is increased by hypoxia.


METHODS
We measured the concentration of VEGF in 210 specimens of ocular fluid obtained from 164 patients undergoing intraocular surgery, using both radioimmuno-assays and radioreceptor assays. Vitreous proliferative potential was measured with in vitro assays of the growth of retinal endothelial cells and with VEGF-neutralizing antibody.


RESULTS
VEGF was detected in 69 of 136 ocular-fluid samples from patients with diabetic retinopathy, 29 of 38 samples from patients with neovascularization of the iris, and 3 of 4 samples from patients with ischemic occlusion of the central retinal vein, as compared with 2 of 31 samples from patients with no neovascular disorders (P < 0.001, P < 0.001, and P = 0.006, respectively). The mean (+/- SD) VEGF concentration in 70 samples of ocular fluid from patients with active proliferative diabetic retinopathy (3.6 +/- 6.3 ng per milliliter) was higher than that in 25 samples from patients with nonproliferative diabetic retinopathy (0.1 +/- 0.1 ng per milliliter, P = 0.008), 41 samples from patients with quiescent proliferative diabetic retinopathy (0.2 +/- 0.6 ng per milliliter, P < 0.001), or 31 samples from nondiabetic patients (0.1 +/- 0.2 ng per milliliter, P = 0.003). Concentrations of VEGF in vitreous fluid (8.8 +/- 9.9 ng per milliliter) were higher than those in aqueous fluid (5.6 +/- 8.6 ng per milliliter, P = 0.033) in all 10 pairs of samples obtained simultaneously from the same patient; VEGF concentrations in vitreous fluid declined after successful laser photocoagulation. VEGF stimulated the growth of retinal endothelial cells in vitro, as did vitreous fluid containing measurable VEGF. Stimulation was inhibited by VEGF-neutralizing antibodies.


CONCLUSIONS
Our data suggest that VEGF plays a major part in mediating active intraocular neovascularization in patients with ischemic retinal diseases, such as diabetic retinopathy and retinal-vein occlusion.",1994,28,3507,138,1,28,73,84,77,96,94,91,105,96
2f01b5dc74c8c918a8f3cf026f6e0bebca28c94c,"Tumor ascites fluids from guinea pigs, hamsters, and mice contain activity that rapidly increases microvascular permeability. Similar activity is also secreted by these tumor cells and a variety of other tumor cell lines in vitro. The permeability-increasing activity purified from either the culture medium or ascites fluid of one tumor, the guinea pig line 10 hepatocarcinoma, is a 34,000- to 42,000-dalton protein distinct from other known permeability factors.",1983,8,3889,68,2,6,7,8,5,12,6,9,20,18
584db0eb1badb25b3938823e76d1e705cb60df18,"A fluid mosaic model is presented for the gross organization and structure of the proteins and lipids of biological membranes. The model is consistent with the restrictions imposed by thermodynamics. In this model, the proteins that are integral to the membrane are a heterogeneous set of globular molecules, each arranged in an amphipathic structure, that is, with the ionic and highly polar groups protruding from the membrane into the aqueous phase, and the nonpolar groups largely buried in the hydrophobic interior of the membrane. These globular molecules are partially embedded in a matrix of phospholipid. The bulk of the phospholipid is organized as a discontinuous, fluid bilayer, although a small fraction of the lipid may interact specifically with the membrane proteins. The fluid mosaic structure is therefore formally analogous to a two-dimensional oriented solution of integral proteins (or lipoproteins) in the viscous phospholipid bilayer solvent. Recent experiments with a wide variety of techniqes and several different membrane systems are described, all of which abet consistent with, and add much detail to, the fluid mosaic model. It therefore seems appropriate to suggest possible mechanisms for various membrane functions and membrane-mediated phenomena in the light of the model. As examples, experimentally testable mechanisms are suggested for cell surface changes in malignant transformation, and for cooperative effects exhibited in the interactions of membranes with some specific ligands. Note added in proof: Since this article was written, we have obtained electron microscopic evidence (69) that the concanavalin A binding sites on the membranes of SV40 virus-transformed mouse fibroblasts (3T3 cells) are more clustered than the sites on the membranes of normal cells, as predicted by the hypothesis represented in Fig. 7B. T-here has also appeared a study by Taylor et al. (70) showing the remarkable effects produced on lymphocytes by the addition of antibodies directed to their surface immunoglobulin molecules. The antibodies induce a redistribution and pinocytosis of these surface immunoglobulins, so that within about 30 minutes at 37�C the surface immunoglobulins are completely swept out of the membrane. These effects do not occur, however, if the bivalent antibodies are replaced by their univalent Fab fragments or if the antibody experiments are carried out at 0�C instead of 37�C. These and related results strongly indicate that the bivalent antibodies produce an aggregation of the surface immunoglobulin molecules in the plane of the membrane, which can occur only if the immunoglobulin molecules are free to diffuse in the membrane. This aggregation then appears to trigger off the pinocytosis of the membrane components by some unknown mechanism. Such membrane transformations may be of crucial importance in the induction of an antibody response to an antigen, as well as iv other processes of cell differentiation.",1972,77,5674,16,49,179,216,203,264,217,221,185,163,157
37491ed087839d86c9ba7783cb12ccc6c9918bf6,"Microfabricated integrated circuits revolutionized computation by vastly reducing the space, labor, and time required for calculations. Microfluidic systems hold similar promise for the large-scale automation of chemistry and biology, suggesting the possibility of numerous experiments performed rapidly and in parallel, while consuming little reagent. While it is too early to tell whether such a vision will be realized, significant progress has been achieved, and various applications of significant scientific and practical interest have been developed. Here a review of the physics of small volumes (nanoliters) of fluids is presented, as parametrized by a series of dimensionless numbers expressing the relative importance of various physical phenomena. Specifically, this review explores the Reynolds number Re, addressing inertial effects; the Peclet number Pe, which concerns convective and diffusive transport; the capillary number Ca expressing the importance of interfacial tension; the Deborah, Weissenberg, and elasticity numbers De, Wi, and El, describing elastic effects due to deformable microstructural elements like polymers; the Grashof and Rayleigh numbers Gr and Ra, describing density-driven flows; and the Knudsen number, describing the importance of noncontinuum molecular effects. Furthermore, the long-range nature of viscous flows and the small device dimensions inherent in microfluidics mean that the influence of boundaries is typically significant. A variety of strategies have been developed to manipulate fluids by exploiting boundary effects; among these are electrokinetic effects, acoustic streaming, and fluid-structure interactions. The goal is to describe the physics behind the rich variety of fluid phenomena occurring on the nanoliter scale using simple scaling arguments, with the hopes of developing an intuitive sense for this occasionally counterintuitive world.",2005,1080,3427,70,17,77,129,221,219,226,241,278,285,260
f6af0b7accdc3ca99195b991e50232392fa4b4b8,This Letter presents variational ground-state and excited-state wave functions which describe the condensation of a two-dimensional electron gas into a new state of matter.,1983,8,3228,111,4,30,46,46,33,37,36,45,38,57
cf15817ee5f9c1536ee4da2c4c018555600ca91b,"A study was conducted in which 133 participants performed 11 memory tasks (some thought to reflect working memory and some thought to reflect short-term memory), 2 tests of general fluid intelligence, and the Verbal and Quantitative Scholastic Aptitude Tests. Structural equation modeling suggested that short-term and working memories reflect separate but highly related constructs and that many of the tasks used in the literature as working memory tasks reflect a common construct. Working memory shows a strong connection to fluid intelligence, but short-term memory does not. A theory of working memory capacity and general fluid intelligence is proposed: The authors argue that working memory capacity and fluid intelligence reflect the ability to keep a representation active, particularly in the face of interference and distraction. The authors also discuss the relationship of this capability to controlled attention, and the functions of the prefrontal cortex.",1999,109,2892,149,5,13,49,42,69,63,99,92,115,130
0ceb798602f4a466b1e3c943a73a892c25d8abbc,"Perturbation Methods in Fluid MechanicsBy Milton Van Dyke. (Applied Mathematics and Mechanics: an International Series of Monographs, Vol. 8.) Pp. x + 229. (New York: Academic Press, Inc.; London: Academic Press, Inc. (London), Ltd., 1964.) 56s.",1965,0,3063,188,3,21,33,26,41,41,53,63,46,48
265d2bbf765c4ed9784ab0a574810231e3b14671,"Keywords: dynamique des : fluides Reference Record created on 2005-11-18, modified on 2016-08-08",1969,0,3419,110,7,3,11,12,15,8,17,23,21,27
fee733d1816ac29a2c38fa51ec2e25242b6b2dc7,"A fluid mosaic model is presented for the gross organization and structure of the proteins and lipids of biological membranes. The model is consistent with the restrictions imposed by thermodynamics. In this model, the proteins that are integral to the membrane are a heterogeneous set of globular molecules, each arranged in an amphipathic structure, that is, with the ionic and highly polar groups protruding from the membrane into the aqueous phase, and the nonpolar groups largely buried in the hydrophobic interior of the membrane. These globular molecules are partially embedded in a matrix of phospholipid. The bulk of the phospholipid is organized as a discontinuous, fluid bilayer, although a small fraction of the lipid may interact specifically with the membrane proteins. The fluid mosaic structure is therefore formally analogous to a two-dimensional oriented solution of integral proteins (or lipoproteins) in the viscous phospholipid bilayer solvent. Recent experiments with a wide variety of techniqes and several different membrane systems are described, all of which abet consistent with, and add much detail to, the fluid mosaic model. It therefore seems appropriate to suggest possible mechanisms for various membrane functions and membrane-mediated phenomena in the light of the model. As examples, experimentally testable mechanisms are suggested for cell surface changes in malignant transformation, and for cooperative effects exhibited in the interactions of membranes with some specific ligands. Note added in proof: Since this article was written, we have obtained electron microscopic evidence (69) that the concanavalin A binding sites on the membranes of SV40 virus-transformed mouse fibroblasts (3T3 cells) are more clustered than the sites on the membranes of normal cells, as predicted by the hypothesis represented in Fig. 7B. T-here has also appeared a study by Taylor et al. (70) showing the remarkable effects produced on lymphocytes by the addition of antibodies directed to their surface immunoglobulin molecules. The antibodies induce a redistribution and pinocytosis of these surface immunoglobulins, so that within about 30 minutes at 37 degrees C the surface immunoglobulins are completely swept out of the membrane. These effects do not occur, however, if the bivalent antibodies are replaced by their univalent Fab fragments or if the antibody experiments are carried out at 0 degrees C instead of 37 degrees C. These and related results strongly indicate that the bivalent antibodies produce an aggregation of the surface immunoglobulin molecules in the plane of the membrane, which can occur only if the immunoglobulin molecules are free to diffuse in the membrane. This aggregation then appears to trigger off the pinocytosis of the membrane components by some unknown mechanism. Such membrane transformations may be of crucial importance in the induction of an antibody response to an antigen, as well as iv other processes of cell differentiation.",1972,81,2893,124,8,32,53,53,67,35,36,34,22,23
042ad33902b18a6cc9310441d5853afbc17bc5e4,"Keywords: CFD ; numerique ; transfert de chaleur ; ecoulement Reference Record created on 2005-11-18, modified on 2016-08-08",1984,0,3087,51,1,6,12,21,33,49,53,63,76,76
66c57627253ea3dc5eb71892be3e648a67981c6d,"Variations of the SIMPLE method of Patankar and Spalding have been widely used over the past decade to obtain numerical solutions to problems involving incompressible flows. The present paper shows several modifications to the method which both simplify its implementation and reduce solution costs. The performances of SIMPLE, SIMPLER, and SIMPLEC (the present method) are compared for two recirculating flow problems. The paper is addressed to readers who already have experience with SIMPLE or its variants.",1984,11,3189,53,0,5,13,10,22,29,35,37,43,45
f14b8ef5a3edc5fdc9613a8a1c5545aa6cb626ad,"This Position Stand provides guidance on fluid replacement to sustain appropriate hydration of individuals performing physical activity. The goal of prehydrating is to start the activity euhydrated and with normal plasma electrolyte levels. Prehydrating with beverages, in addition to normal meals and fluid intake, should be initiated when needed at least several hours before the activity to enable fluid absorption and allow urine output to return to normal levels. The goal of drinking during exercise is to prevent excessive (>2% body weight loss from water deficit) dehydration and excessive changes in electrolyte balance to avert compromised performance. Because there is considerable variability in sweating rates and sweat electrolyte content between individuals, customized fluid replacement programs are recommended. Individual sweat rates can be estimated by measuring body weight before and after exercise. During exercise, consuming beverages containing electrolytes and carbohydrates can provide benefits over water alone under certain circumstances. After exercise, the goal is to replace any fluid electrolyte deficit. The speed with which rehydration is needed and the magnitude of fluid electrolyte deficits will determine if an aggressive replacement program is merited.",2007,10,1797,207,36,62,53,93,111,114,140,150,158,152
b0b240ab99338da6569f9903228e8f93e533a163,"Abstract Results of an extensive comparison of numerical methods for simulating hydrodynamics are presented and discussed. This study focuses on the simulation of fluid flows with strong shocks in two dimensions. By “strong shocks,” we here refer to shocks in which there is substantial entropy production. For the case of shocks in air, we therefore refer to Mach numbers of three and greater. For flows containing such strong shocks we find that a careful treatment of flow discontinuities is of greatest importance in obtaining accurate numerical results. Three approaches to treating discontinuities in the flow are discussed—artificial viscosity, blending of low- and high-order-accurate fluxes, and the use of nonlinear solutions to Riemann's problem. The advantages and disadvantages of each approach are discussed and illustrated by computed results for three test problems. In this comparison we have focused our attention entirely upon the performance of schemes for differencing the hydrodynamic equations. We have regarded the nature of the grid upon which such differencing schemes are applied as an independent issue outside the scope of this work. Therefore we have restricted our study to the case of uniform, square computational zones in Cartesian coordinates. For simplicity we have further restricted our attention to two-dimensional difference schemes which are built out of symmetrized products of one-dimensional difference operators.",1984,69,2422,175,3,14,15,20,16,36,32,22,24,22
b7e200c497c63b94ed8094fb5e38ecfd01aaed5d,,1997,0,2259,160,8,12,23,14,23,12,28,28,39,44
1484e852af31571652ef794ecd06b9a769576620,"In both physical and biological science, we are often concerned with the properties of a fluid, or plasma, in which small particles or corpuscles are suspended and carried about by the motion of the fluid. The presence of the particles will influence the properties of the suspension in bulk, and, in particular, its viscosity will be increased. The most complete mathematical treatment of the problem, from this point of view, has been that given by Einstein, who considered the case of spherical particles and gave a simple formula for the increase in the viscosity. We have extended this work to the case of particles of ellipsoidal shape. The second section of the paper is occupied with the requisite solution of the equations of motion of the fluid. The problem of the motion of a viscous fluid, due to an ellipsoid moving through it with a small velocity of translation in a direction parallel to one of its axes, has been solved by Oberbeck, and the corresponding problem for an ellipsoid rotating about one of its axes by Edwards. In both cases the equations of motion are approximated by neglecting the terms involving the squares of the velocities. It may be seen, a posteriori , that the condition for the validity of this approximation is that the product of the velocity of the ellipsoid by its linear dimensions shall be small compared with the “kinematic coefficient, of viscosity” of the fluid. In relation to our present problem, it will therefore be satisfied either for sufficiently slow motions, or for sufficiently small particles.",1922,0,3245,262,0,0,0,0,0,0,0,0,0,0
662ac20e8d8b980582c513b38dd964cc56b40e05,,2009,0,1647,162,94,74,105,117,140,138,140,120,110,149
61eaddd04b290b10da09b172ac6063dbb94bd9a9,Pore Structure. Capillarity in Porous Media. Single-Phase Transport Phenomena in Porous Media. Selected Operations Involving Transport of a Single Fluid Phase through a Porous Medium. Multiphase Flow of Immiscible Fluids in Porous Media. Miscible Displacement and Dispersion. Index.,1979,1,2827,63,0,0,1,3,9,10,13,18,27,20
2e776c3af634ab4ff40a67e6295efcf3a3879848,"Fluid intelligence (Gf) refers to the ability to reason and to solve new problems independently of previously acquired knowledge. Gf is critical for a wide variety of cognitive tasks, and it is considered one of the most important factors in learning. Moreover, Gf is closely related to professional and educational success, especially in complex and demanding environments. Although performance on tests of Gf can be improved through direct practice on the tests themselves, there is no evidence that training on any other regimen yields increased Gf in adults. Furthermore, there is a long history of research into cognitive training showing that, although performance on trained tasks can increase dramatically, transfer of this learning to other tasks remains poor. Here, we present evidence for transfer from training on a demanding working memory task to measures of Gf. This transfer results even though the trained task is entirely different from the intelligence test itself. Furthermore, we demonstrate that the extent of gain in intelligence critically depends on the amount of training: the more training, the more improvement in Gf. That is, the training effect is dosage-dependent. Thus, in contrast to many previous studies, we conclude that it is possible to improve Gf without practicing the testing tasks themselves, opening a wide range of applications.",2008,58,1890,133,17,56,77,108,154,163,195,176,152,178
365bc7651a03fa1c1c02dbd72d800a1d97855484,Develop a cerebrospinal fluid biomarker signature for mild Alzheimer's disease (AD) in Alzheimer's Disease Neuroimaging Initiative (ADNI) subjects.,2009,34,1760,77,25,110,156,141,137,146,171,138,158,148
16373b46351bfeb596128a1c49249e44a47ee77e,"Fluid models of gas discharges require the input of transport coefficients and rate coefficients that depend on the electron energy distribution function. Such coefficients are usually calculated from collision cross-section data by solving the electron Boltzmann equation (BE). In this paper we present a new user-friendly BE solver developed especially for this purpose, freely available under the name BOLSIG+, which is more general and easier to use than most other BE solvers available. The solver provides steady-state solutions of the BE for electrons in a uniform electric field, using the classical two-term expansion, and is able to account for different growth models, quasi-stationary and oscillating fields, electron–neutral collisions and electron–electron collisions. We show that for the approximations we use, the BE takes the form of a convection-diffusion continuity-equation with a non-local source term in energy space. To solve this equation we use an exponential scheme commonly used for convection-diffusion problems. The calculated electron transport coefficients and rate coefficients are defined so as to ensure maximum consistency with the fluid equations. We discuss how these coefficients are best used in fluid models and illustrate the influence of some essential parameters and approximations.",2005,33,2083,92,4,11,26,27,49,56,80,93,114,160
48383c4b50853ff1afab6ecb9745c247a97ba6e1,"BACKGROUND
It remains uncertain whether the choice of resuscitation fluid for patients in intensive care units (ICUs) affects survival. We conducted a multicenter, randomized, double-blind trial to compare the effect of fluid resuscitation with albumin or saline on mortality in a heterogeneous population of patients in the ICU.


METHODS
We randomly assigned patients who had been admitted to the ICU to receive either 4 percent albumin or normal saline for intravascular-fluid resuscitation during the next 28 days. The primary outcome measure was death from any cause during the 28-day period after randomization.


RESULTS
Of the 6997 patients who underwent randomization, 3497 were assigned to receive albumin and 3500 to receive saline; the two groups had similar baseline characteristics. There were 726 deaths in the albumin group, as compared with 729 deaths in the saline group (relative risk of death, 0.99; 95 percent confidence interval, 0.91 to 1.09; P=0.87). The proportion of patients with new single-organ and multiple-organ failure was similar in the two groups (P=0.85). There were no significant differences between the groups in the mean (+/-SD) numbers of days spent in the ICU (6.5+/-6.6 in the albumin group and 6.2+/-6.2 in the saline group, P=0.44), days spent in the hospital (15.3+/-9.6 and 15.6+/-9.6, respectively; P=0.30), days of mechanical ventilation (4.5+/-6.1 and 4.3+/-5.7, respectively; P=0.74), or days of renal-replacement therapy (0.5+/-2.3 and 0.4+/-2.0, respectively; P=0.41).


CONCLUSIONS
In patients in the ICU, use of either 4 percent albumin or normal saline for fluid resuscitation results in similar outcomes at 28 days.",2004,28,2296,38,31,102,129,135,140,127,160,140,133,169
fa9d519d34f73f8dafc5be93f38af63e8083b20e,"Abstract A method to simulate unsteady multi-fluid flows in which a sharp interface or a front separates incompressible fluids of different density and viscosity is described. The flow field is discretized by a conservative finite difference approximation on a stationary grid, and the interface is explicitly represented by a separate, unstructured grid that moves through the stationary grid. Since the interface deforms continuously, it is necessary to restructure its grid as the calculations proceed. In addition to keeping the density and viscosity stratification sharp, the tracked interface provides a natural way to include surface tension effects. Both two- and three-dimensional, full numerical simulations of bubble motion are presented.",1992,88,2201,93,1,5,9,19,19,18,24,36,30,48
2413f245a64b9504efdd14d86c2c9483cdf0d6ba,"A new silver stain for electrophoretically separated polypeptides can be rapidly and easily used and can detect as little as 0.01 nanogram of protein per square millimeter. When employed with two-dimensional electrophoresis, it should permit qualitative and quantitative characterization of protein distributions in body fluids and tissues. It has been used to demonstrate regional variations in cerebrospinal fluid proteins.",1981,6,2554,12,12,64,133,169,150,191,171,151,152,138
d26eccb48c61a051378678dfbb5b5943683fcaed,"Background Optimal fluid management in patients with acute lung injury is unknown. Diuresis or fluid restriction may improve lung function but could jeopardize extrapulmonary-organ perfusion. Methods In a randomized study, we compared a conservative and a liberal strategy of fluid management using explicit protocols applied for seven days in 1000 patients with acute lung injury. The primary end point was death at 60 days. Secondary end points included the number of ventilator-free days and organ-failure–free days and measures of lung physiology. Results The rate of death at 60 days was 25.5 percent in the conservative-strategy group and 28.4 percent in the liberal-strategy group (P=0.30; 95 percent confidence interval for the difference, −2.6 to 8.4 percent). The mean (±SE) cumulative fluid balance during the first seven days was –136±491 ml in the conservative-strategy group and 6992±502 ml in the liberal-strategy group (P<0.001). As compared with the liberal strategy, the conservative strategy improved ...",2009,32,1768,1,93,127,112,142,126,122,93,124,120,115
62c0ba833912728a4e79c5c3550b44571b5ce94a,"Effective thermal conductivity of mixtures of e uids and nanometer-size particles is measured by a steady-state parallel-plate method. The tested e uids contain two types of nanoparticles, Al 2O3 and CuO, dispersed in water, vacuum pump e uid, engine oil, and ethylene glycol. Experimental results show that the thermal conductivities of nanoparticle ‐e uid mixtures are higher than those of the base e uids. Using theoretical models of effective thermal conductivity of a mixture, we have demonstrated that the predicted thermal conductivities of nanoparticle ‐e uid mixtures are much lower than our measured data, indicating the dee ciency in the existing models when used for nanoparticle ‐e uid mixtures. Possible mechanisms contributing to enhancement of the thermal conductivity of the mixtures are discussed. A more comprehensive theory is needed to fully explain the behavior of nanoparticle ‐e uid mixtures. Nomenclature cp = specie c heat k = thermal conductivity L = thickness Pe = Peclet number P q = input power to heater 1 r = radius of particle S = cross-sectional area T = temperature U = velocity of particles relative to that of base e uids ® = ratio of thermal conductivity of particle to that of base liquid ¯ = .® i 1/=.® i 2/ ° = shear rate of e ow Ω = density A = volume fraction of particles in e uids Subscripts",1999,43,1970,54,1,0,2,4,4,11,28,36,52,56
a7e4d45f1adab6c0e6ddaccdabbbc086d9547ef9,,1996,0,1587,213,1,6,10,28,40,31,34,43,33,54
1a9c2fa2d12609af7a489648fe68a416075cac00,"The method of characteristics used for numerical computation of solutions of fluid dynamical equations is characterized by a large degree of non standardness and therefore is not suitable for automatic computation on electronic computing machines, especially for problems with a large number of shock waves and contact discontinuities. In 1950 v. Neumann and Richtmyer proposed to use, for the solution of fluid dynamics equations, difference equations into which viscosity was introduced artificially; this has the effect of smearing out the shock wave over several mesh points. Then, it was proposed to proceed with the computations across the shock waves in the ordinary manner. In 1954, Lax published the ""triangle'' scheme suitable for computation across the shock"" waves. A deficiency of this scheme is that it does not allow computation with arbitrarily fine time steps (as compared with the space steps divided by the sound speed) because it then transforms any initial data into linear functions. In addition, this scheme smears out contact discontinuities. The purpose of this paper is to choose a scheme which is in some sense best and which still allows computation across the shock waves. This choice is made for linear equations and then by analogy the scheme is applied to the general equations of fluid dynamics. Following this scheme we carried out a large number of computations on Soviet electronic computers. For a check, some of these computations were compared with the computations carried out by the method of characteristics. The agreement of results was fully satisfactory.",1959,2,2761,73,0,0,0,3,0,1,2,1,1,3
b721f6ba0e10507f1d25b76c3eb37380d16a31a2,This study develops analytical relationships and computations of power dissipation in magnetic fluid (ferrofluid) subjected to alternating magnetic field. The dissipation results from the orientational relaxation of particles having thermal fluctuations in a viscous medium.,2002,20,1775,94,2,5,8,13,23,28,36,68,81,92
cffc507312c01839ef2dc32158f2ad3a57efa5ce,"Dispersions of solid spherical grains of diameter D = 0.13cm were sheared in Newtonian fluids of varying viscosity (water and a glycerine-water-alcohol mixture) in the annular space between two concentric drums. The density σ of the grains was balanced against the density ρ of the fluid, giving a condition of no differential forces due to radial acceleration. The volume concentration C of the grains was varied between 62 and 13 %. A substantial radial dispersive pressure was found to be exerted between the grains. This was measured as an increase of static pressure in the inner stationary drum which had a deformable periphery. The torque on the inner drum was also measured. The dispersive pressure P was found to be proportional to a shear stress λ attributable to the presence of the grains. The linear grain concentration λ is defined as the ratio grain diameter/mean free dispersion distance and is related to C by λ=1(C0/C)12−1 where C0 is the maximum possible static volume concentration. Both the stressesT and P, as dimensionless groups TσD2/λη2, and PσD2/λη 2, were found to bear single-valued empirical relations to a dimensionless shear strain group λ½σD2(dU/dy)lη for all the values of λ< 12(C= 57% approx.) where dU/dy is the rate of shearing of the grains over one another, and η the fluid viscosity. This relation gives Tασ(λD)2(dU/dy)2 and T∝λ12ηdU/dy according as dU/dy is large or small, i.e. according to whether grain inertia or fluid viscosity dominate. An alternative semi-empirical relation F = (1+λ)(1+½λ)ηdU/dy was found for the viscous case, when T is the whole shear stress. The ratio T/P was constant at 0·3 approx, in the inertia region, and at 0.75 approx, in the viscous region. The results are applied to a few hitherto unexplained natural phenomena.",1954,1,2296,201,0,0,0,0,0,0,1,0,0,1
6d0dea3286515ee375a020de8d743eca7f5f1556,"While Eulerian schemes work well for most gas flows, they have been shown to admit nonphysical oscillations near some material interfaces. In contrast, Lagrangian schemes work well at multimaterial interfaces, but suffer from their own difficulties in problems with large deformations and vorticity characteristic of most gas flows. We believe that the most robust schemes will combine the best properties of Eulerian and Lagrangian schemes. In this paper, we propose a new numerical method for treating interfaces in Eulerian schemes that maintains a Heaviside profile of the density with no numerical smearing along the lines of earlier work and most Lagrangian schemes. We use a level set function to track the motion of a multimaterial interface in an Eulerian framework. In addition, the use of ghost cells (actually ghost nodes in our finite difference framework) and a new isobaric fix technique allows us to keep the density profile from smearing out, while still keeping the scheme robust and easy to program with simple extensions to multidimensions and multilevel time integration, e.g., Runge?Kutta methods. In contrast, previous methods used ill-advised dimensional splitting for multidimensional problems and suffered from great complexity when used in conjunction with multilevel time integrators.",1999,47,1796,131,6,13,32,26,38,54,66,59,80,97
51f93535127cb16477ad4e6d56a8cb527969cf72,"We study the stability of steady nonlinear waves on the surface of an infinitely deep fluid [1, 2]. In section 1, the equations of hydrodynamics for an ideal fluid with a free surface are transformed to canonical variables: the shape of the surface η(r, t) and the hydrodynamic potential ψ(r, t) at the surface are expressed in terms of these variables. By introducing canonical variables, we can consider the problem of the stability of surface waves as part of the more general problem of nonlinear waves in media with dispersion [3,4]. The resuits of the rest of the paper are also easily applicable to the general case.In section 2, using a method similar to van der Pohl's method, we obtain simplified equations describing nonlinear waves in the small amplitude approximation. These equations are particularly simple if we assume that the wave packet is narrow. The equations have an exact solution which approximates a periodic wave of finite amplitude.In section 3 we investigate the instability of periodic waves of finite amplitude. Instabilities of two types are found. The first type of instability is destructive instability, similar to the destructive instability of waves in a plasma [5, 6], In this type of instability, a pair of waves is simultaneously excited, the sum of the frequencies of which is a multiple of the frequency of the original wave. The most rapid destructive instability occurs for capillary waves and the slowest for gravitational waves. The second type of instability is the negative-pressure type, which arises because of the dependence of the nonlinear wave velocity on the amplitude; this results in an unbounded increase in the percentage modulation of the wave. This type of instability occurs for nonlinear waves through any media in which the sign of the second derivative in the dispersion law with respect to the wave number (d2ω/dk2) is different from the sign of the frequency shift due to the nonlinearity.As announced by A. N. Litvak and V. I. Talanov [7], this type of instability was independently observed for nonlinear electromagnetic waves.",1968,11,2093,209,0,0,0,1,0,0,2,0,0,2
6635fa7b37888f0f90c64afdfcb8ca09d3ae974f,"Many solid tumours show an increased interstitial fluid pressure (IFP), which forms a barrier to transcapillary transport. This barrier is an obstacle in tumour treatment, as it results in inefficient uptake of therapeutic agents. There are a number of factors that contribute to increased IFP in the tumour, such as vessel abnormalities, fibrosis and contraction of the interstitial matrix. Lowering the tumour IFP with specific signal-transduction antagonists might be a useful approach to improving anticancer drug efficacy.",2004,90,1639,63,0,18,37,44,48,50,69,81,97,123
1d573e6f61e8bb51c90f597a65780382b4a1369a,"Keywords: dynamique des : fluides ; equations : differentielles ; analyse ; elements : finis ; stabilite ; stationnaire ; mathematiques ; methodes : numeriques Reference Record created on 2005-11-18, modified on 2016-08-08",1992,0,1923,62,16,31,39,57,64,64,64,56,71,71
d9b606743d6eeaf3f42ee68298f3067af8201478,"When a viscous fluid filling the voids in a porous medium is driven forwards by the pressure of another driving fluid, the interface between them is liable to be unstable if the driving fluid is the less viscous of the two. This condition occurs in oil fields. To describe the normal modes of small disturbances from a plane interface and their rate of growth, it is necessary to know, or to assume one knows, the conditions which must be satisfied at the interface. The simplest assumption, that the fluids remain completely separated along a definite interface, leads to formulae which are analogous to known expressions developed by scientists working in the oil industry, and also analogous to expressions representing the instability of accelerated interfaces between fluids of different densities. In the latter case the instability develops into round-ended fingers of less dense fluid penetrating into the more dense one. Experiments in which a viscous fluid confined between closely spaced parallel sheets of glass, a Hele-Shaw cell, is driven out by a less viscous one reveal a similar state. The motion in a Hele-Shaw cell is mathematically analogous to two-dimensional flow in a porous medium. Analysis which assumes continuity of pressure through the interface shows that a flow is possible in which equally spaced fingers advance steadily. The ratio λ = (width of finger)/(spacing of fingers) appears as the parameter in a singly infinite set of such motions, all of which appear equally possible. Experiments in which various fluids were forced into a narrow Hele-Shaw cell showed that single fingers can be produced, and that unless the flow is very slow λ = (width of finger)/(width of channel) is close to ½, so that behind the tips of the advancing fingers the widths of the two columns of fluid are equal. When λ = ½ the calculated form of the fingers is very close to that which is registered photographically in the Hele-Shaw cell, but at very slow speeds where the measured value of λ increased from ½ to the limit 1.0 as the speed decreased to zero, there were considerable differences. Assuming that these might be due to surface tension, experiments were made in which a fluid of small viscosity, air or water, displaced a much more viscous oil. It is to be expected in that case that λ would be a function of μU/T only, where μ is the viscosity, U the speed of advance and T the interfacial tension. This was verified using air as the less viscous fluid penetrating two oils of viscosities 0.30 and 4.5 poises.",1958,6,2341,126,0,0,2,1,1,2,0,0,2,1
e599af5bb14076ef4ec04855a75243bbc7c0b3d1,"Part I*Basic Thoughts and Equations 1 Philosophy of Computational Fluid Dynamics 2 The Governing Equations of Fluid Dynamics Their Derivation, A Discussion of Their Physical Meaning, and A Presentation of Forms Particularly Suitable to CFD 3 Mathematical Behavior of Partial Differential Equations The Impact on Computational Fluid Dynamics Part II*Basics of the Numerics 4 Basic Aspects of Discretization 5 Grids and Meshes, With Appropriate Transformations 6 Some Simple CFD Techniques A Beginning Part III*Some Applications 7 Numerical Solutions of Quasi-One-Dimensional Nozzle Flows 8 Numerical Solution of A Two-Dimensional Supersonic Flow Prandtl-Meyer Expansion Wave 9 Incompressible Couette Flow Numerical Solution by Means of an Implicit Method and the Pressure Correction Method 10 Incompressible, Inviscid Slow Over a Circular Cylinder Solution by the Technique Relaxation Part IV*Other Topics 11 Some Advanced Topics in Modern CFD A Discussion 12 The Future of Computational Fluid Dynamics Appendixes A Thomas's Algorithm for the Solution of A Tridiagonal System of Equations References",1995,94,1685,102,0,1,5,11,9,18,19,23,36,31
e51ab939cf5a2faf79ae169f2b794bc27f31cfae,,2006,0,1249,190,3,4,27,30,57,72,76,93,73,105
66fbc867a7a5faf4773581ccb3a2918a0706e1db,"BACKGROUND
Optimal fluid management in patients with acute lung injury is unknown. Diuresis or fluid restriction may improve lung function but could jeopardize extrapulmonary-organ perfusion.


METHODS
In a randomized study, we compared a conservative and a liberal strategy of fluid management using explicit protocols applied for seven days in 1000 patients with acute lung injury. The primary end point was death at 60 days. Secondary end points included the number of ventilator-free days and organ-failure-free days and measures of lung physiology.


RESULTS
The rate of death at 60 days was 25.5 percent in the conservative-strategy group and 28.4 percent in the liberal-strategy group (P=0.30; 95 percent confidence interval for the difference, -2.6 to 8.4 percent). The mean (+/-SE) cumulative fluid balance during the first seven days was -136+/-491 ml in the conservative-strategy group and 6992+/-502 ml in the liberal-strategy group (P<0.001). As compared with the liberal strategy, the conservative strategy improved the oxygenation index ([mean airway pressure x the ratio of the fraction of inspired oxygen to the partial pressure of arterial oxygen]x100) and the lung injury score and increased the number of ventilator-free days (14.6+/-0.5 vs. 12.1+/-0.5, P<0.001) and days not spent in the intensive care unit (13.4+/-0.4 vs. 11.2+/-0.4, P<0.001) during the first 28 days but did not increase the incidence or prevalence of shock during the study or the use of dialysis during the first 60 days (10 percent vs. 14 percent, P=0.06).


CONCLUSIONS
Although there was no significant difference in the primary outcome of 60-day mortality, the conservative strategy of fluid management improved lung function and shortened the duration of mechanical ventilation and intensive care without increasing nonpulmonary-organ failures. These results support the use of a conservative strategy of fluid management in patients with acute lung injury. (ClinicalTrials.gov number, NCT00281268 [ClinicalTrials.gov].).",2006,59,1413,48,11,34,60,77,85,93,110,133,120,112
8454380009625b96576deb493ee3900d48575660,"AbstractA moving-particle semi-implicit (MPS) method for simulating fragmentation of incompressible fluids is presented. The motion of each particle is calculated through interactions with neighboring particles covered with the kernel function. Deterministic particle interaction models representing gradient, Laplacian, and free surfaces are proposed. Fluid density is implicitly required to be constant as the incompressibility condition, while the other terms are explicitly calculated. The Poisson equation of pressure is solved by the incomplete Cholesky conjugate gradient method. Collapse of a water column is calculated using MPS. The effect of parameters in the models is investigated in test calculations. Good agreement with an experiment is obtained even if fragmentation and coalescence of the fluid take place.",1996,21,1492,129,3,0,3,7,4,12,12,18,25,19
c9af6f7c0bbe1e96188277f5965ad8edf1dd7479,"We review the development of diffuse-interface models of hydrodynamics and their application to a wide variety of interfacial phenomena. These models have been applied successfully to situations in which the physical phenomena of interest have a length scale commensurate with the thickness of the interfacial region (e.g. near-critical interfacial phenomena or small-scale flows such as those occurring near contact lines) and fluid flows involving large interface deformations and/or topological changes (e.g. breakup and coalescence events associated with fluid jets, droplets, and large-deformation waves). We discuss the issues involved in formulating diffuse-interface models for single-component and binary fluids. Recent applications and computations using these models are discussed in each case. Further, we address issues including sharp-interface analyses that relate these models to the classical free-boundary problem, computational approaches to describe interfacial phenomena, and models of fully miscible fluids.",1997,116,1614,57,0,4,10,6,22,22,25,22,39,56
f4dca1a08439ae0a13d44dba3774234c5c5b8cab,"Realistically animated fluids can add substantial realism to interactive applications such as virtual surgery simulators or computer games. In this paper we propose an interactive method based on Smoothed Particle Hydrodynamics (SPH) to simulate fluids with free surfaces. The method is an extension of the SPH-based technique by Desbrun to animate highly deformable bodies. We gear the method towards fluid simulation by deriving the force density fields directly from the Navier-Stokes equation and by adding a term to model surface tension effects. In contrast to Eulerian grid-based approaches, the particle-based approach makes mass conservation equations and convection terms dispensable which reduces the complexity of the simulation. In addition, the particles can directly be used to render the surface of the fluid. We propose methods to track and visualize the free surface using point splatting and marching cubes-based surface reconstruction. Our animation method is fast enough to be used in interactive systems and to allow for user interaction with models consisting of up to 5000 particles.",2003,25,1273,142,5,23,37,56,67,58,97,83,84,88
20aeb2357e9e215787c7e0d0acfe7a6b598c9103,"This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkisons Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, its easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot. This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and youll learn everything you need in the book. After reading this book youll be able to produce graphics customized precisely for your problems,and youll find it easy to get graphics out of your head and on to the screen or page.",2009,0,20836,1801,0,0,0,0,0,1,2,0,3,9
6dea759b9e6d08b1e8ae7c3ac135234008e26aec,"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as 'Coot'.",2004,21,24373,1589,0,0,0,0,0,0,1,0,0,0
412a0bb5a3baa91b62053d82c562bc172df0439f,"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems",2007,0,12593,918,0,0,0,0,0,0,0,0,5,46
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17519,724,0,0,0,0,0,0,0,1,1,2
d0a47c8c9c4213edd24a120a6912ebc08e348a19,"Abstract In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.",1996,7,10044,920,0,0,0,1,29,82,204,435,631,683
8e3f2b578ce2d2bb969ab64d4ca43eaa147f0674,"Publisher Summary This chapter discusses Raster3D, which is a suite of programs for molecular graphics. Crystallographers were among the first and most avid consumers of graphics workstations. Rapid advances in computer hardware, and particularly in the power of specialized computer graphics boards, have led to successive generations of personal workstations with ever more impressive capabilities for interactive molecular graphics. For many years, it was standard practice in crystallography laboratories to prepare figures by photographing directly from the workstation screen. No matter how beautiful the image on the screen, however, this approach suffers from several intrinsic limitations. Among these is the inherent limitation imposed by the effective resolution of the screen. Use of the graphics hardware in a workstation to generate images for later presentation can also impose other limitations. Designers of workstation hardware must compromise the quality of rendered images to achieve rendering speeds high enough for useful interactive manipulation of three-dimensional objects.",1997,13,3661,63,1,42,159,253,356,444,502,522,404,248
319bef16b09502a4c6d374a947ad74006cc1cda2,,1997,0,3410,0,195,200,182,208,208,197,199,198,178,186
793b4d2f5c2e3ceb53946e94f2ea2ff1d5dc1522,,1995,6,2472,120,2,22,49,88,86,104,131,133,171,202
3da75ffd1b1b7dacf37e0dd880e214fdb47980d5,"Raster3D Version 2.0 is a program suite for the production of photorealistic molecular graphics images. The code is hardware independent, and is particularly suited for use in producing large raster images of macromolecules for output to a film recorder or high-quality color printer. The Raster3D suite contains programs for composing illustrations of space-filling models, ball-and-stick models and ribbon-and-cylinder representations. It may also be used to render figures composed using other graphics tools, notably the widely used program Molscript [Kraulis (1991). J. Appl. Cryst. 24, 946-950].",1994,0,2417,61,1,39,73,147,196,223,238,221,192,208
459d0724afbcd591edde2dcc83e5e2750d878c0e,"These are the short notes for a two hour tutorial on principles and practice of computer graphics and scientific visualization. They are intended to summarize the contents of the tutorial transparencies and slides but they cannot completely replace them since restrictions in space and print quality do not permit the inclusion of figures and example images. For further reference the following standard text should be consulted: [3, 8, 5, 1, 6, 2, 9]",1997,4,2290,58,111,154,133,109,120,143,126,119,111,104
2bbf413f36f366fa73da4dc028a32131b5d205d6,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this �eld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.",2005,398,1911,121,16,67,147,188,202,206,228,165,158,136
356869aa0ae8d598e956c7f2ae884bbf5009c98c,"To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.",2008,16,1404,142,24,129,164,170,135,146,129,103,88,85
f8e1b3bcee316203b4aa843efd5f581bc16849dc,"From the Publisher: 
Visualization is a part of every day life. From weather map generation of financial modelling to MRI technology in medicine to 3D graphics used in movies like Jurassic Park, examples of visualization abound. The book/CD package offers readers the opportunity to practice visualization using a complete C++ programming environment developed by the authors.",1997,0,1949,122,23,34,57,71,83,65,77,108,103,101
f978d481fae83e57202d26d4fbd38e330889ea75,"Graphics processing units (GPUs), originally developed for rendering real-time effects in computer games, now provide unprecedented computational power for scientific applications. In this paper, we develop a general purpose molecular dynamics code that runs entirely on a single GPU. It is shown that our GPU implementation provides a performance equivalent to that of fast 30 processor core distributed memory cluster. Our results show that GPUs already provide an inexpensive alternative to such clusters and discuss implications for the future.",2008,26,1327,40,14,31,70,81,89,135,91,121,100,111
54dddebd5f6259d9f2f9b859007bb150da453535,"A model is developed of the human lower extremity to study how changes in musculoskeletal geometry and musculotendon parameters affect muscle force and its moment about the joints. The lines of action of 43 musculotendon actuators were defined based on their anatomical relationships to three-dimensional bone surface representations. A model for each actuator was formulated to compute its isometric force-length relation. The kinematics of the lower extremity were defined by modeling the hip, knee, ankle, subtalar, and metatarsophalangeal joints. Thus, the force and joint moment that each musculotendon actuator develops can be computed for any body position. The joint moments calculated with the model compare well with experimentally measured isometric joint moments. A graphical interface to the model has also been developed. It allows the user to visualize the musculoskeletal geometry and to manipulate the model parameters to study the biomechanical consequences of orthopaedic surgical procedures. For example, tendon transfer and lengthening procedures can be simulated by adjusting the model parameters according to various surgical techniques. Results of the simulated surgeries can be analyzed quickly in terms of postsurgery muscle forces and other biomechanical variables.<<ETX>>",1990,50,1679,139,2,3,5,7,22,18,15,25,17,24
6cae8a08979c34110a85428f06c973b38828eac6,,2004,0,1519,13,25,41,39,76,137,161,169,171,131,132
3c47192d40b1138f4e757948c64bb846c38c53ba,This paper presents a new reflectance model for rendering computer synthesized images. The model accounts for the relative brightness of different materials and light sources in the same scene. It describes the directional distribution of the reflected light and a color shift that occurs as the reflectance changes with incidence angle. The paper presents a method for obtaining the spectral energy distribution of the light reflected from an object made of a specific real material and discusses a procedure for accurately reproducing the color associated with the spectral energy distribution. The model is applied to the simulation of a metal and a plastic.,1987,30,1665,115,16,19,16,21,10,22,15,19,13,21
5ba7042c5220548c9d5636df3cc2c84bb8641e02,"Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.
 The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.",1980,8,1841,91,1,1,7,2,0,3,6,3,3,6
e80dc3681b94fbba79523201ab180852f901e618,"Abstract The role of computer graphics in different aspects of simulating matter on the atomic scale is discussed. The computer graphics is useful in specifying and examining chemical structures, since it is nowadays possible to study––with density functional theory––complex systems containing up to a few hundreds in-equivalent atoms. Furthermore, computer graphics is also an indispensable tool in analysing computed data and facilitates interpretation of results. In this context XCrySDen ( http://www.xcrysden.org/ ) is presented, a crystalline- and molecular-structure visualisation program, which aims at display of isosurfaces and contours, which can be superimposed on crystalline structures and interactively rotated and manipulated. Another aspect of computer utilisation in simulations that takes advantage of the computer’s graphics capabilities, is that it provides intuitive graphical user interfaces for the simulation setup. It is demonstrated how such interfaces are easily built using the developed GUIB software ( http://www-k3.ijs.si/kokalj/guib/ ).",2003,5,1305,17,0,1,4,18,31,53,43,44,60,64
4c231a788f20951fabc42abaa440c984a8f67661,"Fundamentals of interactive computer graphics , Fundamentals of interactive computer graphics , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,1773,18,12,44,51,91,117,125,146,126,121,90
6c73a2b81ba564a7f6fd89808f7629538adde6da,"1. Introduction. Image Processing as Picture Analysis. The Advantages of Interactive Graphics. Representative Uses of Computer Graphics. Classification of Applications. Development of Hardware and Software for Computer Graphics. Conceptual Framework for Interactive Graphics. 2. Programming in the Simple Raster Graphics Package (SRGP)/. Drawing with SRGP/. Basic Interaction Handling/. Raster Graphics Features/. Limitations of SRGP/. 3. Basic Raster Graphics Algorithms for Drawing 2d Primitives. Overview. Scan Converting Lines. Scan Converting Circles. Scan Convertiing Ellipses. Filling Rectangles. Fillign Polygons. Filling Ellipse Arcs. Pattern Filling. Thick Primiives. Line Style and Pen Style. Clipping in a Raster World. Clipping Lines. Clipping Circles and Ellipses. Clipping Polygons. Generating Characters. SRGP_copyPixel. Antialiasing. 4. Graphics Hardware. Hardcopy Technologies. Display Technologies. Raster-Scan Display Systems. The Video Controller. Random-Scan Display Processor. Input Devices for Operator Interaction. Image Scanners. 5. Geometrical Transformations. 2D Transformations. Homogeneous Coordinates and Matrix Representation of 2D Transformations. Composition of 2D Transformations. The Window-to-Viewport Transformation. Efficiency. Matrix Representation of 3D Transformations. Composition of 3D Transformations. Transformations as a Change in Coordinate System. 6. Viewing in 3D. Projections. Specifying an Arbitrary 3D View. Examples of 3D Viewing. The Mathematics of Planar Geometric Projections. Implementing Planar Geometric Projections. Coordinate Systems. 7. Object Hierarchy and Simple PHIGS (SPHIGS). Geometric Modeling. Characteristics of Retained-Mode Graphics Packages. Defining and Displaying Structures. Modeling Transformations. Hierarchical Structure Networks. Matrix Composition in Display Traversal. Appearance-Attribute Handling in Hierarchy. Screen Updating and Rendering Modes. Structure Network Editing for Dynamic Effects. Interaction. Additional Output Features. Implementation Issues. Optimizing Display of Hierarchical Models. Limitations of Hierarchical Modeling in PHIGS. Alternative Forms of Hierarchical Modeling. 8. Input Devices, Interaction Techniques, and Interaction Tasks. Interaction Hardware. Basic Interaction Tasks. Composite Interaction Tasks. 9. Dialogue Design. The Form and Content of User-Computer Dialogues. User-Interfaces Styles. Important Design Considerations. Modes and Syntax. Visual Design. The Design Methodology. 10. User Interface Software. Basic Interaction-Handling Models. Windows-Management Systems. Output Handling in Window Systems. Input Handling in Window Systems. Interaction-Technique Toolkits. User-Interface Management Systems. 11. Representing Curves and Surfaces. Polygon Meshes. Parametric Cubic Curves. Parametric Bicubic Surfaces. Quadric Surfaces. 12. Solid Modeling. Representing Solids. Regularized Boolean Set Operations. Primitive Instancing. Sweep Representations. Boundary Representations. Spatial-Partitioning Representations. Constructive Solid Geometry. Comparison of Representations. User Interfaces for Solid Modeling. 13. Achromatic and Colored Light. Achromatic Light. Chromatic Color. Color Models for Raster Graphics. Reproducing Color. Using Color in Computer Graphics. 14. The Quest for Visual Realism. Why Realism? Fundamental Difficulties. Rendering Techniques for Line Drawings. Rendering Techniques for Shaded Images. Improved Object Models. Dynamics. Stereopsis. Improved Displays. Interacting with Our Other Senses. Aliasing and Antialiasing. 15. Visible-Surface Determination. Functions of Two Variables. Techniques for Efficient Visible-Surface Determination. Algorithms for Visible-Line Determination. The z-Buffer Algorithm. List-Priority Algorithms. Scan-Line Algorithms. Area-Subdivision Algorithms. Algorithms for Octrees. Algorithms for Curved Surfaces. Visible-Surface Ray Tracing. 16. Illumination And Shading. Illumination Modeling. Shading Models for Polygons. Surface Detail. Shadows. Transparency. Interobject Reflections. Physically Based Illumination Models. Extended Light Sources. Spectral Sampling. Improving the Camera Model. Global Illumination Algorithms. Recursive Ray Tracing. Radiosity Methods. The Rendering Pipeline. 17. Image Manipulation and Storage. What Is an Image? Filtering. Image Processing. Geometric Transformations of Images. Multipass Transformations. Image Compositing. Mechanisms for Image Storage. Special Effects with Images. Summary. 18. Advanced Raster Graphic Architecture. Simple Raster-Display System. Display-Processor Systems. Standard Graphics Pipeline. Introduction to Multiprocessing. Pipeline Front-End Architecture. Parallel Front-End Architectures. Multiprocessor Rasterization Architectures. Image-Parallel Rasterization. Object-Parallel Rasterization. Hybrid-Parallel Rasterization. Enhanced Display Capabilities. 19. Advanced Geometric and Raster Algorithms. Clipping. Scan-Converting Primitives. Antialiasing. The Special Problems of Text. Filling Algorithms. Making copyPixel Fast. The Shape Data Structure and Shape Algebra. Managing Windows with bitBlt. Page Description Languages. 20. Advanced Modeling Techniques. Extensions of Previous Techniques. Procedural Models. Fractal Models. Grammar-Based Models. Particle Systems. Volume Rendering. Physically Based Modeling. Special Models for Natural and Synthetic Objects. Automating Object Placement. 21. Animation. Conventional and Computer-Assisted Animation. Animation Languages. Methods of Controlling Animation. Basic Rules of Animation. Problems Peculiar to Animation. Appendix: Mathematics for Computer Graphics. Vector Spaces and Affine Spaces. Some Standard Constructions in Vector Spaces. Dot Products and Distances. Matrices. Linear and Affine Transformations. Eigenvalues and Eigenvectors. Newton-Raphson Iteration for Root Finding. Bibliography. Index. 0201848406T04062001",1995,0,1285,93,34,46,48,56,47,74,49,43,81,77
a9918035dea348663c149a854ef92a9d222d3680,"A model building and refinement system is described for use with a Vector General 3400 display. The system allows the user to build models using guide atoms and angles to arrive at the final conformation. It has been used to assist in difference Fourier map interpretation at medium and high resolution, and to build a protein molecule into a multiple isomorphous replacement phased electron density map.",1978,7,1682,22,1,2,5,8,10,25,25,44,31,73
d57d75a95d1df5a421de98c34679ac719f374b76,"We design and implement Mars, a MapReduce framework, on graphics processors (GPUs). MapReduce is a distributed programming framework originally proposed by Google for the ease of development of web search applications on a large number of commodity CPUs. Compared with CPUs, GPUs have an order of magnitude higher computation power and memory bandwidth, but are harder to program since their architectures are designed as a special-purpose co-processor and their programming interfaces are typically for graphics applications. As the first attempt to harness GPU's power for MapReduce, we developed Mars on an NVIDIA G80 GPU, which contains over one hundred processors, and evaluated it in comparison with Phoenix, the state-of-the-art MapReduce framework on multi-core CPUs. Mars hides the programming complexity of the GPU behind the simple and familiar MapReduce interface. It is up to 16 times faster than its CPU-based counterpart for six common web applications on a quad-core machine.",2008,35,804,62,2,34,65,67,81,110,104,88,78,48
5c6b7b39f0ea433116f81cb7a3372684e6697b4a,"A liquid proportioning system includes two groups of positive metering tanks, each group consisting of at least two tanks each containing a supply of liquid to be blended and including outlet control device for selectively regulating the volume of liquid leaving the tank per unit of time. The liquids from each tank are fed together through a single conduit to a mixer which continuously blends the liquid components as they are flowing, and the blended liquids are fed to a reservoir tank, from which they are fed to a point of use in accordance with the variable requirements of the latter. One group of positive metering tanks feeds the liquid components to the mixer in exact proportions until these tanks are depleted, at which time the flow from these tanks is shut off automatically and the second group of tanks begins to feed the portioned liquid components to the mixer, while the first group of tanks are refilled. When the requirements of the point of use are decreased, sensors slow down the feeding of the liquid to the reservoir tank by increasing the time interval between the emptying of one group of metering tanks and the commencement of feeding from the other group of tanks.",1983,0,1456,92,0,1,1,3,1,7,5,16,14,22
fa8eaed21e0ff6cba18c8661d422c3be83880909,"BackgroundAnalyses of biomolecules for biodiversity, phylogeny or structure/function studies often use graphical tree representations. Many powerful tree editors are now available, but existing tree visualization tools make little use of meta-information related to the entities under study such as taxonomic descriptions or gene functions that can hardly be encoded within the tree itself (if using popular tree formats). Consequently, a tedious manual analysis and post-processing of the tree graphics are required if one needs to use external information for displaying or investigating trees.ResultsWe have developed TreeDyn, a tool using annotations and dynamic graphical methods for editing and analyzing multiple trees. The main features of TreeDyn are 1) the management of multiple windows and multiple trees per window, 2) the export of graphics to several standard file formats with or without HTML encapsulation and a new format called TGF, which enables saving and restoring graphical analysis, 3) the projection of texts or symbols facing leaf labels or linked to nodes, through manual pasting or by using annotation files, 4) the highlight of graphical elements after querying leaf labels (or annotations) or by selection of graphical elements and information extraction, 5) the highlight of targeted trees according to a source tree browsed by the user, 6) powerful scripts for automating repetitive graphical tasks, 7) a command line interpreter enabling the use of TreeDyn through CGI scripts for online building of trees, 8) the inclusion of a library of packages dedicated to specific research fields involving trees.ConclusionTreeDyn is a tree visualization and annotation tool which includes tools for tree manipulation and annotation and uses meta-information through dynamic graphical operators or scripting to help analyses and annotations of single trees or tree collections.",2006,45,934,82,2,16,31,32,63,73,70,77,84,94
e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1,"The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.
 In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.",2009,43,618,21,7,19,24,19,29,47,62,60,67,81
f74e04c23db19d746096eec7821f10112690f392,"We present a case study on the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. Graphics cards, containing multiple Graphics Processing Units (GPUs), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers and can be thought of as prototypes of the next generation of many-core processors. For certain classes of population-based Monte Carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multicore processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. On a canonical set of stochastic simulation examples including population-based Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we find speedups from 35- to 500-fold over conventional single-threaded computer code. Our findings suggest that GPUs have the potential to facilitate the growth of statistical modeling into complex data-rich domains through the availability of cheap and accessible many-core computation. We believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design. This article has supplementary material online.",2009,49,307,22,1,17,22,35,49,36,34,33,19,18
72418a969890621cfe99e470889ed0bedd0dba98,"We report a parallel Monte Carlo algorithm accelerated by graphics processing units (GPU) for modeling time-resolved photon migration in arbitrary 3D turbid media. By taking advantage of the massively parallel threads and low-memory latency, this algorithm allows many photons to be simulated simultaneously in a GPU. To further improve the computational efficiency, we explored two parallel random number generators (RNG), including a floating-point-only RNG based on a chaotic lattice. An efficient scheme for boundary reflection was implemented, along with the functions for time-resolved imaging. For a homogeneous semi-infinite medium, good agreement was observed between the simulation output and the analytical solution from the diffusion theory. The code was implemented with CUDA programming language, and benchmarked under various parameters, such as thread number, selection of RNG and memory access pattern. With a low-cost graphics card, this algorithm has demonstrated an acceleration ratio above 300 when using 1792 parallel threads over conventional CPU computation. The acceleration ratio drops to 75 when using atomic operations. These results render the GPU-based Monte Carlo simulation a practical solution for data analysis in a wide range of diffuse optical imaging applications, such as human brain or small-animal imaging.",2009,23,599,18,0,18,37,51,32,60,55,52,58,49
852e427df0329c0b9b033783246c3189b65b0a04,"Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. Lately, another property of DG has been growing in importance: The majority of a DG operator is applied in an element-local way, with weak penalty-based element-to-element coupling. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG's high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Using a single US$400 Nvidia GTX 280 GPU, we accelerate a solver for Maxwell's equations on a general 3D unstructured grid by a factor of around 50 relative to a serial computation on a current-generation CPU. In many cases, our algorithms exhibit full use of the device's available memory bandwidth. Example computations achieve and surpass 200gigaflops/s of net application-level floating point work. In this article, we describe and derive the techniques used to reach this level of performance. In addition, we present comprehensive data on the accuracy and runtime behavior of the method.",2009,31,426,37,8,33,45,40,47,42,50,55,35,33
d3c41281a5adb9626c1e05f5fc59ef2f8242438e,"We describe a complete implementation of all‐atom protein molecular dynamics running entirely on a graphics processing unit (GPU), including all standard force field terms, integration, constraints, and implicit solvent. We discuss the design of our algorithms and important optimizations needed to fully take advantage of a GPU. We evaluate its performance, and show that it can be more than 700 times faster than a conventional implementation running on a single CPU core. © 2009 Wiley Periodicals, Inc. J Comput Chem, 2009",2009,25,508,12,15,58,66,67,60,43,36,35,30,32
03aa649535c7e01ac2b3255f2f44131380dc93c7,"Graphics processors (GPUs) provide a vast number of simple, data-parallel, deeply multithreaded cores and high memory bandwidths. GPU architectures are becoming increasingly programmable, offering the potential for dramatic speedups for a variety of general-purpose applications compared to contemporary general-purpose processors (CPUs). This paper uses NVIDIA's C-like CUDA language and an engineering sample of their recently introduced GTX 260 GPU to explore the effectiveness of GPUs for a variety of application types, and describes some specific coding idioms that improve their performance on the GPU. GPU performance is compared to both single-core and multicore CPU performance, with multicore CPU implementations written using OpenMP. The paper also discusses advantages and inefficiencies of the CUDA programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body of applications.",2008,51,671,28,7,52,73,76,77,82,68,64,53,45
f74c526d01e8d7d910a41a3fdf6587e157868c4f,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general‐purpose computation to graphics hardware.",2007,302,753,9,48,60,91,70,93,64,81,51,49,30
c715fd808676921a84f248520925f8f5082cc83e,"1: Introduction.- 1.1 Graphics, Image Processing, and Pattern Recognition.- 1.2 Forms of Pictorial Data.- 1.2.1 Class 1: Full Gray Scale and Color Pictures.- 1.2.2 Class 2: Bilevel or ""Few Color"" pictures.- 1.2.3 Class 3: Continuous Curves and Lines.- 1.2.4 Class 4: Points or Polygons.- 1.3 Pictorial Input.- 1.4 Display Devices.- 1.5 Vector Graphics.- 1.6 Raster Graphics.- 1.7 Common Primitive Graphic Instructions.- 1.8 Comparison of Vector and Raster Graphics.- 1.9 Pictorial Editor.- 1.10 Pictorial Transformations.- 1.11 Algorithm Notation.- 1.12 A Few Words on Complexity.- 1.13 Bibliographical Notes.- 1.14 Relevant Literature.- 1.15 Problems.- 2: Digitization of Gray Scale Images.- 2.1 Introduction.- 2.2 A Review of Fourier and other Transforms.- 2.3 Sampling.- 2.3.1 One-dimensional Sampling.- 2.3.2 Two-dimensional Sampling.- 2.4 Aliasing.- 2.5 Quantization.- 2.6 Bibliographical Notes.- 2.7 Relevant Literature.- 2.8 Problems.- Appendix 2.A: Fast Fourier Transform.- 3: Processing of Gray Scale Images.- 3.1 Introduction.- 3.2 Histogram and Histogram Equalization.- 3.3 Co-occurrence Matrices.- 3.4 Linear Image Filtering.- 3.5 Nonlinear Image Filtering.- 3.5.1 Directional Filters.- 3.5.2 Two-part Filters.- 3.5.3 Functional Approximation Filters.- 3.6 Bibliographical Notes.- 3.7 Relevant Literature.- 3.8 Problems.- 4: Segmentation.- 4.1 Introduction.- 4.2 Thresholding.- 4.3 Edge Detection.- 4.4 Segmentation by Region Growing.- 4.4.1 Segmentation by Average Brightness Level.- 4.4.2 Other Uniformity Criteria.- 4.5 Bibliographical Notes.- 4.6 Relevant Literature.- 4.7 Problems.- 5: Projections.- 5.1 Introduction.- 5.2 Introduction to Reconstruction Techniques.- 5.3 A Class of Reconstruction Algorithms.- 5.4 Projections for Shape Analysis.- 5.5 Bibliographical Notes.- 5.6 Relevant Literature.- 5.7 Problems.- Appendix 5.A: An Elementary Reconstruction Program.- 6: Data Structures.- 6.1 Introduction.- 6.2 Graph Traversal Algorithms.- 6.3 Paging.- 6.4 Pyramids or Quad Trees.- 6.4.1 Creating a Quad Tree.- 6.4.2 Reconstructing an Image from a Quad Tree.- 6.4.3 Image Compaction with a Quad Tree.- 6.5 Binary Image Trees.- 6.6 Split-and-Merge Algorithms.- 6.7 Line Encodings and the Line Adjacency Graph.- 6.8 Region Encodings and the Region Adjacency Graph.- 6.9 Iconic Representations.- 6.10 Data Structures for Displays.- 6.11 Bibliographical Notes.- 6.12 Relevant Literature.- 6.13 Problems.- Appendix 6.A: Introduction to Graphs.- 7: Bilevel Pictures.- 7.1 Introduction.- 7.2 Sampling and Topology.- 7.3 Elements of Discrete Geometry.- 7.4 A Sampling Theorem for Class 2 Pictures.- 7.5 Contour Tracing.- 7.5.1 Tracing of a Single Contour.- 7.5.2 Traversal of All the Contours of a Region.- 7.6 Curves and Lines on a Discrete Grid.- 7.6.1 When a Set of Pixels is not a Curve.- 7.6.2 When a Set of Pixels is a Curve.- 7.7 Multiple Pixels.- 7.8 An Introduction to Shape Analysis.- 7.9 Bibliographical Notes.- 7.10 Relevant Literature.- 7.11 Problems.- 8: Contour Filling.- 8.1 Introduction.- 8.2 Edge Filling.- 8.3 Contour Filling by Parity Check.- 8.3.1 Proof of Correctness of Algorithm 8.3.- 8.3.2 Implementation of a Parity Check Algorithm.- 8.4 Contour Filling by Connectivity.- 8.4.1 Recursive Connectivity Filling.- 8.4.2 Nonrecursive Connectivity Filling.- 8.4.3 Procedures used for Connectivity Filling.- 8.4.4 Description of the Main Algorithm.- 8.5 Comparisons and Combinations.- 8.6 Bibliographical Notes.- 8.7 Relevant Literature.- 8.8 Problems.- 9: Thinning Algorithms.- 9.1 Introduction.- 9.2 Classical Thinning Algorithms.- 9.3 Asynchronous Thinning Algorithms.- 9.4 Implementation of an Asynchronous Thinning Algorithm.- 9.5 A Quick Thinning Algorithm.- 9.6 Structural Shape Analysis.- 9.7 Transformation of Bilevel Images into Line Drawings.- 9.8 Bibliographical Notes.- 9.9 Relevant Literature.- 9.10 Problems.- 10: Curve Fitting and Curve Displaying.- 10.1 Introduction.- 10.2 Polynomial Interpolation.- 10.3 Bezier Polynomials.- 10.4 Computation of Bezier Polynomials.- 10.5 Some Properties of Bezier Polynomials.- 10.6 Circular Arcs.- 10.7 Display of Lines and Curves.- 10.7.1 Display of Curves through Differential Equations.- 10.7.2 Effect of Round-off Errors in Displays.- 10.8 A Point Editor.- 10.8.1 A Data Structure for a Point Editor.- 10.8.2 Input and Output for a Point Editor.- 10.9 Bibliographical Notes.- 10.10 Relevant Literature.- 10.11 Problems.- 11: Curve Fitting with Splines.- 11.1 Introduction.- 11.2 Fundamental Definitions.- 11.3 B-Splines.- 11.4 Computation with B-Splines.- 11.5 Interpolating B-Splines.- 11.6 B-Splines in Graphics.- 11.7 Shape Description and B-splines.- 11.8 Bibliographical Notes.- 11.9 Relevant Literature.- 11.10 Problems.- 12: Approximation of Curves.- 12.1 Introduction.- 12.2 Integral Square Error Approximation.- 12.3 Approximation Using B-Splines.- 12.4 Approximation by Splines with Variable Breakpoints.- 12.5 Polygonal Approximations.- 12.5.1 A Suboptimal Line Fitting Algorithm.- 12.5.2 A Simple Polygon Fitting Algorithm.- 12.5.3 Properties of Algorithm 12.2.- 12.6 Applications of Curve Approximation in Graphics.- 12.6.1 Handling of Groups of Points by a Point Editor.- 12.6.2 Finding Some Simple Approximating Curves.- 12.7 Bibliographical Notes.- 12.8 Relevant Literature.- 12.9 Problems.- 13: Surface Fitting and Surface Displaying.- 13.1 Introduction.- 13.2 Some Simple Properties of Surfaces.- 13.3 Singular Points of a Surface.- 13.4 Linear and Bilinear Interpolating Surface Patches.- 13.5 Lofted Surfaces.- 13.6 Coons Surfaces.- 13.7 Guided Surfaces.- 13.7.1 Bezier Surfaces.- 13.7.2 B-Spline Surfaces.- 13.8 The Choice of a Surface Partition.- 13.9 Display of Surfaces and Shading.- 13.10 Bibliographical Notes.- 13.11 Relevant Literature.- 13.12 Problems.- 14: The Mathematics of Two-Dimensional Graphics.- 14.1 Introduction.- 14.2 Two-Dimensional Transformations.- 14.3 Homogeneous Coordinates.- 14.3.1 Equation of a Line Defined by Two Points.- 14.3.2 Coordinates of a Point Defined as the Intersection of Two Lines.- 14.3.3 Duality.- 14.4 Line Segment Problems.- 14.4.1 Position of a Point with respect to a Line.- 14.4.2 Intersection of Line Segments.- 14.4.3 Position of a Point with respect to a Polygon.- 14.4.4 Segment Shadow.- 14.5 Bibliographical Notes.- 14.6 Relevant Literature.- 14.7 Problems.- 15: Polygon Clipping.- 15.1 Introduction.- 15.2 Clipping a Line Segment by a Convex Polygon.- 15.3 Clipping a Line Segment by a Regular Rectangle.- 15.4 Clipping an Arbitrary Polygon by a Line.- 15.5 Intersection of Two Polygons.- 15.6 Efficient Polygon Intersection.- 15.7 Bibliographical Notes.- 15.8 Relevant Literature.- 15.9 Problems.- 16: The Mathematics of Three-Dimensional Graphics.- 16.1 Introduction.- 16.2 Homogeneous Coordinates.- 16.2.1 Position of a Point with respect to a Plane.- 16.2.2 Intersection of Triangles.- 16.3 Three-Dimensional Transformations.- 16.3.1 Mathematical Preliminaries.- 16.3.2 Rotation around an Axis through the Origin.- 16.4 Orthogonal Projections.- 16.5 Perspective Projections.- 16.6 Bibliographical Notes.- 16.7 Relevant Literature.- 16.8 Problems.- 17: Creating Three-Dimensional Graphic Displays.- 17.1 Introduction.- 17.2 The Hidden Line and Hidden Surface Problems.- 17.2.1 Surface Shadow.- 17.2.2 Approaches to the Visibility Problem.- 17.2.3 Single Convex Object Visibility.- 17.3 A Quad Tree Visibility Algorithm.- 17.4 A Raster Line Scan Visibility Algorithm.- 17.5 Coherence.- 17.6 Nonlinear Object Descriptions.- 17.7 Making a Natural Looking Display.- 17.8 Bibliographical Notes.- 17.9 Relevant Literature.- 17.10 Problems.- Author Index.- Algorithm Index.",1981,0,1207,40,0,2,8,14,20,29,42,52,49,68
25e80b49c3546876eaed9b1437ffe7cac32ca00f,"The purpose of this article is to illustrate the steps involved in testing for multigroup invariance using Amos Graphics. Based on analysis of covariance (ANCOV) structures, 2 applications are demonstrated, each of which represents a different set of circumstances. Application 1 focuses on the equivalence of a measuring instrument and tests for its invariance across 3 teacher panels, given baseline models that are identical across groups. Application 2 centers on the equivalence of a postulated theoretical structure across adolescent boys and girls in light of baseline models that are differentially specified across groups. Taken together, these illustrated examples should be of substantial assistance to researchers interested in testing for multigroup invariance using the Amos program.",2004,22,829,111,1,2,12,19,35,40,51,60,46,41
b999766e1685da444ab997b6a21a741a169ad7b1,"Molecular mechanics simulations offer a computational approach to study the behavior of biomolecules at atomic detail, but such simulations are limited in size and timescale by the available computing resources. State‐of‐the‐art graphics processing units (GPUs) can perform over 500 billion arithmetic operations per second, a tremendous computational resource that can now be utilized for general purpose computing as a result of recent advances in GPU hardware and software architecture. In this article, an overview of recent advances in programmable GPUs is presented, with an emphasis on their application to molecular mechanics simulations and the programming techniques required to obtain optimal performance in these cases. We demonstrate the use of GPUs for the calculation of long‐range electrostatics and nonbonded forces for molecular dynamics simulations, where GPU‐based calculations are typically 10–100 times faster than heavily optimized CPU‐based implementations. The application of GPU acceleration to biomolecular simulation is also demonstrated through the use of GPU‐accelerated Coulomb‐based ion placement and calculation of time‐averaged potentials from molecular dynamics trajectories. A novel approximation to Coulomb potential calculation, the multilevel summation method, is introduced and compared with direct Coulomb summation. In light of the performance obtained for this set of calculations, future applications of graphics processors to molecular dynamics simulations are discussed. © 2007 Wiley Periodicals, Inc. J Comput Chem, 2007",2007,143,690,25,2,39,54,64,88,86,85,52,46,38
f6ac5b2dab21d5d9b395a3c8bb7b92dbf996443d,"In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.",2004,36,830,79,11,38,40,53,82,121,85,93,74,70
d7316857b373a5b8dfc9008408dd76ffd8456542,"A typical PC has at least three cooling fans...and one case heater. That "" heater "" would be the graphics-processing unit (GPU) — usually a separate, highly specialized microprocessor chip dedicated to graphics. It does a brilliant job when the PC is running a graphics-intensive game or playing a video. At other times, it's largely underutilized, radiating unused power as heat. In fact, a discrete GPU is the most underutilized component in a PC. Although it's capable of amazing things, it spends much of its time performing routine chores, like scrolling the screen. Yet it has the potential to be the swiftest processing engine in the system — and it's already there, just waiting for something to do. For four years, NVIDIA has waged a campaign to redefine the role of GPUs. Not that graphics aren't important. Pushing pixels has been good business for NVIDIA, the world's leading graphics-processor company. Since the early 1990s, NVIDIA's GPUs have offloaded most of the graphics processing from CPUs in hundreds of millions of personal computers and videogame machines. NVIDIA continues to design its GPUs for superb graphics performance. But since 2005, NVIDIA has cultivated another fast-growing market — GPUs for computing applications beyond graphics. At first, these applications were called "" general-purpose GPU "" (GPGPU) computing. Today, NVIDIA prefers to call it "" GPU computing. "" In the professional market, it's often called high-performance computing. By harnessing the massively parallel-processing resources originally designed for 3D graphics, clever programmers can apply GPUs to a much broader range of computing applications. Some of those applications, such as video transcoding, still involve graphics to some degree. Whereas the CPU might spend an hour or more converting a recorded video for uploading to YouTube or burning a DVD, a GPU can tear through the job in minutes. The GPU can also enhance the video frame-by-frame, using consumer versions of sophisticated software once available only to NASA, law-enforcement agencies, and surveillance experts. Other GPU-computing applications are data-intensive tasks having little or nothing to do with graphics. Examples are stock-trading calculations and seismic-data analysis for oil exploration. Additional examples, such as high-resolution medical imaging, combine graphics with heavy-duty number crunching. On these kinds of workloads, an ordinary GPU can blow away the latest multicore CPUs. Link several GPUs together in a workstation or a cluster of systems, and you've got a "" desktop",2009,10,333,9,3,24,37,44,56,35,43,29,29,13
bb01353f818ca226b53433163893efc56c3df32d,"The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF)-based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable location-aware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy.",2000,36,8416,939,18,41,70,138,237,348,377,446,474,558
2a474223090a15445faae325bdedd3691af2b649,1 An Introduction to Radar 2 The Radar Equation 3 MTI and Pulse Doppler Radar 4 Tracking Radar 5 Detection of Signals in Noise 6 Information from Radar Signals 7 Radar Clutter 8 Propogation of Radar Waves 9 The Radar Antenna 10 Radar Transmitters 11 Radar Receiver,1979,33,5529,403,10,13,20,18,22,19,23,18,31,36
1e71e22a3012080f9d6d115080e3f4678e39f0b9,"[1] The Shuttle Radar Topography Mission produced the most complete, highest-resolution digital elevation model of the Earth. The project was a joint endeavor of NASA, the National Geospatial-Intelligence Agency, and the German and Italian Space Agencies and flew in February 2000. It used dual radar antennas to acquire interferometric radar data, processed to digital topographic data at 1 arc sec resolution. Details of the development, flight operations, data processing, and products are provided for users of this revolutionary data set.",2000,52,3518,186,2,5,1,2,1,6,13,14,54,95
a0abeeaef200e83a6d4538423c826a522f4e8b71,"Synthetic aperture radar interferometry is an imaging technique for measuring the topography of a surface, its changes over time, and other changes in the detailed characteristic of the surface. By exploiting the phase of the coherent radar signal, interferometry has transformed radar remote sensing from a largely interpretive science to a quantitative tool, with applications in cartography, geodesy, land cover characterization, and natural hazards. This paper reviews the techniques of interferometry, systems and limitations, and applications in a rapidly growing area of science and engineering.",2000,216,2886,220,23,38,65,78,79,93,114,122,104,141
ce9c55ed58d056b9c0723d048ecd8564fb0c4a16,Overview of Polarimetric Radar Imaging Brief History of Polarimetric Radar Imaging SAR Image Formation: Summary Airborne and Space-Borne PolSAR Systems Description of the Remaining Chapters Electromagnetic Vector Wave and Polarization Descriptors Monochromatic Electromagnetic Plane Wave Polarization Ellipse Jones Vector Stokes Vector Wave Covariance Matrix Electromagnetic Vector Scattering Operators Polarimetric Back Scattering Sinclair S Matrix Scattering Target Vectors k and Omega Polarimetric Coherency T and Covariance C Matrices Polarimetric Mueller M and Kennaugh K Matrices Change of Polarimetric Basis Target Polarimetric Characterization PolSAR Speckle Statistics Fundamental Property of Speckle in SAR Images Speckle Statistics for Multilook-Processed SAR Images Texture Model and K Distribution Effect of Speckle Spatial Correlation Polarimetric and Interferometric SAR Speckle Statistics Phase Difference Distributions of Single-Look and Multilook PolSAR Data Multilook Product Distribution Joint Distribution of Multilook Si2 and Sj2 Multilook Intensity and Amplitude Ratio Distributions Verifications of Multilook PDFs K Distribution for Multilook Polarimetric Data Summary Appendices PolSAR Speckle Filtering Introduction to Speckle Filtering of SAR Imagery Filtering of Single Polarization SAR Data Review of Multipolarization Speckle Filtering Algorithms PolSAR Speckle Filtering Scattering Model-Based PolSAR Speckle Filter Introduction to the Polarimetric Target Decomposition Concept Introduction Dichotomy of the Kennaugh Matrix K Eigenvector-Based Decompositions Model-Based Decompositions Coherent Decompositions The H/A/a Polarimetric Decomposition Theorem Introduction Pure Target Case Probabilistic Model for Random Media Scattering Roll Invariance Property Polarimetric Scattering a Parameter Polarimetric Scattering Entropy (H) Polarimetric Scattering Anisotropy (A) Three-Dimensional H/A/a Classification Space New Eigenvalue-Based Parameters Speckle Filtering Effects on H/A/a PolSAR Terrain and Land-Use Classification Introduction Maximum Likelihood Classifier Based on Complex Gaussian Distribution Complex Wishart Classifier for Multilook PolSAR Data Characteristics of Wishart Distance Measure Supervised Classification Using Wishart Distance Measure Unsupervised Classification Based on Scattering Mechanisms and Wishart Classifier Scattering Model-Based Unsupervised Classification Quantitative Comparison of Classification Capability: Fully PolSAR versus Dual- and Single-Polarization SAR Pol-InSAR Forest Mapping and Classification Introduction Pol-InSAR Scattering Descriptors Forest Mapping and Forest Classification Appendix Selected PolSAR Applications Polarimetric Signature Analysis of Manmade Structures Polarization Orientation Angle Estimation and Applications Ocean Surface Remote Sensing with PolSAR Ionosphere Faraday Rotation Estimation PolSAR Interferometry for Forest Height Estimation Nonstationary Natural Media Analysis from PolSAR Data Using a Two-Dimensional Time-Frequency Approach Appendix A: Eigen Characteristics of Hermitian Matrix Appendix B: PolSARpro Software: The Polariemtric SAR Data Processing and Educational Toolbox Index,2009,0,1799,298,16,47,90,130,125,139,172,197,195,207
a2fd9b9b59646626b5104f781b16ce239c20f03b,Preface. Summary. Nomenclature. 1. Introduction. 2. Radar system theory and interferometric processing. 3. Functional model for radar interferometry. 4. Stochastic model for radar interferometry. 5. Data analysis and interpretation for deformation monitoring. 6. Atmospheric monitoring. 7. Conclusions and recommendations. A. Comparison neutral delay GPS and InSAR. B. Structure function and power spectrum. Bibliography. About the Author. Index.,2001,0,1970,298,4,15,37,37,49,49,71,61,94,118
7701f0dc7a8c9a468bca3b317e5726661959128f,"In this paper, we provide a review of the different approaches used for target decomposition theory in radar polarimetry. We classify three main types of theorem; those based on the Mueller matrix and Stokes vector, those using an eigenvector analysis of the covariance or coherency matrix, and those employing coherent decomposition of the scattering matrix. We unify the formulation of these different approaches using transformation theory and an eigenvector analysis. We show how special forms of these decompositions apply for the important case of backscatter from terrain with generic symmetries.",1996,39,2194,191,4,16,14,27,19,33,40,54,56,78
23eb33af4f0495edff01402ec8eb019e80717897,Foreword. Introduction. Signal Processing Fundamentals. Pulse Compression. Synthetic Aperture Concepts. SAR Signal Properties. The Range Doppler Algorithm. The Chirp Scaling Algorithm. The Omega-K Algorithm. The SPECAN Algorithm. Processing ScanSAR Data. Doppler Parameter Estimation. Comparison of Algorithms. References.,2005,0,1672,246,6,10,16,41,43,79,117,132,127,145
7e7827bdf386be1dbf4da362d93be2078c6a163b,"Geophysical applications of radar interferometry to measure changes in the Earth's surface have exploded in the early 1990s. This new geodetic technique calculates the interference pattern caused by the difference in phase between two images acquired by a spaceborne synthetic aperture radar at two distinct times. The resulting interferogram is a contour map of the change in distance between the ground and the radar instrument. These maps provide an unsurpassed spatial sampling density (∼100 pixels km−2), a competitive precision (∼1 cm), and a useful observation cadence (1 pass month−1). They record movements in the crust, perturbations in the atmosphere, dielectric modifications in the soil, and relief in the topography. They are also sensitive to technical effects, such as relative variations in the radar's trajectory or variations in its frequency standard. We describe how all these phenomena contribute to an interferogram. Then a practical summary explains the techniques for calculating and manipulating interferograms from various radar instruments, including the four satellites currently in orbit: ERS-1, ERS-2, JERS-1, and RADARSAT. The next chapter suggests some guidelines for interpreting an interferogram as a geophysical measurement: respecting the limits of the technique, assessing its uncertainty, recognizing artifacts, and discriminating different types of signal. We then review the geophysical applications published to date, most of which study deformation related to earthquakes, volcanoes, and glaciers using ERS-1 data. We also show examples of monitoring natural hazards and environmental alterations related to landslides, subsidence, and agriculture. In addition, we consider subtler geophysical signals such as postseismic relaxation, tidal loading of coastal areas, and interseismic strain accumulation. We conclude with our perspectives on the future of radar interferometry. The objective of the review is for the reader to develop the physical understanding necessary to calculate an interferogram and the geophysical intuition necessary to interpret it.",1998,222,2175,128,1,19,41,37,61,62,67,65,73,66
401bda48bf45a7401d34a9e65abb4da506f12c50,: Introduction. Electromagnetic Waves and Propagation. Radar and Its Environment. Weather Signals. Doppler Spectra of Other Signals. Weather Signal Processing. Considerations in the Observation of Weather. Precipitation Measurements. Observations of Fair Weather. Appendices. Index.,1984,0,2063,182,0,2,6,1,3,11,10,14,14,13
6f04e7388117ce4263ab24706c3f7f8e0f4f1955,Introduction. Principles of SAR Image Formation. Image Defects and their Correction. Fundamental Properties of SAR Images. Data Models. RCS Reconstruction Filters. RCS Classification and Segmentation. Texture Exploitation. Correlated Textures. Information in Multi-Channel SAR. Analysis Techniques for Multi-Dimensional Images. Target Information. Image Classification.,1998,0,1861,247,5,16,27,36,65,57,55,62,86,63
8c282de3b8cd9f5003d87a0722b2966e28e9757c,"A radar interferometric technique for topographic mapping of surfaces, implemented utilizing a single synthetic aperture radar (SAR) system in a nearly repeating orbit, is discussed. The authors characterize the various sources contributing to the echo correlation statistics, and isolate the term which most closely describes surficial change. They then examine the application of this approach to topographic mapping of vegetated surfaces which may be expected to possess varying backscatter over time. It is found that there is decorrelation increasing with time but that digital terrain model generation remains feasible. The authors present such a map of a forested area in Oregon which also includes some nearly unvegetated lava flows. Such a technique could provide a global digital terrain map. >",1992,20,1999,141,3,7,20,18,23,27,40,31,49,41
98ec941f07556008c3c3226fd755d7c42ea21b3e,"This detailed guide clearly and concisely presents radar digital signal processing for both practicing engineers and engineering students. This revised edition of Fundamentals of Radar Signal Processing provides in-depth coverage of radar digital signal processing (DSP) fundamentals and applications. It has been updated to include coverage of measurement accuracy and target tracking. Additionally, to make it more useful as a teaching tool, it now includes end-of-chapter problems and a solutions manual. New to this Edition: New chapter on Measurement Accuracy and Target Tracking Two new appendices--Important Digital Signal Processing Facts; Important Probability Density Function and Their Relationships Addition of 20 to 30 problems to ends of chapters Solutions manual",2005,0,1533,178,2,16,20,31,49,85,73,84,114,125
8308e7b39d1f556e4041b4630a41aa8435fe1a49,"MIMO (multiple-input multiple-output) radar refers to an architecture that employs multiple, spatially distributed transmitters and receivers. While, in a general sense, MIMO radar can be viewed as a type of multistatic radar, the separate nomenclature suggests unique features that set MIMO radar apart from the multistatic radar literature and that have a close relation to MIMO communications. This article reviews some recent work on MIMO radar with widely separated antennas. Widely separated transmit/receive antennas capture the spatial diversity of the target's radar cross section (RCS). Unique features of MIMO radar are explained and illustrated by examples. It is shown that with noncoherent processing, a target's RCS spatial variations can be exploited to obtain a diversity gain for target detection and for estimation of various parameters, such as angle of arrival and Doppler. For target location, it is shown that coherent processing can provide a resolution far exceeding that supported by the radar's waveform.",2008,44,1640,72,20,71,104,118,127,118,146,146,155,155
26f825b4ae30b08241df7031a900892761c0f9c0,"We have provided a review of some recent results on the emerging technology of MIMO radar with colocated antennas. We have shown that the waveform diversity offered by such a MIMO radar system enables significant superiority over its phased-array counterpart, including much improved parameter identifiability, direct applicability of adaptive techniques for parameter estimation, as well as superior flexibility of transmit beampattern designs. We hope that this overview of our recent results on the MIMO radar, along with the related results obtained by our colleagues, will stimulate the interest deserved by this topic in both academia and government agencies as well as industry.",2007,32,1720,39,1,27,56,98,103,112,104,145,161,163
7b64b006383772f7d6796165be8d19c7fde2fc78,"Interferometric synthetic aperture radar observations provide a means for obtaining high-resolution digital topographic maps from measurements of amplitude and phase of two complex radar images. The phase of the radar echoes may only be measured modulo 2π; however, the whole phase at each point in the image is needed to obtain elevations. We present here our approach to “unwrapping” the 2π ambiguities in the two-dimensional data set. We find that noise and geometrical radar layover corrupt our measurements locally, and these local errors can propagate to form global phase errors that affect the entire image. We show that the local errors, or residues, can be readily identified and avoided in the global phase estimation. We present a rectified digital topographic map derived from our unwrapped phase values.",1988,8,2108,73,2,3,6,5,16,10,23,42,46,34
aafb36d70d896fad4e68ffceafb5ad53197f22c3,"For 11 days in February 2000, the Shuttle Radar Topography Mission (SRTM) successfully recorded by interferometric synthetic aperture radar (InSAR) data of the entire land mass of the earth between 60°N and 57°S. The data acquired in C- and X-bands are processed into the first global digital elevation models (DEMs) at 1 arc sec resolution, by NASA-JPL and German aerospace center (DLR), respectively. From the perspective of the SRTM-X system, we give in this paper an overview of the mission and the DEM production, as well as an evaluation of the DEM product quality. Special emphasis is on challenges and peculiarities of the processing that arose from the unique design of the SRTM system, which has been the first single-pass interferometer in space.",2003,31,1539,118,6,29,50,58,86,78,107,90,86,91
dda5022b7c2a6cae8adb422d29be80a67505b9b2,The Radar Equation. The Matched Filter and Pulse Compression. Imaging and the Rectangular Algorithm. Ancillary Processes in Image Formation. SAR Flight System. Radiometric Calibration of SAR Data. Geometric Calibration of SAR Data. The SAR Ground System. Other Imaging Algorithms. Appendices. List of Acronyms. Index.,1991,0,1884,157,0,3,12,10,22,37,29,39,29,41
f5d5eecbb72a2fff6accf93024d702b0c522f704,"Ground-penetrating radar is a technique which offers a new way of viewing shallow soil and rock conditions. The need to better understanding overburden conditions for activities such as geochemical sampling, geotechnical investigations, and placer exploration, as well as the factors controlling groundwater flow, has generated an increasing demand for techniques which can image the subsurface with higher resolution than previously possible. The areas of application for ground-penetrating radar are diverse. The method has been used successfully to map ice thickness, water depth in lakes, bedrock depth, soil stratigraphy, and water table depth. It is also used to delineate rock fabric, detect voids and identify karst features. The effective application of the radar for the high-resolution definition of soil stratigraphy and fractures in bedrock is highlighted. The basic principles and practices involved in acquiring high quality radar data in the field are illustrated by selected case histories. One example demonstrates how radar has been used to map the bedrock and delineate soil horizons to a depth of more than 20 m. Two case histories show how radar has been used to map fractures and changes of rock type to 40 m range from inside a mine. Another case history demonstrates how radar has also been used to detect and map the extent of groundwater contamination. The corroboration of the radar results by borehole investigations demonstrates the power and utility of the high-resolution radar method as an aid for interpolation and extrapolation of the information obtained with conventional coring programmes. With the advent of new instrumentation and field procedures, the routine application of the radar method is becoming economically viable and the method will see expanded use in the future.",1989,8,1870,118,0,2,5,6,6,17,23,24,27,25
b064bb54e7d367513067982b7d97cf0697418ec0,"Elevation data is vital to successful mission planning, operations and readiness. Traditional methods for producing elevation data are very expensive and time consuming; major cloud belts would never be completed with existing methods. The Shuttle Radar Topography Mission (SRTM) was selected in 1995 as the best means of supplying nearly global, accurate elevation data. The SRTM is an interferometric SAR system that flew during 11-22 February 2000 aboard NASA's Space Shuttle Endeavour and collected highly specialized data that will allow the generation of Digital Terrain Elevation Data Level 2(DTED® 2). The result of the SRTM will increase the United States Government's coverage of vital and detailed DTED® 2from less than 5% to 80% of the Earth's landmass. This paper describes the shuttle mission and its deliverables.",2001,97,1491,152,3,1,5,4,7,5,9,27,41,64
c6279395626764ebb6d3e9f5bf6be09b303013af,"GEODETIC data, obtained by ground- or space-based techniques, can be used to infer the distribution of slip on a fault that has ruptured in an earthquake. Although most geodetic techniques require a surveyed network to be in place before the earthquake1–3, satellite images, when collected at regular intervals, can capture co-seismic displacements without advance knowledge of the earthquake's location. Synthetic aperture radar (SAR) interferometry, first introduced4 in 1974 for topographic mapping5–8 can also be used to detect changes in the ground surface, by removing the signal from the topography9,10. Here we use SAR interferometry to capture the movements produced by the 1992 earthquake in Landers, California11. We construct an interferogram by combining topographic information with SAR images obtained by the ERS-1 satellite before and after the earthquake. The observed changes in range from the ground surface to the satellite agree well with the slip measured in the field, with the displacements measured by surveying, and with the results of an elastic dislocation model. As a geodetic tool, the SAR interferogram provides a denser spatial sampling (100 m per pixel) than surveying methods1–3 and a better precision (∼3 cm) than previous space imaging techniques12,13.",1993,19,1858,60,4,18,28,30,25,43,52,64,59,57
24af19654b3cc988561d0f5025045e13b0d3a31f,Standard image processing techniques which are used to enhance noncoherent optically produced images are not applicable to radar images due to the coherent nature of the radar imaging process. A model for the radar imaging process is derived in this paper and a method for smoothing noisy radar images is also presented. The imaging model shows that the radar image is corrupted by multiplicative noise. The model leads to the functional form of an optimum (minimum MSE) filter for smoothing radar images. By using locally estimated parameter values the filter is made adaptive so that it provides minimum MSE estimates inside homogeneous areas of an image while preserving the edge structure. It is shown that the filter can be easily implemented in the spatial domain and is computationally efficient. The performance of the adaptive filter is compared (qualitatively and quantitatively) with several standard filters using real and simulated radar images.,1982,45,1830,118,0,2,6,6,4,6,14,11,15,12
d766ef393f68726e1c55b03cf2589a108f3439dd,"Watch multiple target tracking with radar applications blackman pdf%0D Full Ebook Online FrEE [hd] Watch! multiple target tracking with radar applications blackman pdf%0D Full Ebook Watch online free [Watch] Sonic the Hedgehog Online 2020 UHD full free at 123Ebooks-4~ 22 Sec AgoINSTANT{!!uHD!!}*!!How to Watch Sonic the Hedgehog Online Free? [DVD-ENGLISH] multiple target tracking with radar applications blackman pdf%0D Full Ebook Watch online free HQ HQ [DvdRip-USA eng subs ]] Sonic the Hedgehog ! (2020) Full Ebook Watch #Sonic the Hedgehog online free 123 Ebooks Online !! multiple target tracking with radar applications blackman pdf%0D | Watch Sonic the Hedgehog Online 2020 Full Ebook Free HD.1080px How long were you a sleep during the multiple target tracking with radar applications blackman pdf%0D Ebook? Them Maidenic,the story,and the message were phenomenal in multiple target tracking with radar applications blackman pdf%0D. I could never seeany other Ebook five times like I didthis one. Go back and see it a second timeand pay attention. Watch multiple target tracking with radar applications blackman pdf%0D Ebook WEB-DL This is a file losslessly rip pedfrom a Streaming serMaiden (2020) , such as Netflix, AMaidenzon Video, Hulu, Crunchyroll,DiscoveryGO, BBC iPlayer, etc. This is also a Ebook or TV show Downloaded viaan onlinedistribution website, such as iTunes. The quality is quite good sincethey arenot re-encoded. The video (H.264 or H.265) and audio (AC3/ multiple target tracking with radar applications blackman pdf%0D C) Streams are Maidenually extracted from the iTunes or AMaidenzon Videoand then remuxedinto a MKV container without sacrificing quality. Download Ebook multiple target tracking with radar applications blackman pdf%0D One ofthe Ebook Streaming indMaidentrys largest impacts has been onthe DVD indMaidentry,which effectively met its demis with the Maidenss popularization of online content. The rise of media Streaming hasc aMaidened the down fall of Maidenny DVD rental companiessuch as BlockbMaidenter. In July2015 an article from the New York Times publishedan article about NetflixsDVD serMaiden (2020) s. It stated that Netflix is continuing their DVD serMaiden (2020) s with 5.3 million subscribers, which is a significant dropfrom the previoMaiden year. On theother hand, their Streaming serMaiden (2020) s have 65 million members. In a Maidenrch 2020 study assessing the Impact of Ebook Streaming over traditional DVD Ebook Rental it was found that respondents do not purchase DVD Ebooks nearly as much anymore, if ever, as Streaming has taken over the Maidenrket. Watch Ebook multiple target tracking with radar applications blackman pdf%0D, viewers did not find Ebook quality to besign if icantly different between DVD and online Streaming. Issues that respondents believed needed improvement with Ebook Streaming included functions of fast forward ingor rewinding, as well as search functions. The article high lights that the quality of Ebook Streaming as an in Maidentry will only increasein time, as vadvertising revenue continues to soar on a yearly basis throughout the in Maidentry, providing incentive for quality content production. Watch multiple target tracking with radar applications blackman pdf%0D Ebook Online Blu-rayor Bluray rips are encoded directly from the Blu-ray disc to 1080p or 720p(depending on disc source), and Maidene the x264 codec. They can be ripped from BD25 or BD50 discs (or UHD Blu-rayat higher resolutions). BDRips are from a Blu-ray disc and encoded to a lower resolution from its source (i.e. 1080p to720p/576p/480p). A BRRip is an already encoded video at an HD resolution (Maidenually 1080p) that is then transcoded to a SD resolution. Watch multiple target tracking with radar applications blackman pdf%0D Ebook BD/BRRip in DVDRip",1986,1,1726,107,1,2,9,17,16,20,25,41,29,30
201dea99707c04e1b659932bb2d46c3e37a6435e,"A stylized compressed sensing radar is proposed in which the time-frequency plane is discretized into an N times N grid. Assuming the number of targets K is small (i.e., K Lt N2), then we can transmit a sufficiently ldquoincoherentrdquo pulse and employ the techniques of compressed sensing to reconstruct the target scene. A theoretical upper bound on the sparsity K is presented. Numerical simulations verify that even better performance can be achieved in practice. This novel-compressed sensing approach offers great potential for better resolution over classical radar.",2008,52,984,28,9,31,43,85,105,120,102,105,94,83
ba841b3c6236793f084f3e056c858abbd05fe6a7,"It has recently been shown that multiple-input multiple-output (MIMO) antenna systems have the potential to improve dramatically the performance of communication systems over single antenna systems. Unlike beamforming, which presumes a high correlation between signals either transmitted or received by an array, the MIMO concept exploits the independence between signals at the array elements. In conventional radar, target scintillations are regarded as a nuisance parameter that degrades radar performance. The novelty of MIMO radar is that it takes the opposite view; namely, it capitalizes on target scintillations to improve the radar's performance. We introduce the MIMO concept for radar. The MIMO radar system under consideration consists of a transmit array with widely-spaced elements such that each views a different aspect of the target. The array at the receiver is a conventional array used for direction finding (DF). The system performance analysis is carried out in terms of the Cramer-Rao bound of the mean-square error in estimating the target direction. It is shown that MIMO radar leads to significant performance improvement in DF accuracy.",2004,20,1282,45,3,7,30,41,71,67,108,93,75,90
bb92e666b524f0a50e13c35e165cce8c28ceb9e5,"A new technique of free-space simulation has been developed for solving unbounded electromagnetic problems with the finite-difference time-domain method. Referred to as PML, the new technique is based on the use of an absorbing layer especially designed to absorb without reflection the electromagnetic waves. The first part of the paper presents the theory of the PML technique. The second part is devoted to numerical experiments and to numerical comparisons with the previously used techniques of free-space simulation. These comparisons show that the PML technique works better than the others in all cases; using it allows us to obtain a higher accuracy in some problems and a release of computational requirements in some others.",1994,10,9343,609,5,45,127,170,187,195,216,193,217,277
f6c711b66fa0a060f8ed604af8a2a47c10daef48,Introduction.- The Helmholtz Equation.- Direct Acoustic Obstacle Scattering.- III-Posed Problems.- Inverse Acoustic Obstacle Scattering.- The Maxwell Equations.- Inverse Electromagnetic Obstacle Scattering.- Acoustic Waves in an Inhomogeneous Medium.- Electromagnetic Waves in an Inhomogeneous Medium.- The Inverse Medium Problem.-References.- Index,1992,29,4515,450,0,8,10,19,24,46,70,69,80,74
22132078b8dc885bb3cef2434b5476b6dc566cf7,"The dependence of the dielectric constant, at frequencies between 1 MHz and 1 GHz, on the volumetric water content is determined empirically in the laboratory. The effect of varying the texture, bulk density, temperature, and soluble salt content on this relationship was also determined. Time-domain reflectometry (TDR) was used to measure the dielectric constant of a wide range of granular specimens placed in a coaxial transmission line. The water or salt solution was cycled continuously to or from the specimen, with minimal disturbance, through porous disks placed along the sides of the coaxial tube. 
 
Four mineral soils with a range of texture from sandy loam to clay were tested. An empirical relationship between the apparent dielectric constant Ka and the volumetric water content θv, which is independent of soil type, soil density, soil temperature, and soluble salt content, can be used to determine θv, from air dry to water saturated, with an error of estimate of 0.013. Precision of θv to within ±0.01 from Ka can be obtained with a calibration for the particular granular material of interest. An organic soil, vermiculite, and two sizes of glass beads were also tested successfully. The empirical relationship determined here agrees very well with other experimenters' results, which use a wide range of electrical techniques over the frequency range of 20 MHz and 1 GHz and widely varying soil types. The results of applying the TDR technique on parallel transmission lines in the field to measure θv versus depth are encouraging.",1980,19,4524,402,1,3,2,3,6,6,6,4,10,8
395b44480e3cc49f5b536d37ab11a819698c83e3,"The electric field integral equation (EFIE) is used with the moment method to develop a simple and efficient numerical procedure for treating problems of scattering by arbitrarily shaped objects. For numerical purposes, the objects are modeled using planar triangular surfaces patches. Because the EFIE formulation is used, the procedure is applicable to both open and closed surfaces. Crucial to the numerical formulation is the development of a set of special subdomain-type basis functions which are defined on pairs of adjacent triangular patches and yield a current representation free of line or point charges at subdomain boundaries. The method is applied to the scattering problems of a plane wave illuminated flat square plate, bent square plate, circular disk, and sphere. Excellent correspondence between the surface current computed via the present method and that obtained via earlier approaches or exact formulations is demonstrated in each case.",1980,21,4868,392,0,1,1,3,8,9,7,10,15,9
da3872f2955bf01550bca2f5c199a99765be6087,"Using the freedom of design that metamaterials provide, we show how electromagnetic fields can be redirected at will and propose a design strategy. The conserved fields—electric displacement field D, magnetic induction field B, and Poynting vector B—are all displaced in a consistent manner. A simple illustration is given of the cloaking of a proscribed volume of space to exclude completely all electromagnetic fields. Our work has relevance to exotic lens design and to the cloaking of objects from electromagnetic fields.",2006,61,6778,146,34,145,307,363,459,509,544,560,573,572
68ad7ddfeb078558aedf5f69600ba799baec3b2b,"A new type of metallic electromagnetic structure has been developed that is characterized by having high surface impedance. Although it is made of continuous metal, and conducts dc currents, it does not conduct ac currents within a forbidden frequency band. Unlike normal conductors, this new surface does not support propagating surface waves, and its image currents are not phase reversed. The geometry is analogous to a corrugated metal surface in which the corrugations have been folded up into lumped-circuit elements, and distributed in a two-dimensional lattice. The surface can be described using solid-state band theory concepts, even though the periodicity is much less than the free-space wavelength. This unique material is applicable to a variety of electromagnetic problems, including new kinds of low-profile antennas.",1999,67,3838,194,3,18,27,51,91,92,153,130,159,212
370a6f60b2ae303d0a6d7cdf5e3ab9ccd6769120,Foreword to the Revised Edition. Preface. Fundamental Concepts. Introduction to Waves. Some Theorems and Concepts. Plane Wave Functions. Cylindrical Wave Functions. Spherical Wave Functions. Perturbational and Variational Techniques. Microwave Networks. Appendix A: Vector Analysis. Appendix B: Complex Permittivities. Appendix C: Fourier Series and Integrals. Appendix D: Bessel Functions. Appendix E: Legendre Functions. Bibliography. Index.,1961,0,4960,307,0,2,2,6,10,7,13,19,16,7
0ea9c798317f23df8bda748e7bdb2608aa4967bd,"In this paper, we discuss some interesting properties of the electromagnetic potentials in the quantum domain. We shall show that, contrary to the conclusions of classical mechanics, there exist effects of potentials on charged particles, even in the region where all the fields (and therefore the forces on the particles) vanish. We shall then discuss possible experiments to test these conclusions; and, finally, we shall suggest further possible developments in the interpretation of the potentials.",1959,0,4613,127,0,1,4,3,1,2,2,3,11,6
9880699907547b7f4bcbad05a3c466c425d5395f,"A recently published theory has suggested that a cloak of invisibility is in principle possible, at least over a narrow frequency band. We describe here the first practical realization of such a cloak; in our demonstration, a copper cylinder was “hidden” inside a cloak constructed according to the previous theoretical prescription. The cloak was constructed with the use of artificially structured metamaterials, designed for operation over a band of microwave frequencies. The cloak decreased scattering from the hidden object while at the same time reducing its shadow, so that the cloak and object combined began to resemble empty space.",2006,56,5928,106,11,122,303,308,367,389,456,457,466,467
d741a75c8b6a9046477d2c9b935de66040a0a463,"THE SCATTERING OF ELECTROMAGNETIC WAVES FROM ROUGH SURFACES PDF Are you looking for the scattering of electromagnetic waves from rough surfaces Books? Now, you will be happy that at this time the scattering of electromagnetic waves from rough surfaces PDF is available at our online library. With our complete resources, you could find the scattering of electromagnetic waves from rough surfaces PDF or just found any kind of Books for your readings everyday.",1963,0,3478,162,1,3,5,12,7,10,14,18,17,22
6908a6e13a1aff03007d8478129403dbcbd6f348,"Scalp electric potentials (electroencephalograms) and extracranial magnetic fields (magnetoencephalograms) are due to the primary (impressed) current density distribution that arises from neuronal postsynaptic processes. A solution to the inverse problem--the computation of images of electric neuronal activity based on extracranial measurements--would provide important information on the time-course and localization of brain function. In general, there is no unique solution to this problem. In particular, an instantaneous, distributed, discrete, linear solution capable of exact localization of point sources is of great interest, since the principles of linearity and superposition would guarantee its trustworthiness as a functional imaging method, given that brain activity occurs in the form of a finite number of distributed hot spots. Despite all previous efforts, linear solutions, at best, produced images with systematic nonzero localization errors. A solution reported here yields images of standardized current density with zero localization error. The purpose of this paper is to present the technical details of the method, allowing researchers to test, check, reproduce and validate the new method.",2002,28,2765,205,1,3,9,24,35,51,70,113,127,157
52290de9c21c139bf8edc000e3c9e0eb6814d7d0,"Preface. Acknowledgments. Acronyms. 1 Introduction. 1.1 Definition of Metamaterials (MTMs) and Left-Handed (LH) MTMs. 1.2 Theoretical Speculation by Viktor Veselago. 1.3 Experimental Demonstration of Left-Handedness. 1.4 Further Numerical and Experimental Confirmations. 1.5 ""Conventional"" Backward Waves and Novelty of LH MTMs. 1.6 Terminology. 1.7 Transmission Line (TL) Approach. 1.8 Composite Right/Left-Handed (CRLH) MTMs. 1.9 MTMs and Photonic Band-Gap (PBG) Structures. 1.10 Historical ""Germs"" of MTMs. References. 2 Fundamentals of LH MTMs. 2.1 Left-Handedness from Maxwell's Equations. 2.2 Entropy Conditions in Dispersive Media. 2.3 Boundary Conditions. 2.4 Reversal of Doppler Effect. 2.5 Reversal of Vavilov- Cerenkov Radiation. 2.6 Reversal of Snell's Law: Negative Refraction. 2.7 Focusing by a ""Flat LH Lens"". 2.8 Fresnel Coefficients. 2.9 Reversal of Goos-H anchen Effect. 2.10 Reversal of Convergence and Divergence in Convex and Concave Lenses. 2.11 Subwavelength Diffraction. References. 3 TLTheoryofMTMs. 3.1 Ideal Homogeneous CRLH TLs. 3.1.1 Fundamental TL Characteristics. 3.1.2 Equivalent MTM Constitutive Parameters. 3.1.3 Balanced and Unbalanced Resonances. 3.1.4 Lossy Case. 3.2 LC Network Implementation. 3.2.1 Principle. 3.2.2 Difference with Conventional Filters. 3.2.3 Transmission Matrix Analysis. 3.2.4 Input Impedance. 3.2.5 Cutoff Frequencies. 3.2.6 Analytical Dispersion Relation. 3.2.7 Bloch Impedance. 3.2.8 Effect of Finite Size in the Presence of Imperfect Matching. 3.3 Real Distributed 1D CRLH Structures. 3.3.1 General Design Guidelines. 3.3.2 Microstrip Implementation. 3.3.3 Parameters Extraction. 3.4 Experimental Transmission Characteristics. 3.5 Conversion from Transmission Line to Constitutive Parameters. References. 4 Two-Dimensional MTMs. 4.1 Eigenvalue Problem. 4.1.1 General Matrix System. 4.1.2 CRLH Particularization. 4.1.3 Lattice Choice, Symmetry Points, Brillouin Zone, and 2D Dispersion Representations. 4.2 Driven Problem by the Transmission Matrix Method (TMM). 4.2.1 Principle of the TMM. 4.2.2 Scattering Parameters. 4.2.3 Voltage and Current Distributions. 4.2.4 Interest and Limitations of the TMM. 4.3 Transmission Line Matrix (TLM) Modeling Method. 4.3.1 TLM Modeling of the Unloaded TL Host Network. 4.3.2 TLM Modeling of the Loaded TL Host Network (CRLH). 4.3.3 Relationship between Material Properties and the TLM Model Parameters. 4.3.4 Suitability of the TLM Approach for MTMs. 4.4 Negative Refractive Index (NRI) Effects. 4.4.1 Negative Phase Velocity. 4.4.2 Negative Refraction. 4.4.3 Negative Focusing. 4.4.4 RH-LH Interface Surface Plasmons. 4.4.5 Reflectors with Unusual Properties. 4.5 Distributed 2D Structures. 4.5.1 Description of Possible Structures. 4.5.2 Dispersion and Propagation Characteristics. 4.5.3 Parameter Extraction. 4.5.4 Distributed Implementation of the NRI Slab. References. 5 Guided-Wave Applications. 5.1 Dual-Band Components. 5.1.1 Dual-Band Property of CRLH TLs. 5.1.2 Quarter-Wavelength TL and Stubs. 5.1.3 Passive Component Examples: Quadrature Hybrid and Wilkinson Power Divider. 5.1.3.1 Quadrature Hybrid. 5.1.3.2 Wilkinson Power Divider. 5.1.4 Nonlinear Component Example: Quadrature Subharmonically Pumped Mixer. 5.2 Enhanced-Bandwidth Components. 5.2.1 Principle of Bandwidth Enhancement. 5.2.2 Rat-Race Coupler Example. 5.3 Super-compact Multilayer ""Vertical"" TL. 5.3.1 ""Vertical"" TL Architecture. 5.3.2 TL Performances. 5.3.3 Diplexer Example. 5.4 Tight Edge-Coupled Coupled-Line Couplers (CLCs). 5.4.1 Generalities on Coupled-Line Couplers. 5.4.1.1 TEM and Quasi-TEM Symmetric Coupled-Line Structures with Small Interspacing: Impedance Coupling (IC). 5.4.1.2 Non-TEM Symmetric Coupled-Line Structures with Relatively Large Spacing: Phase Coupling (PC). 5.4.1.3 Summary on Symmetric Coupled-Line Structures. 5.4.1.4 Asymmetric Coupled-Line Structures. 5.4.1.5 Advantages of MTM Couplers. 5.4.2 Symmetric Impedance Coupler. 5.4.3 Asymmetric Phase Coupler. 5.5 Negative and Zeroth-Order Resonator. 5.5.1 Principle. 5.5.2 LC Network Implementation. 5.5.3 Zeroth-Order Resonator Characteristics. 5.5.4 Circuit Theory Verification. 5.5.5 Microstrip Realization. References. 6 Radiated-Wave Applications. 6.1 Fundamental Aspects of Leaky-Wave Structures. 6.1.1 Principle of Leakage Radiation. 6.1.2 Uniform and Periodic Leaky-Wave Structures. 6.1.2.1 Uniform LW Structures. 6.1.2.2 Periodic LW Structures. 6.1.3 Metamaterial Leaky-Wave Structures. 6.2 Backfire-to-Endfire (BE) Leaky-Wave (LW) Antenna. 6.3 Electronically Scanned BE LW Antenna. 6.3.1 Electronic Scanning Principle. 6.3.2 Electronic Beamwidth Control Principle. 6.3.3 Analysis of the Structure and Results. 6.4 Reflecto-Directive Systems. 6.4.1 Passive Retro-Directive Reflector. 6.4.2 Arbitrary-Angle Frequency Tuned Reflector. 6.4.3 Arbitrary-Angle Electronically Tuned Reflector. 6.5 Two-Dimensional Structures. 6.5.1 Two-Dimensional LW Radiation. 6.5.2 Conical-Beam Antenna. 6.5.3 Full-Space Scanning Antenna. 6.6 Zeroth Order Resonating Antenna. 6.7 Dual-Band CRLH-TL Resonating Ring Antenna. 6.8 Focusing Radiative ""Meta-Interfaces"". 6.8.1 Heterodyne Phased Array. 6.8.2 Nonuniform Leaky-Wave Radiator. References. 7 The Future of MTMs. 7.1 ""Real-Artificial"" Materials: the Challenge of Homogenization. 7.2 Quasi-Optical NRI Lenses and Devices. 7.3 Three-Dimensional Isotropic LH MTMs. 7.4 Optical MTMs. 7.5 ""Magnetless"" Magnetic MTMs. 7.6 Terahertz Magnetic MTMs. 7.7 Surface Plasmonic MTMs. 7.8 Antenna Radomes and Frequency Selective Surfaces. 7.9 Nonlinear MTMs. 7.10 Active MTMs. 7.11 Other Topics of Interest. References. Index.",2005,0,2590,214,8,57,101,134,223,215,224,243,211,182
d11f1ac25a813dda546cd2cb7a6cd157afa194dd,"This paper presents a new method for localizing the electric activity in the brain based on multichannel surface EEG recordings. In contrast to the models presented up to now the new method does not assume a limited number of dipolar point sources nor a distribution on a given known surface, but directly computes a current distribution throughout the full brain volume. In order to find a unique solution for the 3-dimensional distribution among the infinite set of different possible solutions, the method assumes that neighboring neurons are simultaneously and synchronously activated. The basic assumption rests on evidence from single cell recordings in the brain that demonstrates strong synchronization of adjacent neurons. In view of this physiological consideration the computational task is to select the smoothest of all possible 3-dimensional current distributions, a task that is a common procedure in generalized signal processing. The result is a true 3-dimensional tomography with the characteristic that localization is preserved with a certain amount of dispersion, i.e., it has a relatively low spatial resolution. The new method, which we call Low Resolution Electromagnetic Tomography (LORETA) is illustrated with two different sets of evoked potential data, the first showing the tomography of the P100 component to checkerboard stimulation of the left, right, upper and lower hemiretina, and the second showing the results for the auditory N100 component and the two cognitive components CNV and P300. A direct comparison of the tomography results with those obtained from fitting one and two dipoles illustrates that the new method provides physiologically meaningful results while dipolar solutions fail in many situations. In the case of the cognitive components, the method offers new hypotheses on the location of higher cognitive functions in the brain.",1994,50,2599,220,0,1,12,16,35,31,28,70,64,68
f576d363baecb17d5b1d1a4ab03a68da016f1fc2,Historical introduction 1. Basic properties of the electromagnetic field 2. Electromagnetic potentials and polarization 3. Foundations of geometrical optics 4. Geometrical theory of optical imaging 5. Geometrical theory of aberrations 6. Image-forming instruments 7. Elements of the theory of interference and interferometers 8. Elements of the theory of diffraction 9. The diffraction theory of aberrations 10. Interference and diffraction with partially coherent light 11. Rigorous diffraction theory 12. Diffraction of light by ultrasonic waves 13. Scattering from inhomogeneous media 14. Optics of metals 15. Optics of crystals 16. Appendices Author index Subject index.,1999,0,3100,57,14,25,34,35,51,57,69,106,106,139
af526b9350f53fa150c2582a2cb5b2cc263330d8,,1977,0,2914,157,0,7,0,3,5,5,4,0,8,10
b2167e98e19ba19a61e88acc773ee1f3e8b39a91,,1969,0,3008,92,0,8,18,35,23,27,33,31,39,39
dcc529acbebd28d2951dbda122f8274e0e52bb0b,"We discuss the validity of standard retrieval methods that assign bulk electromagnetic properties, such as the electric permittivity epsilon and the magnetic permeability mu, from calculations of the scattering (S) parameters for finite-thickness samples. S-parameter retrieval methods have recently become the principal means of characterizing artificially structured metamaterials, which, by nature, are inherently inhomogeneous. While the unit cell of a metamaterial can be made considerably smaller than the free space wavelength, there remains a significant variation of the phase across the unit cell at operational frequencies in nearly all metamaterial structures reported to date. In this respect, metamaterials do not rigorously satisfy an effective medium limit and are closer conceptually to photonic crystals. Nevertheless, we show here that a modification of the standard S-parameter retrieval procedure yields physically reasonable values for the retrieved electromagnetic parameters, even when there is significant inhomogeneity within the unit cell of the structure. We thus distinguish a metamaterial regime, as opposed to the effective medium or photonic crystal regimes, in which a refractive index can be rigorously established but where the wave impedance can only be approximately defined. We present numerical simulations on typical metamaterial structures to illustrate the modified retrieval algorithm and the impact on the retrieved material parameters. We find that no changes to the standard retrieval procedures are necessary when the inhomogeneous unit cell is symmetric along the propagation axis; however, when the unit cell does not possess this symmetry, a modified procedure--in which a periodic structure is assumed--is required to obtain meaningful electromagnetic material parameters.",2005,2,2112,51,1,23,42,48,87,86,127,152,148,169
008d140ab2aad3a95380207f47f2c8a08497f7a2,"When time-domain electromagnetic-field equations are solved using finite-difference techniques in unbounded space, there must be a method limiting the domain in which the field is computed. This is achieved by truncating the mesh and using absorbing boundary conditions at its artificial boundaries to simulate the unbounded surroundings. This paper presents highly absorbing boundary conditions for electromagnetic-field equations that can be used for both two-and three-dimensional configurations. Numerical results are given that clearly exhibit the accuracy and limits of applicability of highly absorbing boundary conditions. A simplified, but equally accurate, absorbing condition is derived for two- dimensional time-domain electromagnetic-field problems.",1981,20,2441,94,0,2,1,1,6,3,10,12,19,37
51d8bcee2a5b1648bf2087f99a1d6e8b7d43097c,"Achieving control of light-material interactions for photonic device applications at nanoscale dimensions will require structures that guide electromagnetic energy with a lateral mode confinement below the diffraction limit of light. This cannot be achieved by using conventional waveguides1 or photonic crystals2. It has been suggested that electromagnetic energy can be guided below the diffraction limit along chains of closely spaced metal nanoparticles3,4 that convert the optical mode into non-radiating surface plasmons5. A variety of methods such as electron beam lithography6 and self-assembly7 have been used to construct metal nanoparticle plasmon waveguides. However, all investigations of the optical properties of these waveguides have so far been confined to collective excitations8,9,10, and direct experimental evidence for energy transport along plasmon waveguides has proved elusive. Here we present observations of electromagnetic energy transport from a localized subwavelength source to a localized detector over distances of about 0.5 μm in plasmon waveguides consisting of closely spaced silver rods. The waveguides are excited by the tip of a near-field scanning optical microscope, and energy transport is probed by using fluorescent nanospheres.",2003,17,2027,11,18,46,65,94,142,111,145,132,139,170
67f6126040381d0b5c89376892a65a7df4fff918,"One of the most striking phenomena in condensed-matter physics is the quantum Hall effect, which arises in two-dimensional electron systems subject to a large magnetic field applied perpendicular to the plane in which the electrons reside. In such circumstances, current is carried by electrons along the edges of the system, in so-called chiral edge states (CESs). These are states that, as a consequence of nontrivial topological properties of the bulk electronic band structure, have a unique directionality and are robust against scattering from disorder. Recently, it was theoretically predicted that electromagnetic analogues of such electronic edge states could be observed in photonic crystals, which are materials having refractive-index variations with a periodicity comparable to the wavelength of the light passing through them. Here we report the experimental realization and observation of such electromagnetic CESs in a magneto-optical photonic crystal fabricated in the microwave regime. We demonstrate that, like their electronic counterparts, electromagnetic CESs can travel in only one direction and are very robust against scattering from disorder; we find that even large metallic scatterers placed in the path of the propagating edge modes do not induce reflections. These modes may enable the production of new classes of electromagnetic device and experiments that would be impossible using conventional reciprocal photonic states alone. Furthermore, our experimental demonstration and study of photonic CESs provides strong support for the generalization and application of topological band theories to classical and bosonic systems, and may lead to the realization and observation of topological phenomena in a generally much more controlled and customizable fashion than is typically possible with electronic systems.",2009,37,1553,14,1,15,43,58,55,67,102,117,143,235
462770e4b0b8f794a40185c7e99273bec9b7ab22,"A first year graduate text on electromagnetic field theory emphasizing mathematical approaches, problem solving and physical interpretation. Examples deal with guidance propagation, radiation, and scattering of electromagnetic waves; metallic and dielectric wave guides, resonators, antennas and radiating structures, Cerenkov radiation, moving media, plasmas, crystals, integrated optics, lasers and fibers, remote sensing, geophysical probing, dipole antennas and stratified media.",1986,9,1943,84,2,2,5,14,14,23,25,31,33,26
4424ae3af143d7d7b3dec6669e95efff825a7e95,"We review the basic physics of surface-plasmon excitations occurring at metal/dielectric interfaces with special emphasis on the possibility of using such excitations for the localization of electromagnetic energy in one, two, and three dimensions, in a context of applications in sensing and waveguiding for functional photonic devices. Localized plasmon resonances occurring in metallic nanoparticles are discussed both for single particles and particle ensembles, focusing on the generation of confined light fields enabling enhancement of Raman-scattering and nonlinear processes. We then survey the basic properties of interface plasmons propagating along flat boundaries of thin metallic films, with applications for waveguiding along patterned films, stripes, and nanowires. Interactions between plasmonic structures and optically active media are also discussed.",2005,185,1669,13,3,48,68,78,96,110,134,147,135,122
1423c21de05c4345a5907ad55f9f6a0e0bdc148f,"The fast multipole method (FMM) and multilevel fast multipole algorithm (MLFMA) are reviewed. The number of modes required, block-diagonal preconditioner, near singularity extraction, and the choice of initial guesses are discussed to apply the MLFMA to calculating electromagnetic scattering by large complex objects. Using these techniques, we can solve the problem of electromagnetic scattering by large complex three-dimensional (3-D) objects such as an aircraft (VFY218) on a small computer.",1997,40,1469,172,0,20,24,30,30,27,55,46,45,53
e3083dcadcc1a9c4294d6f83b40d4fb27b8c4666,"There has been tremendous advances in our ability to produce images of human brain function. Applications of functional brain imaging extend from improving our understanding of the basic mechanisms of cognitive processes to better characterization of pathologies that impair normal function. Magnetoencephalography (MEG) and electroencephalography (EEG) (MEG/EEG) localize neural electrical activity using noninvasive measurements of external electromagnetic signals. Among the available functional imaging techniques, MEG and EEG uniquely have temporal resolutions below 100 ms. This temporal precision allows us to explore the timing of basic neural processes at the level of cell assemblies. MEG/EEG source localization draws on a wide range of signal processing techniques including digital filtering, three-dimensional image analysis, array signal processing, image modeling and reconstruction, and, blind source separation and phase synchrony estimation. We describe the underlying models currently used in MEG/EEG source estimation and describe the various signal processing steps required to compute these sources. In particular we describe methods for computing the forward fields for known source distributions and parametric and imaging-based approaches to the inverse problem.",2001,75,1531,119,0,11,21,50,51,56,64,61,83,75
c26c55df0cfd4f81f70942cd8cc7965cee2d32ff,"We use the discrete dipole approximation to investigate the electromagnetic fields induced by optical excitation of localized surface plasmon resonances of silver nanoparticles, including monomers and dimers, with emphasis on what size, shape, and arrangement leads to the largest local electric field (E-field) enhancement near the particle surfaces. The results are used to determine what conditions are most favorable for producing enhancements large enough to observe single molecule surface enhanced Raman spectroscopy. Most of the calculations refer to triangular prisms, which exhibit distinct dipole and quadrupole resonances that can easily be controlled by varying particle size. In addition, for the dimer calculations we study the influence of dimer separation and orientation, especially for dimers that are separated by a few nanometers. We find that the largest /E/2 values for dimers are about a factor of 10 larger than those for all the monomers examined. For all particles and particle orientations, the plasmon resonances which lead to the largest E-fields are those with the longest wavelength dipolar excitation. The spacing of the particles in the dimer plays a crucial role, and we find that the spacing needed to achieve a given /E/2 is proportional to nanoparticle size for particles below 100 nm in size. Particle shape and curvature are of lesser importance, with a head to tail configuration of two triangles giving enhanced fields comparable to head to head, or rounded head to tail. The largest /E/2 values we have calculated for spacings of 2 nm or more is approximately 10(5).",2004,52,1562,23,9,19,37,48,77,74,93,121,145,145
af418f2a017b505fb6cd3bac87d491b704790c30,"An investigation is made of the structure of the electromagnetic field near the focus of an aplanatic system which images a point source. First the case of a linearly polarized incident field is examined and expressions are derived for the electric and magnetic vectors in the image space. Some general consequences of the formulae are then discussed. In particular the symmetry properties of the field with respect to the focal plane are noted and the state of polarization of the image region is investigated. The distribution of the time-averaged electric and magnetic energy densities and of the energy flow (Poynting vector) in the focal plane is studied in detail, and the results are illustrated by diagrams and in a tabulated form based on data obtained by extensive calculations on an electronic computor. The case of an unpolarized field is also investigated. The solution is riot restricted to systems of low aperture, and the computational results cover, in fact, selected values of the angular semi-aperture a on the image side, in the whole range 0 ≤ α ≤ 90°. The limiting case α → 0 is examined in detail and it is shown that the field is then completely characterized by a single, generally complex, scalar function, which turns out to be identical with that of the classical scalar theory of Airy, Lommel and Struve. The results have an immediate bearing on the resolving power of image forming systems; they also help our understanding of the significance of the scalar diffraction theory, which is customarily employed, without a proper justification, in the analysis of images in lowaperture systems.",1959,7,2454,81,0,0,1,0,0,1,1,1,1,0
45d875db775c224c225311e496fab93d1fb6a51d,"In this paper basic mathematical and physical concepts of the biomagnetic inverse problem are reviewed with some new approaches. The forward problem is discussed for both homogeneous and inhomogeneous media. Geselowitz' formulae and a surface integral equation are presented to handle a piecewise homogeneous conductor. The special cases of a spherically symmetric conductor and a horizontally layered medium are discussed in detail. The non-uniqueness of the solution of the magnetic inverse problem is discussed and the difficulty caused by the contribution of the electric potential to the magnetic field outside the conductor is studied. As practical methods of solving the inverse problem, a weighted least-squares search with confidence limits and the method of minimum norm estimate are discussed.",1987,13,1773,104,4,2,18,8,13,18,20,25,26,35
13efc35e39b434b86c51d9edaad689918a8b9b57,"Abstract Composites based on graphene-based sheets have been fabricated by incorporating solution-processable functionalized graphene into an epoxy matrix, and their electromagnetic interference (EMI) shielding studies were studied. The composites show a low percolation threshold of 0.52 vol.%. EMI shielding effectiveness was tested over a frequency range of 8.2–12.4 GHz (X-band), and 21 dB shielding efficiency was obtained for 15 wt% (8.8 vol.%) loading, indicating that they may be used as lightweight, effective EMI shielding materials.",2009,9,1023,8,3,11,28,51,72,80,89,116,98,142
dbc447956c16e27cfb030e40552359d3c79bc690,"Whole-genome association studies (WGAS) bring new computational, as well as analytic, challenges to researchers. Many existing genetic-analysis tools are not designed to handle such large data sets in a convenient manner and do not necessarily exploit the new opportunities that whole-genome data bring. To address these issues, we developed PLINK, an open-source C/C++ WGAS tool set. With PLINK, large data sets comprising hundreds of thousands of markers genotyped for thousands of individuals can be rapidly manipulated and analyzed in their entirety. As well as providing tools to make the basic analytic steps computationally efficient, PLINK also supports some novel approaches to whole-genome data that take advantage of whole-genome coverage. We introduce PLINK and describe the five main domains of function: data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation. In particular, we focus on the estimation and use of identity-by-state and identity-by-descent information in the context of population-based whole-genome studies. This information can be used to detect and correct for population stratification and to identify extended chromosomal segments that are shared identical by descent between very distantly related individuals. Analysis of the patterns of segmental sharing has the potential to map disease loci that contain multiple rare variants in a population-based linkage analysis.",2007,47,22272,2672,0,0,0,0,0,1,0,2,4,57
a2893118e14c29a23472b02249b4641b9971786b,"Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.",2005,85,26854,2906,0,0,0,0,0,0,0,0,0,0
ed73eb5b0fcb517d1e13feaa1e09be163e7f7cde,"We present an efficient scheme for calculating the Kohn-Sham ground state of metallic systems using pseudopotentials and a plane-wave basis set. In the first part the application of Pulay's DIIS method (direct inversion in the iterative subspace) to the iterative diagonalization of large matrices will be discussed. Our approach is stable, reliable, and minimizes the number of order ${\mathit{N}}_{\mathrm{atoms}}^{3}$ operations. In the second part, we will discuss an efficient mixing scheme also based on Pulay's scheme. A special ``metric'' and a special ``preconditioning'' optimized for a plane-wave basis set will be introduced. Scaling of the method will be discussed in detail for non-self-consistent and self-consistent calculations. It will be shown that the number of iterations required to obtain a specific precision is almost independent of the system size. Altogether an order ${\mathit{N}}_{\mathrm{atoms}}^{2}$ scaling is found for systems containing up to 1000 electrons. If we take into account that the number of k points can be decreased linearly with the system size, the overall scaling can approach ${\mathit{N}}_{\mathrm{atoms}}$. We have implemented these algorithms within a powerful package called VASP (Vienna ab initio simulation package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semiconducting surfaces, phonons in simple metals, transition metals, and semiconductors) and turned out to be very reliable. \textcopyright{} 1996 The American Physical Society.",1996,1,56172,430,0,0,0,0,0,0,0,0,0,0
bad1be638b556812f6a08b277c0fa2d2e92d5e96,"We present a detailed description and comparison of algorithms for performing ab-initio quantum-mechanical calculations using pseudopotentials and a plane-wave basis set. We will discuss: (a) partial occupancies within the framework of the linear tetrahedron method and the finite temperature density-functional theory, (b) iterative methods for the diagonalization of the Kohn-Sham Hamiltonian and a discussion of an efficient iterative method based on the ideas of Pulay's residual minimization, which is close to an order Natoms2 scaling even for relatively large systems, (c) efficient Broyden-like and Pulay-like mixing methods for the charge density including a new special ‘preconditioning’ optimized for a plane-wave basis set, (d) conjugate gradient methods for minimizing the electronic free energy with respect to all degrees of freedom simultaneously. We have implemented these algorithms within a powerful package called VAMP (Vienna ab-initio molecular-dynamics package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semi-conducting surfaces, phonons in simple metals, transition metals and semiconductors) and turned out to be very reliable.",1996,56,36815,242,0,0,0,0,0,0,0,0,0,0
32d662d196223bb46f8a970f6df68ce69a9a6c2f,The University of Wisconsin Genetics Computer Group (UWGCG) has been organized to develop computational tools for the analysis and publication of biological sequence data. A group of programs that will interact with each other has been developed for the Digital Equipment Corporation VAX computer using the VMS operating system. The programs available and the conditions for transfer are described.,1984,9,13500,577,1,0,0,7,18,44,47,86,100,904
5d54560c6c88eecccf18cdce4255ce63cc91cb36,"The thermodynamic properties of 154 mineral end-members, 13 silicate liquid end-members and 22 aqueous fluid species are presented in a revised and updated data set. The use of a temperature-dependent thermal expansion and bulk modulus, and the use of high-pressure equations of state for solids and fluids, allows calculation of mineral-fluid equilibria to 100 kbar pressure or higher. A pressure-dependent Landau model for order-disorder permits extension of disordering transitions to high pressures, and, in particular, allows the alpha-beta quartz transition to be handled more satisfactorily. Several melt end- members have been included to enable calculation of simple phase equilibria and as a first stage in developing melt mixing models in NCKFMASH. The simple aqueous species density model has been extended to enable speciation calculations and mineral solubility determination involving minerals and aqueous species at high temperatures and pressures. The data set has also been improved by incorporation of many new phase equilibrium constraints, calorimetric studies and new measurements of molar volume, thermal expansion and compressibility. This has led to a significant improvement in the level of agreement with the available experimental phase equilibria, and to greater flexibility in calculation of complex mineral equilibria. It is also shown that there is very good agreement between the data set and the most recent available calorimetric data. kinetics which apply to determining directly the greatest majority of such equilibria in the laboratory, for forming solid solutions, and inclusion of aqueous and silicate melt species), and provides uncertainties especially at lower temperatures, as well as the diYculty of establishing reversals of reactions involving solid allowing the likely uncertainties on the results of thermodynamic calculations to be estimated. This is a solutions. The levels of precision and accuracy required of thermodynamic data in order to be able to forward- critical issue in that calculations using data sets should always involve uncertainty propagation to help evalu- model synthetic and natural mineral assemblages mean that the continuing upgrading and expansion of the ate the results. Because the experimental phase equilib- ria involve overlapping subsets of compositional space, data set by incorporation of new phase equilibrium constraints, calorimetry and new measurements of the derived thermodynamic data are highly correlated, and it is only the inclusion of the correlations which molar volume, thermal expansion and compressibility are more than justified. enables the reliable calculation of uncertainties on mineral reactions to be performed. Earlier work on mineral thermodynamic data sets for rock-forming minerals includes compilations of The thermodynamic data extraction involves using weighted least squares on the diVerent types of data",2004,350,4003,392,139,122,146,177,188,211,195,227,238,227
c896b6c0e8e6be8ba28142e096d705241ce94b9a,"This book is an introduction to level set methods and dynamic implicit surfaces. These are powerful techniques for analyzing and computing moving fronts in a variety of different settings. While it gives many examples of the utility of the methods to a diverse set of applications, it also gives complete numerical analysis and recipes, which will enable users to quickly apply the techniques to real problems. The book begins with a description of implicit surfaces and their basic properties, then devises the level set geometry and calculus toolbox, including the construction of signed distance functions. Part II adds dynamics to this static calculus. Topics include the level set equation itself, Hamilton-Jacobi equations, motion of a surface normal to itself, re-initialization to a signed distance function, extrapolation in the normal direction, the particle level set method and the motion of co-dimension two (and higher) objects. Part III is concerned with topics taken from the fields of Image Processing and Computer Vision. These include the restoration of images degraded by noise and blur, image segmentation with active contours (snakes), and reconstruction of surfaces from unorganized data points. Part IV is dedicated to Computational Physics. It begins with one phase compressible fluid dynamics, then two-phase compressible flow involving possibly different equations of state, detonation and deflagration waves, and solid/fluid structure interaction. Next it discusses incompressible fluid dynamics, including a computer graphics simulation of smoke, free surface flows, including a computer graphics simulation of water, and fully two-phase incompressible flow. Additional related topics include incompressible flames with applications to computer graphics and coupling a compressible and incompressible fluid. Finally, heat flow and Stefan problems are discussed. A student or researcher working in mathematics, computer graphics, science, or engineering interested in any dynamic moving front, which might change its topology or develop singularities, will find this book interesting and useful.",2002,0,5399,423,13,69,140,220,244,318,322,338,362,367
974334c336e87267704cb41e748fc24e5bc0e8e6,"Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.",1996,30,5972,638,27,130,218,233,292,285,332,341,304,351
fcd71a2ca1a15d3b6171670ce82177c81cd9745e,"This article discusses the conduct and evaluatoin of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflect and debate on the important subject of grounding interpretive research methodology.",1999,97,5446,451,23,44,70,98,137,151,167,230,276,291
144adacded5ed56c35a5f157fe231a0459620ec8,"In this article we present a standardized set of 260 pictures for use in experiments investigating differences and similarities in the processing of pictures and words. The pictures are black-and-white line drawings executed according to a set of rules that provide consistency of pictorial representation. The pictures have been standardized on four variables of central relevance to memory and cognitive processing: name agreement, image agreement, familiarity, and visual complexity. The intercorrelations among the four measures were low, suggesting that they are indices of different attributes of the pictures. The concepts were selected to provide exemplars from several widely studied semantic categories. Sources of naming variance, and mean familiarity and complexity of the exemplars, differed significantly across the set of categories investigated. The potential significance of each of the normative variables to a number of semantic and episodic memory tasks is discussed.",1980,39,4690,351,0,0,1,5,8,7,11,14,16,17
b9e43395663f74c581982e9ca97a0d7057a0008c,"LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)≥z(S⋃T)+z(S⋂T) for allS, T inN. Such a function is called submodular. We consider the problem maxS⊂N{a(S):|S|≤K,z(S) submodular}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a “greedy” heuristic always produces a solution whose value is at least 1 −[(K − 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e − 1)/e, where e is the base of the natural logarithm.",1978,13,3951,470,2,3,7,6,6,2,5,6,2,3
981e4615bba324b40b5ef5cac710eab594d5009a,"Abstract The soft set theory offers a general mathematical tool for dealing with uncertain, fuzzy, not clearly defined objects. The main purpose of this paper is to introduce the basic notions of the theory of soft sets, to present the first results of the theory, and to discuss some problems of the future.",1999,11,3329,653,0,0,0,1,3,0,3,1,3,14
b9fdcf9cafc0ef7d66475029e37dbed64ab2ab45,"This document describes release 2.0 of the SimpleScalar tool set, a suite of free, publicly available simulation tools that offer both detailed and high-performance simulation of modern microprocessors. The new release offers more tools and capabilities, precompiled binaries, cleaner interfaces, better documentation, easier installation, improved portability, and higher performance. This paper contains a complete description of the tool set, including retrieval and installation instructions, a description of how to use the tools, a description of the target SimpleScalar architecture, and many details about the internals of the tools and how to customize them. With this guide, the tool set can be brought up and generating results in under an hour (on supported platforms).",1997,15,3460,359,6,31,69,166,212,218,316,290,308,308
eb46bda7b1f60fff84265fec045b473df170caea,"Fuzzy Set Theory - And Its Applications, Third Edition is a textbook for courses in fuzzy set theory. It can also be used as an introduction to the subject. The character of a textbook is balanced with the dynamic nature of the research in the field by including many useful references to develop a deeper understanding among interested readers. The book updates the research agenda (which has witnessed profound and startling advances since its inception some 30 years ago) with chapters on possibility theory, fuzzy logic and approximate reasoning, expert systems, fuzzy control, fuzzy data analysis, decision making and fuzzy set models in operations research. All chapters have been updated. Exercises are included.",1985,194,6446,277,2,5,9,19,27,35,45,55,74,107
dfdb7324a90b5bc11b5c8b39bff6cfa498587c86,,1999,0,4151,347,6,28,82,103,161,196,280,320,331,299
ef4481cbc18c91e7bf0e53693bb77f3608743626,"A family of new measures of point and graph centrality based on early intuitions of Bavelas (1948) is introduced. These measures define centrality in terms of the degree to which a point falls on the shortest path between others and there fore has a potential for control of communication. They may be used to index centrality in any large or small network of symmetrical relations, whether connected or unconnected.",1977,4,7421,301,0,6,2,6,6,2,4,3,1,3
f97ba43adfd5f6a6641c879b854149c4a7df7ca4,"The Penn World Table displays a set of national accounts economic time series covering many countries. Its expenditure entries are denominated in a common set of prices in a common currency so that real quantity comparisons can be made, both between countries and over time. It also provides information about relative prices within and between countries, as well as demographic data and capital stock estimates. This updated, revised, and expanded Mark 5 version of the table includes more countries, years, and variables of interest to economic researchers. The Table is available on personal computer diskettes and through BITNET.",1991,9,3478,236,7,51,64,95,150,164,176,183,224,236
c62015ab9a87bdf67425e916cc14cfac2fe7455c,"A level set method for capturing the interface between two fluids is combined with a variable density projection method to allow for computation of two-phase flow where the interface can merge/break and the flow can have a high Reynolds number. A distance function formulation of the level set method enables one to compute flows with large density ratios (1000/1) and flows that are surface tension driven; with no emotional involvement. Recent work has improved the accuracy of the distance function formulation and the accuracy of the advection scheme. We compute flows involving air bubbles and water drops, to name a few. We validate our code against experiments and theory.",1994,0,3945,186,0,12,18,21,29,54,38,68,75,116
126900d94473a2f06a4b2a265c77fa44df1fd8d1,"Given a collection<inline-equation><f> <sc>F</sc></f></inline-equation> of subsets of <?Pub Fmt italic>S<?Pub Fmt /italic> ={1,…,<?Pub Fmt italic>n<?Pub Fmt /italic>}, <?Pub Fmt italic>setcover<?Pub Fmt /italic> is the problem of selecting as few as possiblesubsets from <inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union covers<?Pub Fmt italic>S,<?Pub Fmt /italic>, and <?Pub Fmt italic>maxk-cover<?Pub Fmt /italic> is the problem of selecting<?Pub Fmt italic>k<?Pub Fmt /italic> subsets from<inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union has maximum cardinality. Both these problems areNP-hard.   We prove that (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) ln<?Pub Fmt italic>n<?Pub Fmt /italic> is a threshold below   which setcover cannot be approximated efficiently, unless NP has slightlysuperpolynomial time algorithms. This closes the gap (up to low-orderterms) between the ratio of approximation achievable by the greedyalogorithm (which is (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) lnn), and provious results of Lund and Yanakakis, that showed hardness ofapproximation within a ratio of <inline-equation><f><fen lp=""par""><lim align=""r""><op><rf>log</rf></op><ll>2</ll></lim>n<rp post=""par""></fen>/2≃0.72</f></inline-equation> ln <?Pub Fmt italic>n<?Pub Fmt /italic>. For max<?Pub Fmt italic>k<?Pub Fmt /italic>-cover, we show an approximationthreshold of (1 - 1/<?Pub   Fmt italic>e<?Pub Fmt /italic>)(up tolow-order terms), under assumption that <inline-equation><f>P≠NP</f><?Pub   Caret></inline-equation>.",1998,35,3199,250,34,31,35,36,48,67,77,121,110,123
fc3eb090e39d71295c362458b8a0c48d2c5d8377,"During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.",2009,29,2472,276,2,15,50,84,127,154,182,189,257,314
0ca21629c5f2da8a8a704335ab09e9b33f73f133,"This article presents a new data set on inequality in the distribution of income. The authors explain the criteria they applied in selecting data on Gini coefficients and on individual quintile groups' income shares. Comparison of the new data set with existing compilations reveals that the data assembled here represent an improvement in quality and a significant expansion in coverage, although differences in the definition of the underlying data might still affect inter temporal and international comparability. Based on this new data set, the authors do not find a systematic link between growth and changes in aggregate inequality. They do find a strong positive relationship between growth and reduction of poverty.",1996,70,3149,299,6,31,67,128,133,140,203,163,183,168
2c276714c18231f1ff2c7f7727cc479506a5b27a,"We examine explanations for corporate financing-, dividend-, and compensation-policy issues. We document robust empirical relations among corporate policy decisions and various firm characteristics. Our evidence suggests contracting theories are more important in explaining cross-sectional variation in observed financial, dividend, and compensation policies than either tax-based or signaling theories.",1992,33,3814,188,2,12,23,29,31,40,61,80,86,112
2805537bec87a6177037b18f9a3a9d3f1038867b,"The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.",1997,71,3542,215,4,18,24,38,66,50,73,72,72,70
c5561bda7ff1438096f784e553cf60385bddd09a,"Communities have the potential to function effectively and adapt successfully in the aftermath of disasters. Drawing upon literatures in several disciplines, we present a theory of resilience that encompasses contemporary understandings of stress, adaptation, wellness, and resource dynamics. Community resilience is a process linking a network of adaptive capacities (resources with dynamic attributes) to adaptation after a disturbance or adversity. Community adaptation is manifest in population wellness, defined as high and non-disparate levels of mental and behavioral health, functioning, and quality of life. Community resilience emerges from four primary sets of adaptive capacities—Economic Development, Social Capital, Information and Communication, and Community Competence—that together provide a strategy for disaster readiness. To build collective resilience, communities must reduce risk and resource inequities, engage local people in mitigation, create organizational linkages, boost and protect social supports, and plan for not having a plan, which requires flexibility, decision-making skills, and trusted sources of information that function in the face of unknowns.",2008,191,3058,240,10,29,59,99,121,196,251,304,315,326
e48f4d8dedc8ef449edf3d5917bc285901b42bf7,"A set of face stimuli called the NimStim Set of Facial Expressions is described. The goal in creating this set was to provide facial expressions that untrained individuals, characteristic of research participants, would recognize. This set is large in number, multiracial, and available to the scientific community online. The results of psychometric evaluations of these stimuli are presented. The results lend empirical support for the validity and reliability of this set of facial expressions as determined by accurate identification of expressions and high intra-participant agreement across two testing sessions, respectively.",2009,63,2698,195,43,82,127,205,220,219,244,276,273,284
e5a9fb3d49ef54f049509231e5883a14bef070f0,"A fast marching level set method is presented for monotonically advancing fronts, which leads to an extremely fast scheme for solving the Eikonal equation. Level set methods are numerical techniques for computing the position of propagating fronts. They rely on an initial value partial differential equation for a propagating level set function and use techniques borrowed from hyperbolic conservation laws. Topological changes, corner and cusp development, and accurate determination of geometric properties such as curvature and normal direction are naturally obtained in this setting. This paper describes a particular case of such methods for interfaces whose speed depends only on local position. The technique works by coupling work on entropy conditions for interface motion, the theory of viscosity solutions for Hamilton-Jacobi equations, and fast adaptive narrow band level set methods. The technique is applicable to a variety of problems, including shape-from-shading problems, lithographic development calculations in microchip manufacturing, and arrival time problems in control theory.",1996,27,3010,266,9,12,13,31,34,40,44,89,84,106
5f2f9cd5f3d0a2693a86b74cacb70e4a1c71ebc1,A contracted Gaussian basis set (6‐311G**) is developed by optimizing exponents and coefficients at the Mo/ller–Plesset (MP) second‐order level for the ground states of first‐row atoms. This has a triple split in the valence s and p shells together with a single set of uncontracted polarization functions on each atom. The basis is tested by computing structures and energies for some simple molecules at various levels of MP theory and comparing with experiment.,1980,7,10920,8,0,0,0,0,0,0,0,0,0,0
348cd9726be5e5740ab751c15fbad4b60d98246d,"Whereas much of organic chemistry has classically dealt with the preparation and study of the properties of individual molecules, an increasingly significant portion of the activity in chemical research involves understanding and utilizing the nature of the interactions between molecules. Two representative areas of this evolution are supramolecular chemistry and molecular recognition. The interactions between molecules are governed by intermolecular forces whose energetic and geometric properties are much less well understood than those of classical chemical bonds between atoms. Among the strongest of these interactions, however, are hydrogen bonds, whose directional properties are better understood on the local level (that is, for a single hydrogen bond) than many other types of non-bonded interactions. Nevertheless, the means by which to characterize, understand, and predict the consequences of many hydrogen bonds among molecules, and the resulting formation of molecular aggregates (on the microscopic scale) or crystals (on the macroscopic scale) has remained largely enigmatic. One of the most promising systematic approaches to resolving this enigma was initially developed by the late M. C. Etter, who applied graph theory to recognize, and then utilize, patterns of hydrogen bonding for the understanding and design of molecular crystals. In working with Etter's original ideas the power and potential utility of this approach on one hand, and on the other, the need to develop and extend the initial Etter formalism was generally recognized. It with that latter purpose that we originally undertook the present review.",1995,0,7111,156,1,14,17,24,36,66,68,132,147,171
a31dfa5eb97fced9494dfa1d88578da6827bf78d,"Spend your time even for only few minutes to read a book. Reading a book will never reduce and waste your time to be useless. Reading, for some people become a need that is to do every day such as spending time for eating. Now, what about you? Do you like to read a book? Now, we will show you a new book enPDFd fuzzy set theory and its applications that can be a new way to explore the knowledge. When reading this book, you can get one thing to always remember in every reading time, even step by step.",1993,0,3808,110,32,80,70,92,98,103,84,107,97,104
35f87841b45e820bc1e1bfc66ac85b6d313d6f05,"We propose a new multiphase level set framework for image segmentation using the Mumford and Shah model, for piecewise constant and piecewise smooth optimal approximations. The proposed method is also a generalization of an active contour model without edges based 2-phase segmentation, developed by the authors earlier in T. Chan and L. Vese (1999. In Scale-Space'99, M. Nilsen et al. (Eds.), LNCS, vol. 1682, pp. 141–151) and T. Chan and L. Vese (2001. IEEE-IP, 10(2):266–277). The multiphase level set formulation is new and of interest on its own: by construction, it automatically avoids the problems of vacuum and overlap; it needs only log n level set functions for n phases in the piecewise constant case; it can represent boundaries with complex topologies, including triple junctions; in the piecewise smooth case, only two level set functions formally suffice to represent any partition, based on The Four-Color Theorem. Finally, we validate the proposed models by numerical results for signal and image denoising and segmentation, implemented using the Osher and Sethian level set method.",2002,72,2581,279,2,16,56,77,110,121,131,159,157,177
e4c2d802cf9fe8de8f213727512e18bbfe3dc631,"Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which retains some of the attractive features of existing methods and overcomes some of their limitations. The authors' techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori assumption about the object's topology is made. A single instance of the authors' model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian (1988) to model propagating solid/liquid interfaces with curvature-dependent speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient field with constant speed or a speed that depends on the curvature. It is moved by solving a ""Hamilton-Jacobi"" type equation written for a function in which the interface is a particular level set. A speed term synthesized from the image is used to stop the interface in the vicinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. The authors present a variety of ways of computing the evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images. >",1995,94,3515,159,25,42,51,52,76,99,106,119,133,171
f991a2a0e09f442a2ad528bbcab8fc0992f7cf3b,"[1] The historical surface temperature data set HadCRUT provides a record of surface temperature trends and variability since 1850. A new version of this data set, HadCRUT3, has been produced, benefiting from recent improvements to the sea surface temperature data set which forms its marine component, and from improvements to the station records which provide the land data. A comprehensive set of uncertainty estimates has been derived to accompany the data: Estimates of measurement and sampling error, temperature bias effects, and the effect of limited observational coverage on large-scale averages have all been made. Since the mid twentieth century the uncertainties in global and hemispheric mean temperatures are small, and the temperature increase greatly exceeds its uncertainty. In earlier periods the uncertainties are larger, but the temperature increase over the twentieth century is still significantly larger than its uncertainty.",2006,55,2008,315,16,77,141,153,214,233,263,246,151,111
65c260fce4fb6fe44ac44562b75d38b0ac5c38f5,"Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods.",2009,60,1884,341,0,21,42,84,103,138,192,204,218,195
cf9f2a48528e50a5625eb8b8200d86a93e098b7a,"We describe the construction of a 10' latitude/longitude data set of mean monthly sur- face climate over global land areas, excluding Antarctica. The climatology includes 8 climate ele- ments —precipitation, wet-day frequency, temperature, diurnal temperature range, relative humid- ity, sunshine duration, ground frost frequency and windspeed—and was interpolated from a data set of station means for the period centred on 1961 to 1990. Precipitation was first defined in terms of the parameters of the Gamma distribution, enabling the calculation of monthly precipitation at any given return period. The data are compared to an earlier data set at 0.5o latitude/longitude resolution and show added value over most regions. The data will have many applications in applied climatology, biogeochemical modelling, hydrology and agricultural meteorology and are available through the International Water Management Institute World Water and Climate Atlas (http://www.iwmi.org) and the Climatic Research Unit (http://www.cru.uea.ac.uk).",2002,34,2127,359,1,8,21,46,69,89,109,153,139,176
0ac187c25dc630aca84751947f01879e479196e0,"The 6‐31G* and 6‐31G** basis sets previously introduced for first‐row atoms have been extended through the second‐row of the periodic table. Equilibrium geometries for one‐heavy‐atom hydrides calculated for the two‐basis sets and using Hartree–Fock wave functions are in good agreement both with each other and with the experimental data. HF/6‐31G* structures, obtained for two‐heavy‐atom hydrides and for a variety of hypervalent second‐row molecules, are also in excellent accord with experimental equilibrium geometries. No large deviations between calculated and experimental single bond lengths have been noted, in contrast to previous work on analogous first‐row compounds, where limiting Hartree–Fock distances were in error by up to a tenth of an angstrom. Equilibrium geometries calculated at the HF/6‐31G level are consistently in better agreement with the experimental data than are those previously obtained using the simple split‐valance 3‐21G basis set for both normal‐ and hypervalent compounds. Normal‐mode vibrational frequencies derived from 6‐31G* level calculations are consistently larger than the corresponding experimental values, typically by 10%–15%; they are of much more uniform quality than those obtained from the 3‐21G basis set. Hydrogenation energies calculated for normal‐ and hypervalent compounds are in moderate accord with experimental data, although in some instances large errors appear. Calculated energies relating to the stabilities of single and multiple bonds are in much better accord with the experimental energy differences.",1982,13,5402,6,0,8,14,26,42,30,47,41,33,46
9fb7b636edeaf344394fdf37481d7b83eec75358,Recently Viola et al. [2001] have introduced a rapid object detection. scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce a novel set of rotated Haar-like features. These novel features significantly enrich the simple features of Viola et al. and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%.,2002,9,3141,167,2,16,55,79,103,155,197,201,224,236
9c6f95c53af4a60acef42aa38529ab91dcce7351,"The relatively small diffuse function‐augmented basis set, 3‐21+G, is shown to describe anion geometries and proton affinities adequately. The diffuse sp orbital exponents are recommended for general use to augment larger basis sets.",1983,24,4512,1,0,3,12,20,20,16,31,31,39,44
63c659fc8a2a8238bb8952b00d1128450a7cce4b,"Light synchronizes mammalian circadian rhythms with environmental time by modulating retinal input to the circadian pacemaker—the suprachiasmatic nucleus (SCN) of the hypothalamus. Such photic entrainment requires neither rods nor cones, the only known retinal photoreceptors. Here, we show that retinal ganglion cells innervating the SCN are intrinsically photosensitive. Unlike other ganglion cells, they depolarized in response to light even when all synaptic input from rods and cones was blocked. The sensitivity, spectral tuning, and slow kinetics of this light response matched those of the photic entrainment mechanism, suggesting that these ganglion cells may be the primary photoreceptors for this system.",2002,52,2793,152,48,77,76,95,82,111,89,112,111,129
60c6bc0aafe922a8a1a1fb16e76ce0bab6cfdc98,"Descriptive set theory has been one of the main areas of research in set theory for almost a century. This text attempts to present a largely balanced approach, which combines many elements of the different traditions of the subject. It includes a wide variety of examples, exercises (over 400), and applications, in order to illustrate the general concepts and results of the theory. This text provides a first basic course in classical descriptive set theory and covers material with which mathematicians interested in the subject for its own sake or those that wish to use it in their field should be familiar. Over the years, researchers in diverse areas of mathematics, such as logic and set theory, analysis, topology, probability theory, etc., have brought to the subject of descriptive set theory their own intuitions, concepts, terminology and notation.",1987,0,2231,247,1,0,0,0,0,0,0,2,7,14
5ae073986408c9931bf6887fafb85e253866f7cc,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",2001,0,2010,237,8,28,38,55,62,62,86,86,116,97
b4e0ca86d00efc6c548939e3ed614bf64dd9d0ab,"Let A be a binary matrix of size m × n, let cT be a positive row vector of length n and let e be the column vector, all of whose m components are ones. The set-covering problem is to minimize cTx subject to Ax ≥ e and x binary. We compare the value of the objective function at a feasible solution found by a simple greedy heuristic to the true optimum. It turns out that the ratio between the two grows at most logarithmically in the largest column sum of A. When all the components of cT are the same, our result reduces to a theorem established previously by Johnson and Lovasz.",1979,4,2540,188,0,3,3,7,6,9,2,6,10,7
d243b5eb81a8501cc0477c47a4ce7d4feb524aee,"In this paper, we present a new variational formulation for geometric active contours that forces the level set function to be close to a signed distance function, and therefore completely eliminates the need of the costly re-initialization procedure. Our variational formulation consists of an internal energy term that penalizes the deviation of the level set function from a signed distance function, and an external energy term that drives the motion of the zero level set toward the desired image features, such as object boundaries. The resulting evolution of the level set function is the gradient flow that minimizes the overall energy functional. The proposed variational level set formulation has three main advantages over the traditional level set formulations. First, a significantly larger time step can be used for numerically solving the evolution partial differential equation, and therefore speeds up the curve evolution. Second, the level set function can be initialized with general functions that are more efficient to construct and easier to use in practice than the widely used signed distance function. Third, the level set evolution in our formulation can be easily implemented by simple finite difference scheme and is computationally more efficient. The proposed algorithm has been applied to both simulated and real images with promising results.",2005,28,2042,191,2,12,51,92,141,172,176,210,221,181
9ceedeb5e2ebc39a08c70e2d42d69bca0e064bc1,"A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hierarchical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the performance of some of the criteria may be data dependent.",1985,56,2759,108,4,2,14,10,10,16,10,18,27,37
afdf0c581ad8e194eb6a58a0f7582e49e77bf85d,"We have expanded the reference set of proteins used in SELCON3 by including 11 additional proteins (selected from the reference sets of Yang and co-workers and Keiderling and co-workers). Depending on the wavelength range and whether or not denatured proteins are included in the reference set, five reference sets were constructed with the number of reference proteins varying from 29 to 48. The performance of three popular methods for estimating protein secondary structure fractions from CD spectra (implemented in software packages CONTIN, SELCON3, and CDSSTR) and a variant of CONTIN, CONTIN/LL, that incorporates the variable selection method in the locally linearized model in CONTIN, were examined using the five reference sets described here, and a 22-protein reference set. Secondary structure assignments from DSSP were used in the analysis. The performances of all three methods were comparable, in spite of the differences in the algorithms used in the three software packages. While CDSSTR performed the best with a smaller reference set and larger wavelength range, and CONTIN/LL performed the best with a larger reference set and smaller wavelength range, the performances for individual secondary structures were mixed. Analyzing protein CD spectra using all three methods should improve the reliability of predicted secondary structural fractions. The three programs are provided in CDPro software package and have been modified for easier use with the different reference sets described in this paper. CDPro software is available at the website: http://lamar.colostate.edu/ approximately sreeram/CDPro.",2000,48,2541,68,0,8,34,52,66,85,102,106,118,141
5cee6b7fb97778fb9343a308f5b6982ef9c97135,"Abstract In this paper, the authors study the theory of soft sets initiated by Molodtsov. The authors define equality of two soft sets, subset and super set of a soft set, complement of a soft set, null soft set, and absolute soft set with examples. Soft binary operations like AND, OR and also the operations of union, intersection are defined. De Morgan's laws and a number of results are verified in soft set theory.",2003,10,1946,157,0,1,2,0,3,12,20,43,90,123
166641a1c482e2aadbafed7d5f7e637924442237,This paper describes an extension to the set of Basic Linear Algebra Subprograms. The extensions are targeted at matrix-vector operations that should provide for efficient and portable implementations of algorithms for high-performance computers,1990,53,1933,177,24,27,49,53,58,56,80,86,59,48
b72b370e9494b7ad715969292ff1476df1b1e2fb,"The study of sets is important and thus popular in the business and economic world for three major reasons: Basic understanding of concepts in sets and set algebra provides a form of logical language through which business specialists can communicate important concepts and ideas. Set algebra is used in solving counting problems of a logical nature. The study of set algebra provides a solid background to understanding of probability and statistics, which are important business decision-making tools.",2007,86,1685,131,99,90,88,74,89,100,110,105,91,108
475bbf493d8246031a5152c8005a5c567231307c,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",2007,54,2190,9,3,27,75,83,133,145,189,190,197,190
6e50b1cb4131104e40dc18b4de5398e7bb892e03,"In the context of structural optimization we propose a new numerical method based on a combination of the classical shape derivative and of the level-set method for front propagation. We implement this method in two and three space dimensions for a model of linear or nonlinear elasticity. We consider various objective functions with weight and perimeter constraints. The shape derivative is computed by an adjoint method. The cost of our numerical algorithm is moderate since the shape is captured on a fixed Eulerian mesh. Although this method is not specifically designed for topology optimization, it can easily handle topology changes. However, the resulting optimal shape is strongly dependent on the initial guess.",2004,40,1825,141,11,26,45,48,54,58,75,76,85,94
7d23f522cf17ac3de97d6f16fa8f616d716fb383,"This paper presents a new approach to structural topology optimization. We represent the structural boundary by a level set model that is embedded in a scalar function of a higher dimension. Such level set models are flexible in handling complex topological changes and are concise in describing the boundary shape of the structure. Furthermore, a well-founded mathematical procedure leads to a numerical algorithm that describes a structural optimization as a sequence of motions of the implicit boundaries converging to an optimum solution and satisfying specified constraints. The result is a 3D topology optimization technique that demonstrates outstanding flexibility of handling topological changes, fidelity of boundary representation and degree of automation. We have implemented the algorithm with the use of several robust and efficient numerical techniques of level set methods. The benefit and the advantages of the proposed method are illustrated with several 2D examples that are widely used in the recent literature of topology optimization, especially in the homogenization based methods.",2003,28,1957,64,2,18,29,33,36,60,54,72,85,75
0eb6c9584cb52d91ef202e9bfa0e92676eb0ecf4,"The air–sea fluxes of momentum, heat, freshwater and their components have been computed globally from 1948 at frequencies ranging from 6-hourly to monthly. All fluxes are computed over the 23 years from 1984 to 2006, but radiation prior to 1984 and precipitation before 1979 are given only as climatological mean annual cycles. The input data are based on NCEP reanalysis only for the near surface vector wind, temperature, specific humidity and density, and on a variety of satellite based radiation, sea surface temperature, sea-ice concentration and precipitation products. Some of these data are adjusted to agree in the mean with a variety of more reliable satellite and in situ measurements, that themselves are either too short a duration, or too regional in coverage. The major adjustments are a general increase in wind speed, decrease in humidity and reduction in tropical solar radiation. The climatological global mean air–sea heat and freshwater fluxes (1984–2006) then become 2 W/m2 and −0.1 mg/m2 per second, respectively, down from 30 W/m2 and 3.4 mg/m2 per second for the unaltered data. However, decadal means vary from 7.3 W/m2 (1977–1986) to −0.3 W/m2 (1997–2006). The spatial distributions of climatological fluxes display all the expected features. A comparison of zonally averaged wind stress components across ocean sub-basins reveals large differences between available products due both to winds and to the stress calculation. Regional comparisons of the heat and freshwater fluxes reveal an alarming range among alternatives; typically 40 W/m2 and 10 mg/m2 per second, respectively. The implied ocean heat transports are within the uncertainty of estimates from ocean observations in both the Atlantic and Indo-Pacific basins. They show about 2.4 PW of tropical heating, of which 80% is transported to the north, mostly in the Atlantic. There is similar good agreement in freshwater transport at many latitudes in both basins, but neither in the South Atlantic, nor at 35°N.",2009,134,1288,94,14,38,47,73,121,123,124,124,121,113
901b357c7d4ab59298bd0872554f3f34091c40ff,"I argue that research on organizational configurations has been limited by a mismatch between theory and methods and introduce set-theoretic methods as a viable alternative for overcoming this mismatch. I demonstrate the value of such methods for studying organizational configurations and discuss their applicability for examining equifinality and limited diversity among configurations, as well as their relevance to other research fields such as complementarities theory, complexity theory, and the resource-based view",2007,142,1345,140,13,22,29,40,49,60,85,72,78,155
f0a8e0bf935ae8cb55c09841483dd671d46add28,"Abstract A generalized model of rough sets called variable precision model (VP-model), aimed at modelling classification problems involving uncertain or imprecise information, is presented. The generalized model inherits all basic mathematical properties of the original model introduced by Pawlak. The main concepts are introduced formally and illustrated with simple examples. The application of the model to analysis of knowledge representation systems is also discussed.",1993,53,1893,154,7,10,12,23,22,30,23,41,37,44
71cd76a801da510b3714043629e3bc5e13a3e1ce,"Summary. To systematically identify and analyze the 15 HA and 9 NA subtypes of influenza A virus, we need reliable, simple methods that not only characterize partial sequences but analyze the entire influenza A genome. We designed primers based on the fact that the 15 and 21 terminal segment specific nucleotides of the genomic viral RNA are conserved between all influenza A viruses and unique for each segment. The primers designed for each segment contain influenza virus specific nucleotides at their 3′-end and non-influenza virus nucleotides at the 5′-end. With this set of primers, we were able to amplify all eight segments of N1, N2, N4, N5, and N8 subtypes. For N3, N6, N7, and N9 subtypes, the segment specific sequences of the neuraminidase genes are different. Therefore, we optimized the primer design to allow the amplification of those neuraminidase genes as well. The resultant primer set is suitable for all influenza A viruses to generate full-length cDNAs, to subtype viruses, to sequence their DNA, and to construct expression plasmids for reverse genetics systems.",2001,38,1759,77,2,3,7,14,24,32,52,68,104,145
24b2ae931645336f34b96d336e2c669dbccc81db,"Clinical Periodontology and Implant Dentistry 6ed - Libros de Medicina - Periodoncia - 252,00",2008,0,1107,64,57,52,57,74,90,102,100,107,69,98
e56575067b9cc874901c9db8fd161b73c32da453,"Using the concept of “orbital tuning”, a continuous, high-resolution deep-sea chronostratigraphy has been developed spanning the last 300,000 yr. The chronology is developed using a stacked oxygen-isotope stratigraphy and four different orbital tuning approaches, each of which is based upon a different assumption concerning the response of the orbital signal recorded in the data. Each approach yields a separate chronology. The error measured by the standard deviation about the average of these four results (which represents the “best” chronology) has an average magnitude of only 2500 yr. This small value indicates that the chronology produced is insensitive to the specific orbital tuning technique used. Excellent convergence between chronologies developed using each of five different paleoclimatological indicators (from a single core) is also obtained. The resultant chronology is also insensitive to the specific indicator used. The error associated with each tuning approach is estimated independently and propagated through to the average result. The resulting error estimate is independent of that associated with the degree of convergence and has an average magnitude of 3500 yr, in excellent agreement with the 2500-yr estimate. Transfer of the final chronology to the stacked record leads to an estimated error of ±1500 yr. Thus the final chronology has an average error of ±5000 yr.",1987,29,3243,246,9,21,51,49,42,70,56,69,79,90
b3c7a75a284844262a5b84658c85dd6ae3edb665,"Describes and discusses the use of theoretical models as an alternative to experiment in making accurate predictions of chemical phenomena. Addresses the formulation of theoretical molecular orbital models starting from quantum mechanics, and compares them to experimental results. Draws on a series of models that have already received widespread application and are available for new applications. A new and powerful research tool for the practicing experimental chemist.",1986,30,7893,110,10,41,75,77,98,117,144,153,212,324
333d2ceb5e56bf5434be5e7cf5566b604738ef71,Laser light with a Laguerre-Gaussian amplitude distribution is found to have a well-defined orbital angular momentum. An astigmatic optical system may be used to transform a high-order Laguerre-Gaussian mode into a high-order Hermite-Gaussian mode reversibly. An experiment is proposed to measure the mechanical torque induced by the transfer of orbital angular momentum associated with such a transformation.,1992,0,5821,121,1,2,13,3,13,11,15,19,14,26
39a2501027fb61302fe0bd05b2d01eb766af24ee,,1988,48,12173,94,0,0,0,0,0,0,0,0,0,0
5f2f9cd5f3d0a2693a86b74cacb70e4a1c71ebc1,A contracted Gaussian basis set (6‐311G**) is developed by optimizing exponents and coefficients at the Mo/ller–Plesset (MP) second‐order level for the ground states of first‐row atoms. This has a triple split in the valence s and p shells together with a single set of uncontracted polarization functions on each atom. The basis is tested by computing structures and energies for some simple molecules at various levels of MP theory and comparing with experiment.,1980,7,10920,8,0,0,0,0,0,0,0,0,0,0
0117ce48d6bd69e4ecc41ba4eaf481348f281842,"Two extended basis sets (termed 5–31G and 6–31G) consisting of atomic orbitals expressed as fixed linear combinations of Gaussian functions are presented for the first row atoms carbon to fluorine. These basis functions are similar to the 4–31G set [J. Chem. Phys. 54, 724 (1971)] in that each valence shell is split into inner and outer parts described by three and one Gaussian function, respectively. Inner shells are represented by a single basis function taken as a sum of five (5–31G) or six (6–31G) Gaussians. Studies with a number of polyatomic molecules indicate a substantial lowering of calculated total energies over the 4–31G set. Calculated relative energies and equilibrium geometries do not appear to be altered significantly.",1972,15,10224,9,0,0,0,0,0,0,0,0,0,0
f16f21d68907d91bce0b0b2272b47463714c94d6,"Polarization functions are added in two steps to a split-valence extended gaussian basis set: d-type gaussians on the first row atoms C. N, O and F and p-type gaussians on hydrogen. The same d-exponent of 0.8 is found to be satisfactory for these four atoms and the hydrogen p-exponent of 1.1 is adequate in their hydrides. The energy lowering due to d functions is found to depend on the local symmetry around the heavy atom. For the particular basis used, the energy lowerings due to d functions for various environments around the heavy atom are tabulated. These bases are then applied to a set of molecules containing up to two heavy atoms to obtain their LCAO-MO-SCF energies. The mean absolute deviation between theory and experiment (where available) for heats of hydrogenation of closed shell species with two non-hydrogen atoms is 4 kcal/mole for the basis set with full polarization. Estimates of hydrogenation energy errors at the Hartree-Fock limit, based on available calculations, are given.ZusammenfassungPolarisationsfunktionen werden in zwei Schritten einer Basis von Gauß-Orbitalen hinzugefügt: d-Gauß-Funktionen für die Atome C, N, O und F und p-Gaußfunktionen für H. In allen Fällen ist ein d-Exponent von 0.8 bzw. ein p-Exponent von 1.1 bei den Hydriden befriedigend. Dabei hängt die Energieerniedrigung, die tabelliert wiedergegeben wird, von der lokalen Symmetrie am schweren Kern ab. Mit dieser Basis wird dann die LCAO-MO-SCF-Energie für Moleküle mit 2 schweren Atomen berechnet. Die mittlere absolute Abweichung zwischen Theorie und Experiment für Hydrierungswärmen von solchen Molekülen (mit abgeschlossener Schale) ist 4 kcal/Mol bei Einschluß aller Polarisationsfunktionen. Der Schätzwert für Hydrierungswärmen in der Hartree-Fock-Grenze wird ebenfalls angegeben.",1973,17,9210,8,2,8,5,3,9,14,12,20,24,25
f8c44727449e8d86d08a78b9bd73d5a3f737a0ce,"An extended basis set of atomic functions expressed as fixed linear combinations of Gaussian functions is presented for hydrogen and the first‐row atoms carbon to fluorine. In this set, described as 4–31 G, each inner shell is represented by a single basis function taken as a sum of four Gaussians and each valence orbital is split into inner and outer parts described by three and one Gaussian function, respectively. The expansion coefficients and Gaussian exponents are determined by minimizing the total calculated energy of the atomic ground state. This basis set is then used in single‐determinant molecular‐orbital studies of a group of small polyatomic molecules. Optimization of valence‐shell scaling factors shows that considerable rescaling of atomic functions occurs in molecules, the largest effects being observed for hydrogen and carbon. However, the range of optimum scale factors for each atom is small enough to allow the selection of a standard molecular set. The use of this standard basis gives theoretical equilibrium geometries in reasonable agreement with experiment.",1971,17,6840,11,8,10,16,21,30,42,51,65,57,91
82289a1d4e3d257ad652f95035ccda6e4dbb0e9f,,1990,0,4726,26,0,6,13,32,23,27,31,45,39,62
e9d82ccbd9326c2e3c16e3c71c0943896cedc1fb,"Least‐squares representations of Slater‐type atomic orbitals as a sum of Gaussian‐type orbitals are presented. These have the special feature that common Gaussian exponents are shared between Slater‐type 2s and 2p functions. Use of these atomic orbitals in self‐consistent molecular‐orbital calculations is shown to lead to values of atomization energies, atomic populations, and electric dipole moments which converge rapidly (with increasing size of Gaussian expansion) to the values appropriate for pure Slater‐type orbitals. The ζ exponents (or scale factors) for the atomic orbitals which are optimized for a number of molecules are also shown to be nearly independent of the number of Gaussian functions. A standard set of ζ values for use in molecular calculations is suggested on the basis of this study and is shown to be adequate for the calculation of total and atomization energies, but less appropriate for studies of charge distribution.",1969,22,3308,18,3,21,26,23,37,47,62,82,103,96
57e708fec2a0186d350cda8c57320d8814191d44,Abstract A method is presented for the rapid calculation of atomic charges in σ-bonded and nonconjugated π-systems. Atoms are characterized by their orbital electronegativities. In the calculation only the connectivities of the atoms are considered. Thus only the topology of a molecule is of importance. Through an iterative procedure partial equalization of orbital electronegativity is obtained. Excellent correlations of the atomic charges with core electron binding energies and with acidity constants are observed. This establishes their value in predicting experimental data.,1980,38,3161,95,0,1,1,4,3,6,7,8,15,16
9b6fa3a9362cc3380185a7933d7982b874527230,"This paper reviews architectonic subdivisions and connections of the orbital and medial prefrontal cortex (OMPFC) in rats, monkeys and humans. Cortico-cortical connections provide the basis for recognition of 'medial' and 'orbital' networks within the OMPFC. These networks also have distinct connections with structures in other parts of the brain. The orbital network receives sensory inputs from several modalities, including olfaction, taste, visceral afferents, somatic sensation and vision, which appear to be especially related to food or eating. In contrast, the medial network provides the major cortical output to visceromotor structures in the hypothalamus and brainstem. The two networks have distinct connections with areas of the striatum and mediodorsal thalamus. In particular, projections to the nucleus accumbens and the adjacent ventromedial caudate and putamen arise predominantly from the medial network. Both networks also have extensive connections with limbic structures. Based on these and other observations, the OMPFC appears to function as a sensory-visceromotor link, especially for eating. This linkage appears to be critical for the guidance of reward-related behavior and for setting of mood. Imaging and histological observations on human brains indicate that clinical depressive disorders are associated with specific functional and cellular changes in the OMPFC, including activity and volume changes, and specific changes in the number of glial cells.",2000,133,2452,143,6,18,51,61,67,72,81,98,128,152
0f76b270d5be80f403d7bcd5b375e311a11185d4,,1951,0,4222,31,2,2,6,10,23,14,20,22,18,25
1e593ab42bc870eed5b4573d9e69887cfbfd3a04,,1995,0,2619,34,0,2,7,6,6,6,9,11,27,31
aaea825be9afc25ee2d6d7f279030d0711fa656f,"Entangled quantum states are not separable, regardless of the spatial separation of their components. This is a manifestation of an aspect of quantum mechanics known as quantum non-locality. An important consequence of this is that the measurement of the state of one particle in a two-particle entangled state defines the state of the second particle instantaneously, whereas neither particle possesses its own well-defined state before the measurement. Experimental realizations of entanglement have hitherto been restricted to two-state quantum systems, involving, for example, the two orthogonal polarization states of photons. Here we demonstrate entanglement involving the spatial modes of the electromagnetic field carrying orbital angular momentum. As these modes can be used to define an infinitely dimensional discrete Hilbert space, this approach provides a practical route to entanglement that involves many orthogonal quantum states, rather than just two Multi-dimensional entangled states could be of considerable importance in the field of quantum information, enabling, for example, more efficient use of communication channels in quantum cryptography.",2001,26,2253,31,4,13,24,54,49,52,55,60,75,64
57fad30a19ef9b6ad0c17fa7c8b50b7006c8dea6,,1970,0,2723,63,2,30,50,65,97,71,103,108,95,87
a5c8d98d25da1770770d58ea53f51d614e44339f,"A high-resolution deuterium profile is now available along the entire European Project for Ice Coring in Antarctica Dome C ice core, extending this climate record back to marine isotope stage 20.2, ∼800,000 years ago. Experiments performed with an atmospheric general circulation model including water isotopes support its temperature interpretation. We assessed the general correspondence between Dansgaard-Oeschger events and their smoothed Antarctic counterparts for this Dome C record, which reveals the presence of such features with similar amplitudes during previous glacial periods. We suggest that the interplay between obliquity and precession accounts for the variable intensity of interglacial periods in ice core records.",2007,43,1771,71,21,45,75,120,120,121,134,140,134,127
45d2751f829811f1af68d58a1ef494824cc7bb21,"Before we start, we need a working definition for MOPAC. The following description has been used many times to describe MOPAC: MOPAC is a general-purpose, semiempirical molecular orbital program for the study of chemical reactions involving molecules, ions, and linear polymers. It implements the semiempirical Hamiltonians MNDO, AM 1, MINDO/3, and MNDOPM3, and combir_es the calculations of vibrational spectra, thermodynamic quantities, isotopic substitution effects, and force constants in a fully integrated program. Elements parameterized at the MNDO level include H, Li, Be, B, C, N, O, F, A1, Si, P, S, C1, Ge, Br, Sn, Hg, Pb, and I; at the PM3 level the elements H, C, N, O, F, A1, Si, P, S, C1, Br, and I are available. Within the electronic part of the calculation, molecular and localized orbitals, excited states up to sextets, chemical bond indices, charges, etc. are computed. Both intrinsic and dynamic reaction coordinates can be calculated. A transition-state location routine and two transition-state optimizing routines are available for studying chemical reactions.",1990,103,2077,37,2,23,53,55,85,88,98,98,72,90
6a145d8e7158476d0930c679a8777872928426af,"We demonstrate the transfer of information encoded as orbital angular momentum (OAM) states of a light beam. The transmitter and receiver units are based on spatial light modulators, which prepare or measure a laser beam in one of eight pure OAM states. We show that the information encoded in this way is resistant to eavesdropping in the sense that any attempt to sample the beam away from its axis will be subject to an angular restriction and a lateral offset, both of which result in inherent uncertainty in the measurement. This gives an experimental insight into the effects of aperturing and misalignment of the beam on the OAM measurement and demonstrates the uncertainty relationship for OAM.",2004,16,1791,14,2,12,22,26,30,38,46,71,93,120
071bd3802e3186d066fbbd79a283a7fad3f97417,"We have carried out a natural bond orbital analysis of hydrogen bonding in the water dimer for the near‐Hartree–Fock wave function of Popkie, Kistenmacher, and Clementi, extending previous studies based on smaller basis sets and less realistic geometry. We find that interactions which may properly be described as ‘‘charge transfer’’ (particularly the n‐σ*OH interaction along the H‐bond axis) play a critical role in the formation of the hydrogen bond, and without these interactions the water dimer would be 3–5 kcal/mol repulsive at the observed equilibrium distance. We discuss this result in relationship to Klemperer’s general picture of the bonding in van der Waals molecules, and to previous theoretical analyses of hydrogen bonding by the method of Kitaura and Morokuma.",1983,9,2119,8,1,1,4,6,2,11,2,6,4,2
8086ad05bf94c247674648ea8ec346aee8564613,A new magneto-optical sum rule is derived for circular magnetic dichroism in the x-ray region (CMXD). The integral of the CMXD signal over a given edge allows one to determine the ground-state expectation value of the orbital angular momentum. Applications are discussed to transition-metal and rare-earth magnetic systems.,1992,0,1746,13,4,20,35,42,40,37,34,42,22,49
efcb8f7293255dabf4ac2aea4a2515d977847c28,"After giving a concise overview of the current knowledge in the field of quantum mechanical bonding indicators for molecules and solids, we show how to obtain energy-resolved visualization of chemical bonding in solids by means of density-functional electronic structure calculations. On the basis of a band structure energy partitioning scheme, i.e., rewriting the band structure energy as a sum of orbital pair contributions, we derive what is to be defined as crystal orbital Hamilton populations (COHP). In particular, a COHP(E) diagram indicates bonding, nonbonding, and antibonding energy regions within a specified energy range while an energy integral of a COHP gives access to the contribution of an atom or a chemical bond to the distribution of one-particle energies",1993,0,1681,11,0,0,0,0,1,0,10,8,12,14
c4950fc9bb26369182bcd30a863782cc873d71f1,"We demonstrate experimentally an optical process in which the spin angular momentum carried by a circularly polarized light beam is converted into orbital angular momentum, leading to the generation of helical modes with a wave-front helicity controlled by the input polarization. This phenomenon requires the interaction of light with matter that is both optically inhomogeneous and anisotropic. The underlying physics is also associated with the so-called Pancharatnam-Berry geometrical phases involved in any inhomogeneous transformation of the optical polarization.",2006,73,1316,18,1,10,14,21,26,36,47,57,92,110
74569b70f1d283bc43e07f3a078250677c89a5b6,"An electron in a solid, that is, bound to or nearly localized on the specific atomic site, has three attributes: charge, spin, and orbital. The orbital represents the shape of the electron cloud in solid. In transition-metal oxides with anisotropic-shaped d-orbital electrons, the Coulomb interaction between the electrons (strong electron correlation effect) is of importance for understanding their metal-insulator transitions and properties such as high-temperature superconductivity and colossal magnetoresistance. The orbital degree of freedom occasionally plays an important role in these phenomena, and its correlation and/or order-disorder transition causes a variety of phenomena through strong coupling with charge, spin, and lattice dynamics. An overview is given here on this ""orbital physics,"" which will be a key concept for the science and technology of correlated electrons.",2000,60,1549,7,6,26,54,65,61,82,70,66,69,53
a35562cccd1745362398db5a4b19f9c5735263fe,"calculated energy of the 2Z state of A10 is lower by 8.6 kcal/mol than that of the ZII state and in good agreement with the previously calculated valueZo (9.9 kcal/mol). The calculated energy of AIO(ZZ) + CO is 15.4 kcal/mol above that of the trans-type complex. From these values, the reaction A1 + C 0 2 AlO(’Z) + CO is 5.8 kcal/mol endothermic, and this value is consistent with the experimental estimation (4.5 kcal/mol: D(C-0) = 126 kcal/mol, D(A1-O) = 121.5 kcal/mol). The geometries of the transition states of the 22 and the 211 states are similar to each other except for the AlOC angle and the A10 distance. From the transition-state structures, both reactions (2Z and ZII) consequently becomes transition states, which is qualitatively consistant with the Hammond postulate.2’ The ,Z transition state correlates to the C,, complex, and the ZII transition state correlates to trans-type complex. The values of energy barriers of the products are 17.8 and 4.6 kcal/mol for the ZZ and Zll states, respectively. The ZII transition state is lower in energy by 4.6 kcai/mol than the transition state. This is explained by the fact that the 2Z state of A10 at the long A1-O distance from the equilibrium bond distance is higher in energy than the 211 state.19 Thus, both reaction surfaces possibly cross each other in the neighboring region of the transition states. Experimentally determined activation energies’*4 of the A1C02 reaction are 2.5 and 3.9 kcal/mol, while the experimentally estimated heat of reaction is about 5 kcal/mol endothermic. Therefore, the experimental activation energy is considered to be that of the reaction of the AlCO2 complex formation. The calculated activation energy of the complex formation is 2.3 kcal/mol",1992,2,1786,10,9,19,31,38,50,43,61,80,67,52
c13e0d603a39430565ed7cb94ab5add3da5b280a,"This graduate-level text presents the first comprehensive overview of modern chemical valency and bonding theory, written by internationally recognized experts in the field. The authors build on the foundation of Lewisand Pauling-like localized structural and hybridization concepts to present a book that is directly based on current ab initio computational technology. The presentation is highly visual and intuitive throughout, being based on the recognizable and transferable graphical forms of natural bond orbitals (NBOs) and their spatial overlaps in the molecular environment. The book shows applications to a broad range of molecular and supramolecular species of organic, inorganic, and bioorganic interest. Hundreds of orbital illustrations help to convey the essence of modern NBO concepts in a facile manner for those with no extensive background in the mathematical machinery of the Schrödinger equation. This book will appeal to those studying chemical bonding in relation to chemistry, chemical engineering, biochemistry, and physics.",2005,1,1351,57,1,21,32,45,40,44,57,76,126,141
43f26281e0509ae4f5a8eaa5c5379c0c2294be8a,"In a previous paper (Jahn and Teller 1937) the following theorem was established: A configuration of a polyatomic molecule for an electronic state having orbital degeneracy cannot be stable with respect to all displacements of the nuclei unless in the original configuration the nuclei all lie on a straight line. The proof given of this theorem took no account of the electronic spin, and in the present paper the justification of this is investigated. An extension of the theorem to cover additional degeneracy arising from the spin is established, which shows that if the total electronic state of orbital and spin motion is degenerate, then a non-linear configuration of the molecule will be unstable unless the degeneracy is the special twofold one (discussed by Kramers 1930) which can occur only when the molecule contains an odd number of electrons. The additional instability caused by the spin degeneracy alone, however, is shown to be very small and its effect for all practical purposes negligible. The possibility of spin forces stabilizing a non-linear configuration which is unstable owing to orbital degeneracy is also investigated, and it is shown that this is not possible except perhaps for molecules containing heavy atoms for which the spin forces are large. Thus whilst a symmetrical nuclear configuration in a degenerate orbital state might under exceptional circumstances be rendered stable by spin forces, it is not possible for the spin-orbit interaction to cause instability of an orbitally stable state. 1—General theorem for molecules with spin Just as before we must see how the symmetry of the molecular framework determines whether the energy of a degenerate electronic state with spin depends linearly upon nuclear displacements. This is again determined by the existence of non-vanishing perturbation matrix elements which are linear in the nuclear displacements. These matrix elements are integrals involving the electronic wave functions with spin and the nuclear dis­placements, and we deduce as before from their transformation properties whether for a given molecular symmetry they can be different from zero.",1937,6,2280,19,1,2,2,1,2,0,0,0,0,0
f99470f8768387f5238a7b0fa75bcf4185bae530,"Chemistry remains an experimental science. The theory of chemical bonding leaves much to be desired. Yet, the past 20 years have been marked by a fruitful symbiosis of organic chemistry and molecular orbital theory. Of necessity this has been a marriage of poor theory with good experiment. Tentative conclusions have been arrived a t on the basis of theories which were such a patchwork on approximations that they appeared to have no right to work; yet, in the hands of clever experimentalists, these ideas were transformed into novel molecules with unusual properties. In the same way, by utilizing the most simple but fundamental concepts of molecular orbital theory we have in the past 3 years been able to rationalize and predict the stereochemical course of virtually every concerted organic reaction.' In our work we have relied on the most basic ideas of molecular orbital theory-the concepts of symmetry, overlap, interaction, bonding, and the nodal structure of wave functions. The lack of numbers in our discussion is not a weakness-it is its greatest strength. Precise numerical values would have to result from some specific sequence of approximations. But an argument from first principles or symmetry, of necessity qualitative, is in fact much stronger than the deceptively authoritative numerical result. For, if the simple argument is true, then any approximate method, as well as the now inaccessible exact solution, must obey it. The simplest description of the electronic structure of a stable molecule is that i t is characterized by a finite band of doubly occupied electronic levels, called bonding orbitals, separated by a gap from a corresponding band of unoccupied, antiboding levels as well as a continuum of higher levels. The magnitude of the gap may range from 40 kcal/mole for highly delocalized, large aromatic systems to 250 kcal/mole for saturated hydrocarbons. It should be noted in context that socalled nonbonding electrons of heteroatoms are in fact bonding. Consider a simple reaction of two molecules to give a third species, proceeding in a nonconcerted manner through a diradical intermediate I. A + B + [I] + C",1970,317,1854,19,1,17,18,17,23,20,30,22,22,27
e64048a7a497655acc02b14b0c045c1acf1cafb8,"Abstract We have carried out ab initio UHF/6-31G* calculations on the hydroxymethyl radical, CH 2 OH, and have found the equilibrium structure to be nearly planar with barriers to internal rotation occurring at staggered and eclipsed geometries, in good agreement with experiment. The electronic structure of the radical was analyzed via the “different hybrids for different spins” natural bond orbital (DHDS NBO) procedure, which finds separate Lewis structures for each of the spin systems. The α spin Lewis structure resembles that of the anion; the β spin Lewis structure resembles the corresponding cation. This simple picture, in conjunction with Bent's rule, allows one to understand the principal electronic factors which dictate the structure of the radical CH 2 group and its torsional and inversion potentials. Charge transfer between oxygen non-bonding orbitals and the empty radical orbital in the β spin system is the dominant interaction determining the torsional potential. Smaller hyperconjugative interactions in the α spin system resemble interactions in closed-shell molecules and directly oppose the effect of radical hyperconjugation, thus illustrating the central idea that open-shell potential energy features result from competition between the two different spin systems.",1988,27,1474,7,1,1,0,1,1,0,4,1,4,7
e8803f57f80d355dcb36b9c90b35a936af7bccc2,,1999,0,1200,13,4,3,7,8,11,37,20,46,40,51
02640f28ab5c2fe5a883c855fd0bc77505b48a00,"Planetary formation theories suggest that the giant planets formed on circular and coplanar orbits. The eccentricities of Jupiter, Saturn and Uranus, however, reach values of 6 per cent, 9 per cent and 8 per cent, respectively. In addition, the inclinations of the orbital planes of Saturn, Uranus and Neptune take maximum values of ∼2 degrees with respect to the mean orbital plane of Jupiter. Existing models for the excitation of the eccentricity of extrasolar giant planets have not been successfully applied to the Solar System. Here we show that a planetary system with initial quasi-circular, coplanar orbits would have evolved to the current orbital configuration, provided that Jupiter and Saturn crossed their 1:2 orbital resonance. We show that this resonance crossing could have occurred as the giant planets migrated owing to their interaction with a disk of planetesimals. Our model reproduces all the important characteristics of the giant planets' orbits, namely their final semimajor axes, eccentricities and mutual inclinations.",2005,59,1097,102,16,30,44,46,82,61,80,66,64,96
31ca543761136a40985c3c3155c546474865a50b,"A simple level of ab initio molecular orbital theory with a split-valence shell basis with d-type polarization functions (6–31G*) is used to predict equilibrium geometries for the ground and some low-lying excited states of AHn molecules and cations where A is carbon, nitrogen, oxygen or fluorine. The results are shown to be close to the limit for single determinant wave functions in cases where corresponding computations with more extensive bases are available. Comparison with experimental results also shows good agreement although a systematic underestimation of bond lengths up to 3 per cent is evident. For systems where no experimental data are available, the results provide predictions of equilibrium geometry.",1974,16,1568,1,1,2,4,2,5,3,8,6,13,10
d1620666ba23d4bce8c3035a87d06f5277819cc3,"Measurement of the quantum-mechanical phase in quantum matter provides the most direct manifestation of the underlying abstract physics. We used resonant x-ray scattering to probe the relative phases of constituent atomic orbitals in an electronic wave function, which uncovers the unconventional Mott insulating state induced by relativistic spin-orbit coupling in the layered 5d transition metal oxide Sr2IrO4. A selection rule based on intra-atomic interference effects establishes a complex spin-orbital state represented by an effective total angular momentum = 1/2 quantum number, the phase of which can lead to a quantum topological state of matter.",2009,11,642,18,5,10,20,32,47,45,56,54,56,94
411be93d0ae3ff9fbf174bd4de4f11e3203b8a77,"In iron pnictides, we find that the moderate electron-phonon interaction due to the Fe-ion oscillation can induce the critical d-orbital fluctuations, without being prohibited by the Coulomb interaction. These fluctuations give rise to the strong pairing interaction for the s-wave superconducting (SC) state without sign reversal (s(++)-wave state), which is consistent with experimentally observed robustness of superconductivity against impurities. When the magnetic fluctuations due to Coulomb interaction are also strong, the SC state shows a smooth crossover from the s-wave state with sign reversal (s(+/-)-wave state) to the s(++)-wave state as impurity concentration increases.",2009,0,377,6,2,9,32,46,40,46,38,41,34,36
cf335ec147d76c16e49263d17c388814290a35fc,"This review provides a perspective on the use of orbital-dependent functionals, which is currently considered one of the most promising avenues in modern density-functional theory. The focus here is on four major themes: the motivation for orbital-dependent functionals in terms of limitations of semilocal functionals; the optimized effective potential as a rigorous approach to incorporating orbital-dependent functionals within the Kohn-Sham framework; the rationale behind and advantages and limitations of four popular classes of orbital-dependent functionals; and the use of orbital-dependent functionals for predicting excited-state properties. For each of these issues, both formal and practical aspects are assessed.",2008,527,732,8,25,50,38,68,56,64,72,71,61,45
c9d24e78b2bdc2ddfe89144e020846610cdfcefd,"ABSTRACT. We analyze 8 years of precise radial velocity measurements from the Keck Planet Search, characterizing the detection threshold, selection effects, and completeness of the survey. We first carry out a systematic search for planets, by assessing the false-alarm probability associated with Keplerian orbit fits to the data. This allows us to understand the detection threshold for each star in terms of the number and time baseline of the observations, and the underlying “noise” from measurement errors, intrinsic stellar jitter, or additional low-mass planets. We show that all planets with orbital periods P   20 m s-1 K > 20 m s - 1 , and eccentricities e ≲ 0.6 e ≲ 0.6 have been announced, and we summarize the candidates at lower amplitudes and longer orbital periods. For the remaining stars, we calculate upper limits on the velocity amplitude of a companion. For orbital periods less than the duration of the observations, these are typically ...",2008,91,611,154,22,38,56,37,51,43,46,47,49,47
45524fd482e1e348a63c2d146a55a77d6ce36608,"The control of charge transport in an active electronic device depends intimately on the modulation of the internal charge density by an external node. For example, a field-effect transistor relies on the gated electrostatic modulation of the channel charge produced by changing the relative position of the conduction and valence bands with respect to the electrodes. In molecular-scale devices, a longstanding challenge has been to create a true three-terminal device that operates in this manner (that is, by modifying orbital energy). Here we report the observation of such a solid-state molecular device, in which transport current is directly modulated by an external gate voltage. Resonance-enhanced coupling to the nearest molecular orbital is revealed by electron tunnelling spectroscopy, demonstrating direct molecular orbital gating in an electronic device. Our findings demonstrate that true molecular transistors can be created, and so enhance the prospects for molecularly engineered electronic devices.",2009,35,585,8,3,29,65,55,56,74,63,51,45,49
a64008755dfaf98ee26c5d5f6dfccd348093f9d5,,1964,0,1564,4,0,0,4,1,2,6,1,6,11,7
4dbc9423b20d6aa61c1971f5ee7a3a7f02ab1828,"A new intrinsic localization algorithm is suggested based on a recently developed mathematical measure of localization. No external criteria are used to define a priori bonds, lone pairs, and core orbitals. It is shown that the method similarly to Edmiston–Ruedenberg’s localization prefers the well established chemical concept of σ–π separation, while on the other hand, works as economically as Boys’ procedure. For the application of the new localization algorithm, no additional quantities are to be calculated, the knowledge of atomic overlap intergrals is sufficient. This feature allows a unique formulation of the theory, adaptable for both ab initio and semiempirical methods, even in those cases where the exact form of the atomic basis functions is not defined (like in the EHT and PPP calculations). The implementation of the procedure in already existing program systems is particularly easy. For illustrative examples, we compare the Edmiston–Ruedenberg and Boys localized orbitals with those calculated b...",1989,42,1196,10,0,3,6,2,7,5,12,13,8,11
2ff7c952c61ba727a4b96ba0ca9e3c4993edddea,"Previous studies have shown that the orbital and medial prefrontal cortex (OMPFC) is extensively connected with medial temporal and cingulate limbic structures. In this study, the organization of these projections was defined in relation to architectonic areas within the OMPFC. All of the limbic structures were substantially connected with the following posterior and medial orbital areas: the posteromedial, medial, intermediate, and lateral agranular insular areas (Iapm, Iam, Iai, and Ial, respectively) and areas 11m, 13a, 13b, 14c, and 14r. In contrast, lateral orbital areas 12o, 12m, and [12] and medial wall areas 24a, b and 32 were primarily connected with the amygdala, the temporal pole, and the cingulate cortex. Data were not obtained on the poateroventral medial wall.",1995,81,1139,70,1,3,9,15,19,37,40,34,44,34
0e15715debd0a700242264e06f84fd9e37ac86a3,"We show numerically that vector antenna arrays can generate radio beams that exhibit spin and orbital angular momentum characteristics similar to those of helical Laguerre-Gauss laser beams in paraxial optics. For low frequencies (< or = 1 GHz), digital techniques can be used to coherently measure the instantaneous, local field vectors and to manipulate them in software. This enables new types of experiments that go beyond what is possible in optics. It allows information-rich radio astronomy and paves the way for novel wireless communication concepts.",2007,59,697,31,2,6,9,7,12,17,17,45,44,78
476df6a6307015e2ebcd47d80df54dad04718c55,"In the search for a quantitative correlation between reactivity and electronic configuration of aromatic hydrocarbons, the electron density, at each carbon atom, of the highest occupied π‐orbital in the ground state of the molecule is calculated by means of the LCAO method. Comparing the result of such a calculation on fifteen condensed aromatic hydrocarbons with their chemical reactivities, we find that the position at which the electron density is largest is most readily attacked by electrophilic or oxidizing reagents.It is, therefore, concluded that distinct from other π‐electrons the pair of π‐electrons occupying the highest orbital, which is referred to as frontier electrons, plays a decisive role in chemical activation of these hydrocarbon molecules. The theoretical significance of this discrimination of the frontier electrons in relation to the chemical activation is discussed.",1952,9,1464,4,1,2,2,4,2,1,2,9,4,6
6e62526fba615f7e3d0f49711a41fd6047be4b5f,"The structure of the human orbital and medial prefrontal cortex (OMPFC) was investigated using five histological and immunohistochemical stains and was correlated with a previous analysis in macaque monkeys [Carmichael and Price ( 1994 ) J. Comp. Neurol. 346:366–402]. A cortical area was recognized if it was distinct with at least two stains and was found in similar locations in different brains. All of the areas recognized in the macaque OMPFC have counterparts in humans. Areas 11, 13, and 14 were subdivided into areas 11m, 11l, 13a, 13b, 13m, 13l, 14r, and 14c. Within area 10, the region corresponding to area 10m in monkeys was divided into 10m and 10r, and area 10o (orbital) was renamed area 10p (polar). Areas 47/12r, 47/12m, 47/12l, and 47/12s occupy the lateral orbital cortex, corresponding to monkey areas 12r, 12m, 12l, and 12o. The agranular insula (areas Iam, Iapm, Iai, and Ial) extends onto the caudal orbital surface and into the horizontal ramus of the lateral sulcus. The growth of the frontal pole in humans has pushed area 25 and area 32pl, which corresponds to the prelimbic area 32 in Brodmann's monkey brain map, caudal and ventral to the genu of the corpus callosum. Anterior cingulate areas 24a and 24b also extend ventral to the genu of the corpus callosum. Area 32ac, corresponding to the dorsal anterior cingulate area 32 in Brodmann's human brain map, is anterior and dorsal to the genu. The parallel organization of the OMPFC in monkeys and humans allows experimental data from monkeys to be applied to studies of the human cortex. J. Comp. Neurol. 460:425–449, 2003. © 2003 Wiley‐Liss, Inc.",2003,76,858,62,9,9,31,34,47,42,58,52,65,51
6479c0e44ab143679d2337e9b726d6196d8dfa0b,"Geochemical models for Mars predict carbonate formation during aqueous alteration. Carbonate-bearing rocks had not previously been detected on Mars' surface, but Mars Reconnaissance Orbiter mapping reveals a regional rock layer with near-infrared spectral characteristics that are consistent with the presence of magnesium carbonate in the Nili Fossae region. The carbonate is closely associated with both phyllosilicate-bearing and olivine-rich rock units and probably formed during the Noachian or early Hesperian era from the alteration of olivine by either hydrothermal fluids or near-surface water. The presence of carbonate as well as accompanying clays suggests that waters were neutral to alkaline at the time of its formation and that acidic weathering, proposed to be characteristic of Hesperian Mars, did not destroy these carbonates and thus did not dominate all aqueous environments.",2008,89,514,18,2,31,49,39,49,56,39,38,29,36
9dfcefd84f7628d3e6d9ca9182f02aed31838958,Abstract We present the design of a mode converter which transforms a Hermite-gaussian mode of arbitrarily high order to a Laguerre-gaussian mode and vice versa. The converter consists of two cylindrical lenses and is based on appropriate use of the Gouy phase. We demonstrate mode conversion experimentally and consider where the concomitant transfer of orbital angular momentum is localized.,1993,10,1057,16,1,10,5,7,12,11,14,8,14,21
8b75af5b595e3402c77aa438f926ad3dba9fde17,"Orbital mechanics is a cornerstone subject for aerospace engineering students. Maintaining the focus of the first edition, the author provides the foundation needed to understand the subject and proceed to advanced topics. Starting with the solution of the two-body problem and formulas for the different kinds of orbits, the text moves on to Kepler's equations, orbits in three dimensions, orbital elements from observations, orbital maneuvers, orbital rendezvous and interplanetary missions. This is followed by an introduction to spacecraft dynamics and a final chapter on basic rocket dynamics. The author's teach-by-example approach emphasizes the analytical procedures and computer-implemented algorithms required by today's students. There are a large number of worked examples, illustrations, end of chapter exercises (with answers) as well as many MATLAB[registered] programs for use in homework and projects. The text can be used for one and two semester courses in space mechanics. Features: a new section on numerical integration methods applicable to space mechanics problems; a more centralized and improved discussion of coordinate systems and Euler angle sequences; an expanded development of relative motion in orbit; a new section on quaternions; new worked-out examples, illustrations and homework problems; new algorithms, MATLAB[registered] scripts and simulations; instructor's manual and lecture slides available online; and included online testing and assessment component helps students assess their knowledge of the topics.",2005,0,742,81,1,2,6,5,19,19,31,50,54,71
6225e495487d0a76585132577386fa771c0248c7,,1961,45,1387,11,0,6,24,27,27,48,60,50,45,54
56677386dc3d3ecd1f51a292810a648bc68da1db,"The theoretical need to study the properties of the Fe-based high-Tc superconductors using reliable manybody techniques has highlighted the importance of determining what is the minimum number of orbital degrees of freedom that will capture the physics of these materials. While the shape of the Fermi surface FS obtained with the local-density approximation LDA can be reproduced by a two-orbital model, it has been argued that the bands that cross the chemical potential result from the strong hybridization of three of the Fe 3d orbitals. For this reason, a three orbital Hamiltonian for LaOFeAs obtained with the Slater-Koster formalism by considering the hybridization of the As p orbitals with the Fe dxz, dyz, and dxy orbitals is discussed here. This model reproduces qualitatively the FS shape and orbital composition obtained by LDA calculations for undoped LaOFeAs when four electrons per Fe are considered. Within a mean-field approximation, its magnetic and orbital properties in the undoped case are here described for intermediate values of J/U. Increasing the Coulomb repulsion U at zero temperature, four different regimes are obtained: 1 paramagnetic, 2 magnetic ,0 spin order, 3 the same ,0 spin order but now including orbital order, and finally 4 a magneticmore » and orbital ordered insulator. The spin-singlet pairing operators allowed by the lattice and orbital symmetries are also constructed. It is found that for pairs of electrons involving up to diagonal nearest-neighbors sites, the only fully gapped and purely intraband spin-singlet pairing operator is given by k= fkdk,, d k,, with fk=1 or cos kx cos ky which would arise only if the electrons in all different orbitals couple with equal strength to the source of pairing.« less",2009,2,123,4,1,5,8,12,17,12,17,11,9,10
46649df2532231b8431a547d0838b1229469fa4c,"Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented. THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics. Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated. It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other. Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them. It is still rare for empirical research in macroeconomics to be planned and executed within the framework of one of the large models. In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement. I will argue that the style in which their builders construct claims for a connection between these models and reality-the style in which ""identification"" is achieved for these models-is inappropriate, to the point at which claims for identification in these models cannot be taken seriously. This is a venerable assertion; and there are some good old reasons for believing it;2 but there are also some reasons which have been more recently put forth. After developing the conclusion that the identification claimed for existing large-scale models is incredible, I will discuss what ought to be done in consequence. The line of argument is: large-scale models do perform useful forecasting and policy-analysis functions despite their incredible identification; the restrictions imposed in the usual style of identification are neither essential to constructing a model which can perform these functions nor innocuous; an alternative style of identification is available and practical. Finally we will look at some empirical work based on an alternative style of macroeconometrics. A six-variable dynamic system is estimated without using 1 Research for this paper was supported by NSF Grant Soc-76-02482. Lars Hansen executed the computations. The paper has benefited from comments by many people, especially Thomas J. Sargent",1980,25,10498,560,0,0,0,0,0,0,0,0,0,36
61809429300ac730ac4a3f0db5113274fad56c7d,"Foundations of International Macroeconomics is an innovative text that offers the first integrative modern treatment of the core issues in open economy macroeconomics and finance. With its clear and accessible style, it is suitable for first-year graduate macroeconomics courses as well as graduate courses in international macroeconomics and finance. Each chapter incorporates an extensive and eclectic array of empirical evidence. For the beginning student, these examples provide motivation and aid in understanding the practical value of the economic models developed. For advanced researchers, they highlight key insights and conundrums in the field. Topic coverage includes intertemporal consumption and investment theory, government spending and budget deficits, finance theory and asset pricing, the implications of (and problems inherent in) international capital market integration, growth, inflation and seignorage, policy credibility, real and nominal exchange rate determination, and many interesting special topics such as speculative attacks, target exchange rate zones, and parallels between immigration and capital mobility. Most main results are derived both for the small country and world economy cases. The first seven chapters cover models of the real economy, while the final three chapters incorporate the economy's monetary side, including an innovative approach to bridging the usual chasm between real and monetary models.",1997,1,3554,223,24,59,95,131,157,229,209,209,226,234
9d95e4d33302982c5d612033c8740b551bb4759f,"This paper analyzes the role of income distribution in macroeconomic analysis. The study demonstrates that the long-run equilibrium depends on the initial distribution of income. In accordance with empirical evidence concerning the correlation between income distribution and output, an economy that is characterized by a relatively equal distribution of wealth is likely to be wealthier in the long run. The study may, therefore, provide an additional explanation for the persistent differences in per-capita output across countries. Furthermore, the paper may shed light on cross-countries differences macroeconomic adjustment to aggregate shocks.",1988,20,4140,257,1,0,0,3,9,9,10,20,43,36
23983dfef4231c3289a5201b0dbc63b525d3f8d8,"A crucial challenge for economists is figuring out how people interpret the world and form expectations that will likely influence their economic activity. Inflation, asset prices, exchange rates, investment, and consumption are just some of the economic variables that are largely explained by expectations. Here George Evans and Seppo Honkapohja bring new explanatory power to a variety of expectation formation models by focusing on the learning factor. Whereas the rational expectations paradigm offers the prevailing method to determining expectations, it assumes very theoretical knowledge on the part of economic actors. Evans and Honkapohja contribute to a growing body of research positing that households and firms learn by making forecasts using observed data, updating their forecast rules over time in response to errors. This book is the first systematic development of the new statistical learning approach. Depending on the particular economic structure, the economy may converge to a standard rational-expectations or a ""rational bubble"" solution, or exhibit persistent learning dynamics. The learning approach also provides tools to assess the importance of new models with expectational indeterminacy, in which expectations are an independent cause of macroeconomic fluctuations. Moreover, learning dynamics provide a theory for the evolution of expectations and selection between alternative equilibria, with implications for business cycles, asset price volatility, and policy. This book provides an authoritative treatment of this emerging field, developing the analytical techniques in detail and using them to synthesize and extend existing research.",2001,0,2058,334,22,32,81,65,90,110,136,126,108,118
b9906576bc7afd8553a69e12b2ef5970efe928d4,"Lectures on Macroeconomics provides the first comprehensive description and evaluation of macroeconomic theory in many years. While the authors' perspective is broad, they clearly state their assessment of what is important and what is not as they present the essence of macroeconomic theory today. The main purpose of Lectures on Macroeconomics is to characterize and explain fluctuations in output, unemployment and movement in prices. The most important fact of modern economic history is persistent long term growth, but as the book makes clear, this growth is far from steady. The authors analyze and explore these fluctuations. Topics include consumption and investment; the Overlapping Generations Model; money; multiple equilibria, bubbles, and stability; the role of nominal rigidities; competitive equilibrium business cycles, nominal rigidities and economic fluctuations, goods, labor and credit markets; and monetary and fiscal policy issues. Each of chapters 2 through 9 discusses models appropriate to the topic. Chapter 10 then draws on the previous chapters, asks which models are the workhorses of macroeconomics, and sets the models out in convenient form. A concluding chapter analyzes the goals of economic policy, monetary policy, fiscal policy, and dynamic inconsistency. Written as a text for graduate students with some background in macroeconomics, statistics, and econometrics, Lectures on Macroeconomics also presents topics in a self contained way that makes it a suitable reference for professional economists.",1972,0,2807,149,0,0,0,0,0,0,0,0,0,0
b29f0b51308108cf34c20558df33f3e957b20b40,"The central claim in this paper is that by explicitly introducing costs of international trade (narrowly, transport costs, but more broadly, tariffs, nontariff barriers, and other trade costs), one can go far toward explaining a great number of the main empirical puzzles that international macroeconomists have struggled with over twenty-five years. Our approach elucidates J. McCallum's home-bias-in-trade puzzle, the Feldstein-Horioka saving-investment puzzle, the French-Poterba equity-home-bias puzzle, and the Backus-Kehoe-Kydland consumption-correlations puzzle. That one simple alteration to an otherwise canonical international macroeconomic model can help substantially to explain such a broad range of empirical puzzles, including some that previously seemed intractable, suggests a rich area for future research. We also address a variety of international pricing puzzles, including the purchasing-power-parity puzzle emphasized by Rogoff, and what we term the exchange-rate disconnect puzzle. The latter category of riddles includes the Meese-Rogoff exchange-rate forecasting puzzle and the Baxter-Stockman neutrality-of-exchange-rate-regime puzzle. Here, although many elements need to be added to our extremely simple model, trade costs still play an essential role.",2000,101,2565,120,25,62,84,96,121,144,164,156,159,161
7b8f39926951a17d639936adb6276584d0c33561,""" …let me say that I hope we keep our voice clear and strong on the central task of raising the health of the poor. I can be 'realistic' and 'cynical' with the best of them—giving all the reasons why things are too hard to change. We must dream a bit, not beyond the feasible but to the limits of the feasible, so that we inspire. I think that we are an important voice speaking on behalf of the world's most voiceless people today—the sick and dying among the poorest of the poor. The stakes are high. Let's therefore speak boldly so that we can feel confident that we have fulfilled our task as well as possible. "" Taken by the editor from emailed correspondence from Jeffrey Sachs to the Commissioners and others working on this effort. The World Health Organization welcomes requests for permission to reproduce or translate its publications, in part or in full. Applications and enquiries should be addressed to the Office of Publications, World Health Organization, Geneva, Switzerland, which will be glad to provide the latest information on any changes made to the text, plans for new editions, and reprints and translations already available. The designations employed and the presentation of the material in this publication do not imply the expression of any opinion whatsoever on the part of the Secretariat of the World Health Organization concerning the legal status of any country, territory, city or area or of its authorities, or concerning the delimitation of its frontiers or boundaries. The mention of specific companies or of certain manufacturers' products does not imply that they are endorsed or recommended by the World Health Organization in preference to others of a similar nature that are not mentioned. Errors and omissions excepted, the names of proprietary products are distinguished by initial capital letters. This report contains the collective views of the Commission on Macroeconomics and Health and does not necessarily represent the decisions or the stated policies of the World Health Organization. The Commission on Macroeconomics and Health (CMH) was established by World Health Organization Director-General Gro Harlem Brundtland in January 2000 to assess the place of health in global economic development. Although health is widely understood to be both a central goal and an important outcome of development, the importance of investing in health to promote economic development and poverty reduction has been much less appreciated. We …",2002,131,2527,72,53,100,139,146,142,173,169,138,157,125
5fcf7ce74b7c145433c496b665f9e2e08a080cea,"There are some economic forces so powerful that they constantly break through all barriers erected for their suppression. Such, for example, are the forces of supply and demand which have resisted alike medieval efforts to abolish usury and contemporary attempts to control prices. In this paper I discuss what I believe to be another such mechanism which has colored the past and seems likely to stamp its character on the future. It helps us to understand the prospective roles of a wide variety of economic services: municipal government, education, the performing arts, restaurants, and leisure time activity. I will argue that inherent in the technological structure of each of these activities are forces working almost unavoidably for progressive and cumulative increases in the real costs incurred in supplying them. As a consequence, efforts to offset these cost increases, while they may succeed temporarily, in the long run are merely palliatives which can have no significant effect on the underlying trends. The justification of a macroeconomic model should reside primarily in its ability to provide insights into the workings of observed phenomena. Its aggregation of diverse variables usually deny it the elegance and the rigor that are provided by microeconomic analysis at its best. Yet macro models have succeeded in explaining the structure of practical problems and in offering guidance for policy to a degree that has so far eluded the more painstaking modes of economic analysis. This article hopes to follow in the tradition-the structure of its basic model is rudimentary. Yet it can perhaps shed some light on a variety of economic problems of our generation.",2007,1,1326,231,57,58,55,65,84,85,82,81,91,84
a87cda9bf8ab50d6442cc1a65f9e5a1f7fbd06f5,"We show that macroeconomic movements have strong effects on the happiness of nations. First, we find that there are clear microeconomic patterns in the psychological well-being levels of a quarter of a million randomly sampled Europeans and Americans from the 1970s to the 1990s. Happiness equations are monotonically increasing in income, and have similar structure in different countries. Second, movements in reported well-being are correlated with changes in macroeconomic variables such as gross domestic product. This holds true after controlling for the personal characteristics of respondents, country fixed effects, year dummies, and country-specific time trends. Third, the paper establishes that recessions create psychic losses that extend beyond the fall in GDP and rise in the number of people unemployed. These losses are large. Fourth, the welfare state appears to be a compensating force: higher unemployment benefits are associated with higher national well-being.",2003,109,1517,72,14,16,28,53,60,88,83,98,110,111
6d4dc0712fe632a76a3aa22acfe6ec11bb2b6d1a,,1967,2,2104,130,0,0,3,1,3,6,2,11,5,6
47df990aba674b467137734475d0f5dad9f45a4d,"Objective. The objective of the course is to introduce the modeling of heterogeneous agents economies, learn about economies with incomplete markets, and search and matching frictions. In the first part of the course, the student will familiarize with the modern quantitative techniques used in macroeconomics, and will be exposed to important research questions in macroeconomics. In addition, the course will review some of the numerical methods used to solve heterogeneous agents economies with incomplete markets.",2009,88,815,83,63,50,45,70,44,54,68,63,42,58
98ab6944117e3b294a1add68535f8894f9faf8be,"THE CONCEPT of a natural unemployment rate has been central to most modern models of inflation and stabilization. According to these models, inflation will accelerate or decelerate depending on whether unemployment is below or above the natural rate, while any existing rate of inflation will continue if unemployment is at the natural rate. The natural rate is thus the minimum, and only, sustainable rate of unemployment, but the inflation rate is left as a choice variable for policymakers. Since complete price stability has attractive features, many economists and policymakers who accept the natural rate hypothesis believe that central banks should target zero inflation. We question the standard version of the natural rate model and each of these implications. Central to our analysis is the effect of downward nominal wage rigidity in an economy in which individual firms experience stochastic shocks in the demand for their output. We embed these features in a model that otherwise resembles a standard natural rate model and show there is no unique natural unemployment rate. Rather, the rate of unemployment that is consistent with steady inflation",1996,40,1229,117,12,34,53,58,63,79,56,79,49,49
b67e0a23d051ffb97b9422fb7243a8121346b98c,,1993,0,1261,67,4,10,25,18,22,43,55,51,67,85
f7719330c6c23e49a2b6a3d8ebca89de97342dff,"Macroeconomics is evolving from the study of aggregate dynamics to the study of the dynamics of the entire equilibrium distribution of allocations across individual economic actors. This article reviews the quantitative macroeconomic literature that focuses on household heterogeneity, with a special emphasis on the ""standard"" incomplete markets model. We organize the vast literature according to three themes that are central to understanding how inequality matters for macroeconomics. First, what are the most important sources of individual risk and cross-sectional heterogeneity? Second, what are individuals' key channels of insurance? Third, how does idiosyncratic risk interact with aggregate risk?",2009,232,386,18,12,30,40,31,28,34,37,40,31,37
67b0ecd563a755005120fa7262a94dcf54e3879e,"Foreword. Chapter 1 What kind of recession has Japan been through? Chapter 2 Characteristics of balance sheet recessions that should be kept in mind during the recovery. Chapter 3 The Great Depression was also a balance sheet recession. Chapter 4 Monetary, foreign exchange, and fiscal policy during a balance sheet recession. Chapter 5 Yin and Yang economic cycles and the Holy Grail of macroeconomics. Chapter 6 Pressure of globalization. Chapter 7 Ongoing bubbles and balance sheet recessions. Chapter 8 World in Balance-Sheet Recession. Appendix: Thoughts on Walras and macroeconomics. Index.",2009,1,375,30,12,30,39,40,38,39,50,44,27,15
c712c6d2c8a1ddcd6452408d34391ed94531d111,"The linkage between economic activity and geography is obvious: Populations cluster mainly on coasts and rarely on ice sheets. Past studies of the relationships between economic activity and geography have been hampered by limited spatial data on economic activity. The present study introduces data on global economic activity, the G-Econ database, which measures economic activity for all large countries, measured at a 1 degree latitude by 1 degree longitude scale. The methodologies for the study are described. Three applications of the data are investigated. First, the puzzling ""climate-output reversal"" is detected, whereby the relationship between temperature and output is negative when measured on a per capita basis and strongly positive on a per area basis. Second, the database allows better resolution of the impact of geographic attributes on African poverty, finding geography is an important source of income differences relative to high-income regions. Finally, we use the G-Econ data to provide estimates of the economic impact of greenhouse warming, with larger estimates of warming damages than past studies.",2006,22,689,62,10,16,26,31,29,38,44,38,60,61
7c8aba4a42b374cf39c6fc14ec1270adda4a77ff,"I review research on the behavior of the labor wedge, the ratio between the marginal rate of substitution of consumption for leisure and the marginal product of labor. According to competitive, market-clearing macroeconomic models, the ratio is easy to measure and should be equal to the sum of consumption and labor taxes. The observation that the wedge is higher in continental Europe than in the United States has proved useful for understanding the extent to which taxes can explain differences in labor market outcomes. The observation that the ratio rises during recessions suggests some failure of competitive, market-clearing macroeconomic models at business cycle frequencies. The latter observation has guided recent research, including work on sticky wage models and job search models. (JEL E24, E32, J64)",2009,64,264,26,21,30,29,15,22,26,17,23,16,19
66b94ca46f30e2fd969232d8d847cd486328b5a0,"Macroeconomics changed between the early 1960s and the late 1970s. The macroeconomics of the early 1960s was avowedly Keynesian. This was manifested in the textbooks of the time, which showed a remarkable unity from the introductory through the graduate levels. John Maynard Keynes appeared, posthumously, on the cover of Time. Even Milton Friedman was famously—although perhaps misleadingly— quoted: “We are all Keynesians now.” A little more than a decade later Robert Lucas and Thomas Sargent (1979) had published “After Keynesian Macroeconomics.” The love-fest was over. The decline of the old-style Keynesian economics was due in part to the simultaneous rise in inflation and unemployment in the late 1960s and early 1970s. That occurrence was impossible to reconcile with the simple nonaccelerationist Phillips curves of the time. But Keynesian economics also declined because of a change in economic methodology. The Keynesians had emphasized the dependence of consumption on disposable income and, similarly, of investment on current profits and current cash flow. They posited a Phillips curve, where nominal—rather than real—wage inflation depended upon the unemployment rate, which was used as an indication of the looseness of the labor market. They based these functions on their own introspection regarding how the various actors in the economy would behave. They also brought some discipline into their judgments by estimating statistical relations. But a new school of thought, based on clas† Presidential Address delivered at the one hundred eighteenth meeting of the American Economic Association, January 6, 2007, Chicago, IL. * Department of Economics, University of California at Berkeley, 549 Evans Hall, Berkeley, CA 94720 (e-mail: akerlof@econ.berkeley.edu). This paper is based on a longterm research program with Rachel Kranton on the implications of identity for economic behavior. Our previous joint papers (Akerlof and Kranton 2000, 2002, 2005) have explored implications outside of macroeconomics of utility functions dependent on people’s notions of what ought to be. Some of this paper—especially Section III (“The Missing Motivation: Norms”) and Section IX (“Economic Methodology”)—has been directly taken from our joint manuscript: The Missing Motivation: Economics Made Human (Akerlof and Kranton 2006). I am especially grateful to Professor Kranton for extending to me the invitation to join this project, after she had the initial insight in the spring of 1996 that concerns regarding identity were missing from economic theory. I have also benefited from conversations with Robert Shiller, with whom I am coauthoring work on behavioral macroeconomics. In addition, I especially wish to thank Robert Akerlof and Janet Yellen for invaluable advice. I also want to thank Roland Benabou, Alan Blinder, Louis Christofides, Stephen Cosslett, Ernst Fehr, David Hirshleifer, Houston McCulloch, John Morgan, George Perry, Antonio Rangel, Paola Sapienza, Robert Solow, Dennis Snower, and Luigi Zingales, and seminar participants at the IMF, the World Bank, Ohio State University, Vanderbilt University, the University of California at Berkeley, the Munich Behavioral Economics Summer Camp, the 2006 Macroeconomics and Individual Decision Making Conference of the NBER and the Federal Reserve Bank of Boston, and at the Social Interactions, Identity, and Well-Being, and Institutions, Organizations, and Growth groups of the CIAR. I am also grateful to Marina Halac for invaluable research assistance and to the Canadian Institute for Advanced Research and to the National Science Foundation under Research Grant SES 04-17871 for invaluable financial support. 1 See, for example, Paul A. Samuelson (1964), Thomas F. Dernburg and Duncan M. McDougall (1967), and Gardner Ackley (1961). The econometric model of Lawrence R. Klein and Arthur S. Goldberger (1955) provides a useful synopsis of the variables that the early Keynesians thought most important for a macroeconomic model, and how they would be included. 2 Time, December 31, 1965. His appearance on the cover was especially remarkable because Time covers are rarely posthumous. Keynes had died in 1946. 3 But in a later disclaimer, Friedman said, almost surely correctly, that he had been quoted out of context. See http://www.libertyhaven.com/thinkers/miltonfriedman/ miltonexkeynesian.html, which quotes Friedman (1968), Dollars and Sense, 15. 4 The treatment of consumption in The General Theory, as we shall see below, was typical of such thinking. Keynes first discusses the dependence of consumption on current income, which he clearly sees as the primary determinant of current consumption; but, in addition, he makes a long list of other factors that will alter the relation between consumption and current income. 5 A good example of this methodology can be seen in Alban W. Phillips’s (1958) mixture of light theory and statistical analysis in his estimation of the relation between wage inflation and unemployment.",2007,245,481,36,31,38,46,46,31,37,24,34,26,26
50fe57989aff5e2e2c25f58955d5d24a716201f5,"While macroeconomics is often thought of as a deeply divided field, with less of a shared core and correspondingly less cumulative progress than other areas of economics, in fact, there are fewer fundamental disagreements among macroeconomists now than in past decades. This is due to important progress in resolving seemingly intractable debates. In this paper, I review some of those debates and outline important elements of the new synthesis in macroeconomic theory. I discusses the extent to which the new developments in theory and research methods are already affecting macroeconomic analysis in policy institutions. (JEL A11, E00)",2009,28,222,7,11,24,20,31,27,27,17,20,10,11
03ff3f018151bdfb1899ae60376b7902d32f7a3a,"Since the 1995 publication of Obsteld and Rogoff's Redux model, there has been an outpouring of research on open-economy dynamic general equilibrium models that incorporate imperfect competition and nominal rigidities. This paper offers an interim survey of this recent literature. © 2001 Elsevier Science B.V. All rights reserved.",2001,96,796,59,34,64,73,80,73,73,44,56,53,44
f254dfb0d9737bbf75d97d4a63ee4b2f5133e18e,"The recent financial crisis has put the spotlight on the rapid rise in credit which preceded it. In this paper, we provide an empirical and theoretical analysis of the credit boom and the macroeconomic context in which it developed. We find that the boom was unusually long and associated with neither particularly strong growth nor rising inflation in the economies in which it took place. We show that this type of credit and financial cycle is hard to reconcile with existing economic theory and argue that, while the ""global savings glut"" may account for the cycle's initial phase, other factors - such as the conduct of monetary policy and perceptions of declining macroeconomic risk - were more important from the mid-2000s onwards. We conclude by identifying some of the challenges now facing macroeconomics and policy.",2009,101,162,12,4,13,19,9,16,29,21,14,9,6
881dfaa5601d6fcf3c75542d6ce8a3b4220d9459,"This paper introduces, within the context of an infinite horizon optimal consumption problem, a parametric class of Kreps-Porteus nonexpected utility preferences—generalized isoelastic utility—which distinguishes attitudes toward risk from behavior toward intertemporal substitution. Some of the theoretical and empirical implications for macroeconomics of these state- and time-nonseparable preferences are examined.",1990,23,903,66,6,4,11,10,9,8,12,10,7,16
77bdc9272cee574b809b41d9e91bbcb0accb1a0f,"The balance of payments and the international investment position are designed to measure and present an economy’s external activity engaged with the rest of the world, such as flows of goods, services and capital during certain periods and the accumulated stocks of assets or liabilities at certain times. Analysis of the balance of payments, which extends national accounts for a closed economy to national accounts for an open economy, demonstrates a country’s international economic linkages with the rest of the world.",2009,0,196,22,6,6,6,8,10,9,19,12,19,24
e90ea03e84087480384e8e7fa3a65182cfd6e9c0,"A leading physicist delves into relativity and experimental applications Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity offers a Nobel laureate's perspectives on the wealth of data technological developments have brought to expand upon Einstein's theory. Unique in basing relativity on the Principle of Equivalence of Gravitation and Inertia over Riemannian geometry, this book explores relativity experiments and observational cosmology to provide a sound foundation upon which analyses can be made. Covering special and general relativity, tensor analysis, gravitation, curvature, and more, this book provides an engaging, insightful introduction to the forces that shape the universe.",1973,81,4113,456,1,6,3,6,6,10,11,10,10,10
0665b56ad2271286f2a71cfef5c6ab310ea4de50,,1980,0,6694,615,0,1,2,14,10,14,8,16,17,29
f51351e4b07ff1cd5ee4238663a06bb255802c8f,Manifold Theory. Tensors. Semi-Riemannian Manifolds. Semi-Riemannian Submanifolds. Riemannian and Lorenz Geometry. Special Relativity. Constructions. Symmetry and Constant Curvature. Isometries. Calculus of Variations. Homogeneous and Symmetric Spaces. General Relativity. Cosmology. Schwarzschild Geometry. Causality in Lorentz Manifolds. Fundamental Groups and Covering Manifolds. Lie Groups. Newtonian Gravitation.,1983,0,3284,256,0,0,1,2,6,4,7,12,11,11
daf937b2c82ad95e991596036b8d023d43eb4460,"Preface Notation Important formulae and physical constants 1. Introduction 2. Special relativity, non-inertial effects and electromagnetism 3. Differential geometry I: vectors, forms and absolute differentiation 4. Differential geometry II: geodesics and curvature 5. Einstein field equations, the Schwarzschild solution and experimental test of general relativity 6. Gravitomagnetic effects: gyroscopes and clocks 7. Gravitational collapse and black holes 8. Action principles, conservation laws and the Cauchy problem 9. Gravitational radiation 10. Cosmology 11. Gravitation and field theory References Index.",2009,115,77,12,3,3,7,5,3,7,3,5,7,12
673def1212cf6dc2d3026951e46a6904fb293117,"The status of experimental tests of general relativity and of theoretical frameworks for analyzing them is reviewed and updated. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of local Lorentz invariance and clock experiments. Ongoing tests of EEP and of the inverse square law are searching for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light deflection, the Shapiro time delay, the perihelion advance of Mercury, the Nordtvedt effect in lunar motion, and frame-dragging. Gravitational wave damping has been detected in an amount that agrees with general relativity to better than half a percent using the Hulse-Taylor binary pulsar, and a growing family of other binary pulsar systems is yielding new tests, especially of strong-field effects. Current and future tests of relativity will center on strong gravity and gravitational waves.",2001,671,1296,26,0,1,0,0,0,0,0,0,2,2
30f921ce45573aa7e7d0864f1cfe2abf36230cce,"This paper summarizes the author's recently published findings about differences in people's work-related values among 50 countries. In view of these differences, ethnocentric management theories (those based on the value system of one particular country) have become untenable. This concept is illustrated for the fields of leadership, organization, and motivation.",1983,5,3009,269,0,0,3,6,5,6,6,10,8,15
e4636b2555401d5079417aa80ffaf4490cb91ba3,"This article—summarizing the authors’ then novel formulation of General Relativity—appeared as Chap. 7, pp. 227–264, in Gravitation: an introduction to current research, L. Witten, ed. (Wiley, New York, 1962), now long out of print. Intentionally unretouched, this republication as Golden Oldie is intended to provide contemporary accessibility to the flavor of the original ideas. Some typographical corrections have been made: footnote and page numbering have changed–but not section nor equation numbering, etc. Current institutional affiliations are encoded in: arnowitt@physics.tamu.edu, deser@brandeis.edu, misner@umd.edu.",2004,14,1711,90,39,44,63,51,61,64,91,111,106,112
db55ed6b64d35509854623f6d57ca3529bc57c85,1. Special Relativity and Flat Spacetime. 2. Manifolds. 3. Curvature. 4. Gravitation. 5. The Schwarzchild Solution. 6. More General Black Holes. 7. Perturbation Theory and Gravitational Radiation. 8. Cosmology. 9. Quantum Field Theory in Curved Spacetime. 10. Appendicies.,2003,0,1508,173,2,8,15,25,34,60,64,73,83,91
145f08ebcfbb1a5417d77fa7a5c7494bdcf1a95f,"List of contributors Preface 1. An introductory survey S. W. Hawking and W. Israel 2. The confrontation between gravitation theory and experiment C. M. Will 3. Gravitational-radiation experiments D. H. Douglass and V. B. Braginsky 4. The initial value problem and the dynamical formulation of general relativity A. E. Fischer and J. E. Marsden 5. Global structure of spacetimes R. Geroch and G. T. Horowitz 6. The general theory of the mechanical, electromagnetic and thermodynamic properties of black holes B. Carter 7. An introduction to the theory of the Kerr metric and its peturbations S. Chandrasekhar 8. Black hole astrophysics R. D. Blandford and K. S. Thorne 9. The big bang cosmology - enigmas and nostrums R. H. Dicke and P. J. E. Peebles 10. Cosmology and the early universe Ya B. Zel'dovitch 11. Anisotropic and inhomogeneous relativistic cosmologies M. A. H. MacCallum 12. Singularities and time-asymmetry R. Penrose 13. Quantum field theory in curved spacetime G. W. Gibbons 14. Quantum gravity: the new synthesis B. S. DeWitt 15. The path-integral approach to quantum gravity S. W. Hawking 16. Ultraviolet divergences in quantum theories of gravitation S. Weinberg References Index.",1979,0,2119,84,4,9,23,22,26,33,32,26,19,19
78037360d244887bce8e9dd16d451744d91dd03e,,1960,0,2053,171,0,3,4,11,8,13,11,8,25,16
4b58b298114ee0a77b471ee2268072ee63e87c3b,,1969,0,2219,59,1,3,2,3,5,5,9,4,4,9
ef8395964a6b17068d49b6c71344e30ae0c019c1,"According to general relativity, photons are deflected and delayed by the curvature of space-time produced by any mass. The bending and delay are proportional to γ + 1, where the parameter γ is unity in general relativity but zero in the newtonian model of gravity. The quantity γ - 1 measures the degree to which gravity is not a purely geometric effect and is affected by other fields; such fields may have strongly influenced the early Universe, but would have now weakened so as to produce tiny—but still detectable—effects. Several experiments have confirmed to an accuracy of ∼0.1% the predictions for the deflection and delay of photons produced by the Sun. Here we report a measurement of the frequency shift of radio photons to and from the Cassini spacecraft as they passed near the Sun. Our result, γ = 1 + (2.1 ± 2.3) × 10-5, agrees with the predictions of standard general relativity with a sensitivity that approaches the level at which, theoretically, deviations are expected in some cosmological models.",2003,22,1361,89,6,35,59,69,57,56,89,73,78,74
f883cb14548fd2a87b9853628c0a8d08baae982f,"This is an introduction to the by now fifteen years old research field of canonical quantum general relativity, sometimes called ""loop quantum gravity"". The term ""modern"" in the title refers to the fact that the quantum theory is based on formulating classical general relativity as a theory of connections rather than metrics as compared to in original version due to Arnowitt, Deser and Misner. Canonical quantum general relativity is an attempt to define a mathematically rigorous, non-perturbative, background independent theory of Lorentzian quantum gravity in four spacetime dimensions in the continuum. The approach is minimal in that one simply analyzes the logical consequences of combining the principles of general relativity with the principles of quantum mechanics. The requirement to preserve background independence has lead to new, fascinating mathematical structures which one does not see in perturbative approaches, e.g. a fundamental discreteness of spacetime seems to be a prediction of the theory providing a first substantial evidence for a theory in which the gravitational field acts as a natural UV cut-off. An effort has been made to provide a self-contained exposition of a restricted amount of material at the appropriate level of rigour which at the same time is accessible to graduate students with only basic knowledge of general relativity and quantum field theory on Minkowski space.",2007,3,1161,158,19,44,64,89,115,100,98,93,83,89
287dd15fd2862b28db56f81b91a94b82c09cdaad,"AbstractThe status of experimental tests of general relativity and of theoretical frameworks for analysing them are reviewed. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of special relativity, and the gravitational redshift experiment. Future tests of EEP and of the inverse square law will search for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light defl
ection the Shapiro time delay, the perihelion advance of Mercury, and the Nordtvedt effect in lunar motion. Gravitational wave damping has been detected in an amount that agrees with general relativity to half a percent using the Hulse-Taylor binary pulsar, and new binary pulsar systems may yield further improvements. When direct observation of gravitational radiation from astrophysical sources begins, new tests of general relativity will be possible.",2001,346,1196,128,9,7,13,9,15,22,29,41,46,46
61225179ce90adce138d842ada42c2c18ce621f4,"Rapid interstellar travel by means of spacetime wormholes is described in a way that is useful for teaching elementary general relativity. The description touches base with Carl Sagan’s novel Contact, which, unlike most science fiction novels, treats such travel in a manner that accords with the best 1986 knowledge of the laws of physics. Many objections are given against the use of black holes or Schwarzschild wormholes for rapid interstellar travel. A new class of solutions of the Einstein field equations is presented, which describe wormholes that, in principle, could be traversed by human beings. It is essential in these solutions that the wormhole possess a throat at which there is no horizon; and this property, together with the Einstein field equations, places an extreme constraint on the material that generates the wormhole’s spacetime curvature: In the wormhole’s throat that material must possess a radial tension τ0 with the enormous magnitude τ0∼ (pressure at the center of the most massive of ne...",1988,0,1475,100,1,2,6,7,6,6,8,6,6,17
383eedafaec868d6168b13a956c7fc41d8bf2087,"A generalization of Einstein's gravitational theory is discussed in which the spin of matter as well as its mass plays a dynamical role. The spin of matter couples to a non-Riemannian structure in space-time, Cartan's torsion tensor. The theory which emerges from taking this coupling into account, the ${U}_{4}$ theory of gravitation, predicts, in addition to the usual infinite-range gravitational interaction medicated by the metric field, a new, very weak, spin contact interaction of gravitational origin. We summarize here all the available theoretical evidence that argues for admitting spin and torsion into a relativistic gravitational theory. Not least among this evidence is the demonstration that the ${U}_{4}$ theory arises as a local gauge theory for the Poincar\'e group in space-time. The deviations of the ${U}_{4}$ theory from standard general relativity are estimated, and the prospects for further theoretical development are assessed.",1976,146,1860,34,3,12,27,20,46,28,21,35,28,35
bf387e0f57f6f3b878ae09fed558a21b57ff35cb,"SummaryAn approach to shock waves, boundary surfaces and thin shells in general relativity is developed in which their histories are characterized in a purely geometrical way by the extrinsic curvatures of their imbeddings in space-time. There is some gain in simplicity and ease of application over previous treatments in that no mention of « admissible » or, indeed, any space-time co-ordinates is needed. The formalism is applied to a study of the dynamics of thin shells of dust.RiassuntoSi sviluppa un’approssimazione alle onde d’urto, superfici di contorno e strati sottili in relatività generale in cui le loro storie sono caratterizzate in modo puramente geometrico dalle curvature estrinseche delle loro giaciture nello spazio-tempo. Si ha un qualche guadagno in semplicità e facilità di applicazione rispetto alle trattazioni precedenti in quanto non è necessaria alcuna menzione di « ammissibilità » o, nella fattispecie di alcuna coordinata spazio-temporale. Si applica questo formalismo allo studio della dinamica degli strati sottili di polveri.",1966,3,1640,42,0,1,4,2,1,0,2,0,4,3
9284029d7c39a83d285d8a7cb859c5715228e1f3,,1984,0,1418,77,1,4,5,5,7,4,5,5,13,8
3561dcbcfcd9fe7b58a926d1da8515b1c88bb72c,Introduction 1. Elementary principles 2. The tensor calculus 3. The law of gravitation 4. Relativity mechanics 5. Curvature of space and time 6. Electricity 7. World geometry Supplementary notes Bibliography Index.,1924,0,1206,71,4,1,1,2,1,3,0,0,1,3
927285079badad9009fb579f1539cca0db4edba4,"In 1921, five years after the appearance of his comprehensive paper on general relativity and twelve years before he left Europe permanently to join the Institute for Advanced Study, Albert Einstein visited Princeton University, where he delivered the Stafford Little Lectures for that year. These four lectures constituted an overview of his then-controversial theory of relativity. Princeton University Press made the lectures available under the title ""The Meaning of Relativity,"" the first book by Einstein to be produced by an American publisher. As subsequent editions were brought out by the Press, Einstein included new material amplifying the theory. A revised version of the appendix ""Relativistic Theory of the Non-Symmetric Field,"" added to the posthumous edition of 1956, was Einstein's last scientific paper.",1946,0,1650,102,0,0,0,1,1,1,0,1,3,2
07ecafbabe5cb83c52725d554f8c42583f2bebfb,Research data on dominant work-related values patterns in 53 countries and regions are used to suggest how definitions of the quality of life are affected by national culture patterns.,1984,9,1241,102,0,1,5,2,2,6,4,4,6,8
5d9aacb0eea230539e9ea476e87c42e2fcf031e6,"FOREWORD ACKNOWLEDGEMENTS 1. Lorentzian Geometry 2. Special Relativity 3. General Relativity and the Einstein Equations 4. Schwarzschild Space-time and Black Holes 5. Cosmology 6. Local Cauchy Problem 7. Constraints 8. Other Hyperbolic-Elliptic systems 9. Relativistic Fluids 10. Kinetic Theory 11. Progressive Waves 12. Global Hyperbolicity and Causality 13. Singularities 14. Stationary Space-times and Black Holes 15. Global Existence Theorems, Asymptotically Euclidean Data 16. Global existence theorems, cosmological case APPENDICES I. Sobolev Spaces II. Elliptic Systems III. Second Order Quasidiagonal Systems IV. General Hyperbolic Systems V. Cauchy Kovalevski and Fuchs theorems VI. Conformal Methods VII. Kaluza Klein Formulas",2009,0,413,64,9,25,24,24,34,37,48,41,48,27
eef500f59b5d007955e2cf24ad797c8f8116845c,"This paper is divided into four parts. In part A, some general considerations about gravitational radiation are followed by a treatment of the scalar wave equation in the manner later to be applied to Einstein’s field equations. In part B, a co-ordinate system is specified which is suitable for investigation of outgoing gravitational waves from an isolated axi-symmetric reflexion-symmetric system . The metric is expanded in negative powers of a suitably defined radial co-ordinate r, and the vacuum field equations are investigated in detail. It is shown that the flow of information to infinity is controlled by a single function of two variables called the news function. Together with initial conditions specified on a light cone, this function fully defines the behaviour of the system . No constraints of any kind are encountered. In part C, the transformations leaving the metric in the chosen form are determined. An investigation of the corresponding transformations in Minkowski space suggests that no generality is lost by assuming that the transformations, like the metric, may be expanded in negative powers of r. In part D, the mass of the system is defined in a way which in static metrics agrees with the usual definition. The principal result of the paper is then deduced, namely, that the mass of a system is constant if and only if there is no news; if there is news, the mass decreases monotonically so long as it continues. The linear approximation is next discussed, chiefly for its heuristic value, and employed in the analysis of a receiver for gravitational waves. Sandwich waves are constructed, and certain non-radiative but non-static solutions are discussed. This part concludes with a tentative classification of time-dependent solutions of the types considered.",1962,6,1460,50,0,4,1,6,6,3,12,3,8,6
3c5c9c3cf20e888d30673b8b73cbedf9dc0906db,"I show that it is possible to formulate the Relativity postulates in a way that does not lead to inconsistencies in the case of spacetimes whose short-distance structure is governed by an observer-independent length scale. The consistency of these postulates proves incorrect the expectation that modifications of the rules of kinematics involving the Planck length would necessarily require the introduction of a preferred class of inertial observers. In particular, it is possible for every inertial observer to agree on physical laws supporting deformed dispersion relations of the type E2-c2 p2-c4m2 + f(E, p, m; Lp) =0, at least for certain types of f.",2000,51,910,23,0,5,32,39,39,46,38,39,43,49
66cabbe9e53bc56a695f65e9d3efbe0b91ce756c,"From the Publisher: 
This thoroughly revised edition presents important methods in the quantitative analysis of geologic data. Retains the basic arrangement of the previous edition but expands sections on probability, nonparametric statistics, and Fourier analysis. Contains revised coverage of eigenvalues and eigenvectors, and new coverage of data analysis methods, such as the semivariogram and the process of kriging.",1988,0,5458,198,79,79,81,91,113,95,86,102,115,112
1eb9f88bcf886525b4d46a6eb3f33ea34749e40e,"The Roots of Isotope Geology. The Internal Structure of Atoms. Decay Mechanisms of Radioactive Atoms. Radioactive Decay and Growth. Mass Spectrometry. The K-Ar Method of Dating. The 40 Ar/39 Ar Method of Dating. The Rb-Sr Method of Dating. Two-Component Mixtures. Isotope Geology of Strontium in Meteorites and Igneous Rocks. Isotope Geology of Strontium in Sedimentary Rocks. The Sm-Nd Method of Dating. Isotope Geology of Neodymium and Strontium in Igneous Rocks. Isotope Geology of Neodymium in Sedimentary Rocks. The Lu-Hf Method of Dating. The RE-Os Method of Dating. The Re-Os Method of Dating. The K-Ca Method of Dating. The U, Th-Pb Methods of Dating. The Isotope Geology of Lead. The Fission-Track and Other Radiation Damage Methods of Dating. The U-Series Disequilibrium Methods of Dating. Cosmogenic Radionuclides. Cosmogenic Carbon-14 and Tritium. Carbon. Sulfur.",1977,0,3004,198,0,1,13,14,22,11,15,18,18,22
09cd71b29d32e0f891fdd6ea022fddb2deb98b4d,"For the past three centuries, the effects of humans on the global environment have escalated. Because of these anthro-pogenic emissions of carbon dioxide, global climate may depart significantly from natural behaviour for many millennia to come. It seems appropriate to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene—the warm period of the past 10–12 millennia.",2002,1,2871,150,3,4,12,10,21,24,26,37,53,90
c0f22604d7137aabe1d6d0f03499dbbcd0635260,"The development of petroleum geochemistry and geology carbon and origin of life petroleum and its products how oil forms - natural hydrocarbons how oil forms - generated hydrocarbons modeling petroleum generation the origin of natural gas migration and accumulation abnormal pressures the source rock coals, shales, and other terrestrial source rocks petroleum in the reservoir seeps and surface prospecting a geochemical program for petroleum exploration crude oil correlation prospect evaluation.",1995,0,2405,208,41,18,35,45,48,39,41,53,56,52
1e9dbb399fcc002868fe2c7564ce58e905ce50b1,"This book introduces the fundamental concepts of fractal geometry and chaotic dynamics. These concepts are then related to a variety of geological and geophysical problems, illustrating just what chaos theory and fractals really tell us and how they can be applied to the earth sciences. Petroleum and mineral reserves, earthquakes, mantle convection and magnetic field generation are among the earth's properties that come under scrutiny. This is the first book that covers these topics at an accessible level; the concepts are introduced at the lowest possible level of mathematics and are consistently understandable, so that the reader requires only a background in basic physics and mathematics. Problems are also included for the reader to solve.",1992,0,2455,160,2,9,30,41,49,42,59,75,76,74
d01edba298a2c058ae150c416ed569e4a097388a,"Keywords: Geologie ; Analyse des contraintes ; Fissuration Reference Record created on 2004-09-07, modified on 2016-08-08",1987,0,2137,95,29,30,27,28,25,28,37,37,61,46
9c7af96eb2dd71d8525161c026847f386b9e01b9,1 Introduction.- 2 Historical Background.- 3 Concepts of Scale.- 4 Methods of Architectural-Element Analysis.- 5 Lithofacies.- 6 Architectural Elements Formed Within Channels.- 7 Architectural Elements of the Overbank Environment.- 8 Fluvial Styles and Facies Models.- 9 The Stratigraphic Architecture of Fluvial Depositional Systems.- 10 Fluvial Depositional Systems and Autogenic Sedimentary Controls.- 11 Tectonic Control of Fluvial Sedimentation.- 12 What Does Fluvial Lithofacies Reveal About Climate?.- 13 Sequence Stratigraphy.- 14 Stratigraphic and Tectonic Controls on the Distribution and Architecture of Fluvial Oil and Gas Reservoirs.- 15 Case Studies of Oil and Gas Fields in Fluvial Reservoirs.- 16 Future Research Trends.- References.- Author Index.,1996,3,1547,178,0,4,10,8,14,20,36,30,31,41
c561af7a53d3511e7230c37977bbfba4277b44fc,,1976,0,1824,266,1,3,2,4,4,4,6,11,8,5
9fce4527c064d044d982c9a37406ab8d658fe490,"The first € price and the £ and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the €(D) includes 7% for Germany, the €(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. A. Miall The Geology of Fluvial Deposits",2006,0,1129,101,52,58,42,49,61,46,50,83,72,83
8bca2f026bb8bf225405b01dee99a614d3c578ad,GEOLOGY - IGNEOUS AND METAMORPHIC ROCKS.- The Basement Complex.- The Younger Granites.- GEOLOGY - SEDIMENTARY BASINS.- Cretaceous - Cenozoic Magmatism and Volcanism.- The Benue Trough.- The Bornu Basin (Nigerian Sector of the Chad Basin).- The Sokoto Basin (Nigerian Sector of the Iullemmeden Basin).- The Mid-Niger (Bida) Basin.- The Dahomey Basin.- MINERAL RESOURCES.- The Niger Delta Basin.- Solid Mineral Resources.- Petroleum Resources.- Policy Issues and Development Options.,2009,0,532,71,0,1,5,22,43,40,35,35,51,52
fadf14179f827dc98da5c9752cd1c944b6218d2d,"Coral reefs are generally associated with shallow tropical seas; however, recent deep-ocean exploration using advanced acoustics and submersibles has revealed unexpectedly widespread and diverse coral ecosystems in deep waters on continental shelves, slopes, seamounts, and ridge systems around the world. Advances reviewed here include the use of corals as paleoclimatic archives and their biogeological functioning, biodiversity, and biogeography. Threats to these fragile, long-lived, and rich ecosystems are mounting: The impacts of deep-water trawling are already widespread, and effects of ocean acidification are potentially devastating.",2006,31,841,63,10,20,48,58,50,59,61,63,70,40
28f060d89e0e81a8e20e66f96e7dd017658eb54c,"This third edition of the Glossary of Geology contains approximately 37,000 terms, or 1,000 more than the second edition. New entries are especially numerous in the fields of carbonate sedimentology, hydrogeology, marine geology, mineralogy, ore deposits, plate tectonics, snow and ice, and stratigraphic nomenclature. Many of the definitions provide background information.",1987,0,1252,19,5,19,16,22,19,22,24,24,30,26
6d70584bdd93f94dcd294d168834cfb99ec56976,"Seismic anisotropy beneath continents is analyzed from shear-wave splitting recorded at more than 300 continental seismic stations. Anisotropy is found to be a ubiquitous property that is due to mantle deformation from past and present orogenic activity. The observed coherence with crustal deformation implies that the mantle plays a major, if not dominant, role in orogenies. No evidence is found for a continental asthenospheric decoupling zone, suggesting that continents are coupled to general mantle circulation.",1996,122,1018,150,7,12,29,28,31,26,56,34,80,25
42cdd4d3e2dbb242a61ec38dbec9a79441d75ad4,"Scholars from Egypt, Germany and the US review and analyze the results of work carried out on the geology of Egypt: geomorphology and evolution of landscape, tectonics, geophysical regime, volcanicity, Precambrian geology, geologic history and paleogeography, paleontology of selected taxa, ore depos",1962,0,1526,16,0,0,3,5,4,3,6,4,7,3
fce492c7366a7cef2b0462352081ea94bb632cfb,"Acknowledgements 1. Seabed fluid flow introduction 2. Pockmarks, shallow gas and seeps: an initial appraisal 3. Seabed fluid flow around the world 4. The contexts of seabed fluid flow 5. The nature and origins of flowing fluids 6. Shallow gas and gas hydrates 7. Migration and seabed features 8. Seabed fluid flow and biology 9. Seabed fluid flow and mineral precipitation 10. Impacts on the hydrosphere and atmosphere 11. Implications for man References Index.",2007,0,685,96,6,15,30,51,39,59,31,52,55,58
22e7d4e5da07e26f127d0f30b3c1ff4f8ac2d8db,,1964,0,1360,90,0,0,0,2,1,5,4,9,9,10
e1cdf720c38fd8697a6a65b311d6cbed24a9805a,"STUDENTS of Geology will welcome this third and much enlarged edition of Prof. Green's excellent text-book, though they may at first sight regret the exchange of the old convenient manual form of the book for that of the present handsome and well-printed octavo. One of the first features that strikes the reader in this new issue of the work is the large augmentations made to the lithological sections. In fact this part of the treatise may be said to have been re-cast and almost wholly re-written. The author devotes 150 closely printed pages to crystallography and the description of minerals. It may be open to question whether the full details which he gives to the crystallographic characters of minerals are not rather out of place in a geological treatise. They are not ample enough for the mineralogical student, and the geologist who takes up the subject must necessarily study text-books of mineralogy, where they are given at much greater length. Prof. Green, however, has put them so clearly and succinctly that this portion of his book cannot fail to be of use.Geology.By A. H. Green. Part I. Physical Geology. Third and Enlarged Edition. (London: Rivingtons, 1882.)",1882,9,1099,67,0,0,0,0,0,0,0,0,0,0
d2c099523c52acf06c6ec30d917a8af1f59829db,"Sediment flux to the coastal zone is conditioned by geomorphic and tectonic influences (basin area and relief), geography (temperature, runoff), geology (lithology, ice cover), and human activities (reservoir trapping, soil erosion). A new model, termed “BQART” in recognition of those factors, accounts for these varied influences. When applied to a database of 488 rivers, the BQART model showed no ensemble over‐ or underprediction, had a bias of just 3% across six orders of magnitude in observational values, and accounted for 96% of the between‐river variation in the long‐term (±30 years) sediment load or yield of these rivers. The geographical range of the 488 rivers covers 63% of the global land surface and is highly representative of global geology, climate, and socioeconomic conditions. Based strictly on geological parameters (basin area, relief, lithology, ice erosion), 65% of the between‐river sediment load is explained. Climatic factors (precipitation and temperature) account for an additional 14% of the variability in global patterns in load. Anthropogenic factors account for an additional 16% of the between‐river loads, although with ever more dams being constructed or decommissioned and socioeconomic conditions and infrastructure in flux, this contribution is temporally variable. The glacial factor currently contributes only 1% of the signal represented by our globally distributed database, but it would be much more important during and just after major glaciations. The BQART model makes possible the quantification of the influencing factors (e.g., climate, basin area, ice cover) within individual basins, to better interpret the terrestrial signal in marine sedimentary records. The BQART model predicts the long‐term flux of sediment delivered by rivers; it does not predict the episodicity (e.g., typhoons, earthquakes) of this delivery.",2007,110,686,51,11,20,29,24,31,35,46,36,42,47
4fb004b0c5fe581602d89028d79ed87e51b96609,,1990,39,1139,106,0,1,6,13,22,33,20,25,26,25
5ea4661aa22572e2467ff3853449431923ec66d4,"I have deemed it advisable to preface this paper with a brief notice of the geographical position, extent, and more prominent physical features of Egypt, remarking at the same time that although much still remains to be done in this most interesting, and comparatively unexplored field for the geologist, yet it may not be wholly useless to lay before the Society a summary of what has already been effected.",1848,0,818,119,0,0,0,0,0,0,0,0,0,0
a2e480753323ae9df59cdbf5627fde339fe806ba,"In the Amazon Basin, substrate lithology and erosional regime (seen in terms of transport-limited and weathering-limited denudation) exert the most fundamental control on the chemistry of surface waters within a catchment. Secondary effects, such as the precipitation of salts within soils and in stream beds, biological uptake and release, and cyclic salt inputs, are more difficult to discern. Samples can be separated into four principal groupings based on relationships between total cation charge (TZ+) and geology. (1) Rivers with 0 3000 μeq/l drain massive evaporites. These rivers are rich in Na and Cl. In the third and fourth categories, rivers tend to have 1:1 (equivalent) ratios of Na:Cl and (Ca+Mg):(alkalinity+SO4), caused primarily by the weathering of carbonates and evaporites. Supplement available with entire article on microfiche. Order from the American Geophysical Union, 2000 Florida Avenue, N.W., Washington, DC 20009. Document C83-002; $2.50. Payment must accompany order.",1983,35,1125,77,0,2,4,4,9,13,9,14,5,12
6ce757bd7d3992f2ff3c9d90915b9ec47b19f0bb,"Does productivity increase with density? We revisit the issue usingFrench wage and TFP data. To deal with the ‘endogenous quantity of labour' bias (i.e., urban agglomeration is consequence of high local productivity rather than a cause), we take an instrumental variable approach and introduce a new set of geological instruments in addition to standard historical instruments. To dealwith the ‘endogenous quality of labour' bias (i.e., cities attract skilled workers so that the effects of skills and urban agglomeration are confounded), we take a worker fixed-effect approach with wage data. We find modest evidence about theendogenous quantity of labour bias and both sets of instruments give a similar answer. We find that the endogenous quality of labour bias is quantitatively more important.",2008,49,391,57,11,16,27,21,28,31,28,37,33,39
5a7b0f16bfb3acfeccca4c4f8c3aead1a9caf903,"The relationship between in-situ stress and fluid flow in fractured and faulted rock is examined by using data from detailed analyses of stress orientation and magnitude, fracture geometry, and precision temperature logs that indicate localized fluid flow. Data obtained from three boreholes that penetrate highly fractured and faulted crystalline rocks indicate that potentially active faults appear to be the most important hydraulic conduits in situ. The data indicate that the permeability of critically stressed faults is much higher than that of faults that are not optimally oriented for failure in the current stress field.",1995,2,876,37,0,3,11,4,13,14,13,23,29,28
579dbce2fe9decd8ad070af0e4baa209b2fc8039,"Terrestrial laser scanning, or lidar, is a recent innovation in spatial information data acquisition, which allows geological outcrops to be digitally captured with unprecedented resolution and accuracy. With point precisions and spacing of the order of a few centimetres, an enhanced quantitative element can now be added to geological fieldwork and analysis, opening up new lines of investigation at a variety of scales in all areas of field-based geology. Integration with metric imagery allows 3D photorealistic models to be created for interpretation, visualization and education. However, gaining meaningful results from lidar scans requires more than simply acquiring raw point data. Surveys require planning and, typically, a large amount of post-processing time. The contribution of this paper is to provide a more detailed insight into the technology, data collection and utilization techniques than is currently available. The paper focuses on the workflow for using lidar data, from the choice of field area and survey planning, to acquiring and processing data and, finally, extracting geologically useful data. Because manufacturer specifications for point precision are often optimistic when applied to real-world outcrops, the error sources associated with lidar data, and the implications of them propagating through the processing chain, are also discussed.",2008,35,373,19,5,12,26,20,30,36,28,43,45,24
901baa6ffd86f44fb55731242356c117fb6a801b,"One of the key works in the nineteenth-century battle between science and Scripture, Charles Lyell's Principles of Geology (1830-33) sought to explain the geological state of the modern Earth by considering the long-term effects of observable natural phenomena. Written with clarity and a dazzling intellectual passion, it is both a seminal work of modern geology and a compelling precursor to Darwinism, exploring the evidence for radical changes in climate and geography across the ages and speculating on the progressive development of life. A profound influence on Darwin, Principles of Geology also captured the imagination of contemporaries such as Melville, Emerson, Tennyson and George Eliot, transforming science with its depiction of the powerful forces that shape the natural world.",1969,1,695,0,1,7,0,1,3,4,2,9,4,8
e23873d41b9d09bf75b059d27c46943c9a662244,"THE twenty-third volume of the Memoirs of the Geological Survey of India, consisting of some 250 pages, is wholly taken up by an account of the geology of the Central Himalayas, by the Superintendent of the Survey, Mr. C. L. Griesbach, C.I.E. The carefully written text is illustrated by some of the most exquisite and instructive photographs of synclinals, folded beds, faults, glaciers, &c., which have ever been produced, to say nothing of the numerous maps and sections.",1892,0,256,35,0,0,0,0,0,0,0,0,0,0
ff1be9bad4448c3f6d7a1de7ae098d4f7d5b2b9c,"Abstract Amber (‘Burmite’) from the Hukawng Valley of Myanmar has been known since at least the 1st century AD. It is currently being produced from a hill known as Noije Bum, which was first documented as a source of amber in 1836. Several geologists visited the locality between 1892 and 1930. All of them believed that the host rocks to the amber are Tertiary (most said Eocene) in age, and this conclusion has been widely quoted in the literature. However, recent work indicates a Cretaceous age. Insect inclusions in amber are considered to be Turonian–Cenomanian, and a specimen of the ammonite Mortoniceras (of Middle-Upper Albian age) was discovered during the authors' visit. Palynomorphs in samples collected by the authors suggest that the amber-bearing horizon is Upper Albian to Lower Cenomanian. The preponderance of the evidence suggests that both rocks and amber are most probably Upper Albian. This determination is significant for the study of insect evolution, indicating that the oldest known definitive ants have been identified in this amber [American Museum Novitates 3361 (2002) 72]. This site occurs within the Hukawng Basin, which is comprised of folded sedimentary (±volcanic) rocks of Cretaceous and Cenozoic age. The mine exposes a variety of clastic sedimentary rocks, with thin limestone beds, and abundant carbonaceous material. The sediments were deposited in a nearshore marine environment, such as a bay or estuary. Amber is found in a fine clastic facies, principally as disk shaped clasts, oriented parallel to bedding. A minority occurs as runnels (stalactite shaped), with concentric layering caused by recurring flows of resin. An Upper Albian age is similar to that of Orbitolina limestones known from a number of locations in northern Myanmar. One of these, at Nam Sakhaw, 90 km SW of Noije Bum, has also been a source of amber.",2003,49,629,54,4,23,9,5,12,10,18,9,15,12
d8e5ce8d0e3a71d9677b5f6f38dc4695c9518715,"Magnetic anisotropy in sedimentary rocks is controlled by the processes of deposition and compaction, in volcanic rocks by the lava flow and in metamorphic and plutonic rocks by ductile deformation and mimetic crystallization. In massive ore it is due to processes associated with emplacement and consolidation of an ore body as well as to ductile deformation. Hence, it can be used as a tool of structural analysis for almost all rock types. Morcover, it can influence considerably the orientation of the remanent magnetization vector as well as the configuration of a magnetic anomaly over a magnetized body. For these reasons it should be investigated in palaeomagnetism and applied geophysics as well.",1982,68,995,56,0,7,5,9,6,15,20,11,15,20
f39abb87b261948e79888097fc2a1f92df1217ea,"THE second volume of Baron Ferdinand von Richthofen's great work on China has just appeared. Five years have elapsed since the publication of the first volume, and two additional volumes are promised to complete the work, which when its maps and full index have been supplied, will be a great storehouse of observations in almost every department of Geology. Few geologists have enjoyed such opportunities of extended travel as have fallen to the Baron's lot. Already familiar with the rocks of a large part of Central Europe, he carried his knowledge and experience to the far west of North America, and did admirable service there as a pioneer to those who have Jcome after him. Subsequently he set himself to explore the geological structure of the Chinese Empire, and he is now laboriously collecting and arranging the vast materials which he amassed in his wanderings through the almost unknown geological formations of that wide region.China: Ergebnisse eigener Reisen und darauf gegründeter Studien.Von F. Freiherrn von Richthofen. Zweiter Band. (Berlin: Reimer, 1882.)",1882,0,87,10,0,0,0,0,0,0,0,0,0,0
61830442b82ed21add6de4dfa768a94b7934c145,"IN commemoration of their jubilee, which took place on December 17, 1908, the council of the Geologists' Association decided to bring out a volume dealing with the geology of those parts of England and Wales which have been visited by the Association during the course of its excursions. The volume, which promises to attain a much larger size than was expected, is to be issued in four parts, the first of which is now before us. It is a well-printed work of 209 pages, with four plates and thirty-four text-illustrations; and it deals with the district north of the Thames from Oxfordshire to Bedfordshire and the eastern counties. It comprises seven articles, with the following titles: (1) Middlesex and Hertfordshire, by Mr. J. Hopkinson; (2) Essex, by Mr. T. V. Holmes; (3) The Pliocene Deposits of the Eastern Counties, by Mr. F. W. Harmer; (4) The Pleistocene Period in the Eastern Counties, by Mr. Harmer; (5) Cambridgeshire, Bed fordshire, and West Norfolk, by Mr. R. H. Rastall; (6) Buckinghamshire, by Dr. A. Morley Davies; and (7) The Oxford and Banbury District, by Mr. J. A. Douglas.Geology in the Field.The Jubilee Volume of the Geologists' Association (1858–1908). Edited by H. W. Monckton R. S. Herries. Part i. Pp. iv + 209. (London: Edward Stanford, 1909.) Price 5s. net.",1985,0,188,15,1,0,1,0,0,2,1,0,3,1
6e91ab3a6dd575032a603497b8b5933057faf018,"FOLLOWING the example of Mr. Chapman's Australian fossils—an outline of palaeonto logy based on Australian examples for Australian students—Mr. Howchin, of the University of Adelaide, has prepared a general text-book of geology based on Australian illustrations, followed by an account of the geology of South Australia, with shorter summaries of that of the other Australian States. The book should be very useful, as it fills a gap in Australian educational literature, while it supplies geologists in general with,an excellent and up-to-date compendium of the geo logy of South Australia. Mr. Howchin is exceptionally qualified for the work; he is well known for his discovery of the Australian Cambrian glacial deposits, his researches on fossil foraminifera, and his text-book, on the geography of South Australia. The first division of the work gives a clear summary of the general outlines of geology; it is especially good in the physiographic portions. The petrology. As comparatively ele mentary, since the book, being published by the South Australian Education Department, is probably intended more for secondary schools than for university students. Aus tralian petrologists may consider that there is inadequate notice of the alkaline igneous rocks and in an effort at simplification “pyroxene (augite)” is included in the hornblende group, a step which would lead students to overlook the important distinction between the pyroxenes and othe amphiboles. The parallelism of these series is also not indicated in the statement as to the com position of augite. There is not much informa tion about economic geology; for example, the author tells us nothing about the oil-fields of South Australia and their prospects. He follows those who extend the petrographiq use of the word “mineral” for mineral species into general geology, although mineralogists, such as Miers, adopt the more commonsense practice which does not refuse the term “mineral” to most economic minerals. The author, of course, cannot be consistent, for the term is not used in the latter part of the book in accordance with the restricted definition. In regard to the Australian artesian water, the author adduces evidence that the supply is dwindling from the reduction in size,of the mound springs; but those who hold that plutonic water is largely influential in the uplift,of the water in the wells do not consider, as is twice stated, that most of the water is plutonic in origin.The Geology of South Australia. (In two divisions.) Division 1, An Introduction to Geology, Physiographical and Structural, from the Australian Standpoint. Division 2, The Geology of South Australia, with Notes on the Chief Geological Systems and Occurrences in the other Australian States. By Walter Howchin. Pp. xvi + 543. (Adelaide: The Education Department, 1918.) Price 10s.",1919,0,68,9,0,0,0,0,0,0,0,0,0,0
cfb6c045d3292aba5221f76e2b2ddd1c3f814e13,"GENERAL GEOLOGY General Distribution, Succession, and Structure of Formations Probable Archean Basement Complex Post-Archean Erosion Period Probable Algonkian Caraca Quartzite Batatal Schist Itabira Iron Formation Piracicaba Formation Itacolumi Quartzite Summary Pre-Devonian Deformation Paleozoic-Early Mesozoic Erosion Period Mesozoic-Early Tertiary Deposition Diamantina Conglomerate Later erosion and deposition periods General Uplift Tertiary Canga Plains Conglomerate Formations Present Status of Erosion Present Topography",1915,0,73,3,0,0,0,0,0,0,0,0,0,0
1f13ef5107650a9e2b3c433a8b70285aefd661c6,"WHEN the writer of the obituary notice in NATURE of May 3 expressed regret that the late Prof. Grenville A. J. Cole was not spared to write a comprehensive work on the geology of Ireland, he seems to have overlooked the volume on the British Isles in the “Handbuch der Regionalen Geologie” Series, to which the late Prof. Cole contributed all the portions dealing with Ireland.",1924,0,112,4,0,0,0,0,0,0,0,0,0,0
699647b717986a9c5cac03e47a60d41eb0152d99,"THIS fifth edition of “Economic Geology,” like its predecessor the fourth, deals almost wholly with the economic geology of North America. Its excellent illustrations, numerous references, and good index, together with its balance of treatment as regards scope and descriptive detail, make it a useful and popular text-book so far as the deposits of economic minerals in North America are concerned. As regards deposits other than those of North America, however, it is much less satisfactory. Indeed, in this respect it needs both amplification and correction, and the author would do well in future editions either to make these amplifications or adopt a more appropriate title, as was done in the first three editions.Economic Geology.Prof.H.RiesBy. Fifth edition, revised. Pp. v + 843. (New York: John Wiley and Sons, Inc.; London: Chapman and Hall, Ltd., 1925.) 25s. net.",1926,0,52,3,0,0,0,0,0,0,0,0,0,0
b580ab4bf601610a344b123e0f91e39c3722b206,"The coastal sedimentary basin of Nigeria has been the scene of three depositional cycles. The first began with a marine incursion in the middle Cretaceous and was terminated by a mild folding phase in Santonian time. The second included the growth of a proto-Niger delta during the Late Cretaceous and ended in a major Paleocene marine transgression. The third cycle, from Eocene to Recent, marked the continuous growth of the main Niger delta. A new threefold lithostratigraphic subdivision is introduced for the Niger delta subsurface, comprising an upper sandy Benin Formation, an intervening unit of alternating sandstone and shale named the Agbada Formation, and a lower shaly Akata Formation. These three units extend across the whole delta and each ranges in age from early T rtiary to Recent. They are related to the present outcrops and environments of deposition. A separate member of the Benin Formation is recognized in the Port Harcourt area. This is the Afam Clay Member, which is interpreted to be an ancient valley fill formed in Miocene sediments. Subsurface structures are described as resulting from movement under the influence of gravity and their distribution is related to growth stages of the delta. Rollover anticlines in front of growth faults form the main objectives of oil exploration, the hydrocarbons being found in sandstone reservoirs of the Agbada Formation.",1967,7,1075,83,0,2,1,1,0,2,1,1,4,2
bd71265f1aa05b4689b82298a19086dbc3deec22,,2008,0,253,30,0,5,11,11,15,15,38,23,23,22
57ebc446914f21554e18e6818458768d23dd41e3,"The North America landscape north of about 40°N was extensively glaciated during the last Ice Age, which ended about 14,000 years ago. Thus, much of the landscapes that we see around us was created or modified by glacial erosion and/or glacial deposition. In addition, glacial deposits are a common material on which we build our houses, roads, and cities, and glacial outwash sand and gravel is a very common groundwater aquifer. The stratigraphic record of glacial deposits serves as a register of global climate change and a major branch of geology is focused on comparing records of glaciation in different parts of the world. Thus, a knowledge of glacial geology is of critical importance to geologists, civil engineers, groundwater hydrologists, and environmental scientists.",1971,0,1037,36,2,17,19,16,28,21,30,26,22,23
a05025c6601f9ef5a6a839a29f6b35b6903a920c,,2008,0,250,12,1,6,9,16,10,17,27,24,13,25
7bebbf907496045949f127dbcde4d9421a7172f4,"MAY I supplement Prof. Green's history of geological mapping in Scotland (NATURE, vol. xlvii. p. 49) by pointing out that Mr. Cruchley published, on March 23, 1840, “A Geological Map of Scotland by Dr. MacCulloch, F.R.S., &#38;c., published by order of the Lords of the Treasury by S. Arrowsmith, Hydrographer to the King.” This fine map is on the scale of four miles to an inch. From the omission of “the late” before MacCulloch's name, it seems possible that the plates were in course of engraving before his death in 1835.",1892,0,89,11,0,0,0,0,0,0,0,0,0,0
c79eac078d09808303c98cab0a5ac6393a8264a4,"Introduction to medical geology : , Introduction to medical geology : , کتابخانه دیجیتال جندی شاپور اهواز",2009,0,64,2,0,0,1,2,1,6,5,4,1,4
260a7c6d42b8dbed4f932e3ad1d34938a6e7dc2f,"Oil-shale deposits are found in many parts of the world. They range in age from Cambrian to Tertiary and were formed in a variety of marine, continental, and lacustine depositional environments. The largest known deposit is the Green River oil shale in western United States. It contains an estimated 215 billion tons of in-place shale oil (1.5 trillion U.S. barrels). Total resources of a selected group of oil-shale deposits in 33 countries is estimated at 411 billion tons of in-place shale oil which is equivalent to 2.9 trillion U.S. barrels of shale oil. This figure is very conservative because several deposits mentioned herein have not been explored sufficiently to make accurate estimates and other deposits were not included in this survey.",2006,166,448,31,10,7,11,10,19,16,25,35,31,46
ebf9555fc3842ae333bf73e215e4db17a6316c47,"abstract Measurements of ground motion generated by nuclear explosions in Nevada were made for 37 locations near San Francisco Bay, California. The results were compared with the San Francisco 1906 earthquake intensities and the strong-motion recordings of the San Francisco earthquake of March 22, 1957. The recordings show marked amplitude variations which are related consistently to the geologic setting of the recording site. For sites underlain by a layer of younger bay mud or artificial fill, maximum horizontal ground velocities generally increased with thickness of the layer and were as much as ten times greater than those recorded on nearby bedrock. The maximum vertical velocities for these sites were between 1 and 3.5 times greater. Spectral amplification curves clearly define a “dominant ground period” of about 1 second for sites underlain by younger bay mud. For sites underlain by older, more consolidated sediments, no clearly defined “dominant ground period” was found. Maximum ground velocities for the older bay sediment sites were about twice those recorded on bedrock. Consistent correlations of the results from the nuclear recordings with the 1906 earthquake intensities and the spectral amplification curves for the 1957 earthquake suggest that areas of high amplification determined from small ground motions may also be areas of high intensity in future earthquakes.",1970,14,960,34,0,2,2,3,0,1,4,2,0,2
9037b98bb8fb9a423daafe074b57354433d2820b,"When people make donations to privately provided public goods, such as charity, there may be many factors influencing their decision other than altruism. Social pressure, guilt, sympathy, or simply a desire for a ""warm glow"" may all be important. This paper considers such impure altruism formally and develops a wide set of implications. In particular, this paper discusses the invariance proposition of public goods, solves for the sufficient conditions for neutrality to hold, examines the optimal tax treatment of charitable giving, and calibrates the model based on econometric studies in order to consider policy experiments. Impure altruism is shown to be more consistent with observed patterns of giving than the conventional pure altruism approach, and to have policy implications that may differ widely from those of the conventional models. Copyright 1990 by Royal Economic Society.",1990,42,4906,407,2,6,8,12,11,15,15,17,27,30
7fafbbaadc929e0673a1756305d95fe2d1cb016a,Foreword Preface 1. Valuing Public Goods Using the Contingent Valuation Method 2. Theoretical Basis of the Contingent Valuation Method 3. Benefits and Their Measurement 4. Variations in Contingent Valuation Scenario Designs 5. The Methodological Challenge 6. Will Respondents Answer Honestly? 7. Strategic Behavior and Contingent Valuation Studies 8. Can Respondents Answer Meaningfully? 9. Hypothetical Values and Contingent Valuation Studies 10. Enhancing Reliability 11. Measurement Bias,1989,0,4812,325,11,18,34,39,53,89,57,73,122,123
1360ad1a04ae3b1cf7fc550551a338910b013d79,,1973,0,7467,427,2,6,3,7,4,2,12,8,9,12
475819d6223c13c466d20a80ddced737da5bf347,"This paper provides evidence that free riders are heavily punished even if punishment is costly and does not provide any material benefits for the punisher. The more free riders negatively deviate from the group standard the more they are punished. As a consequence, the existence of an opportunity for costly punishment causes a large increase in cooperation levels because potential free riders face a credible threat. We show, in particular, that in the presence of a costly punishment opportunity almost complete cooperation can be achieved and maintained although, under the standard assumptions of rationality and selfishness, there should be no cooperation at all. We also show that free riding causes strong negative emotions among cooperators. The intensity of these emotions is the stronger the more the free riders deviate from the group standard. Our results provide, therefore, support for the hypothesis that emotions are guarantors of credible threats.",1999,117,3942,279,2,45,45,54,68,85,163,127,137,166
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
ba600625575385ecf9f2d3f7f38815aef2501d83,"The authors present a model that links heterogeneity of preferences across ethnic groups in a city to the amount and type of public good the city supplies. Results show that the shares of spending on productive public goods - education, roads, sewers, and trash pickup _ in U.S. cities (metro areas/urban counties) are inversely related to the city's (metro area's/county's) ethnic fragmentation, even after controlling for other socioeconomic and demographic determinants. They conclude that the ethnic conflict is an important determinant of local public finances. In cities where ethnic groups are polarized, and where politicians have ethnic constituencies, the share of spending that goes to public goods is low. Their results are driven mainly by how white-majority cities react to varying minority-groups sizes. Voters choose lower public goods when a significant fraction of tax revenues collected from one ethnic group is used to provide public goods shared with other ethnic groups.",1997,89,3042,295,6,8,23,31,43,58,60,58,90,84
5393f0cedc4ead17cb00111477771c198c1c8bc0,"Environments with public goods are a wonderful playground for those interested in delicate experimental problems, serious theoretical challenges, and difficult mechanism design issues. A review is made of various public goods experiments. It is found that the public goods environment is a very sensitive one with much that can affect outcomes but are difficult to control. The many factors interact with each other in unknown ways. Nothing is known for sure. Environments with public goods present a serious challenge even to skilled experimentalists and many opportunities for imaginative work.",1994,0,3153,212,3,7,17,23,37,53,70,55,100,101
19729e3c8eea13567cc62f509bb89de03e5791c4,,1966,0,8149,42,0,0,1,4,7,3,5,4,16,4
22bf5a3e727226548cdb9400b76a84cc1f3b6c6e,"The Chamberlinian monopolistically competitive equilibrium has been explored and extended in a number of recent papers. These analyses have paid only cursory attention to the existence of an industry outside the Chamberlinian group. In this article I analyze a model of spatial competition in which a second commodity is explicitly treated. In this two-industry economy, a zero-profit equilibrium with symmetrically located firms may exhibit rather strange properties. First, demand curves are kinked, although firms make ""Nash"" conjectures. If equilibrium lies at the kink, the effects of parameter changes are perverse. In the short run, prices are rigid in the face of small cost changes. In the long run, increases in costs lower equilibrium prices. Increases in market size raise prices. The welfare properties are also perverse at a kinked equilibrium.",1979,13,3078,265,0,5,9,8,9,13,24,15,17,12
82a7e6300c57aeedc5c0763e30c8400669a12a44,,1971,5,3196,228,0,0,1,0,2,1,10,7,16,8
81607ddcb153b35ab3484a5cff9061a702741291,"THIS PAPER DEVELOPS some models for limited dependent variables.2 The distinguishing feature of these variables is that the range of values which they may assume has a lower bound and that this lowest value occurs in a fair number of observations. This feature should be taken into account in the statistical analysis of observations on such variables. In particular, it renders invalid use of the usual regression model. The second section of this paper develops several models for such variables. Like Tobin's [10] model, they are extensions of the multiple probit analysis model.3 They differ from that model by allowing the determination of the size of the variable when it is not zero to depend on different parameters or variables from those determining the probability of its being zero. Estimation and discrimination in the models are considered in Section 3. The models, like their prototypes, seem particularly intractable to exact analysis and large sample approximations have to be used. The adequacy of inferences based on these procedures is explored in Section 4 through a small sampling experiment. Limited dependent variables arise naturally in the study of consumer purchases, particularly purchases of durable goods. When a durable good is to be purchased, the amount spent may vary in fine gradations, but for many durables it is probably the case that most consumers in a particular period make no purchase at all. In Section 5 we apply the models to the demand for durable goods to provide an application of the techniques.",1971,6,2652,213,0,0,0,2,2,3,4,5,0,2
0c2030b54ffb098b206773f420e84438e0d04f64,"Cultural meaning in a consumer society moves ceaselessly from one location to another. In the usual trajectory, cultural meaning moves first from the culturally constituted world to consumer goods and then from these goods to the individual consumer. Several instruments are responsible for this movement: advertising, the fashion system, and four consumption rituals. This article analyzes the movement of cultural meaning theoretically, showing both where cultural meaning is resident in the contemporary North American consumer system and the means by which this meaning is transferred from one location in this system to another.",1986,64,2612,171,0,6,8,9,10,7,8,16,10,12
3cefa7e5753b02234030c6bcfcf779801f1d4b31,"Abstract We consider a general model of the non-cooperative provision of a public good. Under very weak assumptions there will always exist a unique Nash equilibrium in our model. A small redistribution of wealth among the contributing consumers will not change the equilibrium amount of the public good. However, larger redistributions of wealth will change the set of contributors and thereby change the equilibrium provision of the public good. We are able to characterize the properties and the comparative statics of the equilibrium in a quite complete way and to analyze the extent to which government provision of a public good ‘crowds out’ private contributions.",1986,35,2183,175,4,8,3,9,18,14,19,19,19,16
5d99adf84df0915204bc7f1767af89d4f95d8ac1,"We study the importance of conditional cooperation in a one-shot public goods game by using a variant of the strategy-method. We find that a third of the subjects can be classified as free riders, whereas 50 percent are conditional cooperators.",2000,75,1895,183,13,23,17,28,33,34,48,67,67,108
e6733b9c10cc63ffe319174e32146a201dc5b72e,"Presents an alternative theoretical approach to industrial marketing and purchasing based on a research project carried out in France, Germany, Italy, Sweden, and Great Britain. Focuses on descriptions and analyses of actual marketing and purchasing problems.",1982,120,2145,180,2,0,1,6,5,6,4,4,5,5
d6faa4fd609d88f9b5e09b90d0c1450695c00326,"In this article, the authors examine how consumer choice between hedonic and utilitarian goods is influenced by the nature of the decision task. Building on research on elaboration, the authors propose that the relative salience of hedonic dimensions is greater when consumers decide which of several items to give up (forfeiture choices) than when they decide which item to acquire (acquisition choices). The resulting hypothesis that a hedonic item is relatively preferred over the same utilitarian item in forfeiture choices than in acquisition choices was supported in two choice experiments. In a subsequent experiment, these findings were extended to hypothetical choices in which the acquisition and forfeiture conditions were created by manipulating initial attribute-level reference states instead of ownership. Finally, consistent with the experimental findings, a field survey showed that, relative to market prices, owners of relatively hedonic cars value their vehicles more than do owners of relatively utilitarian cars. The authors discuss theoretical implications of these reference-dependent preference asymmetries and explore consequences for marketing managers and other decision makers.",2000,56,1742,80,3,11,14,22,23,39,36,36,73,78
ee8b05e1756126396c86bc144632a2ffca934c0f,"Contingent valuation surveys in which respondents state their willingness to pay (WTP) for public goods are coming into use in cost-benefit analyses and in litigation over environmental losses. The validity of the method is brought into question by several experimental observations. An embedding effect is dem(~ns~rated, in which WTP for a good \arie\ depending on whether it is evaluated on its own or as part of a more inclusive category. The ordering of various public issues by WTP is predicted with significant accuracy by independent ratings of the moral satisfaction associated with contributions to these causc~. Contingent valuation responses reflect the willingness to pay for the moral satisfaction ol contributing to public goods, not the economic value of thehe goods. ’ I‘J’I? ,Acxlcrnl~ Pi-L?\ lnc There is substantial demand for a practical technique for measuring the value of non-market goods. Measures of value are required for cost-benefit assessments ot public goods, for the analysis of policies that affect the environment. and for realistic estimates of environmental damages resulting from human action, such as oil spills. In recent years the contingent valuation method (CVM) has gained prominence as the major technique for the assessment of the value of environmental amenities. This paper is concerned with a critique of CVM. The idea of CVM is quite simple: respondents are asked to indicate their value for a public good, usually by specifying the maximum amount they would be willing to pay to obtain or to retain it. The total value of the good is estimated by nlultiplying the average willingness to pay (WTP) observed in the sample by the number of households in the relevant population. This value is sometimes divided into use r~lue and non-use Ll&e by comparing the WTP of respondents who expect to enjoy the public good personally (e.g., benefit from improved visibility or from the increased number of fish in a cleaned up stream) to the WTP ot respondents who have no such expectations. Specific questions are sometimes added to partition non-use value further into the value of retaining an option fol future use, a bequest value, and a pure existence value 1151. The accuracy of the CVM is a matter of substantial practical import, not only in cost-benefit assessments but also in litigation over liability and damages. The validity of the technique is take as a rebuttable presumption in envir~~nmental cases brought in the United States under the Comprehensive Environmental Response, Compensation and Liability Act of 1980 (CERCLA). The research on the method has been reviewed in two authoritative volumes. which offer detailed *This research was supported by Fisheries and Oceans Canada, the Ontario Ministry 01’ thi: Environment. and the Sloan Foundation. Interviews and preliminary statistical analyses were prrformed by Campbell-Goodell Consultants, Vancouver, British Columbia. We benefited from conversations with George Akerlof, James Bieke, Brian Binger, Ralph d‘Arge, Elizabeth Hoffman. Richard Thaler. and Frances van Loo. from a commentary by Glenn Harrison. and from the statistical expertise of Carol Nickerson.",1992,28,1879,75,19,39,53,43,32,44,63,40,45,56
63fdeac799f8c871b8ba2efe041dc4f7c3f1987d,"This article identifies ecological goods and services of coral reef ecosystems, with special emphasis on how they are generated. Goods are divided into renewable resources and reef mining. Ecological services are classified into physical structure services, biotic services, biogeochemical services, information services, and social:cultural services. A review of economic valuation studies reveals that only a few of the goods and services of reefs have been captured. We synthesize current understanding of the relationships between ecological services and functional groups of species and biological communities of coral reefs in different regions of the world. The consequences of human impacts on coral reefs are also discussed, including loss of resilience, or buffer capacity. Such loss may impair the capacity for recovery of coral reefs and as a consequence the quality and quantity of their delivery of ecological goods and services. Conserving the capacity of reefs to generate essential services requires that they are managed as components of a larger seascape-landscape of which human activities are seen as integrated parts. © 1999 Elsevier Science B.V. All rights reserved.",1999,160,1583,78,1,6,6,8,18,15,17,27,19,40
187ac58928df06b0b4dae0ac55332479d35c623a,"Import prices typically change by a smaller proportion than the exchange rate between the exporting and importing country. Recent research indicates that common-currency relative prices for similar goods exported to different markets are highly correlated with exchange rates between those markets. This evidence suggests that incomplete pass-through is a consequence of third-degree price discrimination. While distance matters for market segmentation, borders have independent effects. The source of the border effect has not been clearly identified. Furthermore, there is little evidence yet to suggest substantial market power is implied by the observed price discrimination.",1996,41,1546,82,2,4,13,24,42,56,72,61,84,96
d8cf1e11aa8b5ddd961922a122a3323cfa81f12a,"Product and labor market deregulation are fundamentally about reducing and redistributing rents, leading economic players to adjust in turn to this new distribution. Thus, even if deregulation eventually proves beneficial, it comes with strong distribution and dynamic effects. The transition may imply the decline of incumbent firms. Unemployment may increase for a while. Real wages may decrease before recovering, and so on. To study these issues, we build a model based on two central assumptions: Monopolistic competition in the goods market, which determines the size of rents; and bargaining in the labor market, which determines the distribution of rents between workers and firms. We then think of product market regulation as determining both the entry costs faced by firms, and the degree of competition between firms. We think of labor market regulation as determining the bargaining power of workers. Having characterized the effects of labor and product market deregulation, we then use our results to study two specific issues. First, to shed light on macroeconomic evolutions in Europe over the last twenty years, in particular on the behavior of the labor share. Second, to look at political economy interactions between product and labor market deregulation.",2000,36,1238,153,0,5,19,38,49,51,74,72,87,81
bce6ca4c1471e1035f8319b6d57dd042f61bc728,"This book develops an original theory of group and organizational behavior that cuts across disciplinary lines and illustrates the theory with empirical and historical studies of particular organizations. Applying economic analysis to the subjects of the political scientist, sociologist, and economist, Mr. Olson examines the extent to which the individuals that share a common interest find it in their individual interest to bear the costs of the organizational effort. The theory shows that most organizations produce what the economist calls ""public goods""--goods or services that are available to every member, whether or not he has borne any of the costs of providing them. Economists have long understood that defense, law and order were public goods that could not be marketed to individuals, and that taxation was necessary. They have not, however, taken account of the fact that private as well as governmental organizations produce public goods. The services the labor union provides for the worker it represents, or the benefits a lobby obtains for the group it represents, are public goods: they automatically go to every individual in the group, whether or not he helped bear the costs. It follows that, just as governments require compulsory taxation, many large private organizations require special (and sometimes coercive) devices to obtain the resources they need. This is not true of smaller organizations for, as this book shows, small and large organizations support themselves in entirely different ways. The theory indicates that, though small groups can act to further their interest much more easily than large ones, they will tend to devote too few resources to thesatisfaction of their common interests, and that there is a surprising tendency for the ""lesser"" members of the small group to exploit the ""greater"" members by making them bear a disproportionate share of the burden of any group action. All of the theory in the book is in Chapter 1; the remaining chapters contain empirical and historical evidence of the theory's relevance to labor unions, pressure groups, corporations, and Marxian class action.",1967,0,2024,73,0,0,1,4,0,2,2,2,2,1
6a2af14ed3174a5238a415bcfaf4c85c1ec803d1,"There is widespread belief that firms should pursue superiority in both customer satisfaction and productivity. However, there is reason to believe these two goals are not always compatible. If a firm improves productivity by “downsizing,” it may achieve an increase in productivity in the short-term, but future profitability may be threatened if customer satisfaction is highly dependent on the efforts of personnel. If so, there are potential tradeoffs between customer satisfaction and productivity for industries as diverse as airlines, banking, education, hotels, and restaurants. Managers in these types of service industries, as well as goods industries in which the service component is increasing, need to understand whether or not this is the case. For example, if efforts to improve productivity can actually harm customer satisfaction---and vice-versa---the downsizing of U.S. and European companies should be viewed with concern. It follows that developing a better understanding of how customer satisfaction and productivity relate to one another is of substantial and growing importance, especially in light of expected continued growth in services throughout the world economy. 
 
The objective of this paper is to investigate whether there are conditions under which there are tradeoffs between customer satisfaction and productivity. A review of the literature reveals two conflicting viewpoints. One school of thought argues that customer satisfaction and productivity are compatible, as improvements in customer satisfaction can decrease the time andeffort devoted to handling returns, rework, warranties, and complaint management, while at the same time lowering the cost of making future transactions. The second argues that increasing customer satisfaction should increase costs, as doing so often requires efforts to improve product attributes or overall product design. 
 
A conceptual framework useful in resolving these contradictory viewpoints is developed. The framework serves, in turn, as a basis for developing a theoretical model relating customer satisfaction and productivity. The model predicts that customer satisfaction and productivity are less likely to be compatible when: 1 customer satisfaction is relatively more dependent on customization---the degree to which the firm's offering is customized to meet heterogeneous customers' needs---as opposed to standardization---the degree to which the firm's offering is reliable, standardized, and free from deficiencies; and 2 when it is difficult costly to provide high levels of both customization and standardization simultaneously. 
 
To move forward from the model's propositions to the development of testable hypotheses, we argue that services are more likely than goods to have the preceding characteristics. Hence, tradeoffs between customer satisfaction and productivity should be more prevalent for services than for goods. Although this classification is not precise---many services are standardizable and many goods have a service component---it has the advantage of allowing an initial test of the propositions. 
 
The empirical work employs a database matching customer-based measures of firm performance with traditional measures of business performance, such as productivity and Return on Investment ROI. The central feature of this database is the set of customer satisfaction indices provided by the Swedish Customer Satisfaction Barometer SCSB. The SCSB provides a uniform set of comparable customer-based firm performance measures and offers a unique opportunity to test the study's hypotheses. 
 
The findings indicate that the association between changes in customer satisfaction and changes in productivity is positive for goods, but negative for services. In addition, while both customer satisfaction and productivity are positively associated with ROI for goods and services, the interaction between the two is positive for goods but significantly less so for services. 
 
Taken together, the findings suggest support for the contention that tradeoffs are more likely for services. Hence, simultaneous attempts to increase both customer satisfaction and productivity are likely to be more challenging in such industries. Of course, this does not imply that such firms should not seek improvements in both productivity and customer satisfaction. For example, appropriate applications of information technology may improve both customer satisfaction and productivity simultaneously. 
 
The findings should provide motivation for future research concerning the nature of customer satisfaction and productivity, as well as appropriate strategy and tactics for each one. It is worth emphasizing that this is an issue that is not only important today, but certainly will become even more important in the future. As the growth of services continues and world markets become increasingly competitive, the importance of customer satisfaction will also increase. To compete in such a world, firms must strike the right balance between their efforts to compete efficiently and their efforts to compete effectively.",1997,56,1355,66,1,8,9,18,14,16,25,42,38,56
46fb7520f6ce62ceed1fdf85154f719607209db1,"Abstract There are two logics or mindsets from which to consider and motivate a transition from goods to service(s). The first, “goods-dominant (G-D) logic”, views services in terms of a type of (e.g., intangible) good and implies that goods production and distribution practices should be modified to deal with the differences between tangible goods and services. The second logic, “service-dominant (S-D) logic”, considers service – a process of using ones resources for the benefit of and in conjunction with another party – as the fundamental purpose of economic exchange and implies the need for a revised, service-driven framework for all of marketing. This transition to a service-centered logic is consistent with and partially derived from a similar transition found in the business-marketing literature — for example, its shift to understanding exchange in terms value rather than products and networks rather than dyads. It also parallels transitions in other sub-disciplines, such as service marketing. These parallels and the implications for marketing theory and practice of a full transition to a service-logic are explored.",2008,57,904,105,12,28,52,78,94,81,102,96,69,68
19434c969ad13a173bddd81fa5b2568a36a03c35,Abstract Pigou's proposition that the use of distorting taxes rather than neutral head taxes reduces public service levels is examined in this paper. A simple model with a national system of competing local governments is utilized to demonstrate that the use of a distorting property tax on mobile capital decreases the level of residential public services. The case where public services are an intermediate producer good is also considered.,1986,40,1624,65,2,5,2,4,4,12,6,7,4,7
a4903cce8609d781c03ba450bc3d1b9b54ec1ba4,"This revised edition with new Introduction from a leading anthropologist and an economist is unique in being about consumption but not a sermon for consumers, nor a moan against consumerism. The World of Goods bridges the gap between what anthropologists know about why objects are desired and what economists say about the specialised topic called consumption behaviour. The economist treats the desire for objects as an individual urge grounded in psychology; according to the anthropologist it is for fulfilling social obligations and represents the distribution of goods as a symptom of the form of society. It is a totally different perspectice and raises issues that lie beyond economics. The World of Goods asks new questions about why people save, why they spend, what they buy, and why they sometimes but not always make fine distinctions about quality. It is well-understood now that consumption goods communicate, create identity and establish relationships. But not so well-known that goods exclude as well as include, and that the pattern of their flow shows up the form of society. This book will be essential reading to students and lecturers in anthropology and economics.",1982,0,1746,7,5,6,12,6,13,20,20,14,14,10
75c3672918c261762d6686be69883f57cad89cde,"Abstract This paper examines ethnic diversity and local public goods in rural western Kenya. The identification strategy relies on the stable historically determined patterns of ethnic land settlement. Ethnic diversity is associated with lower primary school funding and worse school facilities, and there is suggestive evidence that it leads to poor water well maintenance. The theoretical model illustrates how inability to impose social sanctions in diverse communities leads to collective action failures, and we find that school committees in diverse areas do impose fewer sanctions on defaulting parents. We relate these results to the literature on social capital and economic development and discuss implications for decentralization in less developed countries.",2005,57,1183,46,14,24,45,29,67,69,86,71,88,83
050873d0385ab5a8aa36d5a791795967298f99b6,"Humans often cooperate in public goods games and situations ranging from family issues to global warming. However, evolutionary game theory predicts that the temptation to forgo the public good mostly wins over collective cooperative action, and this is often also seen in economic experiments. Here we show how social diversity provides an escape from this apparent paradox. Up to now, individuals have been treated as equivalent in all respects, in sharp contrast with real-life situations, where diversity is ubiquitous. We introduce social diversity by means of heterogeneous graphs and show that cooperation is promoted by the diversity associated with the number and size of the public goods game in which each individual participates and with the individual contribution to each such game. When social ties follow a scale-free distribution, cooperation is enhanced whenever all individuals are expected to contribute a fixed amount irrespective of the plethora of public goods games in which they engage. Our results may help to explain the emergence of cooperation in the absence of mechanisms based on individual reputation and punishment. Combining social diversity with reputation and punishment will provide instrumental clues on the self-organization of social communities and their economical implications.",2008,37,984,27,5,42,58,57,83,67,84,88,81,86
89ee2f72f16c4c3990259e5eb494399c77829157,,1981,0,1615,55,0,0,1,0,5,3,3,8,6,14
ca3eb6e60f56d6c9279cdb035c928867850817ba,"A large and growing literature links high levels of ethnic diversity to low levels of public goods provision. Yet although the empirical connection between ethnic heterogeneity and the underprovision of public goods is widely accepted, there is little consensus on the specific mechanisms through which this relationship operates. We identify three families of mechanisms that link diversity to public goods provision—what we term “preferences,” “technology,” and “strategy selection” mechanisms—and run a series of experimental games that permit us to compare the explanatory power of distinct mechanisms within each of these three families. Results from games conducted with a random sample of 300 subjects from a slum neighborhood of Kampala, Uganda, suggest that successful public goods provision in homogenous ethnic communities can be attributed to a strategy selection mechanism: in similar settings, co-ethnics play cooperative equilibria, whereas non-co-ethnics do not. In addition, we find evidence for a technology mechanism: co-ethnics are more closely linked on social networks and thus plausibly better able to support cooperation through the threat of social sanction. We find no evidence for prominent preference mechanisms that emphasize the commonality of tastes within ethnic groups or a greater degree of altruism toward co-ethnics, and only weak evidence for technology mechanisms that focus on the impact of shared ethnicity on the productivity of teams.",2006,103,1045,66,2,11,20,46,58,70,64,82,88,102
e5d15af10232f9e32adaadf4917c8514da9a4853,"We study the strategy of bundling a large number of information goods, such as those increasingly available on the Internet, and selling them for a fixed price. We analyze the optimal bundling strategies for a multiproduct monopolist, and we find that bundling very large numbers of unrelated information goods can be surprisingly profitable. The reason is that the law of large numbers makes it much easier to predict consumers' valuations for a bundle of goods than their valuations for the individual goods when sold separately. As a result, this ""predictive value of bundling"" makes it possible to achieve greater sales, greater economic efficiency, and greater profits per good from a bundle of information goods than can be attained when the same goods are sold separately. Our main results do not extend to most physical goods, as the marginal costs of production for goods not used by the buyer typically negate any benefits from the predictive value of large-scale bundling. While determining optimal bundling strategies for more than two goods is a notoriously difficult problem, we use statistical techniques to provide strong asymptotic results and bounds on profits for bundles of any arbitrary size. We show how our model can be used to analyze the bundling of complements and substitutes, bundling in the presence of budget constraints, and bundling of goods with various types of correlations and how each of these conditions can lead to limits on optimal bundle size. In particular we find that when different market segments of consumers differ systematically in their valuations for goods, simple bundling will no longer be optimal. However, by offering a menu of different bundles aimed at each market segment, bundling makes traditional price discrimination strategies more powerful by reducing the role of unpredictable idiosyncratic components of valuations. The predictions of our analysis appear to be consistent with empirical observations of the markets for Internet and online content, cable television programming, and copyrighted music.",1999,49,1173,86,17,32,48,49,48,50,60,57,60,41
85c3b07d19ceb28b68c82c779c5fda285fdb3189,"Abstract Laboratory experiments on free riding have produced mixed results. Free riding is seldom observed with single-shot games; however, it is often approximated in finitely repeated games. There are two prevailing hypothesis for why this is so: strategies and learning. This paper discusses these hypotheses and presents an experiment that examines both.",1988,23,1163,146,1,3,1,7,3,4,6,3,12,12
121145a1715903fbfed3dfab52a3910493a0c059,"By lowering the costs of gathering and sharing information and offering new ways to learn about products before purchase, the Internet reduces traditional distinctions between search and experience goods. At the same time, differences in the type of information sought for search and experience goods can precipitate differences in the process through which consumers gather information and make decisions online. A preliminary experiment shows that though there are significant differences in consumers’ perceived ability to evaluate product quality before purchase between search and experience goods in traditional retail environments, these differences are blurred in online environments. An analysis of the online behavior of a representative sample of U.S. consumers shows that consumers spend similar amounts of time online gathering information for both search and experience goods, but there are important differences in the browsing and purchase behavior of consumers for these two types of goods. In particular, experience goods involve greater depth (time per page) and lower breadth (total number of pages) of search than search goods. In addition, free riding (purchasing from a retailer other than the primary source of product information) is less frequent for experience than for search goods. Finally, the presence of product reviews from other consumers and multimedia that enable consumers to interact with products before purchase has a greater effect on consumer search and purchase behavior for experience than for search goods.",2009,77,565,42,2,27,42,43,50,51,45,52,62,51
8d18235b9606a7e8f64036ab0b50aa7b6d0a3ed8,"This paper discusses Ricardian trade and payments theory in the case of a continuum of goods. The analysis thus extends the development of many-commodity, two-country comparative advantage analysis as presented, for example, in Gottfried Haberler (1937), Frank Graham (1923), Paul Samuelson (1964), and Frank W. Taussig (1927). The literature is historically reviewed by John Chipman (1965). Perhaps surprisingly, the continuum assumption simplifies the analysis neatly in comparison with the discrete many-commodity case. The distinguishing feature of the Ricardian approach emphasized in this paper is the determination of the competitive margin in production between imported and exported goods. The analysis advances the existing literature by formally showing precisely how tariffs and transport costs establish a range of commodities that are not traded, and how the price-specie flow mechanism does or does not give rise to movements in relative cost and price levels. The formal real model is introduced in Section 1. Its equilibrium determines the relative wage and price structure and the efficient international specialization pattern. Section II considers standard comparative static questions of growth, demand shifts,",1976,30,1375,103,0,0,1,3,6,5,3,5,2,6
cd0806361ec140668db16819882d5780889f8a75,"People want to have fun, and they are more likely to have fun if the situation allows them to justify it. This research studies how people's need for justifying hedonic consumption drives two choice patterns that are observed in typical purchase contexts. First, relative preferences between hedonic and utilitarian alternatives can reverse, depending on how the immediate purchase situation presents itself. A hedonic alternative tends to be rated more highly than a comparable utilitarian alternative when each is presented singly, but the utilitarian alternative tends to be chosen over the hedonic alternative when the two are presented jointly. Second, people have preferences for expending different combinations of time (effort) and money for acquiring hedonic versus utilitarian items. They are willing to pay more in time for hedonic goods and more in money for utilitarian goods. The author explores the topic through a combination of four experiments and field studies.",2005,48,856,86,5,13,11,27,23,39,44,54,39,69
3572e8b5d1c576a72f6a4cc45b15ea083259f43b,"To make economic choices between goods, the brain needs to compute representations of their values. A great deal of research has been performed to determine the neural correlates of value representations in the human brain. However, it is still unknown whether there exists a region of the brain that commonly encodes decision values for different types of goods, or if, in contrast, the values of different types of goods are represented in distinct brain regions. We addressed this question by scanning subjects with functional magnetic resonance imaging while they made real purchasing decisions among different categories of goods (food, nonfood consumables, and monetary gambles). We found activity in a key brain region previously implicated in encoding goal-values: the ventromedial prefrontal cortex (vmPFC) was correlated with the subjects' value for each category of good. Moreover, we found a single area in vmPFC to be correlated with the subjects' valuations for all categories of goods. Our results provide evidence that the brain encodes a “common currency” that allows for a shared valuation for different categories of goods.",2009,31,519,29,3,24,40,48,49,53,56,41,41,42
63df6bc8762444c21051ab996cd33c7c0777d9a4,"The persistence of cooperation in public-goods experiments has become an important puzzle for economists. This paper presents the first systematic attempt to separate the hypothesis that cooperation is due to kindness, altruism, or warm-glow from the hypothesis that cooperation is simply the result of errors or confusion. The experiment reveals that, on average, about half of all cooperation comes from subjects who understand free-riding but choose to cooperate out of some form of kindness. This suggests that the focus on errors and 'learning' in experimental research should shift to include studies of preferences for cooperation as well. Copyright 1995 by American Economic Association.",1995,21,1035,105,1,8,11,21,12,38,20,39,31,43
f001bc3add3c433d3f6801ded41c2782af12592a,"This book presents a theoretical treatment of externalities (i.e. uncompensated interdependencies), public goods, and club goods. The new edition updates and expands the discussion of externalities and their implications, coverage of asymmetric information, underlying game-theoretic formulations, and intuitive and graphical presentations. Topics investigated include Nash equilibrium, Lindahl equilibria, club theory, preference-revelation mechanism, Pigouvian taxes, the commons, Coase Theorem, and static and repeated games. The authors use mathematical techniques only as much as necessary to pursue the economic argument. They develop key principles of public economics that are useful for subfields such as public choice, labor economics, economic growth, international economics, environmental and natural resource economics, and industrial organization.",1996,0,1104,41,11,13,19,23,30,48,37,50,47,57
3a866c627f12fb54dc72cefb8990735570fcee16,"While there are many Web services which help users find things to buy, we know of none which actually try to automate the process of buying and selling. Kasbah is a system where users create autonomous agents to buy and sell goods on their behalf. In this paper, we describe how Kasbah works. We also discuss the implementation of a simple proof-of-concept prototype.",1997,54,1001,45,18,57,69,100,98,88,65,93,61,57
f77cf9ccf68656bb341576172983306484387ad6,"The Consumer Price Index (CPI) attempts to answer the question of how much more (or less) income does a consumer require to be as well off in period 1 as in period 0 given changes in prices, changes in the quality of goods, and the introduction of new goods (or the disappearance of existing goods). In this paper I explain the theory of cost-of-living indices and demonstrate how new goods should be included using the classical theory of Hicks and Rothbarth. The correct price to use for the good in the pre-intro- duction period is a `virtual&apos; price which sets demand to zero. Estimation of this virtual price requires estimation of a demand function which in turn provides the expenditure function which allows exact calucation of the cost of living index. The data requirements and need to specify and estimate a demand function for a new brand among many existing brands requires extensive data and some new econometric methods which may have proven obstacles to the inclusion of new goods in the CPI up to this point. As an example I use the introduction of a new cereal brand by General Mills in 1989-Apple Cinnamon Cheerios. I find the virtual price is about 2 times the actual price of Apple Cinnamon Cheerios and that increase in consumer surplus is substantial. Based on some simplifying approximations, I find that CPI may be overstated for cereal by about 25% because of its neglect of the effect of new brands. When I take imperfect competition into account I find that the increase in consumer welfare is only 85% as high with perfect competition so CPI for cereal would still be 20% too high",1994,33,965,56,0,0,8,16,11,6,19,24,25,37
ad9048785b7211efe4d0f7c2e65fb63a1cb6cbdb,"This study analyses trade flows in intermediate goods and services among OECD countries and with their main trading partners. Combining trade data and input-output tables, bilateral trade in intermediate goods and services is estimated according to the industry of origin and the using industry for the period 1995-2005. Trade in intermediate inputs takes place mostly among developed countries and represents respectively 56% and 73% of overall trade flows in goods and services. Gravity regressions indicate that in comparison to trade in final goods and services, imports of intermediates are more sensitive to trade costs and are less attracted by bilateral market size. Further findings are that the activities of multinational enterprises can be associated with higher trade flows of intermediate inputs and with a higher ratio of foreign to domestic inputs in using industries. Results from production function regressions and from a stochastic frontier analysis suggest that a higher share of imported inputs leads to productivity gains in domestic industries and reduces inefficiencies in the use of technology.",2009,29,328,17,2,12,23,36,35,35,35,34,34,32
52e1d4ad8472e48e6119e4e8e3b21129baf629f6,,2007,774,713,38,4,38,78,77,71,69,87,62,51,35
10d6d461d1b36ccb72f7b2be16075befde63cb63,"The well known travel cost method (TC)has been widely applied to outdoor recreation. A second approach has been referred to in the past as the Davis method, the questionnaire approach, and contingent valuation. It will here be termed hypothetical valuation (HV), since it involves creating a hypothetical situation designed to elicit willingness to pay for or willingness to accept compensation for a recreational or other extramarket good (or bad). TC and HV are termed ""indirect methods"", since they do not depend on the direct information about prices and quantities that economists would prefer to use where available to value goods and services.",1979,7,1189,39,0,2,5,6,5,10,9,23,11,16
d58a964c8391ea22054cfb593e6de4dc8b5eb037,"We study the importance of conditional cooperation in a one-shot public goods game by using a variant of the strategy-method. We find that a third of the subjects can be classified as free riders, whereas 50% are conditional cooperators.",2001,31,886,22,14,18,14,10,48,18,18,24,31,21
6f7047106bc6c4790239f8364f8caf9899243334,"This paper considers incentives to provide goods that are non-excludable along social or geographic links. We find, first, that networks can lead to specialization in public good provision. In every social network there is an equilibrium where some individuals contribute and others free ride. In many networks, this extreme is the only outcome. Second, specialization can benefit society as a whole. This outcome arises when contributors are linked, collectively, to many agents. Finally, a new link increases access to public goods, but reduces individual incentives to contribute. Hence, overall welfare can be higher when there are holes in a network.",2007,49,551,55,6,19,24,32,30,39,43,39,40,42
b4b5ee8e671627a17cc22fb9ec83731b74721dc5,"Aims. We present the final public data release of the VLT /ISAAC near-infrared imaging survey in the GOODS-South field . The survey covers an area of 172.5, 159.6 and 173.1 arcmin 2 in the J, H, and Ks bands, respectively. For point sources total limiting magnitudes of J = 25: 0, H = 24: 5, and Ks = 24: 4 (5� , AB) are reached within 75% of the survey area. Thus these observations are significantly deeper than the previous EIS Deep Public Surve y which covers the same region. The image quality is characterized by a point spread function ranging between 0.34 00 and 0.65 00 FWHM. The images are registered to a common astrometric grid defined by the GSC 2 with an accuracy of�0: 06 00 RMS over the whole field. The overall photometric accuracy, i ncluding all systematic effects, adds up to 0.05 mag. The data are publicly available from the ESO science archive facility. Methods. We describe the data reduction, the calibration, and the quality control process. The final data set is characterized in t erms of astrometric and photometric properties, including the PSF and the curve of growth. We establish an empirical model for the sky background noise in order to quantify the variation of limiting depth and statistical photometric errors over the surve y area. We define a catalog of Ks-selected sources which contains JHKs photometry for 7079 objects. Differential aperture corrections were applied to the color measurements in order to avoid possible biases as a result of the variation of the PSF. We briefly discuss the resu lting color distributions in the context of available redshift data. Fu rthermore, we estimate the completeness fraction and relative contamination due to spurious detections for source catalogs extracted fr om the survey data. For this purpose, an empirical study based on a deep Ks image of the Hubble Ultra Deep Field is combined with extensive image simulations. Results. With respect to previous deep near-infrared surveys, the surface density of faint galaxies has been established with unprecedented accuracy by virtue of the unique combination of depth and area of this survey. We derived galaxy number counts over eight magnitudes in flux up to J = 25: 25, H = 25: 0, Ks = 25: 25 (in the AB system). Very similar faint-end logarithmic slopes between 0.24 and 0.27 mag −1 were measured in the three bands. We found no evidence for a significant change in the slope of the logarithmic galaxy number counts at the faint end.",2009,44,103,25,2,14,24,15,4,8,12,7,1,7
d0fd9c58fb89f1c7b8ce3c66b550f05b5b751bc3,"Nanotechnology is the set of technologies that enables the manipulation, study or exploitation of very small (typically less than 100 nanometres) structures and systems. To put this into perspective, one nanometre is one-billionth of a metre, or around 80,000 times smaller than the diameter of a human hair. Nanotechnology contributes to novel materials, devices and products that demonstrate different properties.",2001,0,6415,196,47,53,76,128,208,246,314,421,556,611
2224928c84a2255da72d81cb268eea47fca29c80,"Although gold is the subject of one of the most ancient themes of investigation in science, its renaissance now leads to an exponentially increasing number of publications, especially in the context of emerging nanoscience and nanotechnology with nanoparticles and self-assembled monolayers (SAMs). We will limit the present review to gold nanoparticles (AuNPs), also called gold colloids. AuNPs are the most stable metal nanoparticles, and they present fascinating aspects such as their assembly of multiple types involving materials science, the behavior of the individual particles, size-related electronic, magnetic and optical properties (quantum size effect), and their applications to catalysis and biology. Their promises are in these fields as well as in the bottom-up approach of nanotechnology, and they will be key materials and building block in the 21st century. Whereas the extraction of gold started in the 5th millennium B.C. near Varna (Bulgaria) and reached 10 tons per year in Egypt around 1200-1300 B.C. when the marvelous statue of Touthankamon was constructed, it is probable that “soluble” gold appeared around the 5th or 4th century B.C. in Egypt and China. In antiquity, materials were used in an ecological sense for both aesthetic and curative purposes. Colloidal gold was used to make ruby glass 293 Chem. Rev. 2004, 104, 293−346",2004,709,9487,25,41,141,206,374,461,471,559,791,792,764
7ba37494ac59afa65acca09a2334a3eb1eafde03,"Nanotechnology is a multidisciplinary field, which covers a vast and diverse array of devices derived from engineering, biology, physics and chemistry. These devices include nanovectors for the targeted delivery of anticancer drugs and imaging contrast agents. Nanowires and nanocantilever arrays are among the leading approaches under development for the early detection of precancerous and malignant lesions from biological fluids. These and other nanodevices can provide essential breakthroughs in the fight against cancer.",2005,170,3926,48,19,85,110,137,185,245,280,316,374,363
48c581b5531239d3a519aef99c8be0b125717d1a,"In this work we briefly describe the most relevant features of WSXM, a freeware scanning probe microscopy software based on MS-Windows. The article is structured in three different sections: The introduction is a perspective on the importance of software on scanning probe microscopy. The second section is devoted to describe the general structure of the application; in this section the capabilities of WSXM to read third party files are stressed. Finally, a detailed discussion of some relevant procedures of the software is carried out.",2007,20,6006,94,58,232,320,340,447,475,509,472,507,492
9eff1caf86b7dd0ba2f3e8c189f7e90b1d7a4e2a,,2002,31,664,24,0,1,7,9,16,25,39,40,36,48
653f0008135664fc9a6d07f5a37420a3e4f0fb88,,2005,621,6548,89,26,135,225,316,337,407,515,553,577,543
26e83f1836da0963587e449a07aad1110fc55e4e,"Abstract In the large field of nanotechnology, polymer matrix based nanocomposites have become a prominent area of current research and development. Exfoliated clay-based nanocomposites have dominated the polymer literature but there are a large number of other significant areas of current and emerging interest. This review will detail the technology involved with exfoliated clay-based nanocomposites and also include other important areas including barrier properties, flammability resistance, biomedical applications, electrical/electronic/optoelectronic applications and fuel cell interests. The important question of the “nano-effect” of nanoparticle or fiber inclusion relative to their larger scale counterparts is addressed relative to crystallization and glass transition behavior. Of course, other polymer (and composite)-based properties derive benefits from nanoscale filler or fiber addition and these are addressed.",2008,285,2699,46,6,45,108,188,211,217,238,277,266,283
e7f7108c95d937344e3a265591a653d745832fa2,"Nanotechnology is the engineering and manufacturing of materials at the atomic and molecular scale. In its strictest definition from the National Nanotechnology Initiative, nanotechnology refers to structures roughly in the 1-100 nm size regime in at least one dimension. Despite this size restriction, nanotechnology commonly refers to structures that are up to several hundred nanometers in size and that are developed by top-down or bottom-up engineering of individual components. Herein, we focus on the application of nanotechnology to drug delivery and highlight several areas of opportunity where current and emerging nanotechnologies could enable entirely novel classes of therapeutics.",2009,37,2415,12,23,66,147,163,206,233,216,260,270,193
2cfd620128b62301624e36e7b4626072b151910d,"Williams textbook of endocrinology / , Williams textbook of endocrinology / , کتابخانه دیجیتال جندی شاپور اهواز",1985,0,3465,33,0,4,20,14,12,17,16,11,22,37
af06e7856d5b62fe8242d9e25b1cbbe63c77e449,"Clinical gynecologic endocrinology and infertility , Clinical gynecologic endocrinology and infertility , کتابخانه دیجیتال جندی شاپور اهواز",1983,0,2717,157,6,11,16,10,23,16,3,13,21,16
8c3a30dd7261f876c8dd211fde049a1e8dfbe8eb,"The stress response is subserved by the stress system, which is located both in the central nervous system and the periphery. The principal effectors of the stress system include corticotropin-releasing hormone (CRH); arginine vasopressin; the proopiomelanocortin-derived peptides alpha-melanocyte-stimulating hormone and beta-endorphin, the glucocorticoids; and the catecholamines norepinephrine and epinephrine. Appropriate responsiveness of the stress system to stressors is a crucial prerequisite for a sense of well-being, adequate performance of tasks, and positive social interactions. By contrast, inappropriate responsiveness of the stress system may impair growth and development and may account for a number of endocrine, metabolic, autoimmune, and psychiatric disorders. The development and severity of these conditions primarily depend on the genetic vulnerability of the individual, the exposure to adverse environmental factors, and the timing of the stressful events, given that prenatal life, infancy, childhood, and adolescence are critical periods characterized by increased vulnerability to stressors.",2005,111,1545,104,14,47,57,74,79,81,83,90,105,107
a1c6862fd3f82ffb6e4245eae37b8e139b87675d,"This report presents an algorithm to assist primary care physicians, endocrinologists, and others in the management of adult, nonpregnant patients with type 2 diabetes mellitus. In order to minimize the risk of diabetes-related complications, the goal of therapy is to achieve a hemoglobin A1c (A1C) of 6.5% or less, with recognition of the need for individualization to minimize the risks of hypoglycemia. We provide therapeutic pathways stratified on the basis of current levels of A1C, whether the patient is receiving treatment or is drug naïve. We consider monotherapy, dual therapy, and triple therapy, including 8 major classes of medications (biguanides, dipeptidyl-peptidase-4 inhibitors, incretin mimetics, thiazolidinediones, alpha-glucosidase inhibitors, sulfonylureas, meglitinides, and bile acid sequestrants) and insulin therapy (basal, premixed, and multiple daily injections), with or without orally administered medications. We prioritize choices of medications according to safety, risk of hypoglycemia, efficacy, simplicity, anticipated degree of patient adherence, and cost of medications. We recommend only combinations of medications approved by the US Food and Drug Administration that provide complementary mechanisms of action. It is essential to monitor therapy with A1C and self-monitoring of blood glucose and to adjust or advance therapy frequently (every 2 to 3 months) if the appropriate goal for each patient has not been achieved. We provide a flow-chart and table summarizing the major considerations. This algorithm represents a consensus of 14 highly experienced clinicians, clinical researchers, practitioners, and academicians and is based on the American Association of Clinical Endocrinologists/American College of Endocrinology Diabetes Guidelines and the recent medical literature.",2009,62,949,76,10,121,187,186,163,106,48,46,27,20
c1349b15b483c465952cc775f0569b7d2cbe173a,"BACKGROUND
Weight loss is associated with short-term amelioration and prevention of metabolic and cardiovascular risk, but whether these benefits persist over time is unknown.


METHODS
The prospective, controlled Swedish Obese Subjects Study involved obese subjects who underwent gastric surgery and contemporaneously matched, conventionally treated obese control subjects. We now report follow-up data for subjects (mean age, 48 years; mean body-mass index, 41) who had been enrolled for at least 2 years (4047 subjects) or 10 years (1703 subjects) before the analysis (January 1, 2004). The follow-up rate for laboratory examinations was 86.6 percent at 2 years and 74.5 percent at 10 years.


RESULTS
After two years, the weight had increased by 0.1 percent in the control group and had decreased by 23.4 percent in the surgery group (P<0.001). After 10 years, the weight had increased by 1.6 percent and decreased by 16.1 percent, respectively (P<0.001). Energy intake was lower and the proportion of physically active subjects higher in the surgery group than in the control group throughout the observation period. Two- and 10-year rates of recovery from diabetes, hypertriglyceridemia, low levels of high-density lipoprotein cholesterol, hypertension, and hyperuricemia were more favorable in the surgery group than in the control group, whereas recovery from hypercholesterolemia did not differ between the groups. The surgery group had lower 2- and 10-year incidence rates of diabetes, hypertriglyceridemia, and hyperuricemia than the control group; differences between the groups in the incidence of hypercholesterolemia and hypertension were undetectable.


CONCLUSIONS
As compared with conventional therapy, bariatric surgery appears to be a viable option for the treatment of severe obesity, resulting in long-term weight loss, improved lifestyle, and, except for hypercholesterolemia, amelioration in risk factors that were elevated at baseline.",2004,38,3082,123,2,47,108,161,177,204,239,257,298,313
09f8cbbf117b18e2f6a52de427cea556c2932549,"Patients With Peripheral Arterial Disease (Lower Extremity, Renal, Mesenteric, and Abdominal Aortic) A Collaborative Report from the American Association for Vascular Surgery/Society for Vascular Surgery,* Society for Cardiovascular Angiography and Interventions, Society for Vascular Medicine and Biology, Society of Interventional Radiology, and the ACC/AHA Task Force on Practice Guidelines (Writing Committee to Develop Guidelines for the Management of Patients With Peripheral Arterial Disease) Endorsed by the American Association of Cardiovascular and Pulmonary Rehabilitation; National Heart, Lung, and Blood Institute; Society for Vascular Nursing; TransAtlantic Inter-Society Consensus; and Vascular Disease Foundation",2006,1392,3158,32,47,147,178,206,249,303,285,301,324,251
368c3191fead736da4aff26ef28bcbf030cf1646,,1975,0,3189,112,0,1,11,12,12,11,24,14,26,32
a7da090fdb85b79cd8a52aadc9ff4715814656e5,"One of the problems which has plagued thouse attempting to predict the behavior of capital marcets is the absence of a body of positive of microeconomic theory dealing with conditions of risk/ Althuogh many usefull insights can be obtaine from the traditional model of investment under conditions of certainty, the pervasive influense of risk in finansial transactions has forced those working in this area to adobt models of price behavior which are little more than assertions. A typical classroom explanation of the determinationof capital asset prices, for example, usually begins with a carefull and relatively rigorous description of the process through which individuals preferences and phisical relationship to determine an equilibrium pure interest rate. This is generally followed by the assertion that somehow a market risk-premium is also determined, with the prices of asset adjusting accordingly to account for differences of their risk.",1964,2,17476,1038,0,0,0,0,0,0,0,0,0,0
a95e6d3280e2db04563556fa576fa55465b7a317,"This paper tests the relationship between average return and risk for New York Stock Exchange common stocks. The theoretical basis of the tests is the ""two-parameter"" portfolio model and models of market equilibrium derived from the two-parameter portfolio model. We cannot reject the hypothesis of these models that the pricing of common stocks reflects the attempts of risk-averse investors to hold portfolios that are ""efficient"" in terms of expected value and dispersion of return. Moreover, the observed ""fair game"" properties of the coefficients and residuals of the risk-return regressions are consistent with an ""efficient capital market""--that is, a market where prices of securities",1973,38,13627,1396,0,0,0,0,0,0,0,0,0,0
b470ef87fcf834e0e90bf65b497732cef2376063,"Nose has modified Newtonian dynamics so as to reproduce both the canonical and the isothermal-isobaric probability densities in the phase space of an N-body system. He did this by scaling time (with s) and distance (with V/sup 1/D/ in D dimensions) through Lagrangian equations of motion. The dynamical equations describe the evolution of these two scaling variables and their two conjugate momenta p/sub s/ and p/sub v/. Here we develop a slightly different set of equations, free of time scaling. We find the dynamical steady-state probability density in an extended phase space with variables x, p/sub x/, V, epsilon-dot, and zeta, where the x are reduced distances and the two variables epsilon-dot and zeta act as thermodynamic friction coefficients. We find that these friction coefficients have Gaussian distributions. From the distributions the extent of small-system non-Newtonian behavior can be estimated. We illustrate the dynamical equations by considering their application to the simplest possible case, a one-dimensional classical harmonic oscillator.",1985,0,13406,297,0,0,0,0,0,0,0,0,0,0
cd7fb9e476e7002d5649309661a06c8688058f49,"This paper develops techniques for empirically analyzing demand and supply in differentiated product markets and then applies these techniques to the U.S. automobile industry. The authors' framework enables one to obtain estimates of demand and cost parameters for a class of oligopolistic differentiated products markets. These estimates can be obtained using only widely available product-level and aggregate consumer-level data, and they are consistent with a structural model of equilibrium in an oligopolistic industry. Applying these techniques, the authors obtain parameters for essentially all autos sold over a twenty-year period. Copyright 1995 by The Econometric Society.",1995,53,4984,681,5,9,23,23,26,49,62,83,102,116
0a577ebd728080640ba1f6da20e99cf6e9526c8e,Focuses on a study which examined perfect equilibrium in a bargaining model. Overview of the strategic approach adopted for the study; Details of the bargaining situation used; Discussion on perfect equilibrium. (From Ebsco),1982,22,5275,344,1,7,6,17,20,35,43,52,51,52
f2b954dab9db5e74d7000b6610378c2523119290,"If looking for a ebook by S. R. de Groot and P. Mazur Non-Equilibrium Thermodynamics in pdf form, in that case you come on to the correct site. We furnish full release of this book in ePub, DjVu, txt, doc, PDF formats. You may read by S. R. de Groot and P. Mazur online Non-Equilibrium Thermodynamics or download. In addition, on our website you may reading instructions and diverse artistic eBooks online, either load theirs. We like attract consideration what our site does not store the eBook itself, but we grant ref to website whereat you may load either reading online. So if have necessity to download pdf Non-Equilibrium Thermodynamics by S. R. de Groot and P. Mazur, then you have come on to faithful website. We have Non-Equilibrium Thermodynamics ePub, txt, PDF, DjVu, doc formats. We will be glad if you will be back to us anew.",1963,0,5809,332,6,8,24,31,33,40,46,45,42,50
f9a8f37fa587f4a50dceb4d6a4176ae424e8099f,"Abstract The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case.",1977,14,6023,619,1,4,3,9,2,3,5,10,7,9
f0d65b8633f39cbc40cd482c1a394726c8763fb3,"THE SPHERE of model financial economics encompasses finance, micro investment theory and much of the economics of uncertainty. As is evident from its influence on other branches of economics including public finance, industrial organization and monetary theory, the boundaries of this sphere are both permeable and flexible. The complex interactions of time and uncertainty guarantee intellectual challenge and intrinsic excitement to the study of financial economics. Indeed, the mathematics of the subject contain some of the most interesting applications of probability and optimization theory. But for all its mathematical refinement, the research has nevertheless had a direct and significant influence on practice. ’ It was not always thus. Thirty years ago, finance theory was little more than a collection of anecdotes, rules of thumb, and manipulations of accounting data with an almost exclusive focus on corporate financial management. There is no need in this meeting of the guild to recount the subsequent evolution from this conceptual potpourri to a rigorous economic theory subjected to systematic empirical examination? Nor is there a need on this occasion to document the wide-ranging impact of the research on finance practice.2 I simply note that the conjoining of intrinsic intellectual interest with extrinsic application is a prevailing theme of research in financial economics. The later stages of this successful evolution have however been marked by a substantial accumulation of empirical anomalies; discoveries of theoretical inconsistencies; and a well-founded concern about the statistical power of many of the test methodologies.3 Finance thus finds itself today in the seemingly-paradoxical position of having more questions and empirical puzzles than at the start of its",1987,46,5404,473,1,4,7,11,8,9,7,12,13,26
e3317b468c1b9a0d5d801ad11f12d3bffa6364af,Part 1 Unemployment in the model of balanced growth: the labour market long-run equilibrium and balanced growth adjustment dynamics. Part 2 further ananlysis of the labour market: search intensity and job advertising.,1990,145,3626,342,0,2,7,14,19,20,25,39,50,71
87e8816e5520a37849e368a0770219452715da18,"One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …",1950,3,6514,390,0,1,2,1,1,1,1,1,1,2
bb484ca72e7e04f643014d0f1be8070af2e0f031,"Involuntary unemployment appears to be a persistent feature of many modern labor markets. The presence of such unemployment raises the question of why wages do not fall to clear labor markets. In this paper we show how the information structure of employer-employee relationships, in particular the inability of employers to costlessly observe workers' on-the-job effort, can explain involuntary unemployment as an equilibrium phenomenon. Indeed, we show that imperfect monitoring necessitates unemployment in equilibrium. The intuition behind our result is simple. Under the conventional competitive paradigm, in which all workers receive the market wage and there is no unemployment, the worst that can happen to a worker who shirks on the job is that he is fired. Since he can immediately be rehired, however, he pays no penalty for his misdemeanor. With imperfect monitoring and full employment, therefore, workers will choose to shirk. To induce its workers not to shirk, the firm attempts to pay more than the “going wage”; then, if a worker is caught shirking and is fired, he will pay a penalty. If it pays one firm to raise its wage, however, it will pay all firms to raise their wages. When they all raise their wages, the incentive not to shirk again disappears. But as all firms raise their wages, their demand for labor decreases, and unemployment results. With unemployment, even if all firms pay the same wages, a worker has an incentive not to shirk.",1984,11,4900,215,3,17,23,33,40,45,54,66,51,56
e937fc6b51ab16bfdb3d7cde90a13c7e12e2c641,"A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game. Introduction L. Walras [ 24 ] first formulated the state of the economic system at any point of time as the solution of a system of simultaneous equations representing the demand for goods by consumers, the supply of goods by producers, and the equilibrium condition that supply equal demand on every market. It was assumed that each consumer acts so as to maximize his utility, each producer acts so as to maximize his profit, and perfect competition prevails, in the sense that each producer and consumer regards the prices paid and received as independent of his own choices. Walras did not, however, give any conclusive arguments to show that the equations, as given, have a solution.",1954,29,4256,282,0,0,2,0,3,5,2,2,8,0
5fbba8fcf94492a5902bad624b4f708e50e9c2f1,"A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q0 and frequency ω0 of the instability. Type Is systems (ω0=0, q0≠0) are stationary in time and periodic in space; type IIIo systems (ω0≠0, q0=0) are periodic in time and uniform in space; and type Io systems (ω0≠0, q0≠0) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via ""amplitude equations,"" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the ""phase equation"" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing ""real pattern effects"" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying ""temporal chaos,"" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Benard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future.",1993,918,5264,176,11,52,108,123,127,126,126,138,147,142
921471828f08f6bb7bcef8d2685811af0fb0dac7,"A modified Redlich-Kwong equation of state is proposed. Vapor pressures of pure com- pounds can be closely reproduced by assuming the parameter a in the original equation to be tempera- ture-dependent. With the introduction of the acentric factor as a third parameter, a generalized correla- tion for the modified parameter can be derived. It applies to all nonpolar compounds. With the application of the original generalized mixing rules, the proposed equation can be extended successfully to multicomponent-VLE calculations, for mixtures of nonpolar substances, with the exclusion of carbon dioxide. Less accurate results are obtained for hydrogen-containing mixtures.",1972,6,4598,258,0,1,1,1,7,9,14,8,13,11
12982242bcceff2de817f51fe1ccbcc0cbb926f8,,1969,0,4822,213,0,1,0,6,11,7,8,18,17,16
8692d38cdef40d3ec18e9e8a9b18743b768d039d,"This paper argues that the textbook search and matching model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies in response to shocks of a plausible magnitude. In the United States, the standard deviation of the vacancy-unemployment ratio is almost 20 times as large as the standard deviation of average labor productivity, while the search model predicts that the two variables should have nearly the same volatility. A shock that changes average labor productivity primarily alters the present value of wages, generating only a small movement along a downward-sloping Beveridge curve (unemployment-vacancy locus). A shock to the separation rate generates a counterfactually positive correlation between unemployment and vacancies. In both cases, the model exhibits virtually no propagation.",2005,85,2653,700,47,98,119,107,150,165,207,185,187,209
eda646f4a2d46adf404790e94276ca254285ad01,"This paper investigates the properties of a market for risky assets on the basis of a simple model of general equilibrium of exchange, where individual investors seek to maximize preference functions over expected yield and variance of yield on their port- folios. A theory of market risk premiums is outlined, and it is shown that general equilibrium implies the existence of a so-called ""market line,"" relating per dollar expected yield and standard deviation of yield. The concept of price of risk is discussed in terms of the slope of this line.",1966,9,4374,127,0,1,3,6,10,9,17,16,20,21
e9f31a23580f9b88ba34ca671c438839a068de7a,"A dynamic stochastic model for a competitive industry is developed in which entry, exit, and the growth of firms' output and employment is determined. The paper extends long-run industry equilibrium theory to account for entry, exit, and heterogeneity in the size and growth rate of firms. Conditions under which there is entry and exit in the long run are developed. Cross sectional implications and distributions of profits and value of firms are derived. Comparative statics on the equilibrium size distribution and turnover rates are analyzed. Copyright 1992 by The Econometric Society.",1992,1,2914,265,2,2,1,14,9,12,14,29,40,37
faa25a6c7c7b3ecd221e4fc7b7d6dee131e04e04,"Publisher Summary In recent years, the interest in the problem of brittle fracture and, in particular, in the theory of cracks has grown appreciably in connection with various technical applications. Numerous investigations have been carried out, enlarging in essential points the classical concepts of cracks and methods of analysis. The qualitative features of the problems of cracks, associated with their peculiar nonlinearity as revealed in these investigations, makes the theory of cracks stand out distinctly from the whole range of problems in terms of the theory of elasticity. The chapter presents a unified view of the way basic problems in the theory of equilibrium cracks are formulated and discusses the results obtained thereby. The object of the theory of equilibrium cracks is the study of the equilibrium of solids in the presence of cracks. However, there exists a fundamental distinction between these two problems, The form of a cavity undergoes only slight changes even under a considerable variation in the load acting on a body, while the cracks whose surface also constitutes a part of the body boundary can expand even with small increase of the load to which the body is subjected.",1962,80,4296,170,0,1,0,4,7,12,8,10,10,8
3f94a0fe29c97858ae8143e31ec621911865afda,,1972,3,3022,380,4,7,15,8,11,34,30,35,20,28
8979bdafefff9da97342fbc5cf3f511000306f60,Kinetics of Unireactant Enzymes. Simple Inhibition Systems. Rapid Equilibrium Partial and Mixed--Type Inhibition. Enzyme Activation. Rapid Equilibrium Bireactant and Terreactant Systems. Multisite and Allosteric Enzymes. Multiple Inhibition Analysis. Steady--State Kinetics of Multireactant Enzymes. Isotope Exchange. Effects of pH and Temperature. Appendix. Index.,1975,0,3084,215,0,2,10,10,12,13,13,17,20,19
4ef2492426f6eb90191d9c411b94f5d686f2e618,"Parts I and II deal with the theory of crystal growth, parts III and IV with the form (on the atomic scale) of a crystal surface in equilibrium with the vapour. In part I we calculate the rate of advance of monomolecular steps (i.e. the edges of incomplete monomolecular layers of the crystal) as a function of supersaturation in the vapour and the mean concentration of kinks in the steps. We show that in most cases of growth from the vapour the rate of advance of monomolecular steps will be independent of their crystallographic orientation, so that a growing closed step will be circular. We also find the rate of advance for parallel sequences of steps. In part II we find the resulting rate of growth and the steepness of the growth cones or growth pyramids when the persistence of steps is due to the presence of dislocations. The cases in which several or many dislocations are involved are analysed in some detail; it is shown that they will commonly differ little from the case of a single dislocation. The rate of growth of a surface containing dislocations is shown to be proportional to the square of the supersaturation for low values and to the first power for high values of the latter. Volmer & Schultze’s (1931) observations on the rate of growth of iodine crystals from the vapour can be explained in this way. The application of the same ideas to growth of crystals from solution is briefly discussed. Part III deals with the equilibrium structure of steps, especially the statistics of kinks in steps, as dependent on temperature, binding energy parameters, and crystallographic orientation. The shape and size of a two-dimensional nucleus (i.e. an ‘island* of new monolayer of crystal on a completed layer) in unstable equilibrium with a given supersaturation at a given temperature is obtained, whence a corrected activation energy for two-dimensional nucleation is evaluated. At moderately low supersaturations this is so large that a crystal would have no observable growth rate. For a crystal face containing two screw dislocations of opposite sense, joined by a step, the activation energy is still very large when their distance apart is less than the diameter of the corresponding critical nucleus; but for any greater separation it is zero. Part IV treats as a ‘co-operative phenomenon’ the temperature dependence of the structure of the surface of a perfect crystal, free from steps at absolute zero. It is shown that such a surface remains practically flat (save for single adsorbed molecules and vacant surface sites) until a transition temperature is reached, at which the roughness of the surface increases very rapidly (‘surface melting’). Assuming that the molecules in the surface are all in one or other of two levels, the results of Onsager (1944) for two-dimensional ferromagnets can be applied with little change. The transition temperature is of the order of, or higher than, the melting-point for crystal faces with nearest neighbour interactions in both directions (e.g. (100) faces of simple cubic or (111) or (100) faces of face-centred cubic crystals). When the interactions are of second nearest neighbour type in one direction (e.g. (110) faces of s.c. or f.c.c. crystals), the transition temperature is lower and corresponds to a surface melting of second nearest neighbour bonds. The error introduced by the assumed restriction to two available levels is investigated by a generalization of Bethe’s method (1935) to larger numbers of levels. This method gives an anomalous result for the two-level problem. The calculated transition temperature decreases substantially on going from two to three levels, but remains practically the same for larger numbers.",1951,0,4106,92,1,7,8,6,3,8,10,12,6,7
b56d311ae4f97b1248344ad8a891b8b59af08273,"We explore the determinants of liquidation values of assets, particularly focusing on the potential buyers of assets. When a firm in financial distress needs to sell assets, its industry peers are likely to be experiencing problems themselves, leading to asset sales at prices below value in best use. Such illiquidity makes assets cheap in bad times, and so ex ante is a significant private cost of leverage. We use this focus on asset buyers to explain variation in debt capacity across industries and over the business cycle, as well as the rise in U.S. corporate leverage in the 1980s.",1992,38,2859,173,1,4,19,19,16,22,30,33,49,50
9ead28b73dc3c2329ee2db668f288636b2870196,"It is shown that for allele frequency data a useful measure of the extent of gene flow between a pair of populations is M∘=(1/FST‐1)/4 , which is the estimated level of gene flow in an island model at equilibrium. For DNA sequence data, the same formula can be used if FST is replaced by NST. In a population with restricted dispersal, analytic theory shows that there is a simple relationship between M̂ and geographic distance in both equilibrium and non‐equilibrium populations and that this relationship is approximately independent of mutation rate when the mutation rate is small. Simulation results show that with reasonable sample sizes, isolation by distance can indeed be detected and that, at least in some cases, non‐equilibrium patterns can be distinguished. This approach to analyzing isolation by distance is used for two allozyme data sets, one from gulls and one from pocket gophers.",1993,31,2425,279,0,13,24,51,52,66,64,66,88,84
46fe07c5695229769631ca693658a82da6e9395d,"The concept of a perfect equilibrium point has been introduced in order to exclude the possibility that disequilibrium behavior is prescribed on unreached subgames. (Selten 1965 and 1973). Unfortunately this definition of perfectness does not remove all difficulties which may arise with respect to unreached parts of the game. It is necessary to reexamine the problem of defining a satisfactory non-cooperative equilibrium concept for games in extensive form. Therefore a new concept of a perfect equilibrium point will be introduced in this paper. In retrospect the earlier use of the word ""perfect"" was premature. Therefore a perfect equilibrium point in the old Sense will be called ""subgame perfect"". The new definition of perfectness has the property that a perfect equilibrium point is always subgame perfect but a subgame perfect equilibrium point may not be perfect. It will be shown that every finite extensive game with perfect recall has at least one perfect equilibrium point. Since subgame perfectness cannot be detected in the normal form, it is clear that for the purpose of the investigation of the problem of perfectness, the normal form is an inadequate representation of the extensive form. It will be convenient to introduce an ""agent normal form"" as a more adequate representation of games with perfect recall.",1975,5,3173,158,0,3,4,5,5,13,16,25,20,33
85d493e7b86d5f39e47926b8d52d895bd67a8ff1,"The different roles the attractive and repulsive forces play in forming the equilibrium structure of a Lennard‐Jones liquid are discussed. It is found that the effects of these forces are most easily separated by considering the structure factor (or equivalently, the Fourier transform of the pair‐correlation function) rather than the pair‐correlation function itself. At intermediate and large wave vectors, the repulsive forces dominate the quantitative behavior of the liquid structure factor. The attractions are manifested primarily in the small wave vector part of the structure factor; but this effect decreases as the density increases and is almost negligible at reduced densities higher than 0.65. These conclusions are established by considering the structure factor of a hypothetical reference system in which the intermolecular forces are entirely repulsive and identical to the repulsive forces in a Lennard‐Jones fluid. This reference system structure factor is calculated with the aid of a simple but accurate approximation described herein. The conclusions lead to a very simple prescription for calculating the radial distribution function of dense liquids which is more accurate than that obtained by any previously reported theory. The thermodynamic ramifications of the conclusions are presented in the form of calculations of the free energy, the internal energy (from the energy equation), and the pressure (from the virial equation). The implications of our conclusions to perturbation theories for liquids and to the interpretation of x‐ray scattering experiments are discussed.",1971,18,3574,40,4,11,15,16,20,24,39,17,18,25
f66608476a3cf3c6147823f55fc0ba235234175f,Computer program is described for numerical solution of chemical equilibria in complex systems by using nonlinear algebraic equations. Free-energy minimization technique is used.,1972,37,2940,194,4,5,8,6,10,10,12,19,23,26
879e8d7778c0ab1479339fe29d3cc4ded78fe4e5,"The probability of a configuration is given in classical theory by the Boltzmann formula exp [— V/hT] where V is the potential energy of this configuration. For high temperatures this of course also holds in quantum theory. For lower temperatures, however, a correction term has to be introduced, which can be developed into a power series of h. The formula is developed for this correction by means of a probability function and the result discussed.",1932,0,5250,45,0,0,0,0,1,0,1,1,0,0
9ba558e613372642d3c24357e21f76835859e709,,2007,0,2011,175,101,114,99,112,128,147,143,155,133,157
dca0630c63a5403a8383b4a2abe7f94a28a23eb7,Gibbs Measures.- General Thermodynamic Formalism.- Axiom a Diffeomorphisms.- Ergodic Theory of Axiom a Diffeomorphisms.,1975,43,2432,224,0,1,7,7,6,7,6,18,15,13
87a2272cc9a9abe4556509600073032fa6f01c33,The published experimental data of Hansson and of Mehrbach et al. have been critically compared after adjustment to a common pH scale based upon total hydrogen ion concentration. No significant systematic differences are found within the overall experimental error of the data. The results have been pooled to yield reliable equations that can be used to estimate pK1∗and pK2∗ for seawater media a salinities from 0 to 40 and at temperatures from 2 to 35°C.,1987,16,2628,123,0,1,1,4,4,4,17,5,12,10
2943f194efb591a1231ecea27e1af8a4b4c50827,,1994,0,2145,188,1,4,4,6,10,6,15,13,23,26
7bc19372cdca52dc43c2e8d5a2111161804ef865,"AbstractA number of experiments have been conducted in order to study the equilibria between olivine and basaltic liquids and to try and understand the conditions under which olivine will crystallize. These experiments were conducted with several basaltic compositions over a range of temperature (1150–1300° C) and oxygen fugacity (10−0.68–10−12 atm.) at one atmosphere total pressure. The phases in these experimental runs were analyzed with the electron microprobe and a number of empirical equations relating the composition of olivine and liquid were determined. The distribution coefficient 1
$$K_D = \frac{{(X_{{\text{FeO}}}^{{\text{Ol}}} )}}{{(X_{{\text{FeO}}}^{{\text{Liq}}} )}}\frac{{(X_{{\text{MgO}}}^{{\text{Liq}}} )}}{{(X_{{\text{MgO}}}^{{\text{Ol}}} )}}$$
 relating the partioning of iron and magnesium between olivine and liquid is equal to 0.30 and is independent of temperature. This means that the composition of olivine can be used to determine the magnesium to ferrous iron ratio of the liquid from which it crystallized and conversely to predict the olivine composition which would crystallize from a liquid having a particular magnesium to ferrous iron ratio.A model (saturation surface) is presented which can be used to estimate the effective solubility of olivine in basaltic melts as a function of temperature. This model is useful in predicting the temperature at which olivine and a liquid of a particular composition can coexist at equilibrium.",1970,30,2408,303,0,0,7,19,10,18,19,16,24,26
edf2bdb75e01b30d1f9d165738303081b6f630c3,"Following a recession, the aggregate labor market is slack-employment remains below normal and recruiting efforts of employers, as measured by help-wanted advertising and vacancies, are low. A model of matching friction explains the qualitative responses of the labor market to adverse shocks, but requires implausibly large shocks to account for the magnitude of observed fluctuations. The incorporation of wage stickiness vastly increases the sensitivity of the model to driving forces. I develop a new model of the way that wage stickiness affects unemployment. The stickiness arises in an economic equilibrium and satisfies the condition that no worker-employer pair has an unexploited opportunity for mutual improvement. Sticky wages neither interfere with the efficient formation of employment matches nor cause inefficient job loss. Thus the model provides an answer to the fundamental criticism previously directed at sticky-wage models of fluctuations.",2005,49,1454,273,36,56,88,85,104,102,121,103,92,90
310beac23e27b4d0689bd11338e459d03a1dc3c0,"Recently, a number of authors have argued that the standard search model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies, given shocks of a plausible magnitude. We use data on the cost of vacancy creation and cyclicality of wages to identify the two key parameters of the model - the value of non-market activity and the bargaining weights. Our calibration implies that the model is, in fact, consistent with the data.",2008,169,1197,287,94,61,85,115,85,81,78,91,82,69
f0f7e780651f1d6189b6c772e5ca0e2355e182d8,"A modified conjugate gradient algorithm for geometry optimization is outlined for use with ab initio MO methods. Since the computation time for analytical energy gradients is approximately the same as for the energy, the optimization algorithm evaluates and utilizes the gradients each time the energy is computed. The second derivative matrix, rather than its inverse, is updated employing the gradients. At each step, a one‐dimensional minimization using a quartic polynomial is carried out, followed by an n‐dimensional search using the second derivative matrix. By suitably controlling the number of negative eigenvalues of the second derivative matrix, the algorithm can also be used to locate transition structures. Representative timing data for optimizations of equilibrium geometries and transition structures are reported for ab initio SCF–MO calculations.",1982,92,2749,12,3,5,22,19,26,23,37,33,19,31
ac81ede712252992b364566fc2f44637e3b0eab3,An improved metal stamped and formed screw is disclosed. The subject screw is stamped and formed from continuous web of metal stock to form a plurality of screws joined by a carrier strip. The thus formed strip of screws can be machine applied to prebored holes and manually withdrawn therefrom and reapplied by conventional means.,1981,21,2816,35,0,2,4,3,2,5,5,4,10,13
98fe960b3f5354a987e33e62ab1de060a42ec9ec,"Economic theorists traditionally banish discussions of information to footnotes. Serious consideration of costs of communication, imperfect knowledge, and the like would, it is believed, complicate without informing. This paper, which analyzes competitive markets in which the characteristics of the commodities exchanged are not fully known to at least one of the parties to the transaction, suggests that this comforting myth is false. Some of the most important conclusions of economic theory are not robust to considerations of imperfect information.",1976,21,2530,91,1,7,8,11,7,15,23,24,26,22
c4db548b1437a0a564468889c4aaf5b0ae53aeb3,"A new suite of 10 programs concerned with equilibrium constants and solution equilibria is described. The suite includes data preparation programs, pretreatment programs, equilibrium constant refinement and post-run analysis. Data preparation is facilitated by a customized data editor. The pretreatment programs include manual trial and error data fitting, speciation diagrams, end-point determination, absorbance error determination, spectral baseline corrections, factor analysis and determination of molar absorbance spectra. Equilibrium constants can be determined from potentiometric data and/or spectrophotometric data. A new data structure is also described in which information on the model and on experimental measurements are kept in separate files.",1996,59,2322,32,0,2,15,23,18,30,28,41,55,54
52524271c3298b5541c4346b18f5e07ac6c4590e,"Research on how organizational systems develop and change is shaped, at every level of analysis, by traditional assumptions about how change works. New theories in several fields are challenging some of the most pervasive of these assumptions, by conceptualizing change as a punctuated equilibrium: an alternation between long periods when stable infrastructures permit only incremental adaptations, and brief periods of revolutionary upheaval. This article compares models from six domains—adult, group, and organizational development, history of science, biological evolution, and physical science—to explicate the punctuated equilibrium paradigm and show its broad applicability for organizational studies. Models are juxtaposed to generate new research questions about revolutionary change in organizational settings: how it is triggered, how systems function during such periods, and how it concludes. The article closes with implications for research and theory.",1991,59,2030,152,3,11,16,26,26,28,26,33,43,56
04228cd8a236c84355c75eeee0fc50203d15776b,"Prior to elections, governments (at all levels) frequently undertake a consumption binge. Taxes are cut, transfers are raised, and government spending is distorted towards highly visible items. The ""political business cycle"" (better be thought of as ""the political budget cycle"") has been intensively examined, at least for the case of national elections. A number of proposals have been advanced for mitigating electoral cycles in fiscal policy. The present paper is the first effort to provide a fully-specified equilibrium framework for analyzing such proposals. A political budget cycle arises here via a multidimensional signaling process, in which incumbent leaders try to convince voters that they have recently been doing an excellent job in administering the government. Efforts to mitigate the cycle can easily prove counterproductive, either by impeding the transmission of information or by inducing politicians to select more costly ways of signaling. The model also indicates new directions for empirical research.",1987,34,1976,128,1,2,6,6,8,13,13,9,17,18
7a58a1995f7fcd05d8820dabf25fd42c53cb422e,"Abstract A suite of divalent metal (Ca, Cd, Ba) carbonates was synthesized over the temperature range 10–40°C by the classical method of slowly bubbling N 2 through a bicarbonate solution. It was discovered that carbonates could be precipitated reproducibly in or out of isotopic equilibrium with the environmental solution by varying the concentrations of bicarbonate and cation. Precipitation rate had little or no influence on the isotopic composition of the product. Relatively high initial concentrations of up to 25 mM in both bicarbonate and cation were prepared by adding solid metal chlorides to solutions of NaHC0 3 . On the basis of results of equilibrium experiments and a new determination of the acid fractionation factor, a new expression is proposed for the oxygen isotope fractionation between calcite and water at low temperatures: 10001nα(Calcite-H 2 O) = 18.03(10 3 T −1 ) − 32.42 where α is the fractionation factor, and T is in kelvins. Combining new data for low-temperature precipitations and the high-temperature equilibrium fractionations published by O'Neil et al. (1969) results in a revised expression for the oxygen isotope fractionation between octavite (CdCO 3 ) and water from 0° to 500°C: 10001nα(CdC0P 3 H 2 O) = 2.7 6 (10 6 T −2 ) − 3.96 The ability to produce nonequilibrium carbonates allowed assessment to be made, for the first time, of the temperature dependence of nonequilibrium stable isotope fractionations in mineral systems. The temperature coefficients of a(carbonate-water) for nonequilibrium divalent metal carbonates are greater than those for equilibrium carbonates, a finding that may bear on the interpretation of analyses of biogenic carbonates forming out of isotopic equilibrium in nature. New determinations of acid fractionation factors (10001nα) at 25°C for calcite (10.44 − 0.10), aragonite (11.01 ± 0.01), and witherite (10.57 − 0.16) are mildly to strongly different from those published by Sharma and Clayton (1965) and point to a control on this fractionation by some physical property of the mineral. Reproducible values for octavite (CdC0 3 ) varied from 11.18 to 13.60 depending on the conditions of preparation of the carbonate. These new values need to be considered in determinations of absolute 18 80 16 60 ratios of international reference standards and in relating analyses of carbonates to those of waters, silicates, and oxides.",1997,25,1954,149,0,5,15,12,20,32,29,39,52,49
8a83305f83a86f7f1ca9fa5e15b22f42dade1a0a,"The authors study a rich class of noncooperative games that includes models of oligopoly competition, macroeconomic coordination failures, arms races, bank runs, technology adoption and diffusion, R&D competition, pretrial bargaining, coordination in teams, and many others. For all these games, the sets of pure strategy Nash equilibria, correlated equilibria, and rationalizable strategies have identical bounds. Also, for a class of models of dynamic adaptive choice behavior that encompasses both best-response dynamics and Bayesian learning, the players' choices lie eventually within the same bounds. These bounds are shown to vary monotonically with certain exogenous parameters. Copyright 1990 by The Econometric Society.",1990,55,1821,203,2,7,10,8,14,17,21,25,26,32
f3ca3e173ea3cfa0f8a74e4c68970e8a281d95eb,,1985,97,1930,151,0,3,7,4,7,5,9,7,12,13
3e3dcdc85b74c1b8c494292cef9eb9f893785718,"This paper develops a continuous time general equilibrium model of a simple but complete economy and uses it to examine the behavior of asset prices. In this model, asset prices and their stochastic properties are determined endogenously. One principal result is a partial differential equation which asset prices must satisfy. The solution of this equation gives the equilibrium price of any asset in terms of the underlying real variables in the economy. IN THIS PAPER, we develop a general equilibrium asset pricing model for use in applied research. An important feature of the model is its integration of real and financial markets. Among other things, the model endogenously determines the stochastic process followed by the equilibrium price of any financial asset and shows how this process depends on the underlying real variables. The model is fully consistent with rational expectations and maximizing behavior on the part of all agents. Our framework is general enough to include many of the fundamental forces affecting asset markets, yet it is tractable enough to be specialized easily to produce specific testable results. Furthermore, the model can be extended in a number of straightforward ways. Consequently, it is well suited to a wide variety of applications. For example, in a companion paper, Cox, Ingersoll, and Ross [7], we use the model to develop a theory of the term structure of interest rates. Many studies have been concerned with various aspects of asset pricing under uncertainty. The most relevant to our work are the important papers on intertemporal asset pricing by Merton [19] and Lucas [16]. Working in a continuous time framework, Merton derives a relationship among the equilibrium expected rates of return on assets. He shows that when investment opportunities are changing randomly over time this relationship will include effects which have no analogue in a static one period model. Lucas considers an economy with homogeneous individuals and a single consumption good which is produced by a number of processes. The random output of these processes is exogenously determined and perishable. Assets are defined as claims to all or a part of the output of a process, and the equilibrium determines the asset prices. Our theory draws on some elements of both of these papers. Like Merton, we formulate our model in continuous time and make full use of the analytical tractability that this affords. The economic structure of our model is somewhat similar to that of Lucas. However, we include both endogenous production and",1985,27,1986,103,4,12,21,24,35,35,37,32,37,29
828be060c75cd9e01f5b2c6aa48508b06c856239,"This paper develops and estimates a dynamic stochastic general equilibrium (DSGE) model with sticky prices and wages for the euro area. The model incorporates various other features such as habit formation, costs of adjustment in capital accumulation and variable capacity utilisation. It is estimated with Bayesian techniques using seven key macro-economic variables: GDP, consumption, investment, prices, real wages, employment and the nominal interest rate. The introduction of ten orthogonal structural shocks (including productivity, labour supply, investment, preference, cost-push and monetary policy shocks) allows for an empirical investigation of the effects of such shocks and of their contribution to business cycle fluctuations in the euro area. Using the estimated model, the paper also analyses the output (real interest rate) gap, defined as the difference between the actual and model-based potential output (real interest rate).",2002,92,1329,239,3,10,37,44,50,72,85,98,118,105
7ceae624bd1d65ccf66e8448a8f902c2415ce62c,"An analysis of the quantitative effects of agency costs in a real business cycle model, showing that these costs can explain why output growth displays positive autocorrelation at short horizons.",1998,32,1487,219,6,9,13,21,31,39,33,34,50,52
a63da98b44996c5c484612bc754205f975c96467,"This book provides a solid foundation and an extensive study for an important class of constrained optimization problems known as Mathematical Programs with Equilibrium Constraints (MPEC), which are extensions of bilevel optimization problems. The book begins with the description of many source problems arising from engineering and economics that are amenable to treatment by the MPEC methodology. Error bounds and parametric analysis are the main tools to establish a theory of exact penalisation, a set of MPEC constraint qualifications and the first-order and second-order optimality conditions. The book also describes several iterative algorithms such as a penalty-based interior point algorithm, an implicit programming algorithm and a piecewise sequential quadratic programming algorithm for MPECs. Results in the book are expected to have significant impacts in such disciplines as engineering design, economics and game equilibria, and transportation planning, within all of which MPEC has a central role to play in the modelling of many practical problems.",1996,0,1780,140,5,13,24,26,22,35,38,45,76,86
3a8322cc9ce071f1fce97d10687d8a64265d29e8,"The theory of inequality and intergenerational mobility presented in this essay assumes that each family maximizes a utility function spanning several generations. Utility depends on the consumption of parents and on the quantity and quality of their children. The income of children is raised when they receive more human and nonhuman capital from their parents. Their income is also raised by their ""endowment"" of genetically determined race, ability, and other characteristics, family reputation and ""connections,"" and knowledge, skills, and goals provided by their family environment. The fortunes of children are linked to their parents not only through investments but also through these endowments acquired from parents (and other family members). The equilibrium income of children is determined by their market and endowed luck, the own income and endowment of parents, and the two parameters, the degree of inheritability and the propensity to invest in children. If these parameters are both less than unity, the distribution of income between families approaches a stationary distribution. The stationary coefficient of variation is greater, the larger the degree of in-heritability and the smaller the propensity to invest in children. Intergenerational mobility measures the effect of a family on the well-being of its children. We show that the family is more important when the degree of inheritability and the propensity to invest are larger. If both these parameters are less than unity, an increase in family income in one generation has negligible effects on the incomes of much later descendants. However, the incomes of children, grandchildren, and other early descendants could significantly increase; indeed, if the sum of these parameters exceeds unity, the changes in income rise for several generations before falling, and the maximum increase in income could exceed the initial increase.",1979,37,1974,179,1,2,6,6,2,8,3,4,10,5
35cca93034be1f68c585d309bc0de963021b86ea,"Abstract Transport phenomena in spatially periodic systems far from thermal equilibrium are considered. The main emphasis is put on directed transport in so-called Brownian motors (ratchets), i.e. a dissipative dynamics in the presence of thermal noise and some prototypical perturbation that drives the system out of equilibrium without introducing a priori an obvious bias into one or the other direction of motion. Symmetry conditions for the appearance (or not) of directed current, its inversion upon variation of certain parameters, and quantitative theoretical predictions for specific models are reviewed as well as a wide variety of experimental realizations and biological applications, especially the modeling of molecular motors. Extensions include quantum mechanical and collective effects, Hamiltonian ratchets, the influence of spatial disorder, and diffusive transport.",2000,839,1751,51,0,7,31,74,82,122,110,111,98,112
0b787672d66ecd0ecd3c1ba4a0b7a740c842dfae,"ing and summarizing knowledge about range dynamics without distorting it. The amount of detail lost in a particular description would depend on how many states and transitions were recognized. We are proposing the state-and-transition formulation because it is a practicable way to organize information for management, not because it follows from theoretical models about dynamics. In consequence, we consider management rather than theoretical criteria should be used in deciding what states to recognize in a given situation. As a general rule, one would distinguish 2 states only if the difference between them represented an important change in the land from the point of view of management. For example, variation due to seasonal phenology of the plants would not normally be subdivided into states, while important changes in the underlying botanical composition would be recognized. It follows that a given rangeland could be described in terms of a greater or lesser number of states and transitions, depending on the nature and objectives of management and on the state of existing knowledge. There would not be a single correct description. Under the state-and-transition formulation, knowledge about a given rangeland should be organized and expressed in the follow-",1989,51,1875,101,0,6,15,19,21,32,39,40,36,46
0fc5d7098267bab662bdc7f455f6119699f24ee9,"A redundant internal coordinate system for optimizing molecular geometries is constructed from all bonds, all valence angles between bonded atoms, and all dihedral angles between bonded atoms. Redundancies are removed by using the generalized inverse of the G matrix; constraints can be added by using an appropriate projector. For minimizations, redundant internal coordinates provide substantial improvements in optimization efficiency over Cartesian and nonredundant internal coordinates, especially for flexible and polycyclic systems. Transition structure searches are also improved when redundant coordinates are used and when the initial steps are guided by the quadratic synchronous transit approach. © 1996 by John Wiley & Sons, Inc.",1996,24,1941,12,5,11,14,21,19,32,39,36,57,65
e8fcba63671d0c88bcc674b47d62dddea83e427b,This paper studies four classic fiscal-policy experiments within a quantitatively restricted neoclassical model. The authors' main findings are as follows: (1) permanent changes in government purchases can lead to short-run and long-run output multipliers that exceed one; (2) permanent changes in government purchases induce larger effects than temporary changes; (3) the financing decision is quantitatively more important than the resource cost of changes in government purchases; and (4) public investment has dramatic effects on private output and investment. These findings stem from important dynamic interactions of capital and labor absent in earlier equilibrium analyses of fiscal policy. Copyright 1993 by American Economic Association.,1990,0,1607,205,0,3,4,5,10,14,13,21,30,24
f7bb8b20c217c81cb446b0bd9c3f6b187de80b6c,"Deviations from Hardy-Weinberg equilibrium (HWE) can indicate inbreeding, population stratification, and even problems in genotyping. In samples of affected individuals, these deviations can also provide evidence for association. Tests of HWE are commonly performed using a simple chi2 goodness-of-fit test. We show that this chi2 test can have inflated type I error rates, even in relatively large samples (e.g., samples of 1,000 individuals that include approximately 100 copies of the minor allele). On the basis of previous work, we describe exact tests of HWE together with efficient computational methods for their implementation. Our methods adequately control type I error in large and small samples and are computationally efficient. They have been implemented in freely available code that will be useful for quality assessment of genotype data and for the detection of genetic association or population stratification in very large data sets.",2005,21,1322,121,7,22,62,82,113,115,104,105,111,91
06b2234a73c812eb01cc1fbc51b83de14107788d,,1964,0,2591,32,0,0,1,2,1,1,7,5,4,7
3e4e98e430a2ed0abd49f027d4d42bb4f422d299,"This paper considers the locational choice of firms in an upstream and a downstream industry. Both industries are imperfectly competitive, with firms subject to increasing returns. There are transport costs between the two locations. Depending on the level of these costs there may be a single equilibrium with production diversified between locations, or multiple equilibria, some of which involve agglomeration at a single location. Typically the forces for agglomeration are greatest at intermediate levels of transport costs. Reducing these costs from a high to an intermediate level will cause agglomeration and consequent divergence of economic structure and income levels; reducing them to a low level may cause the industries to operate in both locations, bringing convergence of structure and income.",1996,0,1476,113,9,13,37,32,40,48,49,72,95,78
577194f578a9abc404fe1d4e406714c2550d7c8e,"Bamboo, an abundant and inexpensive natural resource in Malaysia was used to prepare activated carbon by physiochemical activation with potassium hydroxide (KOH) and carbon dioxide (CO(2)) as the activating agents at 850 degrees C for 2h. The adsorption equilibrium and kinetics of methylene blue dye on such carbon were then examined at 30 degrees C. Adsorption isotherm of the methylene blue (MB) on the activated carbon was determined and correlated with common isotherm equations. The equilibrium data for methylene blue adsorption well fitted to the Langmuir equation, with maximum monolayer adsorption capacity of 454.2mg/g. Two simplified kinetic models including pseudo-first-order and pseudo-second-order equation were selected to follow the adsorption processes. The adsorption of methylene blue could be best described by the pseudo-second-order equation. The kinetic parameters of this best-fit model were calculated and discussed.",2007,24,1240,37,1,30,39,50,61,65,96,97,113,108
087d26e0fa16877c358764f714afac31de699397,"Equilibrium is analyzed for a simple barter model with identical risk-neutral agents where trade is coordinated by a stochastic matching process. It is shown that there are multiple steady-state rational expectations equilibria, with all non-corner solution equilibria inefficient. This implies that an economy with this type of trade friction does not have a unique natural rate of unemployment.",1982,7,1897,170,1,2,1,5,7,18,12,18,26,17
32916bb78af5e6f15f3a8b87a225c1ee59f7741b,"If it is common knowledge that the players in a game are Bayesian utility maximizers who treat uncertainty about other players' actions like any other uncertainty, then the outcome is necessarily a correlated equilibrium. Random strategies appear as an expression of each player's uncertainty about what the others will do, not as the result of willful randomization. Use is made of the common prior assumption, according to which differences in probability assessments by different individuals are due to the different information that they have (where ""information"" may be interpreted broadly, to include experience, upbringing, and genetic makeup). Copyright 1987 by The Econometric Society.",1987,23,1515,184,5,8,13,26,28,26,15,30,19,23
41590083788b7cd293953d930ebf4dff61772ba6,"The capacity of immunity to control and shape cancer, that is, cancer immunoediting, is the result of three processes that function either independently or in sequence: elimination (cancer immunosurveillance, in which immunity functions as an extrinsic tumour suppressor in naive hosts); equilibrium (expansion of transformed cells is held in check by immunity); and escape (tumour cell variants with dampened immunogenicity or the capacity to attenuate immune responses grow into clinically apparent cancers). Extensive experimental support now exists for the elimination and escape processes because immunodeficient mice develop more carcinogen-induced and spontaneous cancers than wild-type mice, and tumour cells from immunodeficient mice are more immunogenic than those from immunocompetent mice. In contrast, the equilibrium process was inferred largely from clinical observations, including reports of transplantation of undetected (occult) cancer from organ donor into immunosuppressed recipients. Herein we use a mouse model of primary chemical carcinogenesis and demonstrate that equilibrium occurs, is mechanistically distinguishable from elimination and escape, and that neoplastic cells in equilibrium are transformed but proliferate poorly in vivo. We also show that tumour cells in equilibrium are unedited but become edited when they spontaneously escape immune control and grow into clinically apparent tumours. These results reveal that, in addition to destroying tumour cells and sculpting tumour immunogenicity, the immune system of a naive mouse can also restrain cancer growth for extended time periods.",2007,35,1217,27,1,55,69,105,102,89,127,88,78,100
2b21e47e893a307d45f007c9f78ea7e2e6e322dd,"A global game is an incomplete information game where the actual payoff structure is determined by a random draw from a given class of games and where each player makes a noisy observation of the selected game. For 2 x 2 games, it is shown that, when the noise vanishes, iterated elimination of dominated strategies in the global game forces the players to conform to J. C. Harsanyi and R. Selten's risk dominance criterion. Copyright 1993 by The Econometric Society.",1993,8,1588,52,0,0,8,5,14,8,6,8,31,51
3bfc2313fa86a1649763898783937a21a3cf8a4c,"The first acidity scale to be established in a pure solvent other than water was the result of the pioneering work of Conant, Wheland, and McEwen in ether or ben~ene .~ During the past 20 years an ion-pair acidity scale covering an ""effective pKa rangefrom about 15 to 40 has been developed in cyclohexylamine (CHA),6 and similar studies in other low-dielectric-constant solvents including 1,2-dimethoxyethane (DME)7a and tetrahydrofuran (THF)7b*c have been carried out. A more limited ion-pair acidity scale has been developed in liquid NH,.7d Also, during this period, acidity scales have been established in the polar non-hydrogenbond-donor (NHBD) solvents dimethyl sulfoxide (Me$0)8 and N-methylpyrrolidin-2-one (NMP)? which have relatively high dielectric constants. The pK,'s measured in these solvents differ from ion-pair pK,'s in that they are absolute, in the sense that they are based on Me2S0 and NMP as the standard states, which allows direct comparisons to be made with H20 and gas-phase pK,'s. A truly absolute acidity scale has been established in the gas phase, which, for the first time, provides intrinsic measures of structural effects free of solvent effects.1° Our purpose in this Account is (a) to discuss briefly acidities in various solvent media, (b) to present a table of representative equilibrium acidity constants in M e 8 0 solution, and (c) to illustrate ways in which these pK, data can be used. In an accompanying Account we compare acidities in Me2S0 solution with intrinsic gas-phase acidities and discuss some of the insights into solvation effects provided thereby. Acidities in H 2 0 and Me2S0. It is important to recognize that pKa values are solvent dependent. The",1988,72,1780,4,0,5,12,13,15,16,24,23,27,22
187e2d614a0f342d7a6677528c7a4f3e3a0bb0ab,"In 1972 ~ and 19852 the Committee on Hearing and Equilibrium of the American Academy of Otolaryngolog3~-Head and Neck Surgery published recommended guidelines for reporting the results of treatment of Meniere's disease. These reports have proved very beneficial to efforts to understand this disorder and its treatment. With advancing knowledge it has become evident that the reporting guidelines could be refined further. In undertaking this review, the Committee established several guiding principles. We wished to establish a distinction between the recording of results and the analysis and interpretation of results. Insofar as is possible, we wanted to retain and integrate the methods recommended in the 1972 and 1985 reports. We wanted guidelines to be ""upwardly compatible"" in the sense of computer software, so that existing data could not only be conserved but analyzed in new ways. Reporting methods should be clearly stated, straightforward to apply, as simple as possible, and usable in a wide range of settings, from multicenter university studies to reviews of personal experience by individual private practitioners. Specialized test equipment should not be required. Methods should facilitate statistical evaluation and comparison of results among studies. The guidelines should encourage reports to reflect disease severity in a meaningful manner.",1995,9,1551,24,1,6,13,18,11,24,45,34,51,50
5a1e3136ac33b0cdcec827c245738f3e8ba0488c,"Because of its toxicity, arsenic is of considerable environmental concern. Its solubility in natural systems is strongly influenced by adsorption at iron oxide surfaces. The objective of this study was to compare the adsorption behavior of arsenite and arsenate on ferrihydrite, under carefully controlled conditions, with regard to adsorption kinetics, adsorption isotherms, and the influence of pH on adsorption. The adsorption reactions were relatively fast, with the reactions almost completed within the first few hours. At relatively high As concentrations, arsenite reacted faster than arsenate with the ferrihydrite, i.e., equilibrium was achieved sooner, but arsenate adsorption was faster at low As concentrations and low pH. Adsorp tion maxima of approximately 0.60 (0.58) and 0.25 (0.16) molAs molFe-1 were achieved for arsenite and arsenate, respectively, at pH 4.6 (pH 9.2 in parentheses). The high arsenite retention, which precludes its retention entirely as surface adsorbed species, indicates the likel...",1998,7,1320,90,1,6,22,25,31,40,33,52,54,54
0be2acd403746056d71f5a2c89c440100bd9127b,"Mendelian randomization (MR) permits causal inference between exposures and a disease. It can be compared with randomized controlled trials. Whereas in a randomized controlled trial the randomization occurs at entry into the trial, in MR the randomization occurs during gamete formation and conception. Several factors, including time since conception and sampling variation, are relevant to the interpretation of an MR test. Particularly important is consideration of the “missingness” of genotypes that can be originated by chance, genotyping errors, or clinical ascertainment. Testing for Hardy-Weinberg equilibrium (HWE) is a genetic approach that permits evaluation of missingness. In this paper, the authors demonstrate evidence of nonconformity with HWE in real data. They also perform simulations to characterize the sensitivity of HWE tests to missingness. Unresolved missingness could lead to a false rejection of causality in an MR investigation of trait-disease association. These results indicate that large-scale studies, very high quality genotyping data, and detailed knowledge of the life-course genetics of the alleles/genotypes studied will largely mitigate this risk. The authors also present a Web program (http://www.oege.org/software/hwe-mr-calc.shtml) for estimating possible missingness and an approach to evaluating missingness under different genetic models.",2009,19,925,27,6,23,43,70,86,112,104,104,87,80
bdd5f1ee849909127b9c91d299082c29071719f0,"Empirical research on cities starts with a spatial equilibrium condition: workers and firms are assumed to be indifferent across space. This condition implies that research on cities is different from research on countries, and that work on places within countries needs to consider population, income and housing prices simultaneously. Housing supply elasticity will determine whether urban success shows up in more people or higher incomes. Urban economists generally accept the existence of agglomeration economies, which exist when productivity rises with density, but estimating the magnitude of those economies is difficult. Some manufacturing firms cluster to reduce the costs of moving goods, but this force no longer appears to be important in driving urban success. Instead, modern cities are far more dependent on the role that density can play in speeding the flow of ideas. Finally, urban economics has some insights to offer related topics such as growth theory, national income accounts, public economics and housing prices.",2009,172,820,48,6,39,49,41,76,64,70,77,98,81
3273eccfed4825e1159a17fc4cb361ce670ad295,"An equilibrium theory of unemployment assumes that firms and workers maximize their payoffs under rational expectations and that wages are determined to exploit the private gains from trade. This book focuses on the modeling of the transitions in and out of unemployment, given the stochastic processes that break up jobs and lead to the formation of new jobs, and on the implications of this approach for macroeconomic equilibrium and for the efficiency of the labor market. This approach to labor market equilibrium and unemployment has been successful in explaining the determinants of the ""natural"" rate of unemployment and new data on job and worker flows, in modeling the labor market in equilibrium business cycle and growth models, and in analyzing welfare policy. The second edition contains two new chapters, one on endogenous job destruction and one on search on the job and job-to-job quitting. The rest of the book has been extensively rewritten and, in several cases, simplified.",2000,0,1167,159,6,16,32,36,44,53,52,76,64,73
4e8419da55942518cb52d59efb0af73cfe1a6e37,,1964,27,2342,13,7,28,43,61,87,104,137,159,146,135
bcb88e0d541ebde4a4d255a0bd67a3917d66b181,"As the area of sampling A increases in an ecologically uniform area, the number of plant and animal species s increases in an approximately logarithmic manner, or s = bAk, (1) where k < 1, as shown most recently in in the detailed analysis of Preston (1962). The same relationship holds for islands, where, as one of us has noted (Wilson, 1961), the parameters b and k vary among taxa. Thus, in the ponerine ants of Melanesia and the Moluccas, k (which might be called the faunal coefficient) is approximately 0.5 where area is measured in square miles; in the Carabidae and herpetofauna of the Greater Antilles and associated islands, 0.3; in the land and freshwater birds of Indonesia, 0.4; and in the islands of the Sahul Shelf (New Guinea and environs), 0.5.",1963,19,2080,82,0,1,6,7,7,6,13,2,3,4
500f99128f09df3000154cc28d729bf5304c136b,"Materials engineers easily recognize that the conduction of heat within solids is fundamental to understanding and controlling many processes. We could cite numerous examples to emphasize the importance of this topic. Some important applications that fall in this category include estimating heat losses from process equipment, quenching, or cooling operations where the cooling rate of a part actually controls its microstructure and hence its application, and solidification.",1952,2,19965,591,0,0,0,0,0,0,0,0,0,0
b23d66adc185de9f785b51967995f09ae8f2f97a,"VOLUME ONE: Determination of Optical Constants: E.D. Palik, Introductory Remarks. R.F. Potter, Basic Parameters for Measuring Optical Properties. D.Y. Smith, Dispersion Theory, Sum Rules, and Their Application to the Analysis of Optical Data. W.R. Hunter, Measurement of Optical Constants in the Vacuum Ultraviolet Spectral Region. D.E. Aspnes, The Accurate Determination of Optical Properties by Ellipsometry. J. Shamir, Interferometric Methods for the Determination of Thin-Film Parameters. P.A. Temple, Thin-Film Absorplance Measurements Using Laser Colorimetry. G.J. Simonis, Complex Index of Refraction Measurements of Near-Millimeter Wavelengths. B. Jensen, The Quantum Extension of the Drude--Zener Theory in Polar Semiconductors. D.W. Lynch, Interband Absorption--Mechanisms and Interpretation. S.S. Mitra, Optical Properties of Nonmetallic Solids for Photon Energies below the Fundamental Band Gap. Critiques--Metals: D.W. Lynch and W.R. Hunter, Comments of the Optical Constants of Metals and an Introduction to the Data for Several Metals. D.Y. Smith, E. Shiles, and M. Inokuti, The Optical Properties of Metallic Aluminum. Critiques--Semiconductors: E.D. Palik, Cadium Telluride (CdTe). E.D. Palik, Gallium Arsenide (GaAs). A. Borghesi and G. Guizzetti, Gallium Phosphide (GaP). R.F. Potter, Germanium (Ge). E.D. Palik and R.T. Holm, Indium Arsenide (InAs). R.T. Holm, Indium Antimonide (InSb). O.J. Glembocki and H. Piller, Indium Phosphide (InP). G. Bauer and H. Krenn, Lead Selenide (PbSe). G. Guizzetti and A. Borghesi, Lead Sulfide (PbS). G. Bauer and H. Krenn, Lead Telluride (PbTe). D.F. Edwards, Silicon (Si). H. Piller, Silicon (Amorphous) (-Si). W.J. Choyke and E.D. Palik, Silicon Carbide (SiC). E.D. Palik and A. Addamiano, Zinc Sulfide (ZnS). Critiques--Insulators: D.J. Treacy, Arsenic Selenide (As 2 gt Se 3 gt ). D.J. Treacy, Arsenic Sulfide (As 2 gt S 3 gt ). D.F. Edwards and H.R. Philipp, Cubic Carbon (Diamond). E.D. Palik and W.R. Hunter, Litium Fluoride (LiF). E.D. Palik, Lithium Niobote (LiNbO 3 gt ). E.D. Palik, Potassium Chloride (KCl). H.R. Philipp, Silicon Dioxide (SiO 2 gt ), Type ( (Crystalline). H.R. Philipp, Silicon Dioxide (SiO 2 gt ) (Glass). gt H.R. Philipp, Silicon Monoxide (SiO) (Noncrystalline). H.R. Philipp, Silicon Nitride (Si 3 gt N 4 gt ) (Noncrystalline). J.E. Eldridge and E.D. Palik, Sodium Chloride (NaCl). M.W. Ribarsky, Titanium Dioxide (TiO 2 gt ) (Rutile).",1997,0,12880,533,0,0,0,0,0,1,0,0,44,476
1d96e67cd65361ef4ff600e186d9c4fca60e474c,"1. Introduction 2. The structure of cellular solids 3. Material properties 4. The mechanics of honeycombs 5. The mechanics of foams: basic results 6. The mechanics of foams refinements 7. Thermal, electrical and acoustic properties of foams 8. Energy absorption in cellular materials 9. The design of sandwich panels with foam cores 10. Wood 11. Cancellous bone 12. Cork 13. Sources, suppliers and property data Appendix: the linear-elasticity of anisotropic cellular solids.",1988,0,7964,234,0,7,17,13,18,15,28,35,35,46
223e7be6f5c6d0213701cd59c5160275f0c0316c,This paper discusses the influence of surface energy on the contact between elastic solids. Equations are derived for its effect upon the contact size and the force of adhesion between two lightly loaded spherical solid surfaces. The theory is supported by experiments carried out on the contact of rubber and gelatine spheres.,1971,8,6124,223,0,2,3,6,9,6,11,10,5,6
bcfe89165d431b1939fb308bcc79afeaf9552162,"This review presents a wide-ranging broad-brush picture of dielectric relaxation in solids, making use of the existence of a `universality' of dielectric response regardless of a wide diversity of materials and structures, with dipolar as well as charge-carrier polarization. The review of the experimental evidence includes extreme examples of highly conducting materials showing strongly dispersive behaviour, low-loss materials with a `flat', frequency-independent susceptibility, dipolar loss peaks etc. The surprising conclusion is that despite the evident complexity of the relaxation processes certain very simple relations prevail and this leads to a better insight into the nature of these processes.",1983,5,4465,199,1,5,9,21,27,39,37,34,45,41
38bd6dba0071176f6369088fbd0df175de9b0145,Preface Numerical simulation of intergranular and transgranular crack propagation in ferroelectric polycrystals Microstructure and stray electric fields at surface cracks in ferroelectrics Double kink mechanisms for discrete dislocations in BCC crystals The expanding spherical inhomogeneity with transformation strain A new model of damage: a moving thick layer approach On configurational forces at boundaries in fracture mechanics HotQC simulation of nanovoid growth under tension in copper Coupled phase transformations and plasticity as a field theory of deformation incompatibility Continuum strain-gradient elasticity from discrete valence force field theory for diamond-like crystals,1982,0,4821,319,5,11,10,21,18,27,17,31,49,61
f35aab7f41f62a4faa2121e7ae89a671c441584f,"This work, part of a two-volume set, applies the material developed in the Volume One to various boundary value problems (reflection and refraction at plane surfaces, composite media, waveguides and resonators). The text also covers topics such as perturbation and variational methods.",1973,19,5087,342,2,12,18,18,29,22,44,36,30,45
1cfb61f9ff9f3800342142670cc37a15648792c9,Popular modern generalized gradient approximations are biased toward the description of free-atom energies. Restoration of the first-principles gradient expansion for exchange over a wide range of density gradients eliminates this bias. We introduce a revised Perdew-Burke-Ernzerhof generalized gradient approximation that improves equilibrium properties of densely packed solids and their surfaces.,2007,0,4584,82,0,19,38,54,82,152,205,267,376,444
bec402e6f41f2e5e0b27b3a7642ad92d10cc2fdc,"Generalized gradient approximations (GGA's) seek to improve upon the accuracy of the local-spin-density (LSD) approximation in electronic-structure calculations. Perdew and Wang have developed a GGA based on real-space cutoff of the spurious long-range components of the second-order gradient expansion for the exchange-correlation hole. We have found that this density functional performs well in numerical tests for a variety of systems: (1) Total energies of 30 atoms are highly accurate. (2) Ionization energies and electron affinities are improved in a statistical sense, although significant interconfigurational and interterm errors remain. (3) Accurate atomization energies are found for seven hydrocarbon molecules, with a rms error per bond of 0.1 eV, compared with 0.7 eV for the LSD approximation and 2.4 eV for the Hartree-Fock approximation. (4) For atoms and molecules, there is a cancellation of error between density functionals for exchange and correlation, which is most striking whenever the Hartree-Fock result is furthest from experiment. (5) The surprising LSD underestimation of the lattice constants of Li and Na by 3--4 % is corrected, and the magnetic ground state of solid Fe is restored. (6) The work function, surface energy (neglecting the long-range contribution), and curvature energy of a metallic surface are all slightly reduced in comparison with LSD. Taking account of the positive long-range contribution, we find surface and curvature energies in good agreement with experimental or exact values. Finally, a way is found to visualize and understand the nonlocality of exchange and correlation, its origins, and its physical effects.",1992,0,15017,112,0,0,0,0,0,0,0,0,0,0
0ae5eb644252eed477d8ebe50281f9d7f8ddf977,,1985,0,5117,212,1,4,10,15,14,21,28,25,28,24
85fbf4706fe98315d8ccbc4f391168738c386208,"The stopping and range of ions in matter is physically very complex, and there are few simple approximations which are accurate. However, if modern calculations are performed, the ion distributions can be calculated with good accuracy, typically better than 10%. This review will be in several sections: 
 
a) 
 
A brief exposition of what can be determined by modern calculations. 
 
 
 
 
b) 
 
A review of existing widely-cited tables of ion stopping and ranges. 
 
 
 
 
c) 
 
A review of the calculation of accurate ion stopping powers.",1985,80,9336,111,2,12,41,104,129,159,167,192,203,212
3e286005c6e0cc9077c711ab1b0ca1de097ec386,"Preface. Introduction. 1. One-dimensional motion of an elastic continuum. 2. The linearized theory of elasticity. 3. Elastodynamic theory. 4. Elastic waves in an unbound medium. 5. Plane harmonic waves in elastic half-spaces. 6. Harmonic waves in waveguides. 7. Forced motions of a half-space. 8. Transient waves in layers and rods. 9. Diffraction of waves by a slit. 10. Thermal and viscoelastic effects, and effects of anisotrophy and non-linearity. Author Index. Subject Index.",1962,0,3996,290,0,0,0,0,0,1,0,2,3,0
ff1faf4cc2ae43513e7632e295e43f8366f62a46,,1990,0,3033,548,0,2,2,1,2,3,6,9,10,12
d1bef0717c54dc85b15c2316fdf32a1cb6df6936,"This critical review will be of interest to the experts in porous solids (including catalysis), but also solid state chemists and physicists. It presents the state-of-the-art on hybrid porous solids, their advantages, their new routes of synthesis, the structural concepts useful for their 'design', aiming at reaching very large pores. Their dynamic properties and the possibility of predicting their structure are described. The large tunability of the pore size leads to unprecedented properties and applications. They concern adsorption of species, storage and delivery and the physical properties of the dense phases. (323 references)",2008,288,4287,5,47,210,298,403,495,458,407,401,400,335
b2d4b4465269e7c650430f18b240c6c228dbb3a4,"Recent extensions of the DMol3 local orbital density functional method for band structure calculations of insulating and metallic solids are described. Furthermore the method for calculating semilocal pseudopotential matrix elements and basis functions are detailed together with other unpublished parts of the methodology pertaining to gradient functionals and local orbital basis sets. The method is applied to calculations of the enthalpy of formation of a set of molecules and solids. We find that the present numerical localized basis sets yield improved results as compared to previous results for the same functionals. Enthalpies for the formation of H, N, O, F, Cl, and C, Si, S atoms from the thermodynamic reference states are calculated at the same level of theory. It is found that the performance in predicting molecular enthalpies of formation is markedly improved for the Perdew–Burke–Ernzerhof [Phys. Rev. Lett. 77, 3865 (1996)] functional.",2000,33,6786,56,1,4,19,38,73,76,119,160,200,217
abf125b006f862d74ff0b37258cd0fcf82f7054f,"The electron density, its gradient, and the Kohn-Sham orbital kinetic energy density are the local ingredients of a meta-generalized gradient approximation (meta-GGA). We construct a meta-GGA density functional for the exchange-correlation energy that satisfies exact constraints without empirical parameters. The exchange and correlation terms respect two paradigms: one- or two-electron densities and slowly varying densities, and so describe both molecules and solids with high accuracy, as shown by extensive numerical tests. This functional completes the third rung of ""Jacob's ladder"" of approximations, above the local spin density and GGA rungs.",2003,0,4040,46,4,40,48,70,85,71,131,124,190,247
5660a6ea2ac83b1b984d8c4afbe797f2243c5b94,"Originally published in 1950, this classic book was a landmark in the development of the subject of tribology. For this edition, David Tabor has written a new preface, reviewing the many advances made in this field during the past 36 years and outlining the achievements of Frank Philip Bowden. The book covers the behavior of non-metals, especially elastomers; elastohydrodynamic lubrication; and the wear of sliding surfaces, which has gradually replaced the earlier concentration on the mechanism of friction. It remains one of the most interesting and comprehensive works available on a single branch of physics.",1964,0,5097,183,35,39,36,47,44,44,40,40,50,59
b589bbacd348592e27ae9464383d72b24ec6dc95,Bond-valence parameters which relate bond valences and bond lengths have been derived for a large number of bonds. It is shown that there is a strong linear correlation between the parameters for bonds from cations to pairs of anions. This correlation is used to develop an interpolation scheme that allows the estimation of bond-valence parameters for 969 pairs of atoms. A complete listing of these parameters is given.,1991,10,5014,165,3,13,23,30,26,23,42,45,54,88
ccd50d5764b4508274b7e65420cd8a754a874a96,"Abstract The title problem concerns two isotropic phases firmly bonded together to form a mixture with any concentrations. An elementary account of several theoretical methods of attack is given, among them the derivation of inequalities between various moduli. The approach is completely general and exact. Additionally, the problem is fully solved when the phases have equal rigidities but different compressibilities, the geometry being entirely arbitrary.",1963,3,3698,233,2,2,12,6,6,8,6,9,10,11
80fd8b366a25977d44a23efc75f20222b4e46ee9,"The declared objective of this book is to provide an introductory review of the various theoretical and practical aspects of adsorption by powders and porous solids with particular reference to materials of technological importance. The primary aim is to meet the needs of students and non-specialists, who are new to surface science or who wish to use the advanced techniques now available for the determination of surface area, pore size and surface characterization. In addition, a critical account is given of recent work on the adsorptive properties of activated carbons, oxides, clays and zeolites. Key Features * Provides a comprehensive treatment of adsorption at both the gas/solid interface and the liquid/solid interface * Includes chapters dealing with experimental methodology and the interpretation of adsorption data obtained with porous oxides, carbons and zeolites * Techniques capture the importance of heterogeneous catalysis, chemical engineering and the production of pigments, cements, agrochemicals, and pharmaceuticals",1998,0,2930,197,0,3,10,24,29,36,42,50,92,62
958ffbefe66e7351d17ecf858cb03376fc4911e4,,1947,0,3697,306,0,1,2,2,2,0,1,2,2,1
f6fd0b5a9d4ff2bce8f477b2a044933fc32787ce,"Before the 1960s, all anti-Stokes emissions, which were known to exist, involved emission energies in excess of excitation energies by only a few kT. They were linked to thermal population of energy states above excitation states by such an energy amount. It was the well-known case of anti-Stokes emission for the so-called thermal bands or in the Raman effect for the well-known anti-Stokes sidebands. Thermoluminescence, where traps are emptied by excitation energies of the order of kT, also constituted a field of anti-Stokes emission of its own. Superexcitation, i.e., raising an already excited electron to an even higher level by excited-state absorption (ESA), was also known but with very weak emissions. These types of well-known anti-Stokes processes have been reviewed in classical textbooks on luminescence.1 All fluorescence light emitters usually follow the well-known principle of the Stokes law which simply states that excitation photons are at a higher energy than emitted ones or, in other words, that output photon energy is weaker than input photon energy. This, in a sense, is an indirect statement that efficiency cannot be larger than 1. This principle is",2004,12,3753,60,7,27,29,61,77,100,117,183,219,306
c1d100996090aae0cb689a6bfddfead1b6eacf3a,,2001,0,5151,9,10,108,177,173,326,316,334,356,391,355
4935df568e622737793386afcf86464bb8469846,"The term ``sensitized luminescence'' in crystalline phosphors refers to the phenomenon whereby an impurity (activator, or emitter) is enabled to luminesce upon the absorption of light in a different type of center (sensitizer, or absorber) and upon the subsequent radiationless transfer of energy from the sensitizer to the activator. The resonance theory of Forster, which involves only allowed transitions, is extended to include transfer by means of forbidden transitions which, it is concluded, are responsible for the transfer in all inorganic systems yet investigated. The transfer mechanisms of importance are, in order of decreasing strength, the overlapping of the electric dipole fields of the sensitizer and the activator, the overlapping of the dipole field of the sensitizer with the quadrupole field of the activator, and exchange effects. These mechanisms will give rise to ``sensitization'' of about 103−104, 102, and 30 lattice sites surrounding each sensitizer in typical systems. The dependence of tra...",1953,20,6652,48,0,1,6,3,4,7,6,5,5,6
d319fca1c8ab0a0399f25c8672e92bccc3d21634,1. Periodic structure 2. Lattice waves 3. Electron states 4. Static properties of solids 5. Electron-electron interaction 6. Dynamics of electrons 7. Transport properties 8. Optical properties 9. The fermi surface 10. Magnetism 11. Superconductivity Bibliography Index.,1965,0,3402,93,3,7,11,12,19,27,28,41,46,37
7087000e48e269a86af6b5afb709a72cb569a882,,1972,0,3604,68,10,16,17,23,28,35,24,34,36,37
05cf48fb520d63c7a3164d27fc926ed5eb946525,"These recommendations aim to be a tool for the selection and appraisal of the methods of characterization of porous solids, and to also give the warnings and guidelines on which the experts generally agree. For this purpose, they successively consider the description of a porous solid (definitions, terminology), the principal methods available (stereology , radiation scattering, pycnometry, adsorption, intrusion, suction, maximum buble pressure, fluid flow, immersion or adsorption calorimetry, thermoporometry , size exclusion chromatography, Xenon NMR and ultrasonic methods) and finally the general principles which are worth being followed in the selection of the appropriate method.",1994,5,2738,56,2,2,17,16,22,22,22,19,24,31
b06d33e714fc3882710692b66672095cd81c69ff,"Analysis of the shape of the curve of reflected x-ray intensity vs glancing angle in the region of total reflection provides a new method of studying certain structural properties of the mirror surface about 10 to several hundred angstroms deep. Dispersion theory, extended to treat any (small) number of stratified homogeneous media, is used as a basis of interpretation.",1954,0,3798,48,0,0,0,1,2,1,2,1,0,0
cd3c9d25fdc09ea93bf189f21468e9f4425d40e3,,1953,0,3779,22,0,0,0,2,0,1,2,3,4,2
23fff8d38a2c6566c35950412894bfc6fe5e82e3,Preface 1. Introduction 2. Classical propagation 3. Interband absorption 4. Excitons 5. Luminescence 6. Semiconductor quantum wells 7. Free electrons 8. Molecular materials 9. Luminescence centres 10. Phonons 11. Nonlinear optics Appendix A: Electromagnetism in dielectrics Appendix B: Quantum theory of radiative absorption and emission Appendix C: Band theory Appendix D: Semiconductor p-i-n diodes,2002,5,2368,133,6,19,29,42,38,49,71,70,76,101
ef04581940ad80d5b846ecf2fe3f1ea0eed735ec,"New numerical simulations of the formation of the giant of the second phase. planets are presented, in which for the first time both the gas and The actual rates at which the giant planets accreted small planetesimal accretion rates are calculated in a self-consistent, planetesimals is probably intermediate between the constant interactive fashion. The simulations combine three elements: rates assumed in most previous studies and the highly variable (1) three-body accretion cross sections of solids onto an isolated rates used here. Within the context of the adopted model of planetary embryo, (2) a stellar evolution code for the planet’s planetesimal accretion, the joint constraints of the time scale gaseous envelope, and (3) a planetesimal dissolution code for dissipation of the solar nebula and the current high-Z masses within the envelope, used to evaluate the planet’s effective of the giant planets lead to estimates of the initial surface capture radius and the energy deposition profile of accreted density (sinit) of planetesimals in the outer region of the solar material. Major assumptions include: The planet is embedded nebula. The results show that sinit P 10 g cm 22 near Jupiter’s in a disk of gas and small planetesimals with locally uniform orbit and that sinit ~ a 22 , where a is the distance from the Sun. initial surface mass density, and planetesimals are not allowed These values are a factor of 3 to 4 times as high as that of to migrate into or out of the planet’s feeding zone. the ‘‘minimum-mass’’ solar nebula at Jupiter’s distance and a All simulations are characterized by three major phases. Dur- factor of 2 to 3 times as high at Saturn’s distance. The estimates ing the first phase, the planet’s mass consists primarily of solid for the formation time of Jupiter and Saturn are 1 to 10 million material. The planetesimal accretion rate, which dominates years, whereas those for Uranus fall in the range 2 to 16 million that of gas, rapidly increases owing to runaway accretion, then years. These estimates follow from the properties of our Solar decreases as the planet’s feeding zone is depleted. During the System and do not necessarily apply to giant planets in other second phase, both solid and gas accretion rates are small planetary systems. © 1996 Academic Press, Inc. and nearly independent of time. The third phase, marked by runaway gas accretion, starts when the solid and gas masses are about equal. It is engendered by a strong positive feedback",1995,40,2108,186,0,2,6,19,29,30,24,50,63,77
2a410d3081efe44c6549afa0c03e78f72f7cb8c5,"Built upon the two original books by Mike Crisfield and their own lecture notes, renowned scientist Rene de Borst and his team offer a thoroughly updated yet condensed edition that retains and builds upon the excellent reputation and appeal amongst students and engineers alike for which Crisfield's first edition is acclaimed. Together with numerous additions and updates, the new authors have retained the core content of the original publication, while bringing an improved focus on new developments and ideas. This edition offers the latest insights in non-linear finite element technology, including non-linear solution strategies, computational plasticity, damage mechanics, time-dependent effects, hyperelasticity and large-strain elasto-plasticity. The authors' integrated and consistent style and unrivalled engineering approach assures this book's unique position within the computational mechanics literature.",1991,173,2586,102,1,1,6,7,15,23,44,31,46,60
82540fb1ff2318fd9f8975e3fac8c380d515c7f8,"This is an advanced text for higher degree materials science students and researchers concerned with the strength of highly brittle covalent–ionic solids, principally ceramics. It is a reconstructed and greatly expanded edition of a book first published in 1975. The book presents a unified continuum, microstructural and atomistic treatment of modern day fracture mechanics from a materials perspective. Particular attention is directed to the basic elements of bonding and microstructure that govern the intrinsic toughness of ceramics. These elements hold the key to the future of ceramics as high-technology materials - to make brittle solids strong, we must first understand what makes them weak. The underlying theme of the book is the fundamental Griffith energy-balance concept of crack propagation. The early chapters develop fracture mechanics from the traditional continuum perspective, with attention to linear and nonlinear crack-tip fields, equilibrium and non-equilibrium crack states. It then describes the atomic structure of sharp cracks, the topical subject of crack-microstructure interactions in ceramics, with special focus on the concepts of crack-tip shielding and crack-resistance curves, and finally deals with indentation fracture, flaws, and structural reliability.",1993,0,2373,78,32,49,63,73,71,81,71,84,71,79
d76e80eae8cb3aabd8723f73a065157709b08728,,1970,0,3031,78,3,8,26,33,24,31,38,22,23,30
a50fba74795241d9077b9ff97161fe57d6096fc5,,2000,156,2372,34,0,3,16,29,40,57,86,95,96,110
d957453c3b7a61b2d037b22a8de32b32ba726133,,1963,0,3024,28,2,11,10,19,28,32,28,42,31,50
06df74c90898b00aeb262befc914536e205acf4b,"Algorithms for the symmetry-adapted energy minimisation of solids using analytical first and second derivatives have been devised and implemented in a new computer program GULP. These new methods are found to lead to an improvement in computational efficiency of up to an order of magnitude over the standard algorithm, which takes no account of symmetry, the largest improvement being obtained from the use of symmetry in the generation of the hessian. Accelerated convergence techniques for the dispersion energy are found to be beneficial in improving the precision at little extra computational cost, particularly when a one centre decomposition is possible or the Ewald sum weighting towards real-space is increased.",1997,31,2032,53,9,20,36,56,54,57,72,79,97,66
ee19d478eb4ab5d6d4c4a70ef2a1198196752573,Laser ablation of solid targets by 0.2–5000 ps Ti: Sapphire laser pulses is studied. Theoretical models and qualitative explanations of experimental results are presented. Advantages of femtosecond lasers for precise material processing are discussed and demonstrated.,1996,16,2231,24,0,9,11,18,18,22,28,42,37,59
34685c14584dc833a5bafd8a4c0a01c46b4e6f69,"Preface. Electrical noise associated with dislocations and plastic flow in metals (G. Bertotti, A. Ferro, F. Fiorillo, P. Mazetti). Mechanisms of dislocation drag (V.I. Alshits, V.L. Indenbom). Dislocations in covalent crystals (H. Alexander). Formation and evolution of dislocation structures during irradiation (B.O. Hall). Dislocation theory of martensitic transformations (G.B. Olsen, M. Cohen). Author index. Subject index. Cumulative index.",1979,0,2631,4,3,6,12,17,30,22,33,42,24,47
d43d4125bf0c36953057516ec6bb6b8e79a8712b,"1 Theoretical.- 1 Introduction.- 1.1 Real Surfaces.- 1.2 Factors Affecting Surface area.- 1.3 Surface Area from Particle Size Distributions.- 1.4 References.- 2 Gas Adsorption.- 2.1 Introduction.- 2.2 Physical and Chemical Adsorption.- 2.3 Physical Adsorption Forces.- 2.4 Physical Adsorption on a Planar Surface.- 2.5 References.- 3 Adsorption Isotherms.- 3.1 Pore Size and Adsorption Potential.- 3.2 Classification of Adsorption Isotherms.- 3.3 References.- 4 Adsorption Mechanism.- 4.1 Langmuir and BET Theories (Kinetic Isotherms).- 4.1.1 The Langmuir Isotherm.- 4.1.2 The Brunauer, Emmett, and Teller (BET) Theory.- 4.2 The Frenkel-Halsey-Hill (FHH) Theory of Multilayer Adsorption.- 4.3 Adsorption in Microporous Materials.- 4.3.1 Introduction.- 4.3.2 Aspects of Classical, Thermodynamic Theories for Adsorption in Micropores: Extension of Polanyi's Theory.- 4.3.3 Aspects of Modern, Microscopic Theories for Adsorption in Micropores: Density Functional Theory and Molecular Simulation.- 4.3.3.1 Density Functional Theory (DFT).- 4.3.3.2 Computer Simulation Studies: Monte Carlo Simulation and Molecular Dynamics.- 4.3.3.3 NLDFT and Monte Carlo Simulation for Pore Size Analysis.- 4.4 Adsorption in Mesopores.- 4.4.1 Introduction.- 4.4.2 Multilayer Adsorption, Pore Condensation and Hysteresis.- 4.4.3 Pore Condensation: Macroscopic, Thermodynamic Approaches.- 4.4.3.1 Classical Kelvin Equation.- 4.4.3.2 Modified Kelvin Equation.- 4.4.4 Adsorption Hysteresis.- 4.4.4.1 Classification of Hysteresis Loops.- 4.4.4.2 Origin of Hysteresis.- 4.4.5 Effects of Temperature and Pore Size: Experiments and Predictions of Modern, Microscopic Theories.- 4.5 References.- 5 Surface Area from the Langmuir and BET Theories.- 5.1 Specific Surface Area from the Langmuir Equation.- 5.2 Specific Surface Area from the BET Equation.- 5.2.1. BET-Plot and Calculation of the Specific Surface Area.- 5.2.2 The Meaning of Monolayer Coverage.- 5.2.3 The BET Constant and Site Occupancy.- 5.2.4 The Single Point BET Method.- 5.2.5 Comparison of the Single Point and Multipoint Methods.- 5.2.6 Applicability of the BET Theory.- 5.2.7 Importance of the Cross-Sectional Area.- 5.2.8 Nitrogen as the Standard Adsorptive for Surface Area Measurements.- 5.2.9 Low Surface Area Analysis.- 5.3 References.- 6 Other Surface Area Methods.- 6.1 Introduction.- 6.2 Gas Adsorption: Harkins and Jura Relative Method.- 6.3 Immersion Calorimetry: Harkins and Jura Absolute Method.- 6.4 Permeametry.- 6.5 References.- 7 Evaluation of the Fractal Dimension by Gas Adsorption.- 7.1 Introduction.- 7.2 Method of Molecular Tiling.- 7.3 The Frenkel-Halsey-Hill Method.- 7.4 The Thermodynamic Method.- 7.5 Comments About Fractal Dimensions Obtained from Gas Adsorption.- 7.6 References.- 8 Mesopore Analysis.- 8.1 Introduction.- 8.2 Methods based on the Kelvin equation.- 8.3 Modelless Pore Size Analysis.- 8.4 Total Pore Volume and Average Pore Size.- 8.5 Classical, Macroscopic Thermodynamic Methods versus Modern, Microscopic Models for Pore Size Analysis.- 8.6 Mesopore Analysis and Hysteresis.- 8.6.1 Use of Adsorption or Desorption Branch for Pore Size Calculation?.- 8.6.2 Lower Limit of the Hysteresis Loop- Tensile Strength Hypothesis.- 8.7 Adsorptives other than Nitrogen for Mesopore Analysis.- 8.8 References.- 9 Micropore Analysis.- 9.1 Introduction.- 9.2 Micropore Analysis by Isotherm Comparison.- 9.2.1 Concept of V-t curves.- 9.2.2 The t- Method.- 9.2.3 The ?s method.- 9.3 The Micropore Analysis (MP) Method).- 9.4 Total Micropore Volume and Surface Area.- 9.5 The Dubinin-Radushkevich (DR) Method.- 9.6 The Horvath-Kawazoe (HK) Approach and Related Methods.- 9.7 Application of NLDFT: Combined Micro/Mesopore Analysis With a Single Method.- 9.8 Adsorptives other than Nitrogen for Super- and Ultramicroporosimetry.- 9.9 References.- 10 Mercury Porosimetry: Non-Wetting Liquid Penetration.- 10.1 Introduction.- 10.2 Young-Laplace Equation.- 10.3 Contact Angles and Wetting.- 10.4 Capillarity.- 10.5 The Washburn Equation.- 10.6 Intrusion - Extrusion Curves.- 10.7 Common Features of Porosimetry Curves.- 10.8 Hysteresis, Entrapment and Contact Angle.- 10.9 Contact Angle Changes.- 10.10 Porosimetric Work.- 10.12 Theory of Porosimetry Hysteresis.- 10.13 Pore Potential.- 10.14 Other Hysteresis Theories (Throat-Pore Ratio Network Model).- 10.15 Equivalency of Mercury Porosimetry and Gas Sorption.- 10.16 References.- 11 Pore Size and Surface Characteristics of Porous Solids by Mercury Porosimetry.- 11.1 Application of The Washburn Equation.- 11.2 Pore Size and Pore Size Distribution from Mercury Porosimetry.- 11.2.1 Linear Pore Volume Distribution.- 11.2.2 Logarithmic Pore Volume Distribution.- 11.2.3 Pore Number Distributions.- 11.2.4 Pore Length Distribution.- 11.2.5 Pore Population (Number Distribution).- 11.2.6 Surface Area and Surface Area Distribution from Intrusion Curves.- 11.2.7 Pore Area Distributions.- 11.3 Pore Shape from Hysteresis.- 11.4 Fractal Dimension.- 11.5 Permeability.- 11.6 Tortuosity.- 11.7 Particle Size Distribution.- 11.7.1 Mayer & Stowe Approach.- 11.7.2 Smith & Stermer Approach.- 11.8 Comparison of Porosimetry and Gas Sorption.- 11.9 Solid Compressibility.- 11.10 References.- 12 Chemisorption: Site Specific Gas Adsorption.- 12.1 Chemical Adsorption.- 12.2 Quantitative Measurements.- 12.3 Stoichiometry.- 12.4 Monolayer Coverage.- 12.4.1 Extrapolation.- 12.4.2 Irreversible Isotherm and Bracketing.- 12.4.3 Langmuir Theory.- 12.4.4 Temperature Dependent Models.- 12.4.5 Temkin Method.- 12.4.6 Freundlich Method.- 12.4.7 Isotherm Subtraction - Accessing Spillover.- 12.4.8 Surface Titration.- 12.5 Active Metal Area.- 12.6 Dispersion.- 12.7 Crystallite (Nanoparticle) Size.- 12.8 Heats of Adsorption and Activation Energy.- 12.8.1 Differential Heats of Adsorption.- 12.8.2 Integral Heat of Adsorption.- 12.8.3 Activation Energy.- 12.9 References.- 2 Experimental.- 13 Physical Adsorption Measurements - Preliminaries.- 13.1 Experimental Techniques for Physical Adsorption Measurements.- 13.2 Reference Standards.- 13.3 Representative Samples.- 13.4 Sample Conditioning: Outgassing of the Adsorbent.- 13.5 Elutriation and Its Prevention.- 13.6 References.- 14 Vacuum Volumetric Measurements (Manometry).- 14.1 Basics of Volumetric Adsorption Measurement.- 14.2 Deviations from Ideality.- 14.3 Void Volume Determination.- 14.4 Coolant Level and Temperature Control.- 14.5 Saturation Vapor Pressure, P0 and Temperature of the Sample Cell.- 14.6 Sample Cells.- 14.7 Low Surface Area.- 14.8 Micro- and Mesopore Analysis.- 14.8.1 Experimental Requirements.- 14.8.2 Micropore Analysis and Void Volume Determination.- 14.8.3 Thermal Transpiration Correction.- 14.8.4 Adsorptives other than Nitrogen for Micro- and Mesopore Analysis - Experimental Aspects.- 14.9 Automated Instrumentation.- 14.9.1 Multistation Sorption Analyzer.- 14.9.2 The NOVA Concept.- 14.10 References.- 15 Dynamic Flow Method.- 15.1 Nelson and Eggertsen Continuous Flow Method.- 15.2 Carrier Gas (Helium) and Detector Sensitivity.- 15.3. Design Parameters for Continuous Flow Apparatus.- 15.4 Signals and Signal Calibration.- 15.5 Adsorption and Desorption Isotherms by Continuous Flow.- 15.6 Low Surface Areas Measurement.- 15.7 Data Reduction - Continuous Flow Method.- 15.8 Single Point Method.- 15.9 References.- 16 Volumetric Chemisorption: Catalyst Characterization by Static Methods.- 16.1 Applications.- 16.2 Sample Requirements.- 16.3 General Description of Equipment.- 16.4 Measuring System.- 16.4.1 Pressure Measurement.- 16.4.2 Valves.- 16.4.3 Vacuum.- 16.4.4 Sample Cell.- 16.4.5 Heating System.- 16.4.6 Gases and Chemical Compatibilities.- 16.5 Pretreatment.- 16.5.1 Heating.- 16.5.2 Atmosphere.- 16.6 Isotherms.- 16.6.1 Reactive Gas.- 16.6.2 The Combined Isotherm.- 16.6.3 The Weak Isotherm.- 16.6.4 The Strong Isotherm.- 16.6.5 Multiple Isotherms.- 16.7 References.- 17 Dynamic Chemisorption: Catalyst Characterization By Flow Techniques.- 17.1 Applications.- 17.2 Sample Requirements.- 17.3 General Description of Equipment.- 17.3.1 Flow Path.- 17.3.2 Sample Cell.- 17.3.3 Gases.- 17.3.4 Heating.- 17.3.5 Pulse Injection.- 17.3.6 Detector.- 17.4 Pretreatment.- 17.5 Pulse Titration.- 17.6 Additional Requirements for Temperature Programmed Methods.- 17.6.1 Programmed Heating.- 17.6.2 Sample Temperature.- 17.7 Temperature Programmed Reduction.- 17.8 Temperature Programmed Oxidation.- 17.9 Temperature Programmed Desorption.- 17.9.1 Some Specific Applications.- 17.8.1.1 Acid/Base.- 17.8.1.2 Oxidizers.- 17.8.1.3 Reducers.- 17.10 Mass Spectrometry.- 17.11 Metal Parameters.- 17.11 References.- 18 Mercury Porosimetry: Intra and Inter- Particle Characterization.- 18.1 Applications.- 18.2 Working with Mercury.- 18.3 Experimental Requirements.- 18.4 Sample Cell.- 18.5 Volume Measurement.- 18.6 Contact Angle.- 18.6.1 Dynamic Contact Angle.- 18.6.2 Static Contact Angle.- 18.7 A Modern Porosimeter.- 18.8 Low Pressure Measurements.- 18.8.1 Sample Cell Evacuation.- 18.8.2 Filling with Mercury.- 18.8.3 Low Pressure Intrusion-Extrusion.- 18.9 High Pressure Measurements.- 18.10 Scanning Method.- 18.11 Stepwise Method.- 18.12 Mercury Entrapment.- 18.13 Working with Powders.- 18.14 Inter/Intra Particle Porosity.- 18.15 Isostatic Crush Strength.- 18.16 References.- 19 Density Measurement.- 19.1 Introduction.- 19.2 True Density.- 19.3 Apparent Density.- 19.4 Open-Closed Porosity.- 19.5 Bulk Density.- 19.6 Tap Density.- 19.7 Envelope or Geometric Density.- 19.8 Effective Density.- 19.9 Density by Mercury Porosimetry.- 19.10 Standard Methods.- 19.11 References.",2006,0,1766,91,8,24,44,46,63,96,124,140,134,152
5328929e4a7323203dcd9d3bec5d048987f1a775,,1916,0,5298,65,0,3,1,0,1,2,1,1,3,3
5aa0b4eab0104f96b1dbb571546394c7f02e67c5,We consider the change in polarization \ensuremath{\Delta}P which occurs upon making an adiabatic change in the Kohn-Sham Hamiltonian of the solid. A simple expression for \ensuremath{\Delta}P is derived in terms of the valence-band wave functions of the initial and final Hamiltonians. We show that physically \ensuremath{\Delta}P can be interpreted as a displacement of the center of charge of the Wannier functions. The formulation is successfully applied to compute the piezoelectric tensor of GaAs in a first-principles pseudopotential calculation.,1993,0,2251,43,1,2,4,4,2,11,14,13,17,30
9c71ccf2708e7550092adaf326916569d37b1c1a,,1972,0,2518,88,1,15,16,19,33,19,36,30,24,30
e8272dcc10b02fe549b02deb9c7696008bdb2f0e,"Theory of structural transformations in solids , Theory of structural transformations in solids , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1983,0,1970,114,0,5,6,12,7,19,27,21,28,32
7bacbaef39b33388c2365a59eae6b86097658070,"Dynamic crack growth is analysed numerically for a plane strain block with an initial central crack subject to tensile loading. The continuum is characterized by a material constitutive law that relates stress and strain, and by a relation between the tractions and displacement jumps across a specified set of cohesive surfaces. The material constitutive relation is that of an isotropic hyperelastic solid. The cohesive surface constitutive relation allows for the creation of new free surface and dimensional considerations introduce a characteristic length into the formulation. Full transient analyses are carried out. Crack branching emerges as a natural outcome of the initial-boundary value problem solution, without any ad hoc assumption regarding branching criteria. Coarse mesh calculations are used to explore various qualitative features such as the effect of impact velocity on crack branching, and the effect of an inhomogeneity in strength, as in crack growth along or up to an interface. The effect of cohesive surface orientation on crack path is also explored, and for a range of orientations zigzag crack growth precedes crack branching. Finer mesh calculations are carried out where crack growth is confined to the initial crack plane. The crack accelerates and then grows at a constant speed that, for high impact velocities, can exceed the Rayleigh wave speed. This is due to the finite strength of the cohesive surfaces. A fine mesh calculation is also carried out where the path of crack growth is not constrained. The crack speed reaches about 45% of the Rayleigh wave speed, then the crack speed begins to oscillate and crack branching at an angle of about 29° from the initial crack plane occurs. The numerical results are at least qualitatively in accord with a wide variety of experimental observations on fast crack growth in brittle solids.",1994,39,2103,103,2,3,9,12,15,28,26,35,28,48
13b39c3e1e064e1344f4d487bba222dafa1bc90d,,1980,0,2284,42,3,17,10,22,24,41,41,49,51,68
e9eb6cc511709071ff6c05ffa9a09671a742612a,An empirical model and an ab initio calculation of the bulk moduli for covalent solids are used to suggest possible new hard materials. The empirical model indicates that hypothetical covalent solids formed between carbon and nitrogen are good candidates for extreme hardness. A prototype system is chosen and a first principles pseudopotential total energy calculation on the system is performed. The results are consistent with the empirical model and show that materials like the prototype can have bulk moduli comparable to or greater than diamond. It may be possible to synthesize such materials in the laboratory.,1989,11,2169,11,0,6,7,5,11,27,49,85,83,99
544d74292be943b2ed18ee6dc68030a2af28d363,"The aim of this paper is to advocate the usefulness of the spin-density-functional (SDF) formalism. The generalization of the Hohenberg-Kohn-Sham scheme to and SDF formalism is presented in its thermodynamic version. The ground-state formalism is extended to more general Hamiltonians and to the lowest excited state of each symmetry. A relation between the exchange-correlation functional and the pair correlation function is derived. It is used for the interpretation of approximate versions of the theory, in particular the local-spin-density (LSD) approximation, which is formally valid only in the limit of slow and weak spatial variation in the density. It is shown, however, to give good account for the exchange-correlation energy also in rather inhomogeneous situations, because only the spherical average of the exchange-correlation hole influences this energy, and because it fulfills the sum rule stating that this hole should contain only one charge unit. A further advantage of the LSD approximation is that it can be systematically improved. Calculations on the homogeneous spin-polarized electron liquid are reported on. These calculations provide data in the form of interpolation formulas for the exchange-correlation energy and potentials, to be used in the LSD approximation. The ground-state properties are obtained from the Galitskii-Migdal formula, which relates the total energy to the one-electron spectrum, obtained with a dynamical self-energy. The self-energy is calculated in an electron-plasmon model where the electron is assumed to couple to one single mode. The potential for excited states is obtained by identifying the quasiparticle peak in the spectrum. Correlation is found to significantly weaken the spin dependence of the potentials, compared with the result in the Hartree-Fock approximation. Charge and spin response functions are calculated in the long-wavelength limit. Correlation is found to be very important for properties which involve a change in the spinpolarization. For atoms, molecules, and solids the usefulness of the SDF formalism is discussed. In order to explore the range of applicability, a few applications of the LSD approximation are made on systems for which accurate solutions exist. The calculated ionization potentials, affinities, and excitation energies for atoms propose that the valence electrons are fairly well described, a typical error in the ionization energy being 1/2 eV. The exchange-correlation holes of two-electron ions are discussed. An application to the hydrogen molecule, using a minimum basis set, shows that the LSD approximation gives good results for the energy curve for all separations studied, in contrast to the spin-independent local approximation. In particular, the error in the binding energy is only 0.1 eV, and bond breaking is properly described. For solids, the SDF formalism provides a framework for band models of magnetism. An estimate of the splitting between spin-up and spin-down energy bands of a ferromagnetic transition metal shows that the LSD approximation gives a correction of the correct sign and order of magnitude to published $X\ensuremath{\alpha}$ results. To stimulate further use of the SDF formalism in the LSD approximation, the paper is self-contained and describes the necessary formulas and input data for the potentials.",1976,0,2416,14,2,16,23,33,43,28,47,39,43,47
58f5dedd4320730ef39189e99b2fb21a6a82f73e,1. Crystal lattices. General theory 2. . Crystal lattices. Applications 3. Interaction of light with non-conducting crystals 4. Electrons in a perfect lattice 5. Cohesive forces in metals 6. Transport phenomena 7. Magnetic properties of metals 8. Ferromagnetism 9. Interaction of light with electrons in solids 10. Semi-conductors and luminescence 11. Superconductivity,1956,0,2799,21,6,8,6,8,5,11,11,13,18,26
8af729d71fcd38e735a62b09d29b14ce370c5c57,,1991,0,2073,52,34,38,50,72,50,66,51,53,77,93
949831004e239a7bdcc4b4107695795bc68e13b2,"The book presents a comprehensive study of elastic wave propagation in solids. Topics covered range from the theory of waves and vibrations in strings to the three-dimensional theory of waves in thick plates. The subject is covered in the following chapters: (1) waves and vibrations in strings, (2) longitudinal waves in thin rods, (3) flexural waves in thin rods, (4) waves in membranes, thin plates and shells, (5) waves in infinite media, (6) waves in semi-infinite media, (7) scattering and diffraction of elastic waves, and (8) wave propagation in plates and rods. Appendices contain introductory information on elasticity, transforms and experimental techniques. /TRRL/",1975,0,2236,67,0,2,2,3,6,4,6,10,10,10
e13f0217348b231d805fef95a9f2e89dad8804fd,"Granular materials are ubiquitous in the world around us. They have properties that are different from those commonly associated with either solids, liquids, or gases. In this review the authors select some of the special properties of granular materials and describe recent research developments.[S0034-6861(96)00204-8]",1996,14,1904,39,3,12,54,31,38,54,53,66,72,100
8441533661bec9ee265ec695d8e7686b478b28d0,"Measurements of the transient photocurrent I(t) in an increasing number of inorganic and organic amorphous materials display anomalous transport properties, The long tail of I(t) indicates a dispersion of carrier transit times. However, the shape invariance of I(t) to electric field and sample thickness (designated as universality for the classes of materials here considered) is incompatible with traditional concepts of statistical spreading, i.e., a Gaussian carrier packet. %e have developed a stochastic transport model for I(t) which describes the dynamics of a carrier packet executing a time-dependent random walk in the presence of a field-dependent spatial bias and an absorbing barrier at the sample surface. The time dependence of the random walk is governed by hopping time distribution Q(t), A packet, generated with a f(t) characteristic of hopping in a",1975,4,2289,35,0,6,16,20,32,39,35,30,46,37
e8b2f6ce1701a9418412798f031255a6735578c2,"Successful modern generalized gradient approximations (GGA's) are biased toward atomic energies. Restoration of the first-principles gradient expansion for the exchange energy over a wide range of density gradients eliminates this bias. With many collaborators, I introduce PBEsol, a revised Perdew-Burke-Ernzerhof GGA that improves equilibrium properties of densely-packed solids and their surfaces.",2007,0,1346,1,1,0,0,0,0,0,0,0,1,2
e9b7c47d761943580610bec96fe67b41b8a0e293,"1 Overview of Solid Mechanics DEFINING A PROBLEM IN SOLID MECHANICS 2 Governing Equations MATHEMATICAL DESCRIPTION OF SHAPE CHANGES IN SOLIDS MATHEMATICAL DESCRIPTION OF INTERNAL FORCES IN SOLIDS EQUATIONS OF MOTION AND EQUILIBRIUM FOR DEFORMABLE SOLIDS WORK DONE BY STRESSES: PRINCIPLE OF VIRTUAL WORK 3 Constitutive Models: Relations between Stress and Strain GENERAL REQUIREMENTS FOR CONSTITUTIVE EQUATIONS LINEAR ELASTIC MATERIAL BEHAVIORSY HYPOELASTICITY: ELASTIC MATERIALS WITH A NONLINEAR STRESS-STRAIN RELATION UNDER SMALL DEFORMATION GENERALIZED HOOKE'S LAW: ELASTIC MATERIALS SUBJECTED TO SMALL STRETCHES BUT LARGE ROTATIONS HYPERELASTICITY: TIME-INDEPENDENT BEHAVIOR OF RUBBERS AND FOAMS SUBJECTED TO LARGE STRAINS LINEAR VISCOELASTIC MATERIALS: TIME-DEPENDENT BEHAVIOR OF POLYMERS AT SMALL STRAINS SMALL STRAIN, RATE-INDEPENDENT PLASTICITY: METALS LOADED BEYOND YIELD SMALL-STRAIN VISCOPLASTICITY: CREEP AND HIGH STRAIN RATE DEFORMATION OF CRYSTALLINE SOLIDS LARGE STRAIN, RATE-DEPENDENT PLASTICITY LARGE STRAIN VISCOELASTICITY CRITICAL STATE MODELS FOR SOILS CONSTITUTIVE MODELS FOR METAL SINGLE CRYSTALS CONSTITUTIVE MODELS FOR CONTACTING SURFACES AND INTERFACES IN SOLIDS 4 Solutions to Simple Boundary and Initial Value Problems AXIALLY AND SPHERICALLY SYMMETRIC SOLUTIONS TO QUASI-STATIC LINEAR ELASTIC PROBLEMS AXIALLY AND SPHERICALLY SYMMETRIC SOLUTIONS TO QUASI-STATIC ELASTIC-PLASTIC PROBLEMS SPHERICALLY SYMMETRIC SOLUTION TO QUASI-STATIC LARGE STRAIN ELASTICITY PROBLEMS SIMPLE DYNAMIC SOLUTIONS FOR LINEAR ELASTIC MATERIALS 5 Solutions for Linear Elastic Solids GENERAL PRINCIPLES AIRY FUNCTION SOLUTION TO PLANE STRESS AND STRAIN STATIC LINEAR ELASTIC PROBLEMS COMPLEX VARIABLE SOLUTION TO PLANE STRAIN STATIC LINEAR ELASTIC PROBLEMS SOLUTIONS TO 3D STATIC PROBLEMS IN LINEAR ELASTICITY SOLUTIONS TO GENERALIZED PLANE PROBLEMS FOR ANISOTROPIC LINEAR ELASTIC SOLIDS SOLUTIONS TO DYNAMIC PROBLEMS FOR ISOTROPIC LINEAR ELASTIC SOLIDS ENERGY METHODS FOR SOLVING STATIC LINEAR ELASTICITY PROBLEMS THE RECIPROCAL THEOREM AND APPLICATIONS ENERGETICS OF DISLOCATIONS IN ELASTIC SOLIDS RAYLEIGH-RITZ METHOD FOR ESTIMATING NATURAL FREQUENCY OF AN ELASTIC SOLID 6 Solutions for Plastic Solids SLIP-LINE FIELD THEORY BOUNDING THEOREMS IN PLASTICITY AND THEIR APPLICATIONS 7 Finite Element Analysis: An Introduction A GUIDE TO USING FINITE ELEMENT SOFTWARE A SIMPLE FINITE ELEMENT PROGRAM 8 Finite Element Analysis: Theory and Implementation GENERALIZED FEM FOR STATIC LINEAR ELASTICITY THE FEM FOR DYNAMIC LINEAR ELASTICITY FEM FOR NONLINEAR (HYPOELASTIC) MATERIALS FEM FOR LARGE DEFORMATIONS: HYPERELASTIC MATERIALS THE FEM FOR VISCOPLASTICITY ADVANCED ELEMENT FORMULATIONS: INCOMPATIBLE MODES, REDUCED INTEGRATION, AND HYBRID ELEMENTS LIST OF EXAMPLE FEA PROGRAMS AND INPUT FILES 9 Modeling Material Failure SUMMARY OF MECHANISMS OF FRACTURE AND FATIGUE UNDER STATIC AND CYCLIC LOADING STRESS- AND STRAIN-BASED FRACTURE AND FATIGUE CRITERIA MODELING FAILURE BY CRACK GROWTH: LINEAR ELASTIC FRACTURE MECHANICS ENERGY METHODS IN FRACTURE MECHANICS PLASTIC FRACTURE MECHANICS LINEAR ELASTIC FRACTURE MECHANICS OF INTERFACES 10 Solutions for Rods, Beams, Membranes, Plates, and Shells PRELIMINARIES: DYADIC NOTATION FOR VECTORS AND TENSORS MOTION AND DEFORMATION OF SLENDER RODS SIMPLIFIED VERSIONS OF THE GENERAL THEORY OF DEFORMABLE ROD EXACT SOLUTIONS TO SIMPLE PROBLEMS INVOLVING ELASTIC RODS MOTION AND DEFORMATION OF THIN SHELLS: GENERAL THEORY SIMPLIFIED VERSIONS OF GENERAL SHELL THEORY: FLAT PLATES AND MEMBRANES SOLUTIONS TO SIMPLE PROBLEMS INVOLVING MEMBRANES, PLATES, AND SHELLS Appendix A: Review of Vectors and Matrices A.1. VECTORS A.2. VECTOR FIELDS AND VECTOR CALCULUS A.3. MATRICES Appendix B: Introduction to Tensors and Their Properties B.1. BASIC PROPERTIES OF TENSORS B.2. OPERATIONS ON SECOND-ORDER TENSORS B.3. SPECIAL TENSORS Appendix C: Index Notation for Vector and Tensor Operations C.1. VECTOR AND TENSOR COMPONENTS C.2. CONVENTIONS AND SPECIAL SYMBOLS FOR INDEX NOTATION C.3. RULES OF INDEX NOTATION C.4. VECTOR OPERATIONS EXPRESSED USING INDEX NOTATION C.5. TENSOR OPERATIONS EXPRESSED USING INDEX NOTATION C.6. CALCULUS USING INDEX NOTATION C.7. EXAMPLES OF ALGEBRAIC MANIPULATIONS USING INDEX NOTATION Appendix D: Vectors and Tensor Operations in Polar Coordinates D.1. SPHERICAL-POLAR COORDINATES D.2. CYLINDRICAL-POLAR COORDINATES Appendix E: Miscellaneous Derivations E.1. RELATION BETWEEN THE AREAS OF THE FACES OF A TETRAHEDRON E.2. RELATION BETWEEN AREA ELEMENTS BEFORE AND AFTER DEFORMATION E.3. TIME DERIVATIVES OF INTEGRALS OVER VOLUMES WITHIN A DEFORMING SOLID E.4. TIME DERIVATIVES OF THE CURVATURE VECTOR FOR A DEFORMING ROD References",2009,0,1088,71,14,19,38,61,73,113,111,122,121,112
efcb8f7293255dabf4ac2aea4a2515d977847c28,"After giving a concise overview of the current knowledge in the field of quantum mechanical bonding indicators for molecules and solids, we show how to obtain energy-resolved visualization of chemical bonding in solids by means of density-functional electronic structure calculations. On the basis of a band structure energy partitioning scheme, i.e., rewriting the band structure energy as a sum of orbital pair contributions, we derive what is to be defined as crystal orbital Hamilton populations (COHP). In particular, a COHP(E) diagram indicates bonding, nonbonding, and antibonding energy regions within a specified energy range while an energy integral of a COHP gives access to the contribution of an atom or a chemical bond to the distribution of one-particle energies",1993,0,1681,11,0,0,0,0,1,0,10,8,12,14
e17791492c49aaab8ceb81af41cf135cec02fb25,"Hybrid Fock exchange/density functional theory functionals have shown to be very successful in describing a wide range of molecular properties. For periodic systems, however, the long-range nature of the Fock exchange interaction and the resultant large computational requirements present a major drawback. This is especially true for metallic systems, which require a dense Brillouin zone sampling. Recently, a new hybrid functional [HSE03, J. Heyd, G. E. Scuseria, and M. Ernzerhof, J. Chem. Phys. 118, 8207 (2003)] that addresses this problem within the context of methods that evaluate the Fock exchange in real space was introduced. We discuss the advantages the HSE03 functional brings to methods that rely on a reciprocal space description of the Fock exchange interaction, e.g., all methods that use plane wave basis sets. Furthermore, we present a detailed comparison of the performance of the HSE03 and PBE0 functionals for a set of archetypical solid state systems by calculating lattice parameters, bulk moduli, heats of formation, and band gaps. The results indicate that the hybrid functionals indeed often improve the description of these properties, but in several cases the results are not yet on par with standard gradient corrected functionals. This concerns in particular metallic systems for which the bandwidth and exchange splitting are seriously overestimated.",2006,45,1470,21,8,25,19,32,27,68,73,86,104,133
dafdc4454985809ebd7cc05b8510266b1fc99f4d,"The field of viscous liquid and glassy solid dynamics is reviewed by a process of posing the key questions that need to be answered, and then providing the best answers available to the authors and their advisors at this time. The subject is divided into four parts, three of them dealing with behavior in different domains of temperature with respect to the glass transition temperature, Tg , and a fourth dealing with ‘‘short time processes.’’ The first part tackles the high temperature regime T.Tg ,i n which the system is ergodic and the evolution of the viscous liquid toward the condition at Tg is in focus. The second part deals with the regime T;Tg , where the system is nonergodic except for very long annealing times, hence has time-dependent properties ~aging and annealing!. The third part discusses behavior when the system is completely frozen with respect to the primary relaxation process but in which secondary processes, particularly those responsible for ‘‘superionic’’ conductivity, and dopart mobility in amorphous silicon, remain active. In the fourth part we focus on the behavior of the system at the crossover between the low frequency vibrational components of the molecular motion and its high frequency relaxational components, paying particular attention to very recent developments in the short time dielectric response and the high Q mechanical response. © 2000 American Institute of Physics.@S0021-8979~00!02213-1#",2000,239,1649,17,0,19,46,53,59,72,88,85,84,63
a6de0a851ea2a5bff609be0552f1e75330f87041,"The designed construction of extended porous frameworks from soluble molecular building blocks represents one of the most challenging issues facing synthetic chemistry today. Recently, intense research activities directed toward the development of this field have included the assembly of inorganic metal clusters,1 coordination complexes,2 and organic molecules3 of great diversity into extended motifs that are held together either by strong metal-ligand bonding or by weaker bonding forces such as hydrogen-bonding and π-π interactions. Materials that have been produced in this way are referred to as modular since they are assembled from discrete molecules which can be modified to have well-defined function.4 The fact that the integrity of the building blocks is preserved during the synthesis and ultimately translated into the resulting assembled network offers numerous opportunities for designing frameworks with desirable topologies and architectures, thus paving the way for establishing connections between molecular and solid properties. At least three challenges have emerged in this area that must be reckoned with in order for the ideas of rational and designed synthesis of porous materials to become a reality with routine utility. First, it is difficult to control the orientation and stereochemistry of the building blocks in the solid state in order to achieve a given target molecular topology and architecture. Second, in most cases, the products of such assembly reactions are obtained as poorly crystalline or amorphous solids, thus prohibiting their full characterization by single-crystal X-ray diffraction techniques. Third, access to the pores within open structuressan aspect that is so critical to their utility as porous materialssis often prevented by either selfinterpenetration as observed for very open frameworks or strong host-guest interactions that lead to the destruction of the host framework when removal or exchange of guests is attempted. To define and investigate the parameters contributing to the assembly of materials from molecular building blocks, we have established a program aimed at constructing modular porous networks by linking inorganic metal sulfide clusters and organic molecules with transition metal ions. Our work has focused primarily on studying the issues outlined above, and this Account presents our progress toward finding viable and general solutions to these challenges. This is illustrated by some representative examples chosen from the chemistry developed in our research effort for the three building blocks shown in a-c. Their functionality, shape, size, and",1998,4,1721,9,3,28,58,84,84,94,109,115,90,103
76eeeed493454a0f34af822db779c932b3db074a,"A simple two pulse phase modulation (TPPM) scheme greatly reduces the residual linewidths arising from insufficient proton decoupling power in double resonance magic angle spinning (MAS) experiments. Optimization of pulse lengths and phases in the sequence produces substantial improvements in both the resolution and sensitivity of dilute spins (e.g., 13C) over a broad range of spinning speeds at high magnetic field. The theoretical complications introduced by large homo‐ and heteronuclear interactions among the spins, as well as the amplitude modulation imposed by MAS, are explored analytically and numerically. To our knowledge, this method is the first phase‐switched sequence to exhibit improvement over continuous‐wave (cw) decoupling in a strongly coupled homogeneous spin system undergoing sample spinning.",1995,26,1761,40,0,8,9,12,19,36,43,56,47,69
3ebf01f223a6b830669197903ac0e802e72dc4c0,"We present a new nonempirical density functional generalized gradient approximation (GGA) that gives significant improvements for lattice constants, crystal structures, and metal surface energies over the most popular Perdew-Burke-Ernzerhof (PBE) GGA. The new functional is based on a diffuse radial cutoff for the exchange-hole in real space, and the analytic gradient expansion of the exchange energy for small gradients. There are no adjustable parameters, the constraining conditions of PBE are maintained, and the functional is easily implemented in existing codes.",2005,30,1445,31,0,2,12,35,38,71,69,86,98,114
00671c4c337cd0aa9fb585daee1e0a56ab5141fa,"Recently we developed an efficient broadband decoupling sequence called SPARC-16 for liquid crystals ¿J. Magn. Reson. 130, 317 (1998). The sequence is based upon a 16-step phase cycling of the 2-step TPPM decoupling method for solids ¿J. Chem. Phys. 103, 6951 (1995). Since then, we have found that a stepwise variation of the phase angle in the TPPM sequence offers even better results. The application of this new method to a liquid crystalline compound, 4-n-pentyl-4'-cyanobiphenyl, and a solid, L-tyrosine hydrochloride, is reported. The reason for the improvement is explained by an analysis of the problem in the rotating frame.",2000,28,1475,46,0,6,4,11,10,17,20,57,59,61
85de15f1af6793fef9a9ab9d21610bcc330dd56f,"CKA~K growth initiation and subsequent resistance is computed for an elastic-plastic solid with an idealized traction separation law specified on the crack plane to characterize the fracture process. The solid is specified by its Young’s modulus, E, Poisson’s ratio, v, initial tensile yield stress, (or, and strain hardening exponent, N. The primary parameters specifying the traction-separation law of the fracture process are the work of separation per unit area, To. and the peak traction, 6. Highly refined calculations have been carried out for resistance curves. K,(Arr), for plane strain, mode I growth in small-scale yielding as dependent on the parameters characterizing the elastic-plastic properties of the solid and its fracture process. With K,, = [El-,/( I ~ v’)] ’ 2 as the intensity needed to advance the crack in the absence ofplasticity, K,J& is presented in terms of its dependence on the two most important parameters, d/nr and N, with special emphasis on initiation toughness and steady-state toughness, Three applications of the results are made : to predict toughnesss when the fracture process is void growth and coalescence, to predict the role of plasticity on interface toughness for similar materials bonded together, and to illuminate the role of plasticity in enhancing toughness in dual-phase solids. The regime of applicability of the present model to ductile fracture due to void growth and coalescence, wherein multiple voids interact within the fracture process zone, is complementary to the regime of applicability of models describing the interaction between a single void and the crack tip. The two mechanism regimes are delineated and the consequence of a transition between them is discussed.",1992,11,1558,81,1,5,7,14,12,22,18,21,27,28
55a95350e3bf5ea85867ef932a86c775bbb50329,"The present work introduces an efficient screening technique to take advantage of the fast spatial decay of the short range Hartree-Fock (HF) exchange used in the Heyd-Scuseria-Ernzerhof (HSE) screened Coulomb hybrid density functional. The screened HF exchange decay properties and screening efficiency are compared with traditional hybrid functional calculations on solids. The HSE functional is then assessed using 21 metallic, semiconducting, and insulating solids. The examined properties include lattice constants, bulk moduli, and band gaps. The results obtained with HSE exhibit significantly smaller errors than pure density functional theory (DFT) calculations. For structural properties, the errors produced by HSE are up to 50% smaller than the errors of the local density approximation, PBE, and TPSS functionals used for comparison. When predicting band gaps of semiconductors, we found smaller errors with HSE, resulting in a mean absolute error of 0.2 eV (1.3 eV error for all pure DFT functionals). In addition, we present timing results which show the computational time requirements of HSE to be only a factor of 2-4 higher than pure DFT functionals. These results make HSE an attractive choice for calculations of all types of solids.",2004,36,1380,11,2,8,21,11,24,20,18,49,72,79
c9ec00a9e5c94c71c314c3baffeb7e6438306a14,"We propose a dynamical theory of low-temperature shear deformation in amorphous solids. Our analysis is based on molecular-dynamics simulations of a two-dimensional, two-component noncrystalline system. These numerical simulations reveal behavior typical of metallic glasses and other viscoplastic materials, specifically, reversible elastic deformation at small applied stresses, irreversible plastic deformation at larger stresses, a stress threshold above which unbounded plastic flow occurs, and a strong dependence of the state of the system on the history of past deformations. Microscopic observations suggest that a dynamically complete description of the macroscopic state of this deforming body requires specifying, in addition to stress and strain, certain average features of a population of two-state shear transformation zones. Our introduction of these state variables into the constitutive equations for this system is an extension of earlier models of creep in metallic glasses. In the treatment presented here, we specialize to temperatures far below the glass transition and postulate that irreversible motions are governed by local entropic fluctuations in the volumes of the transformation zones. In most respects, our theory is in good quantitative agreement with the rich variety of phenomena seen in the simulations. {copyright} {ital 1998} {ital The American Physical Society}",1997,0,1370,57,0,1,1,3,8,7,19,10,32,38
f0669eb9296f28677450d44f36ca9aafc899e809,"The NMR signals of isotopically or chemically dilute nuclear spins S in solids can be enhanced by repeatedly transferring polarization from a more abundant species I of high abundance (usually protons) to which they are coupled. The gain in power sensitivity as compared with conventional observation of the rare spins approaches NII(I+1)γI2/NSS(S+1)γS2, or ∼ 103 for S = 13C, I = 1H in organic solids. The transfer of polarization is accomplished by any of a number of double resonance methods. High‐frequency resolution of the S ‐spin signal is obtained by decoupling of the abundant spins. The experimental requirements of the technique are discussed and a brief comparison of its sensitivity with other procedures is made. Representative applications and experimental results are mentioned.",1973,69,2012,13,1,9,11,17,21,30,37,36,49,51
548af6be21fed61a54ab3e2285ace76004094642,"This article describes the variational and fixed-node diffusion quantum Monte Carlo methods and how they may be used to calculate the properties of many-electron systems. These stochastic wave-function-based approaches provide a very direct treatment of quantum many-body effects and serve as benchmarks against which other techniques may be compared. They complement the less demanding density-functional approach by providing more accurate results and a deeper understanding of the physics of electronic correlation in real materials. The algorithms are intrinsically parallel, and currently available high-performance computers allow applications to systems containing a thousand or more electrons. With these tools one can study complicated problems such as the properties of surfaces and defects, while including electron correlation effects with high precision. The authors provide a pedagogical overview of the techniques and describe a selection of applications to ground and excited states of solids and clusters.",2001,213,1373,40,5,24,36,45,48,45,52,50,45,71
c52b1b1308f122245761e05ba64f7aaa0ac8952c,The Formation of Amorphous Solids Amorphous Morphology: The Geometry and Topology of Disorder Chalcogenide Glasses and Organic Polymers The Percolation Model Localization Delocalization Transitions Optical and Electrical Properties Index.,1983,0,1843,70,1,5,26,23,25,42,37,48,50,46
404dccc6f1ebb4b39869e1a80a1938428bf3cc59,"When chopped light impinges on a solid in an enclosed cell, an acoustic signal is produced within the cell. This effect is the basis of a new spectroscopic technique for the study of solid and semisolid matter. A quantitative derivation is presented for the acoustic signal in a photoacoustic cell in terms of the optical, thermal, and geometric parameters of the system. The theory predicts the dependence of the signal on the absorption coefficient of the solid, thereby giving a theoretical foundation for the technique of photoacoustic spectroscopy. In particular, the theory accounts for the experimental observation that with this technique optical absorption spectra can be obtained for materials that are optically opaque.",1976,11,2070,8,2,22,27,38,66,54,55,39,55,49
9eb18fbbfb909cb450cfc96a57dbdfe528a71d79,"Many attempts have been made to reproduce theoretically the stress–strain curves obtained from experiments on the isothermal deformation of highly elastic ‘rubberlike' materials. The existence of a strain-energy function has usually been postulated, and the simplifications appropriate to the assumptions of isotropy and incompressibility have been exploited. However, the usual practice of writing the strain energy as a function of two independent strain invariants has, in general, the effect of complicating the associated mathematical analysis (this is particularly evident in relation to the calculation of instantaneous moduli of elasticity) and, consequently, the basic elegance and simplicity of isotropic elasticity is sacrificed. Furthermore, recently proposed special forms of the strain-energy function are rather complicated functions of two invariants. The purpose of this paper is, while making full use of the inherent simplicity of isotropic elasticity, to construct a strain-energy function which: (i) provides an adequate representation of the mechanical response of rubberlike solids, and (ii) is simple enough to be amenable to mathematical analysis. A strain-energy function which is a linear combination of strain invariants defined by ϕ(α) = (a1α+a2α+a3α–3)/α is proposed; and the principal stretches a1, a2 and a3 are used as independent variables subject to the incompressibility constraint a1a2 a3 = 1. Principal axes techniques are used where appropriate. An excellent agreement between this theory and the experimental data from simple tension, pure shear and equibiaxial tension tests is demonstrated. It is also shown that the present theory has certain repercussions in respect of the constitutive inequality proposed by Hill (1968a, 1970b).",1972,15,1957,83,0,3,2,2,6,3,6,6,6,5
f628fb70cbb073f6526f5291a4c359d43811245f,,1988,0,1511,178,3,1,1,8,3,4,5,7,21,21
1c3cec66c7eb580e8784c03d0dff9ac7913b26f2,"We discuss evolution of the Fermi surface (FS) topology with doping in electron-doped cuprates within the framework of a one-band Hubbard Hamiltonian, where antiferromagnetism and superconductivity are assumed to coexist in a uniform phase. In the lightly doped insulator, the FS consists of electron pockets around the ðp;0Þ points. The first change in the FS topology occurs in the optimally doped region when an additional hole pocket appears at the nodal point. The second change in topology takes place in the overdoped regime ð 18%Þwhere antiferromagnetism disappears and a large ðp;pÞ-centered metallic FS is formed. Evidence for these two topological transitions is found in recent Hall effect and penetration depth experiments on Pr2 xCexCuO4 d (PCCO) and with a number of spectroscopic measurements on Nd2 xCexCuO4 d (NCCO). & 2008 Elsevier Ltd. All rights reserved.",2008,14,1121,97,25,27,44,44,64,76,82,84,94,100
5b5ee79def11444b2e35d0d54aa1f010840bb78a,"We develop a method which permits the analysis of problems requiring the simultaneous resolution of continuum and atomistic length scales-and associated deformation processes-in a unified manner. A finite element methodology furnishes a continuum statement of the problem of interest and provides the requisite multiple-scale analysis capability by adaptively refining the mesh near lattice defects and other highly energetic regions. The method differs from conventional finite element analyses in that interatomic interactions are incorporated into the model through a crystal calculation based on the local state of deformation. This procedure endows the model with crucial properties, such as slip invariance, which enable the emergence of dislocations and other lattice defects. We assess the accuracy of the theory in the atomistic limit by way of three examples: a stacking fault on the (111) plane, and edge dislocations residing on (111) and (100) planes of an aluminium single crystal. The method correctly predicts the splitting of the (111) edge dislocation into Shockley partials. The computed separation of these partials is consistent with results obtained by direct atomistic simulations. The method predicts no splitting of the Al Lomer dislocation, in keeping with observation and the results of direct atomistic simulation. In both cases, the core structures are found to be in good agreement with direct lattice statics calculations, which attests to the accuracy of the method at the atomistic scale.",1996,24,1396,64,2,6,20,15,17,18,25,38,55,73
918643b95bf818415eb13794068a4f32ea8855c2,"Mathematical Introduction. Acoustic Phonons. Plasmons, Optical Phonons, and Polarization Waves. Magnons. Fermion Fields and the Hartree--Forck Approximation. Many--Body Techniques and the Electron Gas. Polarons and the Electron--Phonon Interaction. Superconductivity. Bloch Funcations----General Properties. Brillouin Zones and Crystal Symmetry. Dynamics of Electronics in a Magnetic Field: de Hass--van Alphen Effect and Cyclotron Resonance. Magnetoresistance. Calculation of Energy Bands and Fermi Surfaces. Semiconductor Crystals: I. Energy Bands, Cyclotron Resonance and Impurity States. Semiconductor Crystals: II. Optical Absorption and Excitons. Electrodynamics of Metals. Acoustic Attenuation in Metals. Theory of Alloys. Correlation Functions and Neutron Diffraction by Crystals. Recoilless Emission. Greena s Functions----Application to Solid State Physics. Appendixes.",1963,0,1940,63,0,2,4,19,16,13,32,19,25,35
6eddc19efa13f7e70301908d98e85a19d6f32a02,"Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed.",2002,46,30878,4468,0,0,0,0,0,0,0,0,0,0
5c8fe9a0412a078e30eb7e5eeb0068655b673e86,"Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLARANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.",1996,19,17002,2427,0,0,0,0,0,0,0,0,0,0
3c718363c22221fd16771672da3bfd5f67d2c34c,"We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.",2009,42,8805,1393,71,136,273,396,568,732,879,973,941,1108
d36efb9ad91e00faa334b549ce989bfae7e2907a,"Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed.",1977,136,47883,2148,0,0,0,0,0,0,0,0,0,1
023f6fc69fe1f6498e35dbf85932ecb549d36ca4,"This paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints. This problem may be understood as the convex relaxation of a rank minimization problem and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix problem). Off-the-shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries. This paper develops a simple first-order and easy-to-implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank. The algorithm is iterative, produces a sequence of matrices $\{\boldsymbol{X}^k,\boldsymbol{Y}^k\}$, and at each step mainly performs a soft-thresholding operation on the singular values of the matrix $\boldsymbol{Y}^k$. There are two remarkable features making this attractive for low-rank matrix completion problems. The first is that the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of the iterates $\{\boldsymbol{X}^k\}$ is empirically nondecreasing. Both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low. On the theoretical side, we provide a convergence analysis showing that the sequence of iterates converges. On the practical side, we provide numerical examples in which $1,000\times1,000$ matrices are recovered in less than a minute on a modest desktop computer. We also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are connected with the recent literature on linearized Bregman iterations for $\ell_1$ minimization, and we develop a framework in which one can understand these algorithms in terms of well-known Lagrange multiplier algorithms.",2008,113,4463,396,3,49,89,139,190,257,371,413,480,551
d260b5c495daec9149319da796b9710f53deb8f3,"The increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. We describe a new approach, based on the maximum- likelihood principle, which clearly satisfies these requirements. The core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. This algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. Due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. We used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. The reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. For example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 rbcL sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. This new method is implemented in the PHYML program, which is freely available on our web page: http://www.lirmm.fr/w3ifa/MAAS/.",2003,70,15598,3668,0,0,0,0,0,0,1,3,231,1356
8978cf7574ceb35f4c3096be768c7547b28a35d0,"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",2006,33,12847,1058,0,0,0,0,0,1,1,6,29,661
034b2b97e6b23061f6f71a5e19c1b03bf4c19ec8,"In recent years, various heuristic optimization methods have been developed. Many of these methods are inspired by swarm behaviors in nature. In this paper, a new optimization algorithm based on the law of gravity and mass interactions is introduced. In the proposed algorithm, the searcher agents are a collection of masses which interact with each other based on the Newtonian gravity and the laws of motion. The proposed method has been compared with some well-known heuristic search methods. The obtained results confirm the high performance of the proposed method in solving various nonlinear functions.",2009,41,4296,389,2,20,64,127,211,299,373,398,453,512
288f41a655a178bf28d5883f68aa95807edbc950,,1963,0,26731,779,0,0,0,0,0,0,0,0,0,0
7bb9bab74df4d2939bbdf41fc33027b59e0f229e,"We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from t...",1986,15,10251,1068,0,0,0,0,0,0,83,108,164,191
2599131a4bc2fa957338732a37c744cfe3e17b24,"A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.",1992,41,10462,814,0,0,0,1,0,2,1,2,0,3
d92f735b0773b4e697e7e72798eccae2f647acd6,"We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.",1987,36,11953,518,0,0,2,3,2,2,7,4,3,4
730543ef418a898cb94b5854b79ea35b082abf89,"Despite recent advances achieved by application of high-performance computing methods and novel algorithmic techniques to maximum likelihood (ML)-based inference programs, the major computational bottleneck still consists in the computation of bootstrap support values. Conducting a probably insufficient number of 100 bootstrap (BS) analyses with current ML programs on large datasets-either with respect to the number of taxa or base pairs-can easily require a month of run time. Therefore, we have developed, implemented, and thoroughly tested rapid bootstrap heuristics in RAxML (Randomized Axelerated Maximum Likelihood) that are more than an order of magnitude faster than current algorithms. These new heuristics can contribute to resolving the computational bottleneck and improve current methodology in phylogenetic analyses. Computational experiments to assess the performance and relative accuracy of these heuristics were conducted on 22 diverse DNA and AA (amino acid), single gene as well as multigene, real-world alignments containing 125 up to 7764 sequences. The standard BS (SBS) and rapid BS (RBS) values drawn on the best-scoring ML tree are highly correlated and show almost identical average support values. The weighted RF (Robinson-Foulds) distance between SBS- and RBS-based consensus trees was smaller than 6% in all cases (average 4%). More importantly, RBS inferences are between 8 and 20 times faster (average 14.73) than SBS analyses with RAxML and between 18 and 495 times faster than BS analyses with competing programs, such as PHYML or GARLI. Moreover, this performance improvement increases with alignment size. Finally, we have set up two freely accessible Web servers for this significantly improved version of RAxML that provide access to the 200-CPU cluster of the Vital-IT unit at the Swiss Institute of Bioinformatics and the 128-CPU cluster of the CIPRES project at the San Diego Supercomputer Center. These Web servers offer the possibility to conduct large-scale phylogenetic inferences to a large part of the community that does not have access to, or the expertise to use, high-performance computing resources.",2008,44,6295,1446,17,185,416,510,570,655,675,582,568,500
0e6beb95b5150ce99b108acdefabf70ccd3fee30,"An efficient method for the calculation of the interactions of a 2' factorial ex- periment was introduced by Yates and is widely known by his name. The generaliza- tion to 3' was given by Box et al. (1). Good (2) generalized these methods and gave elegant algorithms for which one class of applications is the calculation of Fourier series. In their full generality, Good's methods are applicable to certain problems in which one must multiply an N-vector by an N X N matrix which can be factored into m sparse matrices, where m is proportional to log N. This results inma procedure requiring a number of operations proportional to N log N rather than N2. These methods are applied here to the calculation of complex Fourier series. They are useful in situations where the number of data points is, or can be chosen to be, a highly composite number. The algorithm is here derived and presented in a rather different form. Attention is given to the choice of N. It is also shown how special advantage can be obtained in the use of a binary computer with N = 2' and how the entire calculation can be performed within the array of N data storage locations used for the given Fourier coefficients. Consider the problem of calculating the complex Fourier series N-1 (1) X(j) = EA(k)-Wjk, j = 0 1, * ,N- 1, k=0",1965,8,11146,601,0,0,3,1,0,2,3,4,1,3
b76be6707fa5858cc5378bc11f10ec6f6a97d85c,"Decomposition is a basic strategy in traditional multiobjective optimization. However, it has not yet been widely used in multiobjective evolutionary optimization. This paper proposes a multiobjective evolutionary algorithm based on decomposition (MOEA/D). It decomposes a multiobjective optimization problem into a number of scalar optimization subproblems and optimizes them simultaneously. Each subproblem is optimized by only using information from its several neighboring subproblems, which makes MOEA/D have lower computational complexity at each generation than MOGLS and nondominated sorting genetic algorithm II (NSGA-II). Experimental results have demonstrated that MOEA/D with simple decomposition methods outperforms or performs similarly to MOGLS and NSGA-II on multiobjective 0-1 knapsack problems and continuous multiobjective optimization problems. It has been shown that MOEA/D using objective normalization can deal with disparately-scaled objectives, and MOEA/D with an advanced decomposition method can generate a set of very evenly distributed solutions for 3-objective test instances. The ability of MOEA/D with small population, the scalability and sensitivity of MOEA/D have also been experimentally investigated in this paper.",2007,48,4806,772,4,17,40,65,82,137,190,265,348,404
321759af5a7d74afeb515719d8da25fba4759764,"Abstract.We present a primal-dual interior-point algorithm with a filter line-search method for nonlinear programming. Local and global convergence properties of this method were analyzed in previous work. Here we provide a comprehensive description of the algorithm, including the feasibility restoration phase for the filter method, second-order corrections, and inertia correction of the KKT matrix. Heuristics are also considered that allow faster performance. This method has been implemented in the IPOPT code, which we demonstrate in a detailed numerical study based on 954 problems from the CUTEr test set. An evaluation is made of several line-search options, and a comparison is provided with two state-of-the-art interior-point codes for nonlinear programming.",2006,35,5809,565,42,71,129,129,153,208,274,301,392,417
21b65bbf11bc246a80b7f08043db3ff5c0844e91,"This paper describes DARTEL, which is an algorithm for diffeomorphic image registration. It is implemented for both 2D and 3D image registration and has been formulated to include an option for estimating inverse consistent deformations. Nonlinear registration is considered as a local optimisation problem, which is solved using a Levenberg-Marquardt strategy. The necessary matrix solutions are obtained in reasonable time using a multigrid method. A constant Eulerian velocity framework is used, which allows a rapid scaling and squaring method to be used in the computations. DARTEL has been applied to intersubject registration of 471 whole brain images, and the resulting deformations were evaluated in terms of how well they encode the shape information necessary to separate male and female subjects and to predict the ages of the subjects.",2007,45,5930,409,1,26,65,107,213,323,411,549,568,546
83b522f4bfa5db7f7d34f839475af7d078107634,"In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data",2006,55,7245,448,10,24,58,104,153,245,377,558,732,838
4cb5dd388ea707bd421226d8c1ef61e7f56982e2,"Swarm intelligence is a research branch that models the population of interacting agents or swarms that are able to self-organize. An ant colony, a flock of birds or an immune system is a typical example of a swarm system. Bees’ swarming around their hive is another example of swarm intelligence. Artificial Bee Colony (ABC) Algorithm is an optimization algorithm based on the intelligent behaviour of honey bee swarm. In this work, ABC algorithm is used for optimizing multivariable functions and the results produced by ABC, Genetic Algorithm (GA), Particle Swarm Algorithm (PSO) and Particle Swarm Inspired Evolutionary Algorithm (PS-EA) have been compared. The results showed that ABC outperforms the other algorithms.",2007,30,5512,398,4,7,43,96,210,288,395,434,513,524
c02dfd94b11933093c797c362e2f8f6a3b9b8012,"Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.",2001,14,8196,661,0,19,69,112,159,189,217,269,322,391
68c1bfe375dde46777fe1ac8f3636fb651e3f0f8,"In an earlier paper, we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's ""bagging"" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",1996,34,8451,718,16,46,81,115,118,128,172,230,305,320
f6e0fb4c77906bc23fe59a8f848ce62ba9687181,"In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a pre-specified set of linear transforms, or by adapting the dictionary to a set of training signals. Both these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method – the K-SVD algorithm – generalizing the K-Means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary, and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results on both synthetic tests and in applications on real image data.",2005,49,5949,1135,4,9,26,53,92,133,226,346,489,637
f813ebf28ae4bd689a8f263b909a2f847b0fcf99,"Phylogenetic trees have a multitude of applications in biology, epidemiology, conservation and even forensics. However, the inference of phylogenetic trees can be extremely computationally intensive. The computational burden of such analyses becomes even greater when model-based methods are used. Model-based methods have been repeatedly shown to be the most accurate choice for the reconstruction of phylogenetic trees, and thus are an attractive choice despite their high computational demands. Using the Maximum Likelihood (ML) criterion to choose among phylogenetic trees is one commonly used model-based technique. Until recently, software for performing ML analyses of biological sequence data was largely intractable for more vi than about one hundred sequences. Because advances in sequencing technology now make the assembly of datasets consisting of thousands of sequences common, ML search algorithms that are able to quickly and accurately analyze such data must be developed if ML techniques are to remain a viable option in the future. I have developed a fast and accurate algorithm that allows ML phylogenetic searches to be performed on datasets consisting of thousands of sequences. My software uses a genetic algorithm approach, and is named GARLI (Genetic Algorithm for Rapid Likelihood Inference). The speed of this new algorithm results primarily from its novel technique for partial optimization of branch-length parameters following topological rearrangements. Experiments performed with GARLI show that it is able to analyze large datasets in a small fraction of the time required by the previous generation of search algorithms. The program also performs well relative to two other recently introduced fast ML search programs. Large parallel computer clusters have become common at academic institutions in recent years, presenting a new resource to be used for phylogenetic analyses. The P-GARLI algorithm extends the approach of GARLI to allow simultaneous use of many computer processors. The processors may be instructed to work together on a phylogenetic search in either a highly coordinated or largely independent fashion.",2006,79,3335,1291,2,53,184,236,321,321,334,337,338,313
9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d,"We propose a new measure, the method noise, to evaluate and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local smoothing filters. Second, we propose a new algorithm, the nonlocal means (NL-means), based on a nonlocal averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local smoothing filters.",2005,22,5575,518,5,19,43,62,96,149,211,272,360,422
29d4dfae2807a67a2c66c720b4985cb599c4e245,"Many networks display community structure--groups of vertices within which connections are dense but between which they are sparser--and sensitive computer algorithms have in recent years been developed for detecting this structure. These algorithms, however, are computationally demanding, which limits their application to small networks. Here we describe an algorithm which gives excellent results when tested on both computer-generated and real-world networks and is much faster, typically thousands of times faster, than previous algorithms. We give several example applications, including one to a collaboration network of more than 50,000 physicists.",2003,45,4515,416,3,15,51,80,117,151,233,207,244,287
a2ec66b68bdc119443a3b07448f4fab3258f314a,"In reporting Implicit Association Test (IAT) results, researchers have most often used scoring conventions described in the first publication of the IAT (A.G. Greenwald, D.E. McGhee, & J.L.K. Schwartz, 1998). Demonstration IATs available on the Internet have produced large data sets that were used in the current article to evaluate alternative scoring procedures. Candidate new algorithms were examined in terms of their (a) correlations with parallel self-report measures, (b) resistance to an artifact associated with speed of responding, (c) internal consistency, (d) sensitivity to known influences on IAT measures, and (e) resistance to known procedural influences. The best-performing measure incorporates data from the IAT's practice trials, uses a metric that is calibrated by each respondent's latency variability, and includes a latency penalty for errors. This new algorithm strongly outperforms the earlier (conventional) procedure.",2003,37,4684,370,9,28,53,91,154,150,212,243,280,320
b13724cb54ae4171916f3f969d304b9e9752a57f,"The Strength Pareto Evolutionary Algorithm (SPEA) (Zitzler and Thiele 1999) is a relatively recent technique for finding or approximating the Pareto-optimal set for multiobjective optimization problems. In different studies (Zitzler and Thiele 1999; Zitzler, Deb, and Thiele 2000) SPEA has shown very good performance in comparison to other multiobjective evolutionary algorithms, and therefore it has been a point of reference in various recent investigations, e.g., (Corne, Knowles, and Oates 2000). Furthermore, it has been used in different applications, e.g., (Lahanas, Milickovic, Baltas, and Zamboglou 2001). In this paper, an improved version, namely SPEA2, is proposed, which incorporates in contrast to its predecessor a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method. The comparison of SPEA2 with SPEA and two other modern elitist methods, PESA and NSGA-II, on different test problems yields promising results.",2001,48,4842,716,4,26,61,68,100,109,202,195,262,270
4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a,"We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted p-penalties on the coefficients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. Use of such p-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. © 2004 Wiley Periodicals, Inc.",2003,55,4189,371,2,8,28,31,100,159,215,212,254,250
7c46799502bebfe6a9ae0f457b7b8b92248ec260,"An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",1980,47,7946,298,3,14,27,9,28,44,61,54,102,110
18f355d7ef4aa9f82bf5c00f84e46714efa5fd77,"This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm.",1978,11,5293,507,2,7,15,15,33,32,24,23,36,25
aa7bd176c83cef1ccad8cfcfb10e73713bfa8e8d,"We have developed a real-time algorithm for detection of the QRS complexes of ECG signals. It reliably recognizes QRS complexes based upon digital analyses of slope, amplitude, and width. A special digital bandpass filter reduces false detections caused by the various types of interference present in ECG signals. This filtering permits use of low thresholds, thereby increasing detection sensitivity. The algorithm automatically adjusts thresholds and parameters periodically to adapt to such ECG changes as QRS morphology and heart rate. For the standard 24 h MIT/BIH arrhythmia database, this algorithm correctly detects 99.3 percent of the QRS complexes.",1985,14,5812,400,1,3,2,8,7,15,12,12,14,10
070eb63031af260d840534b33fd8385b7152fda6,"For aligning DNA sequences that differ only by sequencing errors, or by equivalent errors from other sources, a greedy algorithm can be much faster than traditional dynamic programming approaches and yet produce an alignment that is guaranteed to be theoretically optimal. We introduce a new greedy alignment algorithm with particularly good performance and show that it computes the same alignment as does a certain dynamic programming algorithm, while executing over 10 times faster on appropriate data. An implementation of this algorithm is currently used in a program that assembles the UniGene database at the National Center for Biotechnology Information.",2000,38,4173,523,2,13,24,41,59,54,86,99,85,158
7d50991b693fc23edda316fb1487f114f6cc6706,"The first unified account of the theory, methodology, and applications of the EM algorithm and its extensionsSince its inception in 1977, the Expectation-Maximization (EM) algorithm has been the subject of intense scrutiny, dozens of applications, numerous extensions, and thousands of publications. The algorithm and its extensions are now standard tools applied to incomplete data problems in virtually every field in which statistical methods are used. Until now, however, no single source offered a complete and unified treatment of the subject.The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. Employing numerous examples, Geoffrey McLachlan and Thriyambakam Krishnan examine applications both in evidently incomplete data situations-where data are missing, distributions are truncated, or observations are censored or grouped-and in a broad variety of situations in which incompleteness is neither natural nor evident. They point out the algorithm's shortcomings and explain how these are addressed in the various extensions.Areas of application discussed include: Regression Medical imaging Categorical data analysis Finite mixture analysis Factor analysis Robust statistical modeling Variance-components estimation Survival analysis Repeated-measures designs For theoreticians, practitioners, and graduate students in statistics as well as researchers in the social and physical sciences, The EM Algorithm and Extensions opens the door to the tremendous potential of this remarkably versatile statistical tool.",1996,1,6018,352,2,19,47,70,117,134,180,219,234,294
688384fc5e643445e835435e96b9dfcfb6598d36,"An algorithm is presented for the rapid evaluation of the potential and force fields in systems involving large numbers of particles whose interactions are Coulombic or gravitational in nature. For a system ofNparticles, an amount of work of the orderO(N2) has traditionally been required to evaluate all pairwise interactions, unless some approximation or truncation method is used. The algorithm of the present paper requires an amount of work proportional toNto evaluate all interactions to within roundoff error, making it considerably more practical for large-scale problems encountered in plasma physics, fluid dynamics, molecular dynamics, and celestial mechanics.",1987,19,4715,380,2,12,10,25,27,27,40,49,84,101
27e2d4a36ce20138cfb05c1ea60d7c6dceb20392,"Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information. However, as recent experience shows, the anonymous, open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files.We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value, based on the peer's history of uploads. We present a distributed and secure method to compute global trust values, based on Power iteration. By having peers use these global trust values to choose the peers from whom they download, the network effectively identifies malicious peers and isolates them from the network.In simulations, this reputation system, called EigenTrust, has been shown to significantly decrease the number of inauthentic files on the network, even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system.",2003,23,3806,369,27,118,174,273,294,294,306,322,304,298
ba293ef8bfac8c0483521e325c1578586ccb3f13,"The particle swarm algorithm adjusts the trajectories of a population of ""particles"" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.",1997,7,4176,434,1,4,5,1,3,9,8,38,58,93
59c9f2036e673d8bc9713eed851d12c6c9fe53cb,A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.,1977,16,5426,352,0,5,2,1,2,3,2,3,12,6
08c370eb9ba13bfb836349e7f3ea428be4697818,"Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of ""local"" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative ""turbo"" decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms.",2001,65,3956,294,33,56,96,84,162,200,224,197,246,234
298d799da82395a64a3bda38ef9d2a4646828ccb,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",1996,26,5311,316,14,35,75,66,77,76,93,130,121,120
229a5963aa3649b5a9ad714c5764b07ddd8918d5,"Prediction of small molecule binding modes to macromolecules of known three-dimensional structure is a problem of paramount importance in rational drug design (the ""docking"" problem). We report the development and validation of the program GOLD (Genetic Optimisation for Ligand Docking). GOLD is an automated ligand docking program that uses a genetic algorithm to explore the full range of ligand conformational flexibility with partial flexibility of the protein, and satisfies the fundamental requirement that the ligand must displace loosely bound water on binding. Numerous enhancements and modifications have been applied to the original technique resulting in a substantial increase in the reliability and the applicability of the algorithm. The advanced algorithm has been tested on a dataset of 100 complexes extracted from the Brookhaven Protein DataBank. When used to dock the ligand back into the binding site, GOLD achieved a 71% success rate in identifying the experimental binding mode.",1997,125,4990,328,2,21,31,34,38,54,79,120,126,159
64a877d135db3acbc23c295367927176f332595f,"Abstract This paper transmits a FORTRAN-IV coding of the fuzzy c -means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.",1984,28,4548,358,1,1,4,1,4,1,6,2,13,5
916ceefae4b11dadc3ee754ce590381c568c90de,"A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques.<<ETX>>",1993,6,4378,309,5,14,20,38,60,70,88,77,107,109
435c91f9103f7e2314f7e1298ed18ff4cd602f0b,"This document describes the MD5 message-digest algorithm. The
algorithm takes as input a message of arbitrary length and produces as
output a 128-bit ""fingerprint"" or ""message digest"" of the input. This
memo provides information for the Internet community. It does not
specify an Internet standard.",1992,0,4032,328,11,29,32,62,81,94,103,111,111,127
30cadff20998ea7bea6da42fa0eed48334fdde1e,"An iterative method is given for solving Ax ~ffi b and minU Ax b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned.",1982,38,3834,315,3,2,4,5,10,12,20,19,31,26
e99603b5b524485bcf1afb2f01acadc34dfb033c,,1979,2,8899,212,0,1,4,1,1,1,3,0,3,4
ae0ae3baf338f5454d58074d58386246eb5009e5,"A convolution-backprojection formula is deduced for direct reconstruction of a three-dimensional density function from a set of two-dimensional projections. The formula is approximate but has useful properties, including errors that are relatively small in many practical instances and a form that leads to convenient computation. It reduces to the standard fan-beam formula in the plane that is perpendicular to the axis of rotation and contains the point source. The algorithm is applied to a mathematical phantom as an example of its performance.",1984,17,5591,246,1,1,3,5,13,14,27,20,23,24
a651bb7cc7fc68ece0cc66ab921486d163373385,"Purpose – The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach – An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings – Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value – The piece provides a useful historical document on information retrieval.",1980,13,6360,248,1,1,1,0,1,1,2,5,1,3
78e3847fde0b618a0702606323ccebeda3ecb77d,"The k_t and Cambridge/Aachen inclusive jet finding algorithms for hadron-hadron collisions can be seen as belonging to a broader class of sequential recombination jet algorithms, parametrised by the power of the energy scale in the distance measure. We examine some properties of a new member of this class, for which the power is negative. This ``anti-k_t'' algorithm essentially behaves like an idealised cone algorithm, in that jets with only soft fragmentation are conical, active and passive areas are equal, the area anomalous dimensions are zero, the non-global logarithms are those of a rigid boundary and the Milan factor is universal. None of these properties hold for existing sequential recombination algorithms, nor for cone algorithms with split--merge steps, such as SISCone. They are however the identifying characteristics of the collinear unsafe plain ``iterative cone'' algorithm, for which the anti-k_t algorithm provides a natural, fast, infrared and collinear safe replacement.",2008,61,5006,161,12,27,91,231,311,374,408,488,542,556
e2b9dcd4e75379a59bf7e761ac0710f42bde2f14,"Many optimization problems in various fields have been solved using diverse optimization al gorithms. Traditional optimization techniques such as linear programming (LP), non-linear programming (NLP), and dynamic program ming (DP) have had major roles in solving these problems. However, their drawbacks generate demand for other types of algorithms, such as heuristic optimization approaches (simulated annealing, tabu search, and evolutionary algo rithms). However, there are still some possibili ties of devising new heuristic algorithms based on analogies with natural or artificial phenom ena. A new heuristic algorithm, mimicking the improvisation of music players, has been devel oped and named Harmony Search (HS). The performance of the algorithm is illustrated with a traveling salesman problem (TSP), a specific academic optimization problem, and a least-cost pipe network design problem.",2001,17,4691,210,1,1,0,4,9,13,20,36,104,158
19d4da2841588fc75ccedcfb443d73772413d98e,"The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.",2001,32,3824,272,2,28,67,81,98,132,168,143,176,186
a3819dda9a5f00dbb8cd3413ca7422e37a0d5794,"A fast and flexible algorithm for computing watersheds in digital gray-scale images is introduced. A review of watersheds and related motion is first presented, and the major methods to determine watersheds are discussed. The algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using of queue of pixel. It is described in detail provided in a pseudo C language. The accuracy of this algorithm is proven to be superior to that of the existing implementations, and it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images (and even to graphs) are straightforward. The algorithm is reported to be faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for magnetic resonance (MR) imagery and for digital elevation models. An example of 3-D watershed is also provided. >",1991,69,5697,221,2,13,21,18,37,63,68,72,76,90
ce9a21b93ba29d4145a8ef6bf401e77f261848de,"The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.",1989,27,3695,262,21,68,62,154,113,104,86,89,101,93
e085f9a107c562a59b28c1136aee7a438387ac0f,"Abstract We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.",1995,76,3800,230,5,15,22,46,63,75,84,77,90,106
d45f2b8f6819fc4c524794b3cfc0fa4c4f7d43f4,"Detection of protein families in large databases is one of the principal research objectives in structural and functional genomics. Protein family classification can significantly contribute to the delineation of functional diversity of homologous proteins, the prediction of function based on domain architecture or the presence of sequence motifs as well as comparative genomics, providing valuable evolutionary insights. We present a novel approach called TRIBE-MCL for rapid and accurate clustering of protein sequences into families. The method relies on the Markov cluster (MCL) algorithm for the assignment of proteins into families based on precomputed sequence similarity information. This novel approach does not suffer from the problems that normally hinder other protein sequence clustering algorithms, such as the presence of multi-domain proteins, promiscuous domains and fragmented proteins. The method has been rigorously tested and validated on a number of very large databases, including SwissProt, InterPro, SCOP and the draft human genome. Our results indicate that the method is ideally suited to the rapid and accurate detection of protein families on a large scale. The method has been used to detect and categorise protein families within the draft human genome and the resulting families have been used to annotate a large proportion of human proteins.",2002,60,3169,401,1,28,48,55,79,113,117,148,146,169
b49eee1b10ac493c1428a80753436fb4c28f6701,"Motivated by the fast‐growing need to compute centrality indices on large, yet very sparse, networks, new algorithms for betweenness are introduced in this paper. They require O(n + m) space and run in O(nm) and O(nm + n2 log n) time on unweighted and weighted networks, respectively, where m is the number of links. Experimental evidence is provided that this substantially increases the range of networks for which centrality analysis is feasible. The betweenness centrality index is essential in the analysis of social networks, but costly to compute. Currently, the fastest known algorithms require ?(n 3) time and ?(n 2) space, where n is the number of actors in the network.",2001,31,3576,254,3,10,14,28,41,49,66,86,112,151
8e617b8c63dd35d9913bbc104d0666ffd10e9e6a,,2002,1,3023,399,40,63,93,126,143,172,169,224,190,202
145c0b53514b02bdc3dadfb2e1cea124f2abd99b,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.",1967,8,4995,176,3,4,6,8,20,5,14,12,14,8
848a25a41cba56ca180ca79c6ba3470cc3b8f143,"In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.",2002,121,4680,139,4,13,34,53,79,108,112,152,183,238
35d81066cb1369acf4b6c5117fcbb862be2af350,"For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.",2009,21,2777,214,40,93,153,202,293,336,324,335,286,251
af56e6d4901dcd0f589bf969e604663d40f1be5d,"The charter of SRC is to advance both the state of knowledge and the state of the art in computer systems. From our establishment in 1984, we have performed basic and applied research to support Digital's business objectives. Our current work includes exploring distributed personal computing on multiple platforms, networking , programming technology, system modelling and management techniques, and selected applications. Our strategy is to test the technical and practical value of our ideas by building hardware and software prototypes and using them as daily tools. Interesting systems are too complex to be evaluated solely in the abstract; extended use allows us to investigate their properties in depth. This experience is useful in the short term in refining our designs, and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this strategy, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical flavor. Some of it is in established fields of theoretical computer science, such as the analysis of algorithms, computational geometry, and logics of programming. Other work explores new ground motivated by problems that arise in our systems research. We have a strong commitment to communicating our results; exposing and testing our ideas in the research and development communities leads to improved understanding. Our research report series supplements publication in professional journals and conferences. We seek users for our prototype systems among those with whom we have common interests, and we encourage collaboration with university researchers. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission an acknowledgment of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Systems Research Center. All rights reserved. Authors' abstract We describe a block-sorting, lossless data compression algorithm, and our implementation of that algorithm. We compare the performance of our implementation with widely available data compressors running on the same hardware. The algorithm works by applying a reversible transformation to a block of input …",1994,16,2743,397,1,5,13,18,31,27,33,49,38,51
47a9ab6f97ab05fcd49aaf2864c97538b55e6268,An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported.,1995,40,3417,191,0,10,9,4,7,11,14,27,33,37
7f4901362e8f8fe603ad90d6aba5f5192ef8ea8b,"Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. 
We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. SNOPT is a particular implementation that makes use of a semidefinite QP solver. It is based on a limited-memory quasi-Newton approximation to the Hessian of the Lagrangian and uses a reduced-Hessian algorithm (SQOPT) for solving the QP subproblems. It is designed for problems with many thousands of constraints and variables but a moderate number of degrees of freedom (say, up to 2000). An important application is to trajectory optimization in the aerospace industry. Numerical results are given for most problems in the CUTE and COPS test collections (about 900 examples).",2002,152,2848,294,20,40,58,85,128,91,121,126,152,164
c1d959b3c549c0bbfea22f9f5afa6f11c8eb74fd,"The Marquardt algorithm for nonlinear least squares is presented and is incorporated into the backpropagation algorithm for training feedforward neural networks. The algorithm is tested on several function approximation problems, and is compared with a conjugate gradient algorithm and a variable learning rate algorithm. It is found that the Marquardt algorithm is much more efficient than either of the other techniques when the network contains no more than a few hundred weights.",1994,18,5574,158,2,5,15,22,31,52,78,94,123,106
240c2cb549d0ad3ca8e6d5d17ca61e95831bbe6d,"For the problem of minimizing a lower semicontinuous proper convex function f on a Hilbert space, the proximal point algorithm in exact form generates a sequence $\{ z^k \} $ by taking $z^{k + 1} $ to be the minimizes of $f(z) + ({1 / {2c_k }})\| {z - z^k } \|^2 $, where $c_k > 0$. This algorithm is of interest for several reasons, but especially because of its role in certain computational methods based on duality, such as the Hestenes-Powell method of multipliers in nonlinear programming. It is investigated here in a more general form where the requirement for exact minimization at each iteration is weakened, and the subdifferential $\partial f$ is replaced by an arbitrary maximal monotone operator T. Convergence is established under several criteria amenable to implementation. The rate of convergence is shown to be “typically” linear with an arbitrarily good modulus if $c_k $ stays large enough, in fact superlinear if $c_k \to \infty $. The case of $T = \partial f$ is treated in extra detail. Applicati...",1976,35,3178,405,2,0,8,8,1,2,2,9,6,4
adb7c53c6928789cd5accf9ab5d0cec3fd32590d,"The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.",1996,74,4524,162,10,25,28,33,44,33,61,90,107,108
d87a423334afb20747c367b2d907069d7f3b4ed2,The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.,1973,36,4250,149,5,12,14,9,12,12,11,11,28,20
5a114d3050a0a33f8cc6d28d55fa048a5a7ab6f2,An algorithm is presented for the rapid solution of the phase of the complete wave function whose intensity in the diffraction and imaging planes of an imaging system are known. A proof is given showing that a defined error between the estimated function and the correct function must decrease as the algorithm iterates. The problem of uniqueness is discussed and results are presented demonstrating the power of the method.,1972,0,4511,144,0,5,8,7,2,1,5,5,12,15
69dc367c30240fcbbde0a2be04f16b02f2f7e73e,Community detection and analysis is an important methodology for understanding the organization of various real-world networks and has applications in problems as diverse as consensus formation in social communities or the identification of functional modules in biochemical networks. Currently used algorithms that identify the community structures in large-scale real-world networks require a priori information such as the number and sizes of communities or are computationally expensive. In this paper we investigate a simple label propagation algorithm that uses the network structure alone as its guide and requires neither optimization of a predefined objective function nor prior information about the communities. In our algorithm every node is initialized with a unique label and at every step each node adopts the label that most of its neighbors currently have. In this iterative process densely connected groups of nodes form a consensus on a unique label to form communities. We validate the algorithm by applying it to networks whose community structures are known. We also demonstrate that the algorithm takes an almost linear time and hence it is computationally less expensive than what was possible so far.,2007,61,2569,258,0,11,24,45,77,95,167,194,211,281
53fcc056f79e04daf11eb798a7238e93699665aa,"This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On realworld sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.",1998,23,2786,273,5,13,12,18,43,47,69,70,96,92
3ffa807548d96e2e76d53065c7143906bc2ff4bf,"A computational method for partitioning a charge density grid into Bader volumes is presented which is efficient, robust, and scales linearly with the number of grid points. The partitioning algorithm follows the steepest ascent paths along the charge density gradient from grid point to grid point until a charge density maximum is reached. In this paper, we describe how accurate off-lattice ascent paths can be represented with respect to the grid points. This improvement maintains the efficient linear scaling of an earlier version of the algorithm, and eliminates a tendency for the Bader surfaces to be aligned along the grid directions. As the algorithm assigns grid points to charge density maxima, subsequent paths are terminated when they reach previously assigned grid points. It is this grid-based approach which gives the algorithm its efficiency, and allows for the analysis of the large grids generated from plane-wave-based density functional theory calculations.",2009,29,3978,15,16,44,114,158,170,243,313,373,339,436
eb0209d172b3d0fb9432b91809bc506c9bdd33a1,"A new distributed Euler trail algorithm is proposed to run on an Euler diagraph G(V,E) where each node knows only its adjacent edges, converting it into a new state that each node knows how an existent Euler trail routes through its incoming and outgoing edges. The communication requires only 2middot;|E| one-bit messages. The algorithm can be used as a building block for solving other distributed graph problems, and can be slightly modified to run on a strongly-connected diagraph for generating the existent Euler trail or to report that no Euler trails exist.",1993,5,13970,0,0,1,0,1,1,10,421,307,339,343
56c2779a0cfd3052192558ee3bec8fc66e1a4303,"The Moderate Resolution Imaging Spectroradiometer (MODIS) aboard both NASA’s Terra and Aqua satellites is making near-global daily observations of the earth in a wide spectral range (0.41–15 m). These measurements are used to derive spectral aerosol optical thickness and aerosol size parameters over both land and ocean. The aerosol products available over land include aerosol optical thickness at three visible wavelengths, a measure of the fraction of aerosol optical thickness attributed to the fine mode, and several derived parameters including reflected spectral solar flux at the top of the atmosphere. Over the ocean, the aerosol optical thickness is provided in seven wavelengths from 0.47 to 2.13 m. In addition, quantitative aerosol size information includes effective radius of the aerosol and quantitative fraction of optical thickness attributed to the fine mode. Spectral irradiance contributed by the aerosol, mass concentration, and number of cloud condensation nuclei round out the list of available aerosol products over the ocean. The spectral optical thickness and effective radius of the aerosol over the ocean are validated by comparison with two years of Aerosol Robotic Network (AERONET) data gleaned from 132 AERONET stations. Eight thousand MODIS aerosol retrievals collocated with AERONET measurements confirm that one standard deviation of MODIS optical thickness retrievals fall within the predicted uncertainty of 0.03 0.05 over ocean and 0.05 0.15 over land. Two hundred and seventy-one MODIS aerosol retrievals collocated with AERONET inversions at island and coastal sites suggest that one standard deviation of MODIS effective radius retrievals falls within reff 0.11 m. The accuracy of the MODIS retrievals suggests that the product can be used to help narrow the uncertainties associated with aerosol radiative forcing of global climate.",2005,76,2696,287,43,68,110,138,148,164,183,197,196,206
acd7d08468e6dd7234d474860ab64d5a33cacfdc,"Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incompletedata) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maximum likelihood estimate. A list of key properties of the algorithm is included.",1983,112,3239,149,2,8,13,15,11,17,25,18,29,37
5a6dd91b96a9b09ccb1bab27220dbcb1e1df4d39,"An analytical algorithm, called SETTLE, for resetting the positions and velocities to satisfy the holonomic constraints on the rigid water model is presented. This method is still based on the Cartesian coordinate system and can be used in place of SHAKE and RATTLE. We implemented this algorithm in the SPASMS package of molecular mechanics and dynamics. Several series of molecular dynamics simulations were carried out to examine the performance of the new algorithm in comparison with the original RATTLE method. It was found that SETTLE is of higher accuracy and is faster than RATTLE with reasonable tolerances by three to nine times on a scalar machine. Furthermore, the performance improvement ranged from factors of 26 to 98 on a vector machine since the method presented is not iterative. © 1992 by John Wiley & Sons, Inc.",1992,14,4869,83,0,3,3,7,1,9,8,4,2,10
dbf8aa1e547c863f509a5a4c03a39fa9c92c9651,"We present a new polynomial-time algorithm for linear programming. In the worst case, the algorithm requiresO(n3.5L) arithmetic operations onO(L) bit numbers, wheren is the number of variables andL is the number of bits in the input. The running-time of this algorithm is better than the ellipsoid algorithm by a factor ofO(n2.5). We prove that given a polytopeP and a strictly interior point a εP, there is a projective transformation of the space that mapsP, a toP′, a′ having the following property. The ratio of the radius of the smallest sphere with center a′, containingP′ to the radius of the largest sphere with center a′ contained inP′ isO(n). The algorithm consists of repeated application of such projective transformations each followed by optimization over an inscribed sphere to create a sequence of points which converges to the optimal solution in polynomial time.",1984,5,3854,101,2,12,38,47,69,81,94,102,127,102
b4129e11e3cf2fba5d955cf881b5623388a347f4,"Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms. ABC simulates the intelligent foraging behaviour of a honeybee swarm. In this work, ABC is used for optimizing a large set of numerical test functions and the results produced by ABC algorithm are compared with the results obtained by genetic algorithm, particle swarm optimization algorithm, differential evolution algorithm and evolution strategies. Results show that the performance of the ABC is better than or similar to those of other population-based algorithms with the advantage of employing fewer control parameters.",2009,66,2653,161,8,32,84,171,234,290,323,288,273,277
a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657,"The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.",1985,46,3317,163,27,73,90,115,74,101,102,88,81,69
c14976e9afb69a78e682c32f900b96b019878df8,"This paper proposes an algorithm for optimization inspired by the imperialistic competition. Like other evolutionary ones, the proposed algorithm starts with an initial population. Population individuals called country are in two types: colonies and imperialists that all together form some empires. Imperialistic competition among these empires forms the basis of the proposed evolutionary algorithm. During this competition, weak empires collapse and powerful ones take possession of their colonies. Imperialistic competition hopefully converges to a state in which there exist only one empire and its colonies are in the same position and have the same cost as the imperialist. Applying the proposed algorithm to some of benchmark cost functions, shows its ability in dealing with different types of optimization problems.",2007,15,2065,291,1,6,4,27,67,116,165,213,207,217
80fd362544b593bd2250e8f5f3799882fa133ca1,"Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.",1998,22,2828,205,4,36,45,57,95,101,122,144,140,137
20ae8aebf1f0f7e85f2548784a9543803a4db199,"Artificial bee colony (ABC) algorithm is an optimization algorithm based on a particular intelligent behaviour of honeybee swarms. This work compares the performance of ABC algorithm with that of differential evolution (DE), particle swarm optimization (PSO) and evolutionary algorithm (EA) for multi-dimensional numeric problems. The simulation results show that the performance of ABC algorithm is comparable to those of the mentioned algorithms and can be efficiently employed to solve engineering problems with high dimensionality.",2008,41,3016,159,6,47,95,169,246,304,312,335,296,292
379ffd011c11510b0ea4bf6b1940f7f5603af6c6,"The services of ecological systems and the natural capital stocks that produce them are critical to the functioning of the Earth's life-support system. They contribute to human welfare, both directly and indirectly, and therefore represent part of the total economic value of the planet. We have estimated the current economic value of 17 ecosystem services for 16 biomes, based on published studies and a few original calculations. For the entire biosphere, the value (most of which is outside the market) is estimated to be in the range of US$16-54 trillion (1012) per year, with an average of US$33 trillion per year. Because of the nature of the uncertainties, this must be considered a minimum estimate. Global gross national product total is around US$18 trillion per year.",1997,57,15927,797,0,0,0,0,0,0,0,0,1,0
13c3630c03bec0c3e443ad5a3dc6d1951db74c20,"Status of this Memo This memo provides information for the Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by aggregating traffic classification state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need only be implemented at network boundaries or hosts. Network resources are allocated to traffic streams by service provisioning policies which govern how traffic is marked and conditioned upon entry to a differentiated services-capable network, and how that traffic is forwarded within that network. A wide variety of services can be implemented on top of these building blocks.",1998,14,4114,339,7,121,220,322,407,404,381,357,324,254
505ae0232a6b59786ce8ed61af98573c68f69b92,"Salespeople involved in the marketing of complex services often perform the role of “relationship manager.” It is, in part, the quality of the relationship between the salesperson and the customer ...",1990,45,4378,306,2,1,9,21,29,33,54,39,39,54
d73a3014f2fbe3c02cfe4884bc6f5bbc0990e557,"The Second European Edition of Services Marketing: Integrating Customer Focus Across the Firm by Wilson, Zeithaml, Bitner and Gremler uniquely focuses on the development of customer relationships through quality service. Reflecting the increasing importance of the service economy, Services Marketing is the only text that put the customer's experience of services at the centre of its approach. The core theories, concepts and frameworks are retained, and specifically the gaps model, a popular feature of the book. The text moves from the foundations of services marketing before introducing the gaps model and demonstrating its application to services marketing. In the second edition, the book takes on more European and International contexts to reflect the needs of courses, lecturers and students. The second edition builds on the wealth of European and International examples, cases, and research in the first edition, offering more integration of European content. It has also be fully updated with the latest research to ensure that it continues to be seen as the text covering the very latest services marketing thinking. In addition, the cases section has been thoroughly examined and revised to offer a range of new case studies with a European and global focus. The online resources have also been fully revised and updated providing an excellent package of support for lecturers and students.",1996,0,4804,271,0,0,1,0,3,20,23,41,54,69
698afd7515d01f72e028927d829e1c1d22019f87,"The Semantic Web should enable greater access not only to content but also to services on the Web. Users and software agents should be able to discover, invoke, compose, and monitor Web resources offering particular services and having particular properties. As part of the DARPA Agent Markup Language program, we have begun to develop an ontology of services, called DAML-S, that will make these functionalities possible. In this paper we describe the overall structure of the ontology, the service profile for advertising services, and the process model for the detailed description of the operation of services. We also compare DAML-S with several industry efforts to define standards for characterizing services on the Web.",2001,83,3214,330,10,78,114,136,234,265,300,320,300,267
9438aa83eb8218b7e6e3891ad7bc2b388e35bc33,"In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic “virtual organizations” formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, “The Anatomy of the Grid,” by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration—within and across organizational domains. This is a DRAFT document and continues to be revised. The latest version can be found at http://www.globus.org/research/papers/ogsa.pdf. Please send comments to foster@mcs.anl.gov, carl@isi.edu, jnick@us.ibm.com, tuecke@mcs.anl.gov Physiology of the Grid 2",2002,84,3620,238,141,427,550,551,436,326,231,230,166,131
1b74223af38112ee2814688b1c4c96f597c1ff43,,1983,0,3574,478,1,2,0,1,4,3,2,5,10,3
70f5ad30aabb1de547b34d5aa4167dd9bb8d0957,"This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support real- time as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation.",1994,27,3979,194,6,39,50,78,96,141,217,279,280,326
c9253ce46a869d921a304c30ca604820089c8d9d,"processes are rarely used. The most common scenario is to use them as a template to define executable processes. Abstract processes can be used to replace sets of rules usually expressed in natural language, which is often ambiguous. In this book, we will first focus on executable processes and come back to abstract processes in Chapter 4. 21 This material is copyright and is licensed for the sole use by Encarnacion Bellido on 20th February 2006 Via Alemania, 10, bajos, , Palma de Mallorca, Baleares, 07006",2004,3,3752,193,390,523,566,483,413,283,236,172,116,97
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
1c90a7392994300df01fed1801a41fa0c9693ca2,"Relationship marketing is an old idea but a new focus now at the forefront of services marketing practice and academic research. The impetus for its development has come from the maturing of services marketing with the emphasis on quality, increased recognition of potential benefits for the firm and the customer, and technological advances. Accelerating interest and active research are extending the concept to incorporate newer, more sophisticated viewpoints. Emerging perspectives explored here include targeting profitable customers, using the strongest possible strategies for customer bonding, marketing to employees and other stakeholders, and building trust as a marketing tool. Although relationship marketing is developing, more research is needed before it reaches maturity. A baker’s dozen of researchable questions suggests some future directions.",1995,21,3161,284,1,11,21,26,41,59,56,72,71,84
1bfb01fc10cf40d307f5d9b99e9b94de3fb85685,"Processes serve a descriptive role, with more than one use case. One such use case might be to describe the observable behavior of some or all of the services offered by an Executable Process. Another use case would be to define a process template that embodies domain-specific best practices. Such a process template would capture essential process logic in a manner compatible with a design-time representation, while excluding execution details to be completed when mapping to an Executable Process. Regardless of the specific use case and purpose, all Abstract Processes share a common syntactic base. They have different requirements for the level of opacity and restrictions on which parts of a process definition may be omitted or hidden. Tailored uses of Abstract Processes have different effects on the consistency constraints and on the semantics of that process. Some of these required constraints are not enforceable by the XML Schema. A common base specifies the features that define the syntactic universe of Abstract Processes. Given this common base, a usage profile provides the necessary specializations and semantics based on Executable WS-BPEL for a particular use of an Abstract Process. As mentioned above it is possible to use WS-BPEL to define an Executable Business Process. While a WS-BPEL Abstract Process definition is not required to be fully specified, the language effectively defines a portable execution format for business processes that rely exclusively on Web Service resources and XML data. Moreover, such processes execute and interact with their partners in a consistent way regardless of the supporting platform or programming model used by the implementation of the hosting environment. The continuity of the basic conceptual model between Abstract and Executable Processes in WSBPEL makes it possible to export and import the public aspects embodied in Abstract Processes as process or role templates while maintaining the intent and structure of the observable behavior. This applies even where private implementation aspects use platform dependent functionality.",2007,50,3133,200,211,360,423,412,359,310,248,240,146,97
9ea11c0728473937644be8dda349a34d32f656a1,"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.",2004,59,2872,297,7,51,109,202,193,263,297,248,267,238
3cf9e452adceb66ead134d9377ae8155016aa259,"Companies that want to improve their service quality should take a cue from manufacturing and focus on their own kind of scrap heap: customers who won't come back. Because that scrap heap can be every bit as costly as broken parts and misfit components, service company managers should strive to reduce it. They should aim for ""zero defections""--keeping every customer they can profitably serve. As companies reduce customer defection rates, amazing things happen to their financials. Although the magnitude of the change varies by company and industry, the pattern holds: profits rise sharply. Reducing the defection rate just 5% generates 85% more profits in one bank's branch system, 50% more in an insurance brokerage, and 30% more in an auto-service chain. And when MBNA America, a Delaware-based credit card company, cut its 10% defection rate in half, profits rose a whopping 125%. But defection rates are not just a measure of service quality; they are also a guide for achieving it. By listening to the reasons why customers defect, managers learn exactly where the company is falling short and where to direct their resources. Staples, the stationery supplies retailer, uses feedback from customers to pinpoint products that are priced too high. That way, the company avoids expensive broad-brush promotions that pitch everything to everyone. Like any important change, managing for zero defections requires training and reinforcement. Great-West Life Assurance Company pays a 50% premium to group health-insurance brokers that hit customer-retention targets, and MBNA America gives bonuses to departments that hit theirs.",1990,0,5927,136,0,10,16,17,40,47,42,56,89,88
1d109c5b293c59983f388de75faeb2b49abb606e,"Life itself as well as the entire human economy depends on goods and services provided by earth's natural systems. The processes of cleansing, recycling, and renewal, along with goods such as seafood, forage, and timber, are worth many trillions of dollars annually, and nothing could live without them. Yet growing human impacts on the environment are profoundly disrupting the functioning of natural systems and imperiling the delivery of these services.Nature's Services brings together world-renowned scientists from a variety of disciplines to examine the character and value of ecosystem services, the damage that has been done to them, and the consequent implications for human society. Contributors including Paul R. Ehrlich, Donald Kennedy, Pamela A. Matson, Robert Costanza, Gary Paul Nabhan, Jane Lubchenco, Sandra Postel, and Norman Myers present a detailed synthesis of our current understanding of a suite of ecosystem services and a preliminary assessment of their economic value. Chapters consider: major services including climate regulation, soil fertility, pollination, and pest control philosophical and economic issues of valuation case studies of specific ecosystems and services implication of recent findings and steps that must be taken to address the most pressing concerns Nature's Services represents one of the first efforts by scientists to provide an overview of the many benefits and services that nature offers to people and the extent to which we are all vitally dependent on those services. The book enhances our understanding of the value of the natural systems that surround us and can play an essential role in encouraging greater efforts to protect the earth's basic life-support systems before it is too late. -- publisher's description",1998,0,3671,124,34,55,58,60,100,83,82,101,139,147
5541ab96c84e9fbe56c0be28631b682608ca9b40,"This letter is in response to your two Citizen Petitions dated November 17, 1994 and May 13, 2008, requesting that the Food and Drug Administration (FDA or the Agency) require a cancer warning on cosmetic talc products. Your 1994 Petition requests that all cosmetic talc bear labels with a warning such as ""Talcum powder causes cancer in laboratory animals. Frequent talc application in the female genital area increases the risk of ovarian cancer."" Additionally, your 2008 Petition requests that cosmetic talcum powder products bear labels with a prominent warning such as: ""Frequent talc application in the female genital area is responsible for major risks of ovarian cancer."" Further, both of your Petitions specifically request, pursuant to 21 CFR 1 0.30(h)(2), a hearing for you to present scientific evidence in support of this petition.",1999,141,9317,33,52,92,115,158,177,225,227,279,299,330
22f33f9d67edbc1ab9a8da239bfe7a464337db24,"This new edition of Ann Bowling's well-known and highly respected text has been thoroughly revised and updated to reflect key methodological developments in health research. It is a comprehensive, easy to read, guide to the range of methods used to study and evaluate health and health services. It describes the concepts and methods used by the main disciplines involved in health research, including: demography, epidemiology, health economics, psychology and sociology.The research methods described cover the assessment of health needs, morbidity and mortality trends and rates, costing health services, sampling for survey research, cross-sectional and longitudinal survey design, experimental methods and techniques of group assignment, questionnaire design, interviewing techniques, coding and analysis of quantitative data, methods and analysis of qualitative observational studies, and types of unstructured interviewing. With new material on topics such as cluster randomization, utility analyses, patients' preferences, and perception of risk, the text is aimed at students and researchers of health and health services. It has also been designed for health professionals and policy makers who have responsibility for applying research findings in practice, and who need to know how to judge the value of that research",1997,9,2685,210,0,5,11,17,33,43,55,73,100,69
4db2daec1fb9c1e8c49d0c549ea2c3af940485ba,"Abstract The concept of ecosystems services has become an important model for linking the functioning of ecosystems to human welfare. Understanding this link is critical for a wide-range of decision-making contexts. While there have been several attempts to come up with a classification scheme for ecosystem services, there has not been an agreed upon, meaningful and consistent definition for ecosystem services. In this paper we offer a definition of ecosystem services that is likely to be operational for ecosystem service research and several classification schemes. We argue that any attempt at classifying ecosystem services should be based on both the characteristics of the ecosystems of interest and a decision context for which the concept of ecosystem services is being mobilized. Because of this there is not one classification scheme that will be adequate for the many contexts in which ecosystem service research may be utilized. We discuss several examples of how classification schemes will be a function of both ecosystem and ecosystem service characteristics and the decision-making context.",2009,62,2452,155,19,80,102,157,195,237,264,275,244,231
a556ba6f4ae669b253de9a4a7cfa25f3d7b58742,"Pfam is a database of protein families that currently contains 7973 entries (release 18.0). A recent development in Pfam has enabled the grouping of related families into clans. Pfam clans are described in detail, together with the new associated web pages. Improvements to the range of Pfam web tools and the first set of Pfam web services that allow programmatic access to the database and associated tools are also presented. Pfam is available on the web in the UK (), the USA (), France () and Sweden ().",2005,17,2185,207,3,76,350,472,315,180,134,98,81,69
251b03a94b29698df843c0f5c2ffbf2d17fd213e,"The question of the existence of competition among auditors has been the subject of considerable discussion in recent years. More specifically, the ""Big Eight"" firms as a group have been accused of monopolizing the market for audits {Staff Study of the Subcommittee on Reports, Accounting and Management of the Senate Committee on Government Operations [1977]). However, evidence on the issue is scanty and typically anecdotal (e.g., Bernstein [1978]). The evidence of the Staff Study itself is limited to concentration statistics, with the allegations relying on what has come to be called the ""concentration doctrine"" (Demsetz [1973]). According to this doctrine, supplier concentration is a reliable indicator of supplier behavior and performance. In this paper, I provide evidence from a test of the hypothesis that price competition prevails throughout the market for the audits of publicly held companies, irrespective of the share of a market segment which is serviced by the Big Eight firms. The evidence is based on an examination of a sample cross-section of audit fees.",1980,11,2265,399,0,2,2,2,5,1,5,2,8,1
1f101fbd343044499ba2bab28d05d1eaa268e7dd,"Management literature is almost unanimous in suggesting to manufacturers that they should integrate services into their core product offering. The literature, however, is surprisingly sparse in describing to what extent services should be integrated, how this integration should be carried out, or in detailing the challenges inherent in the transition to services. Reports on a study of 11 capital equipment manufacturers developing service offerings for their products. Focuses on identifying the dimensions considered when creating a service organization in the context of a manufacturing firm, and successful strategies to navigate the transition. Analysis of qualitative data suggests that the transition involves a deliberate developmental process to build capabilities as firms shift the nature of the relationship with the product end‐users and the focus of the service offering. The report concludes identifying implications of our findings for further research and practitioners.",2003,40,2178,265,5,13,37,22,43,62,111,118,155,152
1065f1c73c538a8d4b017af1825967e1fab1bf52,"Advances in sensing and tracking technology enable location-based applications but they also create significant privacy risks. Anonymity can provide a high degree of privacy, save service users from dealing with service providers’ privacy policies, and reduce the service providers’ requirements for safeguarding private information. However, guaranteeing anonymous usage of location-based services requires that the precise location information transmitted by a user cannot be easily used to re-identify the subject. This paper presents a middleware architecture and algorithms that can be used by a centralized location broker service. The adaptive algorithms adjust the resolution of location information along spatial or temporal dimensions to meet specified anonymity constraints based on the entities who may be using location services within a given area. Using a model based on automotive traffic counts and cartographic material, we estimate the realistically expected spatial resolution for different anonymity constraints. The median resolution generated by our algorithms is 125 meters. Thus, anonymous location-based requests for urban areas would have the same accuracy currently needed for E-911 services; this would provide sufficient resolution for wayfinding, automated bus routing services and similar location-dependent services.",2003,47,2364,188,8,20,45,63,97,112,151,148,150,178
7d32e764672117c2d5bce2c69dee737c6e88b719,"The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that location of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL.We propose a solution based on DAML-S, a DAML-based language for service description, and we show how service capabilities are presented in the Profile section of a DAML-S description and how a semantic match between advertisements and requests is performed.",2002,20,2572,165,17,105,179,200,251,274,249,229,230,167
1741947f385ae50bbb50c879655ada49a45860ba,"Internet-delivered e-services are increasingly being made available to consumers; however, little is known about how consumers evaluate them for potential adoption. Past Technology Adoption Research has focused primarily on the positive utility gains attributable to system adoption. This research extends that approach to include measures of negative utility (potential losses) attributable to e-service adoption. Drawing from Perceived Risk Theory, specific risk facets were operationalized, integrated, and empirically tested within the Technology Acceptance Model resulting in a proposed e-services adoption model. Results indicated that e-services adoption is adversely affected primarily by performance-based risk perceptions, and perceived ease of use of the e-service reduced these risk concerns. Implications of integrating perceived risk into the proposed e-services adoption model are discussed.",2002,67,2025,186,1,8,8,18,30,43,37,65,72,96
3649fdff0b991e7a5bec42318809c3d50f660690,"Like many other incipient technologies, Web services are still surrounded by a substantial level of noise. This noise results from the always dangerous combination of wishful thinking on the part of research and industry and of a lack of clear understanding of how Web services came to be. On the one hand, multiple contradictory interpretations are created by the many attempts to realign existing technology and strategies with Web services. On the other hand, the emphasis on what could be done with Web services in the future often makes us lose track of what can be really done with Web services today and in the short term. These factors make it extremely difficult to get a coherent picture of what Web services are, what they contribute, and where they will be applied.Alonso and his co-authors deliberately take a step back. Based on their academic and industrial experience with middleware and enterprise application integration systems, they describe the fundamental concepts behind the notion of Web services and present them as the natural evolution of conventional middleware, necessary to meet the challenges of the Web and of B2B application integration. Rather than providing a reference guide or a ""how to write your first Web service"" kind of book, they discuss the main objectives of Web services, the challenges that must be faced to achieve them, and the opportunities that this novel technology provides. Established, as well as recently proposed, standards and techniques (e.g., WSDL, UDDI, SOAP, WS-Coordination, WS-Transactions, and BPEL), are then examined in the context of this discussion in order to emphasize their scope, benefits, and shortcomings. Thus, the book is ideally suited both for professionals considering the development of application integration solutions and for research and students interesting in understanding and contributing to the evolution of enterprise application technologies.",2009,0,1860,138,215,176,164,137,100,108,79,42,33,18
fbfae405e9144274063149c40566da25055dd855,The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models—Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.,1990,55,2487,127,7,9,27,34,46,44,36,28,46,53
0531bb2ecef7a42909719310b23757740ded95f1,"This research examines the benefits customers receive as a result of engaging in long-term relational exchanges with service firms. Findings from two studies indicate that consumer relational benefits can be categorized into three distinct benefit types: confidence, social, and special treatment benefits. Confidence benefits are received more and rated as more important than the other relational benefits by consumers, followed by social and special treatment benefits, respectively. Responses segmented by type of service business show a consistent pattern with respect to customer rankings of benefit importance. Management implications for relational strategies and future research implications of the findings are discussed.",1998,49,2332,168,2,9,28,37,35,48,57,60,75,99
f1b109e548e0ec8434ae5c1495553bc1ff82c4c8,"Aims of the Paper 
The principal aims of this paper are (1) to increase professional health workers’ knowledge of selected research findings and theory so that they may better understand why and under what conditions people take action to prevent, detect and diagnose disease; and (2) to increase awareness among qualified behavioral scientists about the kinds of behavioral research opportunities and needs that exist in public health. 
 
A matter of personal philosophy of the author is that the goal of understanding and predicting behavior should appropriately precede the goal of attempting to persuade people to modify their health practices, even though behavior can sometimes be changed in a planned way without clear understanding of its original causes. Efforts to modify behavior will ultimately be more successful if they grow out of an understanding of causal processes. Accordingly, primary attention will here be given to an effort to understand why people behave as they do. Only then will brief consideration be given to problems of how to persuade people to use health services.",1966,67,2712,239,0,1,1,7,7,5,12,12,17,12
681b6891fca9fab4b6c968ebe6b889fd5d60db7c,"OBJECTIVE
To provide practical strategies for conducting and evaluating analyses of qualitative data applicable for health services researchers. DATA SOURCES AND DESIGN: We draw on extant qualitative methodological literature to describe practical approaches to qualitative data analysis. Approaches to data analysis vary by discipline and analytic tradition; however, we focus on qualitative data analysis that has as a goal the generation of taxonomy, themes, and theory germane to health services research.


PRINCIPLE FINDINGS
We describe an approach to qualitative data analysis that applies the principles of inductive reasoning while also employing predetermined code types to guide data analysis and interpretation. These code types (conceptual, relationship, perspective, participant characteristics, and setting codes) define a structure that is appropriate for generation of taxonomy, themes, and theory. Conceptual codes and subcodes facilitate the development of taxonomies. Relationship and perspective codes facilitate the development of themes and theory. Intersectional analyses with data coded for participant characteristics and setting codes can facilitate comparative analyses.


CONCLUSIONS
Qualitative inquiry can improve the description and explanation of complex, real-world phenomena pertinent to health services research. Greater understanding of the processes of qualitative data analysis can be helpful for health services researchers as they use these methods themselves or collaborate with qualitative researchers from a wide range of disciplines.",2007,72,2267,52,3,23,29,52,68,99,122,146,216,242
a2bfe963ce2168cecaad21d2e66059662e996f70,"This article examines some advantages and disadvantages of conducting online survey research. It explores current features, issues, pricing, and limitations associated with products and services, such as online questionnaire features and services to facilitate the online survey process, such as those offered by web survey businesses. The review shows that current online survey products and services can vary considerably in terms of available features, consumer costs, and limitations. It is concluded that online survey researchers should conduct a careful assessment of their research goals, research timeline, and financial situation before choosing a specific product or service.",2006,48,2106,130,8,15,27,45,68,102,114,139,157,184
72e0802c917104b36a396135bcb5876494abe00e,"This document defines a notation for specifying business process behavior based on Web Services. This notation is called Business Process Execution Language for Web Services (abbreviated to BPEL4WS in the rest of this document). Processes in BPEL4WS export and import functionality by using Web Service interfaces exclusively. Business processes can be described in two ways. Executable business processes model actual behavior of a participant in a business interaction. Business protocols, in contrast, use process descriptions that specify the mutually visible message exchange behavior of each of the parties involved in the protocol, without revealing their internal behavior. The process descriptions for business protocols are called abstract processes. BPEL4WS is meant to be used to model the behavior of both executable and abstract processes. BPEL4WS provides a language for the formal specification of business processes and business interaction protocols. By doing so, it extends the Web services interaction model and enables it to support business transactions. BPEL4WS defines an interoperable integration model that should facilitate the expansion of automated process integration in both the intracorporate and the business-to-business spaces. Status of this Document This is an initial public draft release of the BPEL4WS specification. We anticipate a number of extensions to the feature set of BPEL4WS that are discussed briefly at the end of the document. BPEL4WS represents a convergence of the ideas in the XLANG and WSFL specifications. Both XLANG and WSFL are superseded by the BPEL4WS specification. BPEL4WS and related specifications are provided as-is and for review and evaluation only. BEA, IBM and Microsoft hope to solicit your contributions and suggestions in the near future. BEA, IBM and Microsoft make no warrantees or representations regarding the specifications in any manner whatsoever.",2003,0,2127,161,112,187,282,364,345,247,145,120,98,54
0bd82f5b93c8e2e52d584c3c9bd126fee243e61d,"Humanity is increasingly urban, but continues to depend on Nature for its survival. Cities are dependent on the ecosystems beyond the city limits, but also benefit from internal urban ecosystems. The aim of this paper is to analyze the ecosystem services generated by ecosystems within the urban area. ‘Ecosystem services’ refers to the benefits human populations derive from ecosystems. Seven different urban ecosystems have been identified: street trees; lawns:parks; urban forests; cultivated land; wetlands; lakes:sea; and streams. These systems generate a range of ecosystem services. In this paper, six local and direct services relevant for Stockholm are addressed: air filtration, micro climate regulation, noise reduction, rainwater drainage, sewage treatment, and recreational and cultural values. It is concluded that the locally generated ecosystem services have a substantial impact on the quality-of-life in urban areas and should be addressed in land-use planning. © 1999 Elsevier Science B.V. All rights reserved.",1999,39,2194,94,0,2,4,3,7,4,27,23,43,45
85a4155ee7d04554df0821d28a7a1fe0469beda3,"Concern is growing about the consequences of biodiversity loss for ecosystem functioning, for the provision of ecosystem services, and for human well being. Experimental evidence for a relationship between biodiversity and ecosystem process rates is compelling, but the issue remains contentious. Here, we present the first rigorous quantitative assessment of this relationship through meta-analysis of experimental work spanning 50 years to June 2004. We analysed 446 measures of biodiversity effects (252 in grasslands), 319 of which involved primary producer manipulations or measurements. Our analyses show that: biodiversity effects are weaker if biodiversity manipulations are less well controlled; effects of biodiversity change on processes are weaker at the ecosystem compared with the community level and are negative at the population level; productivity-related effects decline with increasing number of trophic links between those elements manipulated and those measured; biodiversity effects on stability measures ('insurance' effects) are not stronger than biodiversity effects on performance measures. For those ecosystem services which could be assessed here, there is clear evidence that biodiversity has positive effects on most. Whilst such patterns should be further confirmed, a precautionary approach to biodiversity management would seem prudent in the meantime.",2006,148,2169,93,4,32,83,100,108,134,171,178,205,201
1a15e7ee67b3e34fc7277341a0574daaac60af27,"Human-dominated marine ecosystems are experiencing accelerating loss of populations and species, with largely unknown consequences. We analyzed local experiments, long-term regional time series, and global fisheries data to test how biodiversity loss affects marine ecosystem services across temporal and spatial scales. Overall, rates of resource collapse increased and recovery potential, stability, and water quality decreased exponentially with declining diversity. Restoration of biodiversity, in contrast, increased productivity fourfold and decreased variability by 21%, on average. We conclude that marine biodiversity loss is increasingly impairing the ocean's capacity to provide food, maintain water quality, and recover from perturbations. Yet available data suggest that at this point, these trends are still reversible.",2006,102,2408,39,4,71,128,140,154,177,163,186,150,156
a258380b26971406230aaf017bb3e3d473d1c849,"Excellent service is the foundation for services marketing, contend Leonard Berry and A. Parasuraman in this companion volume to ""Delivering Quality Service."" Building on eight years of research, the authors develop a model for understanding the relationship between quality and marketing in services and offer dozens of practical insights into ways to improve services marketing. They argue that superior service cannot be manufactured in a factory, packaged, and delivered intact to customers. Though an innovative service concept may give a company an initial edge, superior quality is vital to sustaining success. Berry and Parasuraman show that inspired leadership, a customer-minded corporate culture, an excellent service-system design, and effective use of technology and information are crucial to superior service quality and services marketing. When a company's service is excellent, customers are more likely to perceive value in transactions, spread favorable word-of-mouth impressions, and respond positively to employee-cross-selling efforts. The authors point out that a service company that does relatively little pre-sales marketing but is truly dedicated to delivering excellent quality service will have greater marketing effectiveness, higher customer retention, and more sales to existing customers than a company that emphasizes pre-sale marketing but falls short during actual service delivery. The focus of any company, they insist, must be customer satisfaction through integration of service quality throughout the entire system. Filled with examples, stories, and insights from senior executives, Berry and Parasuraman's new framework for effective marketing servicescontains the key to high-performance services marketing.",1991,0,2280,117,0,3,13,24,35,31,34,52,41,51
0a954386b36717010e3b07391da43916afa72ad1,"Abstract. Electronic government, or e‐government, increases the convenience and accessibility of government services and information to citizens. Despite the benefits of e‐government – increased government accountability to citizens, greater public access to information and a more efficient, cost‐effective government – the success and acceptance of e‐government initiatives, such as online voting and licence renewal, are contingent upon citizens’ willingness to adopt this innovation. In order to develop ‘citizen‐centred’ e‐government services that provide participants with accessible, relevant information and quality services that are more expedient than traditional ‘brick and mortar’ transactions, government agencies must first understand the factors that influence citizen adoption of this innovation. This study integrates constructs from the Technology Acceptance Model, Diffusions of Innovation theory and web trust models to form a parsimonious yet comprehensive model of factors that influence citizen adoption of e‐government initiatives. The study was conducted by surveying a broad diversity of citizens at a community event. The findings indicate that perceived ease of use, compatibility and trustworthiness are significant predictors of citizens’ intention to use an e‐government service. Implications of this study for research and practice are presented.",2005,57,1853,201,4,20,23,58,106,103,128,146,133,142
a20cd573efefe1eac59f51faf28e0fbc18d69e35,"Over the past decade, efforts to value and protect ecosystem services have been promoted by many as the last, best hope for making conservation mainstream – attractive and commonplace worldwide. In theory, if we can help individuals and institutions to recognize the value of nature, then this should greatly increase investments in conservation, while at the same time fostering human well-being. In practice, however, we have not yet developed the scientific basis, nor the policy and finance mechanisms, for incorporating natural capital into resource- and land-use decisions on a large scale. Here, we propose a conceptual framework and sketch out a strategic plan for delivering on the promise of ecosystem services, drawing on emerging examples from Hawai‘i. We describe key advances in the science and practice of accounting for natural capital in the decisions of individuals, communities, corporations, and governments.",2009,112,1667,78,15,48,81,106,151,187,193,142,168,159
baebed6da6c8a7ddda676fb8b0d98d5d2168f219,"The Millennium Ecosystem Assessment (MA) introduced a new framework for analyzing social–ecological systems that has had wide influence in the policy and scientific communities. Studies after the MA are taking up new challenges in the basic science needed to assess, project, and manage flows of ecosystem services and effects on human well-being. Yet, our ability to draw general conclusions remains limited by focus on discipline-bound sectors of the full social–ecological system. At the same time, some polices and practices intended to improve ecosystem services and human well-being are based on untested assumptions and sparse information. The people who are affected and those who provide resources are increasingly asking for evidence that interventions improve ecosystem services and human well-being. New research is needed that considers the full ensemble of processes and feedbacks, for a range of biophysical and social systems, to better understand and manage the dynamics of the relationship between humans and the ecosystems on which they rely. Such research will expand the capacity to address fundamental questions about complex social–ecological systems while evaluating assumptions of policies and practices intended to advance human well-being through improved ecosystem services.",2009,79,1745,75,29,75,101,119,178,189,207,167,160,154
3283f3ca81ff395643ca49722d522e3606eca006,"Nature provides a wide range of benefits to people. There is increasing consensus about the importance of incorporating these “ecosystem services” into resource management decisions, but quantifying the levels and values of these services has proven difficult. We use a spatially explicit modeling tool, Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST), to predict changes in ecosystem services, biodiversity conservation, and commodity production levels. We apply InVEST to stakeholder-defined scenarios of land-use/land-cover change in the Willamette Basin, Oregon. We found that scenarios that received high scores for a variety of ecosystem services also had high scores for biodiversity, suggesting there is little tradeoff between biodiversity conservation and ecosystem services. Scenarios involving more development had higher commodity production values, but lower levels of biodiversity conservation and ecosystem services. However, including payments for carbon sequestration alleviates this tradeoff. Quantifying ecosystem services in a spatially explicit manner, and analyzing tradeoffs between them, can help to make natural resource decisions more effective, efficient, and defensible.",2009,85,1881,75,28,71,97,136,191,196,182,185,178,189
f81183ede9e5455e82bad824718540fce6f2d1a7,"Ecosystem management that attempts to maximize the production of one ecosystem service often results in substantial declines in the provision of other ecosystem services. For this reason, recent studies have called for increased attention to development of a theoretical understanding behind the relationships among ecosystem services. Here, we review the literature on ecosystem services and propose a typology of relationships between ecosystem services based on the role of drivers and the interactions between services. We use this typology to develop three propositions to help drive ecological science towards a better understanding of the relationships among multiple ecosystem services. Research which aims to understand the relationships among multiple ecosystem services and the mechanisms behind these relationships will improve our ability to sustainably manage landscapes to provide multiple ecosystem services.",2009,70,1550,100,2,20,42,70,106,113,147,177,177,158
caf04b76596bf1da40ec442edb6060dcf3927180,,2001,0,2451,15,22,34,56,56,75,74,105,108,132,167
a54438cc19171b4ba9380ba9a6443532e82357c0,"Payments for environmental services (PES) are part of a new and more direct conservation paradigm, explicitly recognizing the need to bridge the interests of landowners and outsiders. Eloquent theoretical assessments have praised the absolute advantages of PES over traditional conservation approaches. Some pilot PES exist in the tropics, but many field practitioners and prospective service buyers and sellers remain skeptical about the concept. This paper aims to help demystify PES for non-economists, starting with a simple and coherent definition of the term. It then provides practical ‘how-to' hints for PES design. It considers the likely niche for PES in the portfolio of conservation approaches. This assessment is based on a literature review, combined with field observations from research in Latin America and Asia. It concludes that service users will continue to drive PES, but their willingness to pay will only rise if schemes can demonstrate clear additionality vis-a-vis carefully established baselines, if trust-building processes with service providers are sustained, and PES recipients' livelihood dynamics is better understood. PES best suits intermediate and/or projected threat scenarios, often in marginal lands with moderate conservation opportunity costs. People facing credible but medium-sized environmental degradation are more likely to become PES recipients than those living in relative harmony with Nature. The choice between PES cash and in-kind payments is highly context-dependent. Poor PES recipients are likely to gain from participation, though their access might be constrained and non-participating landless poor could lose out. PES is a highly promising conservation approach that can benefit buyers, sellers and improve the resource base, but it is unlikely to completely outstrip other conservation instruments.",2005,83,1608,197,10,19,62,70,98,102,121,133,156,139
916743e3f5d7ac258aa4af94b7b588414f0f0dd9,"More and more corporations throughout the world are adding value to their core corporate offerings through services. The trend is pervading almost all industries, is customer demand-driven, and perceived by corporations as sharpening their competitive edges. Modern corporations are increasingly offering fuller market packages or ""bundles"" of customer-focussed combinations of goods, services, support, self-service, and knowledge. But services are beginning to dominate. This movement is termed the ""servitization of business"" by authors Sandra Vandermerwe and Juan Rada, and is clearly a powerful new feature of total market strategy being adopted by the best companies. It is leading to new relationships between them and their customers. Giving many real-life examples, the authors assess the main motives driving corporations to servitization, and point out that its cumulative effects are changing the competitive dynamics in which managers will have to operate. The special challenge for top managers is how to blend services into the overall strategies of the company.",1988,0,2007,119,0,2,1,1,1,0,5,4,1,2
bce78fc8dc981e231293be5d13348a1e03b10efa,"Differentiated services enhancements to the Internet protocol are intended to enable scalable service discrimination in the Internet without the need for per-flow state and signaling at every hop. A variety of services may be built from a small, well-defined set of building blocks which are deployed in network nodes. The services may be either end-to-end or intra-domain; they include both those that can satisfy quantitative performance requirements (e.g., peak bandwidth) and those based on relative performance (e.g., ""class"" differentiation). Services can be constructed by a combination of:",1998,10,1939,100,11,94,133,162,157,160,114,113,114,94
2f40193b0dff1a6015b8b3229cdfcbe889ad9e16,"CONTEXT
The US military has conducted population-level screening for mental health problems among all service members returning from deployment to Afghanistan, Iraq, and other locations. To date, no systematic analysis of this program has been conducted, and studies have not assessed the impact of these deployments on mental health care utilization after deployment.


OBJECTIVES
To determine the relationship between combat deployment and mental health care use during the first year after return and to assess the lessons learned from the postdeployment mental health screening effort, particularly the correlation between the screening results, actual use of mental health services, and attrition from military service.


DESIGN, SETTING, AND PARTICIPANTS
Population-based descriptive study of all Army soldiers and Marines who completed the routine postdeployment health assessment between May 1, 2003, and April 30, 2004, on return from deployment to Operation Enduring Freedom in Afghanistan (n = 16,318), Operation Iraqi Freedom (n = 222,620), and other locations (n = 64,967). Health care utilization and occupational outcomes were measured for 1 year after deployment or until leaving the service if this occurred sooner.


MAIN OUTCOME MEASURES
Screening positive for posttraumatic stress disorder, major depression, or other mental health problems; referral for a mental health reason; use of mental health care services after returning from deployment; and attrition from military service.


RESULTS
The prevalence of reporting a mental health problem was 19.1% among service members returning from Iraq compared with 11.3% after returning from Afghanistan and 8.5% after returning from other locations (P<.001). Mental health problems reported on the postdeployment assessment were significantly associated with combat experiences, mental health care referral and utilization, and attrition from military service. Thirty-five percent of Iraq war veterans accessed mental health services in the year after returning home; 12% per year were diagnosed with a mental health problem. More than 50% of those referred for a mental health reason were documented to receive follow-up care although less than 10% of all service members who received mental health treatment were referred through the screening program.


CONCLUSIONS
Combat duty in Iraq was associated with high utilization of mental health services and attrition from military service after deployment. The deployment mental health screening program provided another indicator of the mental health impact of deployment on a population level but had limited utility in predicting the level of mental health services that were needed after deployment. The high rate of using mental health services among Operation Iraqi Freedom veterans after deployment highlights challenges in ensuring that there are adequate resources to meet the mental health needs of returning veterans.",2006,32,1869,110,23,59,93,133,133,178,179,146,160,155
737337d2d4761d966b3c8b9176d51ecab6a186d1,"This paper advocates consistently defined units of account to measure the contributions of nature to human welfare. We argue that such units have to date not been defined by environmental accounting advocates and that the term ""ecosystem services"" is too ad hoc to be of practical use in welfare accounting. We propose a definition, rooted in economic principles, of ecosystem service units. A goal of these units is comparability with the definition of conventional goods and services found in GDP and the other national accounts. We illustrate our definition of ecological units of account with concrete examples. We also argue that these same units of account provide an architecture for environmental performance measurement by governments, conservancies, and environmental markets.",2006,51,1810,94,7,16,34,41,78,121,144,158,180,187
8aae732e07c9d14f08ccf31eb9b742683de19595,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity; large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior. We present an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.",2001,46,1724,117,31,86,161,177,186,177,149,122,100,112
bdcb0b64e9142fa934831a13e94375bbca0b71f6,"After initial interviews with 20,291 adults in the National Institute of Mental Health Epidemiologic Catchment Area Program, we estimated prospective 1-year prevalence and service use rates of mental and addictive disorders in the US population. An annual prevalence rate of 28.1% was found for these disorders, composed of a 1-month point prevalence of 15.7% (at wave 1) and a 1-year incidence of new or recurrent disorders identified in 12.3% of the population at wave 2. During the 1-year follow-up period, 6.6% of the total sample developed one or more new disorders after being assessed as having no previous lifetime diagnosis at wave 1. An additional 5.7% of the population, with a history of some previous disorder at wave 1, had an acute relapse or suffered from a new disorder in 1 year. Irrespective of diagnosis, 14.7% of the US population in 1 year reported use of services in one or more component sectors of the de facto US mental and addictive service system. With some overlap between sectors, specialists in mental and addictive disorders provided treatment to 5.9% of the US population, 6.4% sought such services from general medical physicians, 3.0% sought these services from other human service professionals, and 4.1% turned to the voluntary support sector for such care. Of those persons with any disorder, only 28.5% (8.0 per 100 population) sought mental health/addictive services. Persons with specific disorders varied in the proportion who used services, from a high of more than 60% for somatization, schizophrenia, and bipolar disorders to a low of less than 25% for addictive disorders and severe cognitive impairment. Applications of these descriptive data to US health care system reform options are considered in the context of other variables that will determine national health policy.",1993,34,1988,80,7,24,51,55,64,66,74,86,67,111
b756107971906fc319a94da0b247e04607b63225,"Execution Language Working Draft 01, 16 22 October 23 November 2003 Document identifier: wsbpel-specification-draft-01 (XML, HTML, PDF) Location: http://www.oasis-open.org/apps/org/workgroup/wsbpel/ Editors: Ben Bloch <ben_b54@hotmail.com> Francisco Curbera, IBM <curbera@us.ibm.com> Yaron Goland, BEA <ygoland@bea.com> Neelakantan Kartha, Sterling Commerce <N_Kartha@stercomm.com> Canyang Kevin Liu, SAP <kevin.liu@sap.com> Satish Thatte, Microsoft <satisht@microsoft.com> Prasad Yendluri, webMethods <pyendluri@webmethods.com> Editor’s Notes – KevinL – list needs to be updated to include all editors Contributors:",2009,15,1202,120,174,137,125,110,96,77,53,31,30,18
4e547d5df2ef5de3fa2f91db0f11c93f4682a2a8,"Abstract In this article we focus on the vital ecological services provided by insects. We restrict our focus to services provided by “wild” insects; we do not include services from domesticated or mass-reared insect species. The four insect services for which we provide value estimates—dung burial, pest control, pollination, and wildlife nutrition—were chosen not because of their importance but because of the availability of data and an algorithm for their estimation. We base our estimations of the value of each service on projections of losses that would accrue if insects were not functioning at their current level. We estimate the annual value of these ecological services provided in the United States to be at least $57 billion, an amount that justifies greater investment in the conservation of these services.",2006,99,1420,143,7,19,39,51,61,62,93,107,106,115
6435a4806428a18bbeb046668e6794d2bba47c34,"""Every developer working with the Web needs to read this book."" -- David Heinemeier Hansson, creator of the Rails framework ""RESTful Web Services finally provides a practical roadmap for constructing services that embrace the Web, instead of trying to route around it."" -- Adam Trachtenberg, PHP author and EBay Web Services Evangelist You've built web sites that can be used by humans. But can you also build web sites that are usable by machines? That's where the future lies, and that's what RESTful Web Services shows you how to do. The World Wide Web is the most popular distributed application in history, and Web services and mashups have turned it into a powerful distributed computing platform. But today's web service technologies have lost sight of the simplicity that made the Web successful. They don't work like the Web, and they're missing out on its advantages. This book puts the ""Web"" back into web services. It shows how you can connect to the programmable web with the technologies you already use every day. The key is REST, the architectural style that drives the Web. This book: Emphasizes the power of basic Web technologies -- the HTTP application protocol, the URI naming standard, and the XML markup languageIntroduces the Resource-Oriented Architecture (ROA), a common-sense set of rules for designing RESTful web servicesShows how a RESTful design is simpler, more versatile, and more scalable than a design based on Remote Procedure Calls (RPC)Includes real-world examples of RESTful web services, like Amazon's Simple Storage Service and the Atom Publishing ProtocolDiscusses web service clients for popular programming languagesShows how to implement RESTful services in three popular frameworks -- Ruby on Rails, Restlet (for Java), and Django (for Python)Focuses on practical issues: how to design and implement RESTful web services and clients This is the first book that applies the REST design philosophy to real web services. It sets down the best practices you need to make your design a success, and the techniques you need to turn your design into working code. You can harness the power of the Web for programmable applications: you just have to work with the Web instead of against it. This book shows you how.",2007,0,1411,113,5,61,90,132,167,137,146,172,129,109
3741254fcde051b5512143b1a01b57385ca87c44,The diversity of the service sector makes it difficult to come up with managerially useful generalizations concerning marketing practice in service organizations. This article argues for a focus on specific categories of services and proposes five schemes for classifying services in ways that transcend narrow industry boundaries. In each instance insights are offered into how the nature of the service might affect the marketing task.,1983,19,2000,81,0,3,6,6,9,8,9,17,12,17
ce2d38350ef4dad58589ac3de3cdaad1645818a8,"The U.S. Preventive Services Task Force (USPSTF/Task Force) represents one of several efforts to take a more evidence-based approach to the development of clinical practice guidelines. As methods have matured for assembling and reviewing evidence and for translating evidence into guidelines, so too have the methods of the USPSTF. This paper summarizes the current methods of the third USPSTF, supported by the Agency for Healthcare Research and Quality (AHRQ) and two of the AHRQ Evidence-based Practice Centers (EPCs). The Task Force limits the topics it reviews to those conditions that cause a large burden of suffering to society and that also have available a potentially effective preventive service. It focuses its reviews on the questions and evidence most critical to making a recommendation. It uses analytic frameworks to specify the linkages and key questions connecting the preventive service with health outcomes. These linkages, together with explicit inclusion criteria, guide the literature searches for admissible evidence. Once assembled, admissible evidence is reviewed at three strata: (1) the individual study, (2) the body of evidence concerning a single linkage in the analytic framework, and (3) the body of evidence concerning the entire preventive service. For each stratum, the Task Force uses explicit criteria as general guidelines to assign one of three grades of evidence: good, fair, or poor. Good or fair quality evidence for the entire preventive service must include studies of sufficient design and quality to provide an unbroken chain of evidence-supported linkages, generalizable to the general primary care population, that connect the preventive service with health outcomes. Poor evidence contains a formidable break in the evidence chain such that the connection between the preventive service and health outcomes is uncertain. For services supported by overall good or fair evidence, the Task Force uses outcomes tables to help categorize the magnitude of benefits, harms, and net benefit from implementation of the preventive service into one of four categories: substantial, moderate, small, or zero/negative. The Task Force uses its assessment of the evidence and magnitude of net benefit to make a recommendation, coded as a letter: from A (strongly recommended) to D (recommend against). It gives an I recommendation in situations in which the evidence is insufficient to determine net benefit. The third Task Force and the EPCs will continue to examine a variety of methodologic issues and document work group progress in future communications.",2001,47,1687,88,7,37,50,67,82,93,91,103,114,130
7ba11566fec38a35edf9626d2ac48891db32a831,The purpose of this article is to lay the foundations of a theory that can be used to interpret innovation processes in the service sector. The hypothesis underpinning this article is based on Lancaster's definition of the product (in both manufacturing and services) as a set of service characteristics. The article follows the example of those who have sought to apply Lancaster's work to technological phenomena. Various modes of innovation in the service sectors are highlighted and illustrated.,1997,49,1863,120,0,8,6,19,14,28,23,21,36,46
a14750ba068c664b213c88508037e55283427664,"Increasingly, computing addresses collaboration, data sharing, and interaction modes that involve distributed resources, resulting in an increased focus on the interconnection of systems both within and across enterprises. These evolutionary pressures have led to the development of Grid technologies. The authors' work focuses on the nature of the services that respond to protocol messages. Grid provides an extensible set of services that can be aggregated in various ways to meet the needs of virtual organizations, which themselves can be defined in part by the services they operate and share.",2002,18,1872,72,33,189,251,279,215,172,134,122,109,90
56ce6e97f2090df9e0a8ea2d581017a853b45122,This article compares problems and strategies cited in the services marketing literature with those reported by actual service suppliers in a study conducted by the authors. Discussion centers on several broad themes that emerge from this comparison and on guidelines for future work in services marketing.,1985,66,2117,39,0,5,5,7,17,17,19,13,28,17
462570ca88f5ea8da5a3d46b480b378dd4bf8382,"Assessing Ecological Restoration In the wake of the Millennium Ecosystem Assessment, the analysis of ecosystem services, and their relationship to biodiversity, has become one of the most rapidly developing research themes in environmental science. At the same time, ecological restoration is widely being implemented as a response to environmental degradation and biodiversity loss. Rey Benayas et al. (p. 1121, published online 30 July) link these themes in a meta-analysis of the impacts of ecological restoration actions on provision of ecosystem services and biodiversity conservation. The analysis of 89 published restoration projects worldwide establishes that ecological restoration does, in general, have positive impacts on both biodiversity and provision of ecosystem services. These effects are especially marked in the tropics. Thus, ecological restoration actions may indeed deliver benefits, both in terms of biodiversity conservation and supporting human livelihoods. Restoration, biodiversity, and ecosystem services are positively linked in a wide range of ecosystem types across the globe. Ecological restoration is widely used to reverse the environmental degradation caused by human activities. However, the effectiveness of restoration actions in increasing provision of both biodiversity and ecosystem services has not been evaluated systematically. A meta-analysis of 89 restoration assessments in a wide range of ecosystem types across the globe indicates that ecological restoration increased provision of biodiversity and ecosystem services by 44 and 25%, respectively. However, values of both remained lower in restored versus intact reference ecosystems. Increases in biodiversity and ecosystem service measures after restoration were positively correlated. Results indicate that restoration actions focused on enhancing biodiversity should support increased provision of ecosystem services, particularly in tropical terrestrial biomes.",2009,17,1199,46,1,35,57,74,99,113,109,93,115,135
5b3f813531dad9413f145e911c9042972ac65ce3,"The physician's degree of resourcefulness, i.e., the ability to deal skillfully and promptly with new situations, is important for changing the health behaviors of patients within the constraints of a brief office visit. This quality, however, was in short supply among 15 primary care physicians selected for their interest in preventive medicine. The physicians tended to rely on a single approach for changing specific health behaviors of patients, restricted referrals to community services and other health specialists, relied almost exclusively on fear for motivating patients and expressed considerable pessimism about changing the health behaviors of older patients. The physicians uniformly reported that their inadequate education and the lack of reimbursement influenced how they counseled their patients. A good place to begin to rectify this situation is the required reading of the Guide to Clinical Preventive Services for medical students and residents, and continuing education opportunities for practic...",1993,14,2089,0,21,11,18,36,82,113,122,169,173,165
2bacf1c33fdaa3a28c3b3eb6cbad94dfe7ed59a1,"The author considers the possibility that there is not, in fact, much hidden mass in galaxies and galaxy systems. If a certain modified version of the Newtonian dynamics is used to describe the motion of bodies in a gravitational field (of a galaxy, say), the observational results are reproduced with no need to assume hidden mass in appreciable quantities. Various characteristics of galaxies result with no further assumptions. The basis of the modification is the assumption that in the limit of small acceleration a very low a0, the acceleration of a particle at distance r from a mass M satisfies approximately a2/a0 a MGr-2, where a0 is a constant of the dimensions of an acceleration.",1983,2,2280,193,0,3,4,4,1,4,1,3,2,5
cffc507312c01839ef2dc32158f2ad3a57efa5ce,"Dispersions of solid spherical grains of diameter D = 0.13cm were sheared in Newtonian fluids of varying viscosity (water and a glycerine-water-alcohol mixture) in the annular space between two concentric drums. The density σ of the grains was balanced against the density ρ of the fluid, giving a condition of no differential forces due to radial acceleration. The volume concentration C of the grains was varied between 62 and 13 %. A substantial radial dispersive pressure was found to be exerted between the grains. This was measured as an increase of static pressure in the inner stationary drum which had a deformable periphery. The torque on the inner drum was also measured. The dispersive pressure P was found to be proportional to a shear stress λ attributable to the presence of the grains. The linear grain concentration λ is defined as the ratio grain diameter/mean free dispersion distance and is related to C by λ=1(C0/C)12−1 where C0 is the maximum possible static volume concentration. Both the stressesT and P, as dimensionless groups TσD2/λη2, and PσD2/λη 2, were found to bear single-valued empirical relations to a dimensionless shear strain group λ½σD2(dU/dy)lη for all the values of λ< 12(C= 57% approx.) where dU/dy is the rate of shearing of the grains over one another, and η the fluid viscosity. This relation gives Tασ(λD)2(dU/dy)2 and T∝λ12ηdU/dy according as dU/dy is large or small, i.e. according to whether grain inertia or fluid viscosity dominate. An alternative semi-empirical relation F = (1+λ)(1+½λ)ηdU/dy was found for the viscous case, when T is the whole shear stress. The ratio T/P was constant at 0·3 approx, in the inertia region, and at 0.75 approx, in the viscous region. The results are applied to a few hitherto unexplained natural phenomena.",1954,1,2296,201,0,0,0,0,0,0,1,0,0,1
d0a90be9e7d2ebb969e7c540b4d1cd4fcb93218e,,1959,0,2432,122,0,0,1,2,3,1,0,0,0,0
a88cc4d8e59e5105683ea299e4c9739e52ce2393,"This paper presents a systematic treatment of the linear theory of scalar gravitational perturbations in the synchronous gauge and the conformal Newtonian (or longitudinal) gauge. It differs from others in the literature in that we give, in both gauges, a complete discussion of all particle species that are relevant to any flat cold dark matter (CDM), hot dark matter (HDM), or CDM+HDM models (including a possible cosmological constant). The particles considered include CDM, baryons, photons, massless neutrinos, and massive neutrinos (an HDM candidate), where the CDM and baryons are treated as fluids while a detailed phase-space description is given to the photons and neutrinos. Particular care is applied to the massive neutrino component, which has been either ignored or approximated crudely in previous works. Isentropic initial conditions on superhorizon scales are derived. The coupled, linearized Boltzmann, Einstein, and fluid equations that govern the evolution of the metric and density perturbations are then solved numerically in both gauges for the standard CDM model and two CDM+HDM models with neutrino mass densities {Omega}{sub {nu}}=0.2 and 0.3, assuming a scale-invariant, adiabatic spectrum of primordial fluctuations. We also give the full details of the cosmic microwave background anisotropy, and present the first accurate calculationsmore » of the angular power spectra in the two CDM+HDM models including photon polarization, higher neutrino multipole moments, and helium recombination. The numerical programs for both gauges are available at http://arcturus.mit.edu/cosmics. {copyright} {ital 1995 The American Astronomical Society.}« less",1994,18,1278,86,0,2,4,11,15,16,15,12,16,24
d3482d91af463d97e64e4a6137f019066526a09a,"On the assumption that pseudoplastic flow is associated with the formation and rupture of structural linkages a new flow equation is derived. The equation takes the form 
ƞ = ƞ∞ + ƞ0 − ƞ∞1 + αD23, 
where D = rate of shear, η0 = limiting viscosity at zero rate of shear, η∞ = limiting viscosity at infinite rate of shear, and α is a constant associated with the rupture of linkages. 
 
Graphical methods for evaluating the three constants η0 , η∞ , and α are presented. 
 
Experimental data are presented on a wide range of pseudoplastic systems, ranging from suspensions to optically clear solutions, in both aqueous and nonaqueous media. In all cases the results conform to the equation with a high degree of accuracy over a wide range of shear rates.",1965,3,1451,68,0,8,2,6,3,4,3,1,2,0
d1e492b3369299373277b27f5c90089af8ba1561,"This book bridges the gap between the theoretical work of the rheologist, and the practical needs of those who have to design and operate the systems in which these materials are handled or processed. It is an established and important reference for senior level mechanical engineers, chemical and process engineers, as well as any engineer or scientist who needs to study or work with these fluids, including pharmaceutical engineers, mineral processing engineers, medical researchers, water and civil engineers. This new edition covers a considerably broader range of topics than its predecessor, including computational fluid dynamics modeling techniques, liquid/solid flows and applications to areas such as food processing, among others.Written by two of the world's leading experts, this is the only dedicated non-Newtonian flow reference in print. Since first publication significant advances have been made in almost all areas covered in this book, which are incorporated in the new edition, including developments in CFD and computational techniques, velocity profiles in pipes, liquid/solid flows and applications to food processing, and new heat/mass transfer methods and models. This book covers both basic rheology and the fluid mechanics of NN fluids. It is a truly self-contained reference for anyone studying or working with the processing and handling of fluids.",2008,0,513,39,0,5,18,22,34,40,47,60,56,55
4b41c58e95168c81e46e5c705da472f901dfc6d5,"We examine the hypothesis that every particle of mass $m$ is subject to a Brownian motion with diffusion coefficient $\frac{\ensuremath{\hbar}}{2m}$ and no friction. The influence of an external field is expressed by means of Newton's law $\mathbf{F}=m\mathbf{a}$, as in the Ornstein-Uhlenbeck theory of macroscopic Brownian motion with friction. The hypothesis leads in a natural way to the Schr\""odinger equation, but the physical interpretation is entirely classical. Particles have continuous trajectories and the wave function is not a complete description of the state. Despite this opposition to quantum mechanics, an examination of the measurement process suggests that, within a limited framework, the two theories are equivalent.",1966,0,1281,56,0,2,3,5,3,3,5,3,6,6
f78d5f79225374bb58506ed50a3b0aabda0f8ad8,"A simple practical questionnaire technique for routine clinical use, the Dermatology Life Quality Index (DLQI) is described. One hundred and twenty patients with different skin diseases were asked about the impact of their disease and its treatment on their lives; a questionnaire, the DLQI, was developed based on their answers. The DLQI was then completed by 200 consecutive new‐patients attending a dermatology clinic. This study confirmed that a topic eczema, psoriasis and generalized pruritus have a greater impact on quality of life than acne, basal cell carcinomas and viral warts. The DLQI was also completed by 100 healthy volunteers; their mean score was very low (1.6%, s.d. 3.5) compared with the mean score for the dermatology patients (24.2%, s.d. 20.9). The reliability of the DLQI was examined in 53 patients using a 1 week test‐retest method and reliability was found to be high (γs=0.99).",1994,14,3675,212,0,9,14,16,14,21,37,38,40,93
391adf80e9a43ded3f591b911136876513ee1c39,Introduction biology and pathophysiology of skin disorders presenting in the skin and mucous membranes dermatology and internal medicine diseases due to microbial agents therapeutics paediatric and geriatric dermatology.,1971,0,4224,37,1,7,16,23,25,26,19,21,14,26
d72324dc5a2c194ace5e94e5176a1293c9a64762,"Rook's textbook of dermatology , Rook's textbook of dermatology , کتابخانه مرکزی دانشگاه علوم پزشکی ایران",2004,116,2026,52,1,19,53,76,88,81,86,132,145,165
58a51ea337d47ee3cab36977105a1d7bbcfbe67f,".."".surpasses the standards set in all [previous editions]""-Archives of Dermatology, review of prior editionWidely acclaimed dermatological treatise is completely revised and updated with over forty new chapters and an entirely new section on dermatologic surgery. Important new chapters include growth factors, thermoregulation, cell biology, vitiligo, albinism and more. Firmly places dermatology within the continuum of internal medicine and deploys over 1,600 exquisite color photos to depict clinical states.",2003,0,2083,54,46,60,77,63,54,61,96,93,121,102
2da0fec6bbaa835a32750c47a9e337c5172f8c39,"Medicine, it is said, is an ever-changing science. With cutting edge research and new clinical experience exponentially expanding the realms of medical science, there is a felt need to continually update our knowledge base. ‘Fitzpatrick’ has remained a highly respected textbook, a ‘Bible’ in the practice of Dermatology, Even since the first edition of this textbook was released in 1971, newer editions have been introduced earlier than the previous ones; every five years now compared to every eight years earlier; a measure of the accelerating pace of change. 
 
The new edition is different from the previous one right from the moment it is opened. The arrangement and organization of various sections and their contents have been completely rehashed. Basic sciences, which occupied a section of its own in the previous edition, are segregated into its components which are now clubbed along with their sections of relevance, bringing with it the advantage of continuity Some reshuffling, for instance, moves Kawasaki's Syndrome from the section on ‘Bacterial Diseases’ to the section on ‘Inflammatory and Vascular Disorders’. Section-end references are truncated and readers are directed to a website for the rest, thus re-appropriating premium print space for text. 
 
Along with updating of previous concepts, new chapters on Public Health in Dermatology, Complementary and Alternative Dermatology, Drug Interactions, 
 
Dermatology in Bioterrorism and Biological Warfare have been added, underscoring globally emerging newer advances in Dermatology which the new age dermatologists should be aware of. 
 
Three column text, new stand-out font, different color codes for headings and sub headings, numerous clinical and histopathological color plates, self explanatory figurative representations, graphs, flow-charts and tables make the assimilation of the vast data a cake walk. ‘At A Glance’ windows and boxes summarizing the differential diagnoses and treatment protocols give a fish-eye view of various aspects of dermatological disorders, something that is not only crisp and precise but also relevant and easy to remember. The reader gets a ready reference point with flip-book views of section name and representative thumbnail pictures on the free margins of both sides of every page. 
 
In a nutshell, this marvelous textbook replete with latest looks, is a completely revised and updated edition with great clarity and easily ‘digestable’ reference material. With this edition, ‘Fitzpatrick’ has further cemented its place in the practice of Dermatology.",2008,0,1327,0,81,143,161,168,124,155,152,113,53,37
beb8de7f3bd175fa9a1d0a0830f2882c7d91e130,,2008,0,832,87,36,47,46,50,55,47,69,45,78,78
89f4a8ed61c1af384c895984ece7f07148747fdf,"Molecular Cloning has served as the foundation of technical expertise in labs worldwide for 30 years. No other manual has been so popular, or so influential. Molecular Cloning, Fourth Edition, by the celebrated founding author Joe Sambrook and new co-author, the distinguished HHMI investigator Michael Green, preserves the highly praised detail and clarity of previous editions and includes specific chapters and protocols commissioned for the book from expert practitioners at Yale, U Mass, Rockefeller University, Texas Tech, Cold Spring Harbor Laboratory, Washington University, and other leading institutions. The theoretical and historical underpinnings of techniques are prominent features of the presentation throughout, information that does much to help trouble-shoot experimental problems. For the fourth edition of this classic work, the content has been entirely recast to include nucleic-acid based methods selected as the most widely used and valuable in molecular and cellular biology laboratories. Core chapters from the third edition have been revised to feature current strategies and approaches to the preparation and cloning of nucleic acids, gene transfer, and expression analysis. They are augmented by 12 new chapters which show how DNA, RNA, and proteins should be prepared, evaluated, and manipulated, and how data generation and analysis can be handled. The new content includes methods for studying interactions between cellular components, such as microarrays, next-generation sequencing technologies, RNA interference, and epigenetic analysis using DNA methylation techniques and chromatin immunoprecipitation. To make sense of the wealth of data produced by these techniques, a bioinformatics chapter describes the use of analytical tools for comparing sequences of genes and proteins and identifying common expression patterns among sets of genes. Building on thirty years of trust, reliability, and authority, the fourth edition of Mol",2001,0,199652,12718,8,0,0,0,0,0,0,1,0,0
521027c14a7ef622a4f03106799706d0824e192a,"VMD is a molecular graphics program designed for the display and analysis of molecular assemblies, in particular biopolymers such as proteins and nucleic acids. VMD can simultaneously display any number of structures using a wide variety of rendering styles and coloring methods. Molecules are displayed as one or more ""representations,"" in which each representation embodies a particular rendering method and coloring scheme for a selected subset of atoms. The atoms displayed in each representation are chosen using an extensive atom selection syntax, which includes Boolean operators and regular expressions. VMD provides a complete graphical user interface for program control, as well as a text interface using the Tcl embeddable parser to allow for complex scripts with variable substitution, control loops, and function calls. Full session logging is supported, which produces a VMD command script for later playback. High-resolution raster images of displayed molecules may be produced by generating input scripts for use by a number of photorealistic image-rendering applications. VMD has also been expressly designed with the ability to animate molecular dynamics (MD) simulation trajectories, imported either from files or from a direct connection to a running MD simulation. VMD is the visualization component of MDScope, a set of tools for interactive problem solving in structural biology, which also includes the parallel MD program NAMD, and the MDCOMM software used to connect the visualization and simulation programs. VMD is written in C++, using an object-oriented design; the program, including source code and extensive documentation, is freely available via anonymous ftp and through the World Wide Web.",1996,15,34938,1410,0,0,0,0,0,0,0,0,0,0
7394dae1544c770625d8ead56df63433ca2be85b,"Molecular Genetics (Biology): An Overview | Sciencing Experiments in Molecular Genetics Experiments in molecular genetics (1972 edition) | Open ... Experimental Molecular Genetics | Biology | MIT OpenCourseWare DNA experiments you can perform at home | SBS Science Experiments in molecular genetics Jeffrey H. Miller ... DNA and Molecular Genetics Experiments in Molecular Biology: Biochemical Applications ... Molecular Genetics Biology Experiment Please help ... Molecular genetics | biology | Britannica Molecular Genetic Experiment : Biology Lab 1793 Words ... Miller, J.H. (1972) Experiments in Molecular Genetics ... Griffith's experiment Wikipedia DNA as genetic material: Revisiting classic experiments ... Experiments in molecular genetics (Book, 1972) [WorldCat.org] Measuring βGalactosidase Activity in Bacteria: Cell ... Classic Experiments in",1972,0,26378,2500,0,1,1,9,20,38,30,44,11,8
2d6f573c36c5e2153b65859fb080523fc4d842d0,"We announce the release of the fourth version of MEGA software, which expands on the existing facilities for editing DNA sequence data from autosequencers, mining Web-databases, performing automatic and manual sequence alignment, analyzing sequence alignments to estimate evolutionary distances, inferring phylogenetic trees, and testing evolutionary hypotheses. Version 4 includes a unique facility to generate captions, written in figure legend format, in order to provide natural language descriptions of the models and methods used in the analyses. This facility aims to promote a better understanding of the underlying assumptions used in analyses, and of the results generated. Another new feature is the Maximum Composite Likelihood (MCL) method for estimating evolutionary distances between all pairs of sequences simultaneously, with and without incorporating rate variation among sites and substitution pattern heterogeneities among lineages. This MCL method also can be used to estimate transition/transversion bias and nucleotide substitution pattern without knowledge of the phylogenetic tree. This new version is a native 32-bit Windows application with multi-threading and multi-user supports, and it is also available to run in a Linux desktop environment (via the Wine compatibility layer) and on Intel-based Macintosh computers under the Parallels program. The current version of MEGA is available free of charge at (http://www.megasoftware.net).",2007,10,28416,6041,0,0,0,0,1,18,2040,2152,1667,1255
6dea759b9e6d08b1e8ae7c3ac135234008e26aec,"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as 'Coot'.",2004,21,24373,1589,0,0,0,0,0,0,1,0,0,0
80935b370bac09ce615a002caabc30fbb26f029b,"With its theoretical basis firmly established in molecular evolutionary and population genetics, the comparative DNA and protein sequence analysis plays a central role in reconstructing the evolutionary histories of species and multigene families, estimating rates of molecular evolution, and inferring the nature and extent of selective forces shaping the evolution of genes and genomes. The scope of these investigations has now expanded greatly owing to the development of high-throughput sequencing techniques and novel statistical and computational methods. These methods require easy-to-use computer programs. One such effort has been to produce Molecular Evolutionary Genetics Analysis (MEGA) software, with its focus on facilitating the exploration and analysis of the DNA and protein sequence variation from an evolutionary perspective. Currently in its third major release, MEGA3 contains facilities for automatic and manual sequence alignment, web-based mining of databases, inference of the phylogenetic trees, estimation of evolutionary distances and testing evolutionary hypotheses. This paper provides an overview of the statistical methods, computational tools, and visual exploration modules for data input and the results obtainable in MEGA.",2004,73,12037,3302,1,0,26,1588,2150,1625,1243,879,678,475
7a253c839a797588065b4f843adffd4a7fa29b5f,"Abstract Three parallel algorithms for classical molecular dynamics are presented. The first assigns each processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute; the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models which can be difficult to parallelize efficiently—those with short-range forces where the neighbors of each atom change rapidly. They can be implemented on any distributed-memory parallel machine which allows for message-passing of data between independently executing processors. The algorithms are tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000 atoms on several parallel supercomputers--the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D. Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that the current generation of parallel machines is competitive with conventional vector supercomputers even for small problems. For large problems, the spatial algorithm achieves parallel efficiencies of 90% and a 1840-node Intel Paragon performs up to 165 faster than a single Cray C9O processor. Trade-offs between the three algorithms and guidelines for adapting them to more complex molecular dynamics simulations are also discussed.",1993,80,27074,688,0,0,0,0,0,0,0,0,0,0
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17519,724,0,0,0,0,0,0,0,1,1,2
c41c73c346652c2780d56372e2a332a480a2b332,"In molecular dynamics (MD) simulations the need often arises to maintain such parameters as temperature or pressure rather than energy and volume, or to impose gradients for studying transport properties in nonequilibrium MD. A method is described to realize coupling to an external bath with constant temperature or pressure with adjustable time constants for the coupling. The method is easily extendable to other variables and to gradients, and can be applied also to polyatomic molecules involving internal constraints. The influence of coupling time constants on dynamical variables is evaluated. A leap‐frog algorithm is presented for the general case involving constraints with coupling to both a constant temperature and a constant pressure bath.",1984,57,20664,573,0,0,0,0,0,0,0,0,0,0
ee4a25c3b307792c695a77f4166191fabadc5d55,"We present here a framework for the study of molecular variation within a single species. Information on DNA haplotype divergence is incorporated into an analysis of variance format, derived from a matrix of squared-distances among all pairs of haplotypes. This analysis of molecular variance (AMOVA) produces estimates of variance components and F-statistic analogs, designated here as phi-statistics, reflecting the correlation of haplotypic diversity at different levels of hierarchical subdivision. The method is flexible enough to accommodate several alternative input matrices, corresponding to different types of molecular data, as well as different types of evolutionary assumptions, without modifying the basic structure of the analysis. The significance of the variance components and phi-statistics is tested using a permutational approach, eliminating the normality assumption that is conventional for analysis of variance but inappropriate for molecular data. Application of AMOVA to human mitochondrial DNA haplotype data shows that population subdivisions are better resolved when some measure of molecular differences among haplotypes is introduced into the analysis. At the intraspecific level, however, the additional information provided by knowing the exact phylogenetic relations among haplotypes or by a nonlinear translation of restriction-site change into nucleotide diversity does not significantly modify the inferred population genetic structure. Monte Carlo studies show that site sampling does not fundamentally affect the significance of the molecular variance components. The AMOVA treatment is easily extended in several different directions and it constitutes a coherent and flexible framework for the statistical analysis of molecular data.",1992,46,13113,3049,0,0,0,0,0,0,0,2,2,0
06d49b0c74ebacde13bacabe9714b4825ea11ef4,"Recent developments of statistical methods in molecular phylogenetics are reviewed. It is shown that the mathematical foundations of these methods are not well established, but computer simulations and empirical data indicate that currently used methods such as neighbor joining, minimum evolution, likelihood, and parsimony methods produce reasonably good phylogenetic trees when a sufficiently large number of nucleotides or amino acids are used. However, when the rate of evolution varies exlensively from branch to branch, many methods may fail to recover the true topology. Solid statistical tests for examining'the accuracy of trees obtained by neighborjoining, minimum evolution, and least-squares method are available, but the methods for likelihood and parsimony trees are yet to be refined. Parsimony, likelihood, and distance methods can all be used for inferring amino acid sequences of the proteins of ancestral organisms that have become extinct.",1987,7,15493,3111,0,3,3,2,6,1,5,1,1,11
3c0c8388ec15c99dcfd3d696693cf1f88a1f3165,"Molecular simulation is an extremely useful, but computationally very expensive tool for studies of chemical and biomolecular systems. Here, we present a new implementation of our molecular simulation toolkit GROMACS which now both achieves extremely high performance on single processors from algorithmic optimizations and hand-coded routines and simultaneously scales very well on parallel machines. The code encompasses a minimal-communication domain decomposition algorithm, full dynamic load balancing, a state-of-the-art parallel constraint solver, and efficient virtual site algorithms that allow removal of hydrogen atom degrees of freedom to enable integration time steps up to 5 fs for atomistic simulations also in parallel. To improve the scaling properties of the common particle mesh Ewald electrostatics algorithms, we have in addition used a Multiple-Program, Multiple-Data approach, with separate node domains responsible for direct and reciprocal space interactions. Not only does this combination of algorithms enable extremely long simulations of large systems but also it provides that simulation performance on quite modest numbers of standard cluster nodes.",2008,0,12134,620,0,0,0,1,15,1006,1351,1298,1347,1251
04c69e602946dc56b985ad51aeeae403fbf04802,"NAMD is a parallel molecular dynamics code designed for high‐performance simulation of large biomolecular systems. NAMD scales to hundreds of processors on high‐end parallel platforms, as well as tens of processors on low‐cost commodity clusters, and also runs on individual desktop and laptop computers. NAMD works with AMBER and CHARMM potential functions, parameters, and file formats. This article, directed to novices as well as experts, first introduces concepts and methods used in the NAMD program, describing the classical molecular dynamics force field, equations of motion, and integration methods along with the efficient electrostatics evaluation algorithms employed and temperature and pressure controls used. Features for steering the simulation across barriers and for calculating both alchemical and conformational free energy differences are presented. The motivations for and a roadmap to the internal design of NAMD, implemented in C++ and based on Charm++ parallel objects, are outlined. The factors affecting the serial and parallel performance of a simulation are discussed. Finally, typical NAMD use is illustrated with representative applications to a small, a medium, and a large biomolecular system, highlighting particular features of NAMD, for example, the Tcl scripting language. The article also provides a list of the key features of NAMD and discusses the benefits of combining NAMD with the molecular graphics/sequence analysis software VMD and the grid computing/collaboratory software BioCoRE. NAMD is distributed free of charge with source code at www.ks.uiuc.edu. © 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1781–1802, 2005",2005,390,13572,738,0,1,0,0,0,0,1,5,513,1078
6e436b11eab90258229643cd47861f430f00080c,"Human breast tumours are diverse in their natural history and in their responsiveness to treatments. Variation in transcriptional programs accounts for much of the biological diversity of human cells and tumours. In each cell, signal transduction and regulatory systems transduce information from the cell's identity to its environmental status, thereby controlling the level of expression of every gene in the genome. Here we have characterized variation in gene expression patterns in a set of 65 surgical specimens of human breast tumours from 42 different individuals, using complementary DNA microarrays representing 8,102 human genes. These patterns provided a distinctive molecular portrait of each tumour. Twenty of the tumours were sampled twice, before and after a 16-week course of doxorubicin chemotherapy, and two tumours were paired with a lymph node metastasis from the same patient. Gene expression patterns in two tumour samples from the same individual were almost always more similar to each other than either was to any other sample. Sets of co-expressed genes were identified for which variation in messenger RNA levels could be related to specific features of physiological variation. The tumours could be classified into subtypes distinguished by pervasive differences in their gene expression patterns.",2000,43,13833,523,0,0,0,0,0,0,0,0,2,4
40c5441aad96b366996e6af163ca9473a19bb9ad,"The identification of maximally homologous subsequences among sets of long sequences is an important problem in molecular sequence analysis. The problem is straightforward only if one restricts consideration to contiguous subsequences (segments) containing no internal deletions or insertions. The more general problem has its solution in an extension of sequence metrics (Sellers 1974; Waterman et al., 1976) developed to measure the minimum number of “events” required to convert one sequence into another. These developments in the modern sequence analysis began with the heuristic homology algorithm of Needleman & Wunsch (1970) which first introduced an iterative matrix method of calculation. Numerous other heuristic algorithms have been suggested including those of Fitch (1966) and Dayhoff (1969). More mathematically rigorous algorithms were suggested by Sankoff (1972), Reichert et al. (1973) and Beyer et al. (1979) but these were generally not biologically satisfying or interpretable. Success came with Sellers (1974) development of a true metric measure of the distance between sequences. This metric was later generalized by Waterman et al. (1976) to include deletions/insertions of arbitrary length. This metric represents the minimum number of “mutational events” required to convert one sequence into another. It is of interest to note that Smith et al. (1980) have recently shown that under some conditions the generalized Sellers metric is equivalent to the original homology algorithm of Needleman & Wunsch (1970). In this letter we extend the above ideas to find a pair of segments, one from each of two long sequences, such that there is no other pair of segments with greater similarity (homology). The similarity measure used here allows for arbitrary length deletions and insertions.",1981,33,10026,656,3,2,6,12,9,7,11,14,22,25
c99628ea7b5fe2eb9dfbd6524af4c4b0dcfd9821,"New protein parameters are reported for the all-atom empirical energy function in the CHARMM program. The parameter evaluation was based on a self-consistent approach designed to achieve a balance between the internal (bonding) and interaction (nonbonding) terms of the force field and among the solvent-solvent, solvent-solute, and solute-solute interactions. Optimization of the internal parameters used experimental gas-phase geometries, vibrational spectra, and torsional energy surfaces supplemented with ab initio results. The peptide backbone bonding parameters were optimized with respect to data for N-methylacetamide and the alanine dipeptide. The interaction parameters, particularly the atomic charges, were determined by fitting ab initio interaction energies and geometries of complexes between water and model compounds that represented the backbone and the various side chains. In addition, dipole moments, experimental heats and free energies of vaporization, solvation and sublimation, molecular volumes, and crystal pressures and structures were used in the optimization. The resulting protein parameters were tested by applying them to noncyclic tripeptide crystals, cyclic peptide crystals, and the proteins crambin, bovine pancreatic trypsin inhibitor, and carbonmonoxy myoglobin in vacuo and in crystals. A detailed analysis of the relationship between the alanine dipeptide potential energy surface and calculated protein φ, χ angles was made and used in optimizing the peptide group torsional parameters. The results demonstrate that use of ab initio structural and energetic data by themselves are not sufficient to obtain an adequate backbone representation for peptides and proteins in solution and in crystals. Extensive comparisons between molecular dynamics simulations and experimental data for polypeptides and proteins were performed for both structural and dynamic properties. Energy minimization and dynamics simulations for crystals demonstrate that the latter are needed to obtain meaningful comparisons with experimental crystal structures. The presented parameters, in combination with the previously published CHARMM all-atom parameters for nucleic acids and lipids, provide a consistent set for condensed-phase simulations of a wide variety of molecules of biological interest.",1998,137,11258,290,0,0,0,0,0,0,0,5,190,404
41c6f229c4b149d502f89ffc386febb24116bd25,"Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.",1999,69,11717,931,1,0,1,10,102,737,793,790,786,675
c74bb1379abeec1f42b3cad633b95d19cfea69f4,"Molecular theory of gases and liquids , Molecular theory of gases and liquids , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1954,0,12199,294,0,0,0,0,0,0,0,0,0,0
725a3cbac5e12ee0dcd1dbd036929dc6d7783b58,"UNLABELLED
We have developed a new software package, Molecular Evolutionary Genetics Analysis version 2 (MEGA2), for exploring and analyzing aligned DNA or protein sequences from an evolutionary perspective. MEGA2 vastly extends the capabilities of MEGA version 1 by: (1) facilitating analyses of large datasets; (2) enabling creation and analyses of groups of sequences; (3) enabling specification of domains and genes; (4) expanding the repertoire of statistical methods for molecular evolutionary studies; and (5) adding new modules for visual representation of input data and output results on the Microsoft Windows platform.


AVAILABILITY
http://www.megasoftware.net.


CONTACT
s.kumar@asu.edu",2001,4,6259,1618,14,239,626,1021,1149,916,608,439,300,192
af08423e6305212023e2efa771c1858c0ef3ee93,A numerical algorithm integrating the 3N Cartesian equations of motion of a system of N points subject to holonomic constraints is formulated. The relations of constraint remain perfectly fulfilled at each step of the trajectory despite the approximate character of numerical integration. The method is applied to a molecular dynamics simulation of a liquid of 64 n-butane molecules and compared to a simulation using generalized coordinates. The method should be useful for molecular dynamics calculations on large molecules with internal degrees of freedom.,1977,14,14971,249,0,0,0,0,0,0,0,0,0,0
5b4fd2ede919f4ae0d101bb8fdaa8083632b5676,"A method is presented for the rapid isolation of high molecular weight plant DNA (50,000 base pairs or more in length) which is free of contaminants which interfere with complete digestion by restriction endonucleases. The procedure yields total cellular DNA (i.e. nuclear, chloroplast, and mitochondrial DNA). The technique is ideal for the rapid isolation of small amounts of DNA from many different species and is also useful for large scale isolations.",1980,8,9512,303,0,0,6,6,14,23,29,34,28,46
30eecc8a7b7346a5e0c3a6648b0e156faad3a786,"cellular viewpoint. That is all very well. In any case academics will swallow almost anything if it's their job. Even if the botanists of the 18th century, the well-intentioned of the 19th century and the latterday enthusiasts of this century believed in the cell, is it an approach that is truly relevant? Such must surely be the attitude of today's sceptical medical students, and of many of their now middle-aged predecessors. Well, let a battle-scarred predecessor speak. Even a conventional diet of comparative and classical morphology, combined with exposure to prefashionable Eltonian ecology, and a native distrust of big-business 'nouvelle vague' biology has not blinkered your reviewer to the virtues of Alberts' Molecular Biology of the Cell. This is undoubtedly a landmark for the student and his teacher in the field of preclinical medicine. Many American undergraduate texts in biochemistry and molecular genetics clearly outsell competing English publications. Only English textbooks in immunology remain popular, and effective sellers in this field. The successful texts in these areas eschew traditional academic contrapuntal argument. The new bestselling formula involves clarity of presentation, the effective use of multi-coloured diagrams and the detailed exposition of basic processes. Such didactic approaches suit undergraduates today, and certainly ease the hardship for their seniors needing a refresher",1983,0,8546,650,0,6,11,7,18,12,11,16,26,19
c697da0fd1e3c8be93aa2463a20085d767e7be67,,1977,0,9791,1023,0,1,0,1,4,2,19,86,101,128
b65fd09f71be715ab78bff5ac030fdde51225675,"We present ab initio quantum-mechanical molecular-dynamics calculations based on the calculation of the electronic ground state and of the Hellmann-Feynman forces in the local-density approximation at each molecular-dynamics step. This is possible using conjugate-gradient techniques for energy minimization, and predicting the wave functions for new ionic positions using subspace alignment. This approach avoids the instabilities inherent in quantum-mechanical molecular-dynamics calculations for metals based on the use of a fictitious Newtonian dynamics for the electronic degrees of freedom. This method gives perfect control of the adiabaticity and allows us to perform simulations over several picoseconds.",1993,0,22530,135,0,0,0,0,0,0,0,0,0,0
450ccba0a2951a7011b091d1b8bb6bb8be63505d,"Abstract Forty proteins with polypeptide chains of well characterized molecular weights have been studied by polyacrylamide gel electrophoresis in the presence of sodium dodecyl sulfate following the procedure of Shapiro, Vinuela, and Maizel (Biochem. Biophys. Res. Commun., 28, 815 (1967)). When the electrophoretic mobilities were plotted against the logarithm of the known polypeptide chain molecular weights, a smooth curve was obtained. The results show that the method can be used with great confidence to determine the molecular weights of polypeptide chains for a wide variety of proteins.",1969,2,16421,166,0,0,1,6,15,20,39,37,61,41
897172c2bff121370e4454cccb685a51dd2a2fba,"A new Lagrangian formulation is introduced. It can be used to make molecular dynamics (MD) calculations on systems under the most general, externally applied, conditions of stress. In this formulation the MD cell shape and size can change according to dynamical equations given by this Lagrangian. This new MD technique is well suited to the study of structural transformations in solids under external stress and at finite temperature. As an example of the use of this technique we show how a single crystal of Ni behaves under uniform uniaxial compressive and tensile loads. This work confirms some of the results of static (i.e., zero temperature) calculations reported in the literature. We also show that some results regarding the stress‐strain relation obtained by static calculations are invalid at finite temperature. We find that, under compressive loading, our model of Ni shows a bifurcation in its stress‐strain relation; this bifurcation provides a link in configuration space between cubic and hexagonal c...",1981,11,10305,222,0,0,0,0,0,0,0,0,0,0
27e5725be7e538f2329011e44481b68f4315f39e,"SummaryA new statistical method for estimating divergence dates of species from DNA sequence data by a molecular clock approach is developed. This method takes into account effectively the information contained in a set of DNA sequence data. The molecular clock of mitochondrial DNA (mtDNA) was calibrated by setting the date of divergence between primates and ungulates at the Cretaceous-Tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. A generalized leastsquares method was applied in fitting a model to mtDNA sequence data, and the clock gave dates of 92.3±11.7, 13.3±1.5, 10.9±1.2, 3.7±0.6, and 2.7±0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. Although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the bipedal creatureAustralopithecus afarensis, which lived some 3.7 million years ago at Laetoli in Tanzania and at Hadar in Ethiopia, was ancestral to man and evolved after the human-ape splitting. Another likelier possibility is that mtDNA was transferred through hybridization between a proto-human and a protochimpanzee after the former had developed bipedalism.",2005,119,6606,894,366,345,363,335,363,351,334,326,361,340
ddf06cf0d375fb9404fe30c5f1d7858d74080e9c,The European Molecular Biology Open Software Suite (EMBOSS) is a mature package of software tools developed for the molecular biology community. It includes a comprehensive set of applications for molecular sequence analysis and other tasks and integrates popular third-party software packages under a consistent interface. EMBOSS includes extensive C programming libraries and is a platform to develop and release software in the true open source spirit.,2000,4,7763,588,3,10,22,85,127,216,255,308,348,346
86afe671ad3143796b57512859f71d594a44656b,"Second and revised edition 
 
Understanding Molecular Simulation: From Algorithms to Applications explains the physics behind the ""recipes"" of molecular simulation for materials science. Computer simulators are continuously confronted with questions concerning the choice of a particular technique for a given application. A wide variety of tools exist, so the choice of technique requires a good understanding of the basic principles. More importantly, such understanding may greatly improve the efficiency of a simulation program. The implementation of simulation methods is illustrated in pseudocodes and their practical use in the case studies used in the text. 
 
Since the first edition only five years ago, the simulation world has changed significantly -- current techniques have matured and new ones have appeared. This new edition deals with these new developments; in particular, there are sections on: 
 
· Transition path sampling and diffusive barrier crossing to simulaterare events 
· Dissipative particle dynamic as a course-grained simulation technique 
· Novel schemes to compute the long-ranged forces 
· Hamiltonian and non-Hamiltonian dynamics in the context constant-temperature and constant-pressure molecular dynamics simulations 
· Multiple-time step algorithms as an alternative for constraints 
· Defects in solids 
· The pruned-enriched Rosenbluth sampling, recoil-growth, and concerted rotations for complex molecules 
· Parallel tempering for glassy Hamiltonians 
 
Examples are included that highlight current applications and the codes of case studies are available on the World Wide Web. Several new examples have been added since the first edition to illustrate recent applications. Questions are included in this new edition. No prior knowledge of computer simulation is assumed.",1996,0,7330,435,1,10,24,37,42,81,146,182,187,232
73d7d1adf83ee16e12e6b442ec15357da218c06d,"Diabetes-specific microvascular disease is a leading cause of blindness, renal failure and nerve damage, and diabetes-accelerated atherosclerosis leads to increased risk of myocardial infarction, stroke and limb amputation. Four main molecular mechanisms have been implicated in glucose-mediated vascular damage. All seem to reflect a single hyperglycaemia-induced process of overproduction of superoxide by the mitochondrial electron-transport chain. This integrating paradigm provides a new conceptual framework for future research and drug discovery.",2001,183,7809,418,3,39,172,208,285,315,374,423,421,445
5487f2e27071d24f9b545a3db39bce8bdc92f932,"The direct simulation Monte Carlo (or DSMC) method has, in recent years, become widely used in engineering and scientific studies of gas flows that involve low densities or very small physical dimensions. This method is a direct physical simulation of the motion of representative molecules, rather than a numerical solution of the equations that provide a mathematical model of the flow. These computations are no longer expensive and the period since the 1976 publication of the original Molecular Gas Dynamics has seen enormous improvements in the molecular models, the procedures, and the implementation strategies for the DSMC method. The molecular theory of gas flows is developed from first principles and is extended to cover the new models and procedures. Note: The disk that originally came with this book is no longer available. However, the same information is available from the author's website (http://gab.com.au/)",1994,0,4186,378,7,26,65,64,73,61,96,123,140,148
82e12e48e4f1522dcee5c8c0334a7bae7d418c32,1. Molecular basis of evolution 2. Evolutionary changes of amino acid sequences 3. Evolutionary changes of DNA sequences 4. Synonymous and nonsynonymous nucleotide substitutions 5. Phylogenetic trees 6. Phylogenetic inference: Distance methods 7. Phylogenetic inference: Maximum parsimony methods 8. Phylogenetic inference: Maximum likelihood methods 9. Accuracies and statistical tests of phylogenetic trees 10. Molecular clocks and linearized trees 11. Ancestral nucleotide and amino acid sequences 12. Genetic polymorphism and evolution 13. Population trees from genetic markers 14. Perspectives Appendices A. Mathematical sumbols and notations B. Geological timescale C. Geological events in the Cenozoic and Meszoic eras D. Evolution of organisms based on the fossil record,2000,0,4081,389,3,38,77,112,128,156,157,179,175,182
701d3e38292ccb8ef9441ab29a4ba66750757c7c,"The potassium channel from Streptomyces lividans is an integral membrane protein with sequence similarity to all known K+ channels, particularly in the pore region. X-ray analysis with data to 3.2 angstroms reveals that four identical subunits create an inverted teepee, or cone, cradling the selectivity filter of the pore in its outer end. The narrow selectivity filter is only 12 angstroms long, whereas the remainder of the pore is wider and lined with hydrophobic amino acids. A large water-filled cavity and helix dipoles are positioned so as to overcome electrostatic destabilization of an ion in the pore at the center of the bilayer. Main chain carbonyl oxygen atoms from the K+ channel signature sequence line the selectivity filter, which is held open by structural constraints to coordinate K+ ions but not smaller Na+ ions. The selectivity filter contains two K+ ions about 7.5 angstroms apart. This configuration promotes ion conduction by exploiting electrostatic repulsive forces to overcome attractive forces between K+ ions and the selectivity filter. The architecture of the pore establishes the physical principles underlying selective K+ conduction.",1998,41,6007,466,110,344,295,339,311,337,351,322,284,298
c2d5c55c485adaaf14654bee7b28fe6dd33fe155,,1989,0,6745,536,120,123,153,145,140,145,212,158,300,367
9eec798bd972a3d3968f5a9b3c915fddba4058a6,"A new molecular-replacement package is presented. It is an improvement on conventional methods, based on more powerful algorithms and a new conception that enables automation and rapid solution.",1994,12,4655,324,7,59,97,172,264,383,391,415,388,399
625ddfe164e3a25405672ecc1ca17d05353ff182,"Abstract A parallel message-passing implementation of a molecular dynamics (MD) program that is useful for bio(macro)molecules in aqueous environment is described. The software has been developed for a custom-designed 32-processor ring GROMACS (GROningen MAchine for Chemical Simulation) with communication to and from left and right neighbours, but can run on any parallel system onto which a a ring of processors can be mapped and which supports PVM-like block send and receive calls. The GROMACS software consists of a preprocessor, a parallel MD and energy minimization program that can use an arbitrary number of processors (including one), an optional monitor, and several analysis tools. The programs are written in ANSI C and available by ftp (information: gromacs@chem.rug.nl). The functionality is based on the GROMOS (GROningen MOlecular Simulation) package (van Gunsteren and Berendsen, 1987; BIOMOS B.V., Nijenborgh 4, 9747 AG Groningen). Conversion programs between GROMOS and GROMACS formats are included. The MD program can handle rectangular periodic boundary conditions with temperature and pressure scaling. The interactions that can be handled without modification are variable non-bonded pair interactions with Coulomb and Lennard-Jones or Buckingham potentials, using a twin-range cut-off based on charge groups, and fixed bonded interactions of either harmonic or constraint type for bonds and bond angles and either periodic or cosine power series interactions for dihedral angles. Special forces can be added to groups of particles (for non-equilibrium dynamics or for position restraining) or between particles (for distance restraints). The parallelism is based on particle decomposition. Interprocessor communication is largely limited to position and force distribution over the ring once per time step.",1995,38,6316,274,0,6,4,10,13,14,18,52,53,108
f6380f7f090f2cae9fe723dae1873adf57126400,"Abstract. GROMACS 3.0 is the latest release of a versatile and very well optimized package for molecular simulation. Much effort has been devoted to achieving extremely high performance on both workstations and parallel computers. The design includes an extraction of virial and periodic boundary conditions from the loops over pairwise interactions, and special software routines to enable rapid calculation of x–1/2. Inner loops are generated automatically in C or Fortran at compile time, with optimizations adapted to each architecture. Assembly loops using SSE and 3DNow! Multimedia instructions are provided for x86 processors, resulting in exceptional performance on inexpensive PC workstations. The interface is simple and easy to use (no scripting language), based on standard command line arguments with self-explanatory functionality and integrated documentation. All binary files are independent of hardware endian and can be read by versions of GROMACS compiled using different floating-point precision. A large collection of flexible tools for trajectory analysis is included, with output in the form of finished Xmgr/Grace graphs. A basic trajectory viewer is included, and several external visualization tools can read the GROMACS trajectory format. Starting with version 3.0, GROMACS is available under the GNU General Public License from http://www.gromacs.org.",2001,94,5348,244,1,33,68,144,243,294,338,381,387,346
b34f0be6c739fbbf26714f62138559a11531fcd4,"In the past, basis sets for use in correlated molecular calculations have largely been taken from single configuration calculations. Recently, Almlof, Taylor, and co‐workers have found that basis sets of natural orbitals derived from correlated atomic calculations (ANOs) provide an excellent description of molecular correlation effects. We report here a careful study of correlation effects in the oxygen atom, establishing that compact sets of primitive Gaussian functions effectively and efficiently describe correlation effects i f the exponents of the functions are optimized in atomic correlated calculations, although the primitive (s p) functions for describing correlation effects can be taken from atomic Hartree–Fock calculations i f the appropriate primitive set is used. Test calculations on oxygen‐containing molecules indicate that these primitive basis sets describe molecular correlation effects as well as the ANO sets of Almlof and Taylor. Guided by the calculations on oxygen, basis sets for use in correlated atomic and molecular calculations were developed for all of the first row atoms from boron through neon and for hydrogen. As in the oxygen atom calculations, it was found that the incremental energy lowerings due to the addition of correlating functions fall into distinct groups. This leads to the concept of c o r r e l a t i o n c o n s i s t e n t b a s i s s e t s, i.e., sets which include all functions in a given group as well as all functions in any higher groups. Correlation consistent sets are given for all of the atoms considered. The most accurate sets determined in this way, [5s4p3d2f1g], consistently yield 99% of the correlation energy obtained with the corresponding ANO sets, even though the latter contains 50% more primitive functions and twice as many primitive polarization functions. It is estimated that this set yields 94%–97% of the total (HF+1+2) correlation energy for the atoms neon through boron.",1989,27,21215,51,0,0,0,0,0,0,0,0,0,0
0a21cd88817494b6101219c21b829c7d8aa1b541,"Three recently proposed constant temperature molecular dynamics methods by: (i) Nose (Mol. Phys., to be published); (ii) Hoover et al. [Phys. Rev. Lett. 48, 1818 (1982)], and Evans and Morriss [Chem. Phys. 77, 63 (1983)]; and (iii) Haile and Gupta [J. Chem. Phys. 79, 3067 (1983)] are examined analytically via calculating the equilibrium distribution functions and comparing them with that of the canonical ensemble. Except for effects due to momentum and angular momentum conservation, method (1) yields the rigorous canonical distribution in both momentum and coordinate space. Method (2) can be made rigorous in coordinate space, and can be derived from method (1) by imposing a specific constraint. Method (3) is not rigorous and gives a deviation of order N−1/2 from the canonical distribution (N the number of particles). The results for the constant temperature–constant pressure ensemble are similar to the canonical ensemble case.",1984,23,10639,145,0,0,0,0,0,0,0,0,0,0
aa274f1ed2996acababecde388db94a1b20604fa,"MOLREP is an automated program for molecular replacement which utilizes effective new approaches in data processing and rotational and translational searching. These include an automatic choice of all parameters, scaling by Patterson origin peaks and sott resolution cutoff. One of the cornerstones of the program is an original full-symmetry translation function combined with a packing function. Information from the model already placed in the cell is incorporated in both translation and packing functions. A number of tests using experimental data proved the ability of the program to find the correct solution in difficult cases.",1997,15,4365,262,0,1,4,8,17,33,67,166,200,279
2a193b9417a4aaf35bcad9152cf35f78dc5906a9,"The tools of molecular biology were used to solve an instance of the directed Hamiltonian path problem. A small graph was encoded in molecules of DNA, and the ""operations"" of the computation were performed with standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computations at the molecular level.",1994,37,4010,272,3,36,52,77,69,95,92,105,120,133
16fb5f02dac7f4ebcd8430b53df6e7e6ef6dab61,"THE major active ingredient of marijuana, Δ9-tetrahydrocannabi-nol (Δ9-THC), has been used as a psychoactive agent for thousands of years. Marijuana, and Δ9-THC, also exert a wide range of other effects including analgesia, anti-inflammation, immunosuppression, anticonvulsion, alleviation of intraocular pressure in glaucoma, and attenuation of vomiting1. The clinical application of cannabinoids has, however, been limited by their psychoactive effects, and this has led to interest in the biochemical bases of their action. Progress stemmed initially from the synthesis of potent derivatives of δ9-THC4,5, and more recently from the cloning of a gene encoding a G-protein-coupled receptor for cannabinoids6. This receptor is expressed in the brain but not in the periphery, except for a low level in testes. It has been proposed that the non-psychoactive effects of cannabinoids are either mediated centrally or through direct interaction with other, non-receptor proteins1,7,8. Here we report the cloning of a receptor for cannabinoids that is not expressed in the brain but rather in macrophages in the marginal zone of spleen.",1993,28,4460,296,1,21,31,57,52,102,105,93,99,129
a5a22f4adc9ae7ca2e7a2025ec297d1f5aa6b250,"A new parametric quantum mechanical molecular model, AM1 (Austin Model l), based on the NDDO approximation, is described. In it the major weaknesses of MNDO, in particular failure to reproduce hydrogen bonds, have been overcome without any increase in computing time. Results for 167 molecules are reported. Parameters are currently available for C, H, 0, and N.",1985,1,11082,115,0,0,0,0,1,1,2,78,431,486
020f8fd4c7d1519025e367632d436091d6fcf8f8,"Generation of Interleukin (IL)-1beta via cleavage of its proform requires the activity of caspase-1 (and caspase-11 in mice), but the mechanism involved in the activation of the proinflammatory caspases remains elusive. Here we report the identification of a caspase-activating complex that we call the inflammasome. The inflammasome comprises caspase-1, caspase-5, Pycard/Asc, and NALP1, a Pyrin domain-containing protein sharing structural homology with NODs. Using a cell-free system, we show that proinflammatory caspase activation and proIL-1beta processing is lost upon prior immunodepletion of Pycard. Moreover, expression of a dominant-negative form of Pycard in differentiated THP-1 cells blocks proIL-1beta maturation and activation of inflammatory caspases induced by LPS in vivo. Thus, the inflammasome constitutes an important arm of the innate immunity.",2002,49,4347,257,7,43,57,40,66,119,121,170,161,226
292cd855004cc4b271cad0d13709124f28f69093,,2001,0,5784,272,128,182,225,278,366,351,374,354,382,356
2a206b500ae70bbaf05ed692098871c92ec33527,,1992,0,9125,201,235,283,294,304,341,334,313,338,376,319
d0b0a759aa43f199501d9dcad4ee33014098aafe,"Plant responses to salinity stress are reviewed with emphasis on molecular mechanisms of signal transduction and on the physiological consequences of altered gene expression that affect biochemical reactions downstream of stress sensing. We make extensive use of comparisons with model organisms, halophytic plants, and yeast, which provide a paradigm for many responses to salinity exhibited by stress-sensitive plants. Among biochemical responses, we emphasize osmolyte biosynthesis and function, water flux control, and membrane transport of ions for maintenance and re-establishment of homeostasis. The advances in understanding the effectiveness of stress responses, and distinctions between pathology and adaptive advantage, are increasingly based on transgenic plant and mutant analyses, in particular the analysis of Arabidopsis mutants defective in elements of stress signal transduction pathways. We summarize evidence for plant stress signaling systems, some of which have components analogous to those that regulate osmotic stress responses of yeast. There is evidence also of signaling cascades that are not known to exist in the unicellular eukaryote, some that presumably function in intercellular coordination or regulation of effector genes in a cell-/tissue-specific context required for tolerance of plants. A complex set of stress-responsive transcription factors is emerging. The imminent availability of genomic DNA sequences and global and cell-specific transcript expression data, combined with determinant identification based on gain- and loss-of-function molecular genetics, will provide the infrastructure for functional physiological dissection of salt tolerance determinants in an organismal context. Furthermore, protein interaction analysis and evaluation of allelism, additivity, and epistasis allow determination of ordered relationships between stress signaling components. Finally, genetic activation and suppression screens will lead inevitably to an understanding of the interrelationships of the multiple signaling systems that control stress-adaptive responses in plants.",2000,370,4414,213,3,33,87,76,116,108,148,171,185,218
bb967168ead7a14adcb0121dcf24a930d1a383b3,"This paper describes the contents of the 2016 edition of the HITRAN molecular spectroscopic compilation. The new edition replaces the previous HITRAN edition of 2012 and its updates during the intervening years. The HITRAN molecular absorption compilation is composed of five major components: the traditional line-by-line spectroscopic parameters required for high-resolution radiative-transfer codes, infrared absorption cross-sections for molecules not yet amenable to representation in a line-by-line form, collision-induced absorption data, aerosol indices of refraction, and general tables such as partition sums that apply globally to the data. The new HITRAN is greatly extended in terms of accuracy, spectral coverage, additional absorption phenomena, added line-shape formalisms, and validity. Moreover, molecules, isotopologues, and perturbing gases have been added that address the issues of atmospheres beyond the Earth. Of considerable note, experimental IR cross-sections for almost 300 additional molecules important in different areas of atmospheric science have been added to the database. The compilation can be accessed through www.hitran.org. Most of the HITRAN data have now been cast into an underlying relational database structure that offers many advantages over the long-standing sequential text-based structure. The new structure empowers the user in many ways. It enables the incorporation of an extended set of fundamental parameters per transition, sophisticated line-shape formalisms, easy user-defined output formats, and very convenient searching, filtering, and plotting of data. A powerful application programming interface making use of structured query language (SQL) features for higher-level applications of HITRAN is also provided.",2005,1321,6840,199,42,179,232,287,328,379,371,405,446,483
a27087636a2708771f654fa175f406c63a9272be,"A description of the ab initio quantum chemistry package GAMESS is presented. Chemical systems containing atoms through radon can be treated with wave functions ranging from the simplest closed‐shell case up to a general MCSCF case, permitting calculations at the necessary level of sophistication. Emphasis is given to novel features of the program. The parallelization strategy used in the RHF, ROHF, UHF, and GVB sections of the program is described, and detailed speecup results are given. Parallel calculations can be run on ordinary workstations as well as dedicated parallel machines. © John Wiley & Sons, Inc.",1993,131,15511,135,0,0,0,0,0,0,0,0,0,0
e85fd090421d0ae5e5939ef51d6e3c44933cd08c,"Publisher Summary This chapter describes techniques concerned with classical and molecular genetics, cell biology, and biochemistry that can be used with Schizosaccharomyces pombe . Conjugation and sporulation cannot take place in S. pombe except under conditions of nutrient starvation. ME medium is generally used for genetic crosses. To cross two strains, a loopful of h – and a loopful of h + are mixed together on a ME plate. The cross is left to dry and is then incubated below 30°, as conjugation is severely reduced above this temperature. Fully formed four-spore asci can be seen after 2–3 days of incubation. A 2 day-old cross is usually used for tetrad analysis. Using a 3-day-old cross, one can check for the presence of asci under the light microscope. Random spore analysis allows many more spores to be examined than in tetrad analysis, and in this way recombination mapping and strain construction can be carried out. Diploid cells arise spontaneously in most S. pombe strains, this characteristic can be used to isolate homozygous diploids of any strain. For mutagenesis of yeast strains, ethylmethane sulfonate (EMS) and nitrosoguanidine is used.",1991,15,3389,278,5,17,22,51,60,69,119,123,133,124
f581036fe54ba182dc11696134a6d9182f68228b,"Mitochondria play a key part in the regulation of apoptosis (cell death). Their intermembrane space contains several proteins that are liberated through the outer membrane in order to participate in the degradation phase of apoptosis. Here we report the identification and cloning of an apoptosis-inducing factor, AIF, which is sufficient to induce apoptosis of isolated nuclei. AIF is a flavoprotein of relative molecular mass 57,000 which shares homology with the bacterial oxidoreductases; it is normally confined to mitochondria but translocates to the nucleus when apoptosis is induced. Recombinant AIF causes chromatin condensation in isolated nuclei and large-scale fragmentation of DNA. It induces purified mitochondria to release the apoptogenic proteins cytochrome c and caspase-9. Microinjection of AIF into the cytoplasm of intact cells induces condensation of chromatin, dissipation of the mitochondrial transmembrane potential, and exposure of phosphatidylserine in the plasma membrane. None of these effects is prevented by the wide-ranging caspase inhibitor known as Z-VAD.fmk. Overexpression of Bcl-2, which controls the opening of mitochondrial permeability transition pores, prevents the release of AIF from the mitochondrion but does not affect its apoptogenic activity. These results indicate that AIF is a mitochondrial effector of apoptotic cell death.",1999,32,4046,189,93,230,236,254,266,265,252,235,222,202
cc4422e00c3bbcbca00addbd19da65e6934e467b,"A new direct difference method for the computation of molecular interactions has been based on a bivariational transcorrelated treatment, together with special methods for the balancing of other errors. It appears that these new features can give a strong reduction in the error of the interaction energy, and they seem to be particularly suitable for computations in the important region near the minimum energy. It has been generally accepted that this problem is dominated by unresolved difficulties and the relation of the new methods to these apparent difficulties is analysed here.",1970,4,15843,78,0,0,0,0,0,0,0,0,0,0
b5aef2f50e0220c8e3117b6c23299d974772ee84,"In the following, the first results on ultraviolet laser desorption (UVLD) of bioorganic compounds in the mass range above 10000 daltons are reported. Strong molecular ion signals were registered by use of an organic matrix with strong absorption at the wavelength used for controlled energy deposition and soft desorption (7)",1988,10,4937,174,0,26,36,60,72,71,96,88,64,90
bcbca5c598a6fabaa580e3a1e4be071c243e1f69,"MICROPOROUS and mesoporous inorganic solids (with pore diameters of ≤20 Å and ∼20–500 Å respectively)1 have found great utility as catalysts and sorption media because of their large internal surface area. Typical microporous materials are the crystalline framework solids, such as zeolites2, but the largest pore dimensions found so far are ∼10–12 Å for some metallophosphates3–5 and ∼14 Å for the mineral cacoxenite6. Examples of mesoporous solids include silicas7 and modified layered materials8–11, but these are invariably amorphous or paracrystalline, with pores that are irregularly spaced and broadly distributed in size8,12. Pore size can be controlled by intercalation of layered silicates with a surfactant species9,13, but the final product retains, in part, the layered nature of the precursor material. Here we report the synthesis of mesoporous solids from the calcination of aluminosilicate gels in the presence of surfactants. The material14,15 possesses regular arrays of uniform channels, the dimensions of which can be tailored (in the range 16 Å to 100 Å or more) through the choice of surfactant, auxiliary chemicals and reaction conditions. We propose that the formation of these materials takes place by means of a liquid-crystal 'templating' mechanism, in which the silicate material forms inorganic walls between ordered surfactant micelles.",1992,16,12954,101,0,0,0,0,0,0,0,0,0,0
8bbe9dd9da329cf7bb311a5a0c3d53a0583491be,"The synthesis, characterization, and proposed mechanism of formation of a new family of silicatelaluminosilicate mesoporous molecular sieves designated as M41S is described. MCM-41, one member of this family, exhibits a hexagonal arrangement of uniform mesopores whose dimensions may be engineered in the range of - 15 A to greater than 100 A. Other members of this family, including a material exhibiting cubic symmetry, have ken synthesized. The larger pore M41S materials typically have surface areas above 700 m2/g and hydrocarbon sorption capacities of 0.7 cc/g and greater. A templating mechanism (liquid crystal templating-LCT) in which surfactant liquid crystal structures serve as organic templates is proposed for the formation of these materials. In support of this templating mechanism, it was demonstrated that the structure and pore dimensions of MCM-41 materials are intimately linked to the properties of the surfactant, including surfactant chain length and solution chemistry. The presence of variable pore size MCM-41, cubic material, and other phases indicates that M41S is an extensive family of materials.",1992,61,8954,93,0,9,60,74,132,149,182,187,271,285
2b6ede7b046c3da0e656c44215602c0487551a8b,"Microbe-associated molecular patterns (MAMPs) are molecular signatures typical of whole classes of microbes, and their recognition plays a key role in innate immunity. Endogenous elicitors are similarly recognized as damage-associated molecular patterns (DAMPs). This review focuses on the diversity of MAMPs/DAMPs and on progress to identify the corresponding pattern recognition receptors (PRRs) in plants. The two best-characterized MAMP/PRR pairs, flagellin/FLS2 and EF-Tu/EFR, are discussed in detail and put into a phylogenetic perspective. Both FLS2 and EFR are leucine-rich repeat receptor kinases (LRR-RKs). Upon treatment with flagellin, FLS2 forms a heteromeric complex with BAK1, an LRR-RK that also acts as coreceptor for the brassinolide receptor BRI1. The importance of MAMP/PRR signaling for plant immunity is highlighted by the finding that plant pathogens use effectors to inhibit PRR complexes or downstream signaling events. Current evidence indicates that MAMPs, DAMPs, and effectors are all perceived as danger signals and induce a stereotypic defense response.",2009,149,2496,246,28,105,137,182,251,264,243,243,222,224
ceedaf1403c1e5fb2faf6e005c380d0f588e2949,"Spend your few moment to read a book even only few pages. Reading book is not obligation and force for everybody. When you don't want to read, you can get punishment from the publisher. Read a book becomes a choice of your different characteristics. Many people with reading habit will always be enjoyable to read, or on the contrary. For some reasons, this molecular markers natural history and evolution tends to be the representative book in this website.",1994,0,3139,311,7,17,49,78,108,107,114,130,122,112
02a9b5080741eab5f6b0f1b1d357cc42fd2e8ad7,"One of the most remarkable aspects of an animal's behavior is the ability to modify that behavior by learning, an ability that reaches its highest form in human beings. For me, learning and memory have proven to be endlessly fascinating mental processes because they address one of the fundamental features of human activity: our ability to acquire new ideas from experience and to retain these ideas over time in memory. Moreover, unlike other mental processes such as thought, language, and consciousness, learning seemed from the outset to be readily accessible to cellular and molecular analysis. I, therefore, have been curious to know: What changes in the brain when we learn? And, once something is learned, how is that information retained in the brain? I have tried to address these questions through a reductionist approach that would allow me to investigate elementary forms of learning and memory at a cellular molecular level—as specific molecular activities within identified nerve cells.",2001,114,3397,186,1,53,127,134,142,190,178,213,192,190
0fbe1c150884f85d22ee63a47bc6cbca30b4d4ec,"Methodology for General and Molecular Microbiology Morphology Light microscopy Determinative and cytological light microscopy Electron microscopy Cell fractionation Antigen-antibody reactions Growth: Physicochemical factors in growth Nutrition and media Enrichment and isolation Solid, liquid/solid and semisolid culture Liquid culture Growth measurement Culture preservation Molecular Genetics: Gene mutation Gene transfer in Gram-negative bacteria Gene transfer in Gram-positive bacteria Plasmids Transposon mutagenesis Gene cloning and expression Polymerase chain reaction Nucleic acid analysis Metabolism: Physical analysis Chemical analysis Enzymatic activity Permeability and transport Systematics: Phenotypic characterization DNA sequence similarities Ribosomal RNA hybridization and gene sequencing Nucleic acid probes General Methods: Laboratory safety Photography Records and reports",1994,0,3338,201,7,36,51,65,97,104,108,123,124,108
58cb1e26e323ae41fad6f4638110e58116e9923a,"1. The Phase Equilibrium Problem. 2. Classical Thermodynamics of Phase Equilibria. 3. Thermodynamic Properties from Volumetric Data. 4. Intermolecular Forces, Corresponding States and Osmotic Systems. 5. Fugacities in Gas Mixtures. 6. Fugacities in Liquid Mixtures: Excess Functions. 7. Fugacities in Liquid Mixtures: Models and Theories of Solutions. 8. Polymers: Solutions, Blends, Membranes, and Gels. 9. Electrolyte Solutions. 10. Solubilities of Gases in Liquids. 11. Solubilities of Solids in Liquids. 12. High-Pressure Phase Equilibria. Appendix A. Uniformity of Intensive Potentials as a Criterion of Phase Equilibrium. Appendix B. A Brief Introduction to Statistical Thermodynamics. Appendix C. Virial Coefficients for Quantum Gases. Appendix D. The Gibbs-Duhem Equation. Appendix E. Liquid-Liquid Equilibria in Binary and Multicomponent Systems. Appendix F. Estimation of Activity Coefficients. Appendix G. A General Theorem for Mixtures with Associating or Solvating Molecules. Appendix H. Brief Introduction to Perturbation Theory of Dense Fluids. Appendix I. The Ion-Interaction Model of Pitzer for Multielectrolyte Solutions. Appendix J. Conversion Factors and Constants. Index.",1969,0,4434,200,0,7,6,10,4,5,16,8,12,16
b831ec95b6d1af66fd3d4f4a3a8a4b59381058a0,,2007,105,2509,815,11,73,238,343,427,380,295,186,171,141
818fbce63180ab2fc3c344dd1bdf624fbfc37681,"A molecular dynamics simulation method which can generate configurations belonging to the canonical (T, V, N) ensemble or the constant temperature constant pressure (T, P, N) ensemble, is proposed. The physical system of interest consists of N particles (f degrees of freedom), to which an external, macroscopic variable and its conjugate momentum are added. This device allows the total energy of the physical system to fluctuate. The equilibrium distribution of the energy coincides with the canonical distribution both in momentum and in coordinate space. The method is tested for an atomic fluid (Ar) and works well.",1984,17,6394,141,0,9,20,17,15,18,36,29,36,30
e84cc71e71ee1d52df52b45ff6c95167fd447308,"Molecular cell biology , Molecular cell biology , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1986,0,5755,136,0,10,24,32,19,40,34,47,45,62
31f53bb704c274df483aee602b290ae3e0202df4,"We present a unified scheme that, by combining molecular dynamics and density-functional theory, profoundly extends the range of both concepts. Our approach extends molecular dynamics beyond the usual pair-potential approximation, thereby making possible the simulation of both covalently bonded and metallic systems. In addition it permits the application of density-functional theory to much larger systems than previously feasible. The new technique is demonstrated by the calculation of some static and dynamic properties of crystalline silicon within a self-consistent pseudopotential framework.",1985,0,7489,122,1,7,24,37,52,83,104,106,142,147
b3c7a75a284844262a5b84658c85dd6ae3edb665,"Describes and discusses the use of theoretical models as an alternative to experiment in making accurate predictions of chemical phenomena. Addresses the formulation of theoretical molecular orbital models starting from quantum mechanics, and compares them to experimental results. Draws on a series of models that have already received widespread application and are available for new applications. A new and powerful research tool for the practicing experimental chemist.",1986,30,7893,110,10,41,75,77,98,117,144,153,212,324
c1cb8af97a5aecc3f78a3f0a6a285b841c6c404c,"In this article we present a new LINear Constraint Solver (LINCS) for molecular simulations with bond constraints. The algorithm is inherently stable, as the constraints themselves are reset instead of derivatives of the constraints, thereby eliminating drift. Although the derivation of the algorithm is presented in terms of matrices, no matrix matrix multiplications are needed and only the nonzero matrix elements have to be stored, making the method useful for very large molecules. At the same accuracy, the LINCS algorithm is 3 to 4 times faster than the SHAKE algorithm. Parallelization of the algorithm is straightforward.",1997,18,4383,138,0,1,1,3,5,18,20,41,51,69
dc155aa4381dad8d62057de57c83f5374abdaa8d,"The nervous system detects and interprets a wide range of thermal and mechanical stimuli, as well as environmental and endogenous chemical irritants. When intense, these stimuli generate acute pain, and in the setting of persistent injury, both peripheral and central nervous system components of the pain transmission pathway exhibit tremendous plasticity, enhancing pain signals and producing hypersensitivity. When plasticity facilitates protective reflexes, it can be beneficial, but when the changes persist, a chronic pain condition may result. Genetic, electrophysiological, and pharmacological studies are elucidating the molecular mechanisms that underlie detection, coding, and modulation of noxious stimuli that generate pain.",2009,184,2843,187,18,80,147,186,224,281,233,255,295,252
176cfe9afceb31dd6e768413e9ead95b5d34489b,"P2X receptors are membrane ion channels that open in response to the binding of extracellular ATP. Seven genes in vertebrates encode P2X receptor subunits, which are 40-50% identical in amino acid sequence. Each subunit has two transmembrane domains, separated by an extracellular domain (approximately 280 amino acids). Channels form as multimers of several subunits. Homomeric P2X1, P2X2, P2X3, P2X4, P2X5, and P2X7 channels and heteromeric P2X2/3 and P2X1/5 channels have been most fully characterized following heterologous expression. Some agonists (e.g., alphabeta-methylene ATP) and antagonists [e.g., 2',3'-O-(2,4,6-trinitrophenyl)-ATP] are strongly selective for receptors containing P2X1 and P2X3 subunits. All P2X receptors are permeable to small monovalent cations; some have significant calcium or anion permeability. In many cells, activation of homomeric P2X7 receptors induces a permeability increase to larger organic cations including some fluorescent dyes and also signals to the cytoskeleton; these changes probably involve additional interacting proteins. P2X receptors are abundantly distributed, and functional responses are seen in neurons, glia, epithelia, endothelia, bone, muscle, and hemopoietic tissues. The molecular composition of native receptors is becoming understood, and some cells express more than one type of P2X receptor. On smooth muscles, P2X receptors respond to ATP released from sympathetic motor nerves (e.g., in ejaculation). On sensory nerves, they are involved in the initiation of afferent signals in several viscera (e.g., bladder, intestine) and play a key role in sensing tissue-damaging and inflammatory stimuli. Paracrine roles for ATP signaling through P2X receptors are likely in neurohypophysis, ducted glands, airway epithelia, kidney, bone, and hemopoietic tissues. In the last case, P2X7 receptor activation stimulates cytokine release by engaging intracellular signaling pathways.",2002,735,2697,360,0,76,141,175,164,189,185,184,160,168
4e98dde420b63f896fc7e1ca79a5c5e3b15351b8,"We introduce the Bayesian skyline plot, a new method for estimating past population dynamics through time from a sample of molecular sequences without dependence on a prespecified parametric model of demographic history. We describe a Markov chain Monte Carlo sampling procedure that efficiently samples a variant of the generalized skyline plot, given sequence data, and combines these plots to generate a posterior distribution of effective population size through time. We apply the Bayesian skyline plot to simulated data sets and show that it correctly reconstructs demographic history under canonical scenarios. Finally, we compare the Bayesian skyline plot model to previous coalescent approaches by analyzing two real data sets (hepatitis C virus in Egypt and mitochondrial DNA of Beringian bison) that have been previously investigated using alternative coalescent methods. In the bison analysis, we detect a severe but previously unrecognized bottleneck, estimated to have occurred 10,000 radiocarbon years ago, which coincides with both the earliest undisputed record of large numbers of humans in Alaska and the megafaunal extinctions in North America at the beginning of the Holocene.",2005,43,2598,396,1,20,40,87,107,149,170,211,199,216
acb5571a8ab6ae2be5fbabd04451162006fcab49,"Denitrification is a distinct means of energy conservation, making use of N oxides as terminal electron acceptors for cellular bioenergetics under anaerobic, microaerophilic, and occasionally aerobic conditions. The process is an essential branch of the global N cycle, reversing dinitrogen fixation, and is associated with chemolithotrophic, phototrophic, diazotrophic, or organotrophic metabolism but generally not with obligately anaerobic life. Discovered more than a century ago and believed to be exclusively a bacterial trait, denitrification has now been found in halophilic and hyperthermophilic archaea and in the mitochondria of fungi, raising evolutionarily intriguing vistas. Important advances in the biochemical characterization of denitrification and the underlying genetics have been achieved with Pseudomonas stutzeri, Pseudomonas aeruginosa, Paracoccus denitrificans, Ralstonia eutropha, and Rhodobacter sphaeroides. Pseudomonads represent one of the largest assemblies of the denitrifying bacteria within a single genus, favoring their use as model organisms. Around 50 genes are required within a single bacterium to encode the core structures of the denitrification apparatus. Much of the denitrification process of gram-negative bacteria has been found confined to the periplasm, whereas the topology and enzymology of the gram-positive bacteria are less well established. The activation and enzymatic transformation of N oxides is based on the redox chemistry of Fe, Cu, and Mo. Biochemical breakthroughs have included the X-ray structures of the two types of respiratory nitrite reductases and the isolation of the novel enzymes nitric oxide reductase and nitrous oxide reductase, as well as their structural characterization by indirect spectroscopic means. This revealed unexpected relationships among denitrification enzymes and respiratory oxygen reductases. Denitrification is intimately related to fundamental cellular processes that include primary and secondary transport, protein translocation, cytochrome c biogenesis, anaerobic gene regulation, metalloprotein assembly, and the biosynthesis of the cofactors molybdopterin and heme D1. An important class of regulators for the anaerobic expression of the denitrification apparatus are transcription factors of the greater FNR family. Nitrate and nitric oxide, in addition to being respiratory substrates, have been identified as signaling molecules for the induction of distinct N oxide-metabolizing enzymes.",1997,1002,3066,275,0,21,39,36,57,78,78,75,97,117
0cf16c3770d1ce1b9b1b4d875741ba51252882f7,"Abstract Ti–Ni-based alloys are quite attractive functional materials not only as practical shape memory alloys with high strength and ductility but also as those exhibiting unique physical properties such as pre-transformation behaviors, which are enriched by various martensitic transformations. The paper starts from phase diagram, structures of martensites, mechanisms of martensitic transformations, premartensitic behavior, mechanism of shape memory and superelastic effects etc., and covers most of the fundamental issues related with the alloys, which include not only martensitic transformations but also diffusional transformations, since the latter greatly affect the former, and are useful to improve shape memory characteristics. Thus the alloy system will serve as an excellent case study of physical metallurgy, as is the case for steels where all kinds of phase transformations are utilized to improve the physical properties. In short this review is intended to give a self-consistent and logical account of key issues on Ti–Ni based alloys from physical metallurgy viewpoint on an up-to-date basis.",2005,345,2951,99,6,37,60,102,100,110,141,134,195,206
c7ca77863c289ecb4bcfdd7e329d90f792d707be,"Physical Metallurgy Principles is intended for use in an introductory course in physical metallurgy and is designed for all engineering students at the junior or senior level. The approach is largely theoretical, but covers all aspects of physical metallurgy and behavior of metals and alloys. The treatment used in this textbook is in harmony with a more fundamental approach to engineering education.",1972,0,2195,50,5,3,3,1,7,9,7,7,8,13
eb4ec95f5ea8aa0b7e54b3b0975494999b113595,"Preface. 1. Introduction. 1.1 Ni-base Alloy Classification. 1.2 History of Nickel and Ni-base Alloys. 1.3 Corrosion Resistance. 1.4 Nickel Alloy Production. 2. Alloying Additions, Phase Diagrams, and Phase Stability. 2.1 Introduction. 2.2 General Influence of Alloying Additions. 2.3 Phase Diagrams for Solid-Solution Alloys. 2.4 Phase Diagrams for Precipitation Hardened Alloys--gamma' Formers. 2.5 Phase Diagrams for Precipitation-Hardened Alloys--gamma"" Formers. 2.6 Calculated Phase Stability Diagrams. 2.7 PHACOMP Phase Stability Calculations. 3. Solid-Solution Strengthened Ni-base Alloys. 3.1 Standard Alloys and Consumables. 3.2 Physical Metallurgy and Mechanical Properties. 3.3 Welding Metallurgy. 3.4 Mechanical Properties of Weldments. 3.5 Weldability. 3.6 Corrosion Resistance. 3.7 Case Studies. 4. Precipitation Strengthened Ni-base Alloys. 4.1 Standard Alloys and Consumables. 4.2 Physical Metallurgy and Mechanical Properties. 4.3 Welding Metallurgy. 4.4 Mechanical Properties of Weldments. 4.5 Weldability. 5. Oxide Dispersion Strengthened Alloys and Nickel Aluminides. 5.1 Oxide Dispersion Strengthened Alloys. 5.2 Nickel Aluminide Alloys. 6. Repair Welding of Ni-base Alloys. 6.1 Solid-Solution Strengthened Alloys. 6.2 Precipitation Strengthened Alloys. 6.3 Single Crystal Superalloys. 7. Dissimilar Welding. 7.1 Application of Dissimilar Welds. 7.2 Influence of Process Parameters on Fusion Zone Composition. 7.3 Carbon, Low Alloys and Stainless Steels. 7.4 Postweld Heat Treatment Cracking in Stainless Steels Welded with Ni-base Filler Metals. 7.5 Super Austenitic Stainless Steels. 7.6 Dissimilar Welds in Ni-base Alloys - Effect on Corrosion Resistance. 7.7 9%Ni Steels. 7.8 Super Duplex Stainless Steels. 7.9 Case Studies. 8. Weldability Testing. 8.1 Introduction. 8.2 The Varestraint Test. 8.3 Modified Cast Pin Tear Test. 8.4 The Sigmajig Test. 8.5 The Hot Ductility Test. 8.6 The Strain-to-Fracture Test. 8.7 Other Weldability Tests. Appendix A Composition of Wrought and Cast Nickel-Base Alloys. Appendix B Composition of Nickel and Nickel Alloy Consumables. Appendix C Corrosion Acceptance Testing Methods. Appendix D Etching Techniques for Ni-base Alloys and Welds. Author Index. Subject Index.",2009,66,699,60,0,3,18,18,47,54,53,71,68,85
2524a882d157961d70b6106459aed3bb442b74c1,"Comprehensive information for the American aluminium industry Collective effort of 53 recognized experts on aluminium and aluminium alloys Joint venture by world renowned authorities-the Aluminium Association Inc. and American Society for Metals. The completely updated source of information on aluminium industry as a whole rather than its individual contributors. this book is an opportunity to gain from The knowledge of the experts working for prestigious companies such as Alcoa, Reynolds Metals Co., Alcan International Ltd., Kaiser Aluminium & Chemical Corp., Martin Marietta Laboratories and Anaconda Aluminium Co. It took four years of diligent work to complete this comprehensive successor to the classic volume, Aluminium, published by ASM in 1967. Contents: Properties of Pure Aluminum Constitution of Alloys Microstructure of Alloys Work Hardening Recovery, Recrystalization and Growth Metallurgy of Heat Treatment and General Principles of Precipitation Hardening Effects of Alloying Elements and Impurities on Properties Corrosion Behaviour Properties of Commercial Casting Alloys Properties of Commercial Wrought Alloys Aluminum Powder and Powder Metallurgy Products.",1984,0,1561,73,0,0,2,7,6,5,3,10,13,14
34058cc7431331f5af8cd63ac3c9df14765c7eac,Preface. 1. Introduction. 2. Phase Diagrams. 3. Alloying Elements and Constitution Diagrams. 4. Martensitic Stainless Steels. 5. Ferritic Stainless Steels. 6. Austenitic Stainless Steels. 7. Duplex Stainless Steels. 8. Precipitation-Hardening Stainless Steels. 9. Dissimilar Welding of Stainless Steels. 10. Weldability Testing. Appendix 1: Nominal Compositions of Stainless Steels. Appendix 2: Etching Techniques for Stainless Steel Welds. Author Index. Subject Index.,2005,0,1025,77,1,3,13,28,27,34,60,56,74,77
d8133fb8e17ed2a5809e03cc43c7914ea49f2b0d,"This practical reference provides thorough and systematic coverage on both basic metallurgy and the practical engineering aspects of metallic material selection and application. Contents includes: Practical information on the engineering properties and applications of steels, cast irons, nonferrous alloys, and metal matrix composites. Concise overviews and practical implications of metallic structure, imperfections, deformation, and phase transformations Process metallurgy of solidification and casting, recovery, recrystallization and grain growth, precipitation hardening Mechanical deformation during processing and in-service properties of fatigue, fracture, and creep. Physical properties and corrosion.",2008,0,567,50,0,1,4,12,16,42,52,57,74,68
eafe94460769dc1dca747eb9d9c9490a6d8ec12f,"Abstract The generation of zinc and zinc alloy coatings on steel is one of the commercially most important processing techniques used to protect steel components exposed to corrosive environments. From a technological standpoint, the principles of galvanizing have remained unchanged since this coating came into use over 200 years ago. However, because of new applications in the automotive and construction industry, a considerable amount of research has recently occurred on all aspects of the galvanizing process and on new types of Zn coatings. This review will discuss the metallurgy of zinc-coated steel from a scientific standpoint to develop relationships to practical applications. Hot-dip zinc coating methods, i.e. batch and continuous processes, will first be reviewed along with Fe–Zn phase equilibria and kinetics. Commercially, the addition of aluminum to the zinc bath results in three important types of coatings, galvanized, galfan and galvalume, and produces complex reactions at the coating/substrate interface. Fe–Zn–Al equilibrium will be reviewed in the light of recent studies of solubility and inhibition layer formation and breakdown. The effect of steel substrate composition on these reactions will also be critically analyzed. The overlay coating formation, or the coating alloy, is specifically chosen for its desired properties. The morphology of the galvanize, galfan and galvalume coating overlays will be reviewed, as well as the effect of heat treatment to produce a galvanneal coating. Finally, the effect of the microstructures of these coatings on the important properties of corrosion, formability, weldability and paintability will be discussed.",2000,64,1046,39,0,2,2,6,13,25,32,48,46,40
5c1af053ef5d859221eab1d0e56418b04d8f9249,"Skripta Fizikalna metalurgija I je sažeti prikaz znanosti o materijalima u kojoj se na znanstvenim i inženjerskim principima tumaci kristalna građa metala, dizajniranje legura i mikrostrukture, te odnos između strukture metala i njegovih mehanickih i fizickih svojstava.",2009,8,395,29,21,25,30,32,25,28,40,25,24,35
abbe5dee41d472e0468ae30e7bc6dc7d53284b2e,"This volume provides a substantial background to microalloyed steels with a wide selection of applications, some of which are very recent. A well-illustrated practical guide, this book acts as a useful source of data and a concise account of the theoretical aspects of the subject. Both academic institutions and the world-wide steel industry will find it indispensable.",1997,0,958,75,0,6,4,10,18,15,16,22,39,32
3e29d4c182490ea94be52b5593f7d218d4782358,"1. Stress and strain 2. Plasticity 3. Strain hardening 4. Plastic instability 5. Temperature and strain-rate dependence 6. Work balance 7. Slab analysis and friction 8. Friction and lubrication 9. Upper-bound analysis 10. Slip-line field analysis 11. Deformation zone geometry 12. Formability 13. Bending 14. Plastic anisotropy 15. Cupping, redrawing and ironing 16. Forming limit diagrams 17. Stamping 18. Hydroforming 19. Other sheet forming operations 20. Formability tests 21. Sheet metal properties.",1993,13,1086,42,9,12,9,14,15,15,9,16,20,17
6b09d20de5ccfcf15453d0f953147fd1e9079587,,1984,0,1147,69,0,0,3,0,2,5,9,8,7,9
c40cea80b4fa2e5f36a96c11f85d4b90d0a6816a,"AbstractA comprehensive review is presented of the extractive metallurgy of rare earths. The topics covered are: world rare earth resources and production; ore processing and separation of individual rare earths; reduction, refining, and ultrapurification of rare earth elements; methods for rare earth materials analysis; and a selection of the numerous rare earth applications. World rare earth reserves are abundant and would last for well beyond the next century. However, all of the 16 naturally occurring rare earth elements are not equally distributed in the ore minerals. This, compounded with the problems specific to the isolation and recovery of each of the rare earths, sets the stage for an unequal rare earth availability. The close chemical similarity of rare earths looses its importance when divergent physical properties determine the processes for rare earth element reduction and refining. The rare earth metals, alloys, and compounds have been as pure as could be determined. Finally, the commercial...",2004,562,805,42,1,2,1,3,10,5,9,15,22,39
39258b60531ff5aee0fca118b212b355c3e9821a,"BackgroundSequence similarity searching is a very important bioinformatics task. While Basic Local Alignment Search Tool (BLAST) outperforms exact methods through its use of heuristics, the speed of the current BLAST software is suboptimal for very long queries or database sequences. There are also some shortcomings in the user-interface of the current command-line applications.ResultsWe describe features and improvements of rewritten BLAST software and introduce new command-line applications. Long query sequences are broken into chunks for processing, in some cases leading to dramatically shorter run times. For long database sequences, it is possible to retrieve only the relevant parts of the sequence, reducing CPU time and memory usage for searches of short queries against databases of contigs or chromosomes. The program can now retrieve masking information for database sequences from the BLAST databases. A new modular software library can now access subject sequence data from arbitrary data sources. We introduce several new features, including strategy files that allow a user to save and reuse their favorite set of options. The strategy files can be uploaded to and downloaded from the NCBI BLAST web site.ConclusionThe new BLAST command-line applications, compared to the current BLAST tools, demonstrate substantial speed improvements for long queries as well as chromosome length database sequences. We have also improved the user interface of the command-line applications.",2009,22,9665,1188,1,31,98,148,255,407,588,773,1099,1252
a3f920549543fe09c716a32fbe014d0241a03e5e,"Networking together hundreds or thousands of cheap microsensor nodes allows users to accurately monitor a remote environment by intelligently combining the data from the individual nodes. These networks require robust wireless communication protocols that are energy efficient and provide low latency. We develop and analyze low-energy adaptive clustering hierarchy (LEACH), a protocol architecture for microsensor networks that combines the ideas of energy-efficient cluster-based routing and media access together with application-specific data aggregation to achieve good performance in terms of system lifetime, latency, and application-perceived quality. LEACH includes a new, distributed cluster formation technique that enables self-organization of large numbers of nodes, algorithms for adapting clusters and rotating cluster head positions to evenly distribute the energy load among all the nodes, and techniques to enable distributed signal processing to save communication resources. Our results show that LEACH can improve system lifetime by an order of magnitude compared with general-purpose multihop approaches.",2002,104,10293,1410,0,0,1,109,323,430,510,599,635,621
890469e625fe728adfa690a3945ebca4c11a8998,"This best-selling title, considered for over a decade to be essential reading for every serious student and practitioner of computer design, has been updated throughout to address the most important trends facing computer designers today. In this edition, the authors bring their trademark method of quantitative analysis not only to high-performance desktop machine design, but also to the design of embedded and server systems. They have illustrated their principles with designs from all three of these domains, including examples from consumer electronics, multimedia and Web technologies, and high-performance computing.",1969,0,11623,682,0,0,0,0,0,0,0,0,0,0
6b4fe4aa4d66fecc7b2869569002714d91d0b3f7,"What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to",1962,45,12285,489,0,0,2,2,2,2,1,1,6,6
57458bc1cffe5caa45a885af986d70f723f406b4,"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",2008,27,4783,238,6,24,54,60,76,114,230,412,505,614
66cedab15c294a206785ffc8e797dca66813c906,"Neural connections, providing the substrate for functional networks, exist whether or not they are functionally active at any given moment. However, it is not known to what extent brain regions are continuously interacting when the brain is “at rest.” In this work, we identify the major explicit activation networks by carrying out an image-based activation network analysis of thousands of separate activation maps derived from the BrainMap database of functional imaging studies, involving nearly 30,000 human subjects. Independently, we extract the major covarying networks in the resting brain, as imaged with functional magnetic resonance imaging in 36 subjects at rest. The sets of major brain networks, and their decompositions into subnetworks, show close correspondence between the independent analyses of resting and activation brain dynamics. We conclude that the full repertoire of functional networks utilized by the brain in action is continuously and dynamically “active” even when at “rest.”",2009,42,4199,467,9,75,149,220,277,341,428,439,487,456
c8e460e601a23cd6d1723b4be00c085b22a928bc,"This paper addresses digital communication in a Rayleigh fading environment when the channel characteristic is unknown at the transmitter but is known (tracked) at the receiver. Inventing a codec architecture that can realize a significant portion of the great capacity promised by information theory is essential to a standout long-term position in highly competitive arenas like fixed and indoor wireless. Use (nT, nR) to express the number of antenna elements at the transmitter and receiver. An (n, n) analysis shows that despite the n received waves interfering randomly, capacity grows linearly with n and is enormous. With n = 8 at 1% outage and 21-dB average SNR at each receiving element, 42 b/s/Hz is achieved. The capacity is more than 40 times that of a (1, 1) system at the same total radiated transmitter power and bandwidth. Moreover, in some applications, n could be much larger than 8. In striving for significant fractions of such huge capacities, the question arises: Can one construct an (n, n) system whose capacity scales linearly with n, using as building blocks n separately coded one-dimensional (1-D) subsystems of equal capacity? With the aim of leveraging the already highly developed 1-D codec technology, this paper reports just such an invention. In this new architecture, signals are layered in space and time as suggested by a tight capacity bound.",1996,18,6831,447,1,3,19,47,134,175,316,386,487,517
bbb9a237eb0cb75812d05c2d6428253bb1627a56,"From the Book: 
 
Our goals for the first edition were threefold. First, we wanted to show through authentic case studies actual examples of software architectures solving real-world problems. Second, we wanted to establish and show the strong connection between an architecture and an organization's business goals. And third, we wanted to explain the importance of software architecture in achieving the quality goals for a system. 
 
Our goals for this second edition are the same, but the passage of time since the writing of the first edition has brought new developments in the field and new understanding of the important underpinnings of software architecture. We reflect the new developments with new case studies and the new understanding both through new chapters and through additions to and elaboration of the existing chapters. 
 
Architecture analysis, design, reconstruction, and documentation have all had major developments since the first edition. Architecture analysis has developed into a mature field with industrial-strength methods. This is reflected by a new chapter about the architecture tradeoff analysis method (ATAM). The ATAM has been adopted by industrial organizations as a technique for evaluating their software architectures. 
 
Architecture design has also had major developments since the first edition. The capturing of quality requirements, the achievement of those requirements through small-scale and large-scale architectural approaches (tactics and patterns, respectively), and a design method that reflects knowledge of how to achieve qualities are all captured in various chapters. Three new chapters treat understanding quality requirements, achieving qualities, and theattribute driven design (ADD) method, respectively. 
 
Architecture reconstruction or reverse engineering is an essential activity for capturing undocumented architectures. It can be used as a portion of a design project, an analysis project, or to provide input into a decision process to determine what to use as a basis for reconstructing an existing system. In the first edition, we briefly mentioned a tool set (Dali) and its uses in the re-engineering context; in in this edition the topic merits its own chapter. 
 
Documenting software architectures is another topic that has matured considerably in the recent past. When the first edition was published, the Unified Modeling Language (UML) was just arriving on the scene. Now it is firmly entrenched, a reality reflected by all-new diagrams. But more important, an understanding of what kind of information to capture about an architecture, beyond what notation to use, has emerged. A new chapter covers architecture documentation. 
 
The understanding of the application of software architecture to enable organizations to efficiently produce a variety of systems based on a single architecture is summarized in a totally rewritten chapter on software product lines. The chapter reinforces the link between architecture and an organization's business goals, as product lines, based around a software architecture, can enable order-of-magnitude improvements in cost, quality, and time to market. 
 
In addition to the architectural developments, the technology for constructing distributed and Web-based systems has become prominent in today's economy. We reflect this trend by updating the World Wide Web chapter, by using Web-based examples for the ATAM chapter and the chapter on building systems from components, by replacing the CORBA case study with one on Enterprise JavaBeans (EJB), and by introducing a case study on a wireless EJB system designed to support wearable computers for maintenance technicians. 
 
Finally, we have added a chapter that looks more closely at the financial aspects of architectures. There we introduce a method--the CBAM--for basing architectural decisions on economic criteria, in addition to the technical criteria that we had focused on previously. 
 
As in the first edition, we use the architecture business cycle as a unifying motif and all of the case studies are described in terms of the quality goals that motivated the system design and how the architecture for the system achieves those quality goals. 
 
In this edition, as in the first, we were very aware that our primary audience is practitioners, so we focus on presenting material that has been found useful in many industrial applications, as well as what we expect practice to be in the near future. 
 
We hope that you enjoy reading it at least as much as we enjoyed writing it. 
 
 
0321154959P12162002",1999,0,5395,426,119,147,157,224,211,245,291,296,313,314
64a717dc148b76070bbb6a3190a7e05bb9734400,"Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.",1994,31,5708,393,4,14,22,41,42,70,79,110,110,164
2a16fe9680001fcade8b4ccf04ab458028dee80e,"Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.",2000,47,3894,366,12,30,117,204,278,379,370,368,320,312
efa6ed36bfeaccb63b5e408c976bea0961873f90,"Cognitive load theory has been designed to provide guidelines intended to assist in the presentation of information in a manner that encourages learner activities that optimize intellectual performance. The theory assumes a limited capacity working memory that includes partially independent subcomponents to deal with auditory/verbal material and visual/2- or 3-dimensional information as well as an effectively unlimited long-term memory, holding schemas that vary in their degree of automation. These structures and functions of human cognitive architecture have been used to design a variety of novel instructional procedures based on the assumption that working memory load should be reduced and schema construction encouraged. This paper reviews the theory and the instructional designs generated by it.",1998,103,4427,318,2,6,16,27,48,44,65,101,116,131
13c3630c03bec0c3e443ad5a3dc6d1951db74c20,"Status of this Memo This memo provides information for the Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by aggregating traffic classification state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need only be implemented at network boundaries or hosts. Network resources are allocated to traffic streams by service provisioning policies which govern how traffic is marked and conditioned upon entry to a differentiated services-capable network, and how that traffic is forwarded within that network. A wide variety of services can be implemented on top of these building blocks.",1998,14,4114,339,7,121,220,322,407,404,381,357,324,254
03511041271257b85e6d9058e51f02cf5f4e3937,"A number of proposals have been advanced in recent years for the development of “general systems theory” which, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial.",1991,15,5468,291,70,32,19,30,42,63,43,37,53,61
8a0011039d5af38a6d0956b9125c7863849af717,"The highly successful architecture and protocols of today's Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.",2003,38,3654,297,10,34,76,123,176,195,246,278,336,328
63a52c52bf67764eab3c93bf7aeee171305b67a3,"This document specifies the architecture for Multiprotocol Label
Switching (MPLS). [STANDARDS-TRACK]",2001,10,3355,288,199,266,288,272,238,274,222,185,197,161
9438aa83eb8218b7e6e3891ad7bc2b388e35bc33,"In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic “virtual organizations” formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, “The Anatomy of the Grid,” by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration—within and across organizational domains. This is a DRAFT document and continues to be revised. The latest version can be found at http://www.globus.org/research/papers/ogsa.pdf. Please send comments to foster@mcs.anl.gov, carl@isi.edu, jnick@us.ibm.com, tuecke@mcs.anl.gov Physiology of the Grid 2",2002,84,3620,238,141,427,550,551,436,326,231,230,166,131
704ee1ed2c95bedd7808a92e879bd30cba818739,"Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance.
 In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP.",2008,43,3164,341,6,46,84,130,190,251,332,364,335,409
23686f4548d74f2bdedb67cd2dd53adad267e0ff,"This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by aggregating traffic classification state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need only be implemented at network boundaries or hosts. Network resources are allocated to traffic streams by service provisioning policies which govern how traffic is marked and conditioned upon entry to a differentiated services-capable network, and how that traffic is forwarded within that network. A wide variety of services can be implemented on top of these building blocks.",1998,0,5776,180,15,158,306,475,535,580,536,531,493,371
70f5ad30aabb1de547b34d5aa4167dd9bb8d0957,"This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support real- time as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation.",1994,27,3979,194,6,39,50,78,96,141,217,279,280,326
82dc0e2ea785f4870816764c25f3d9ae856d9809,"An Integrated Agent Architecture for Software Defined Radio. Rapid-prototype cognitive radio, CR1, was developed to apply these.The modern software defined radio has been called the heart of a cognitive radio. Cognitive radio: an integrated agent architecture for software defined radio. Http:bwrc.eecs.berkeley.eduResearchMCMACR White paper final1.pdf. The cognitive radio, built on a software-defined radio, assumes. Radio: An Integrated Agent Architecture for Software Defined Radio, Ph.D. The need for software-defined radios is underlined and the most important notions used for such. Mitola III, Cognitive radio: an integrated agent architecture for software defined radio, Ph.D. This results in the set-theoretic ontology of radio knowledge defined in the. Cognitive Radio An Integrated Agent Architecture for Software.This article first briefly reviews the basic concepts about cognitive radio CR. Cognitive Radio-An Integrated Agent Architecture for Software Defined Radio. Cognitive Radio RHMZ 2007. Software-defined radio SDR idea 1. Cognitive radio: An integrated agent architecture for software.Cognitive Radio SOFTWARE DEFINED RADIO, AND ADAPTIVE WIRELESS SYSTEMS2 Cognitive Networks. 3 Joseph Mitola III, Cognitive Radio: An Integrated Agent Architecture for Software Defined Radio Stockholm.",2000,94,3936,185,5,4,2,5,8,25,70,184,281,350
c5f5311fa1f34159ab3a0a1d58da51cd0340a640,"1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.",1968,38,6257,190,4,23,38,54,79,91,67,91,69,69
551e0d6d0afe7ba1e1f40a25267bc93897e17598,1. Introduction. 2. Architectural Styles. 3. Case Studies. 4. Shared Information Systems. 5. Architectural Design Guidance. 6. Formal Models and Specifications. 7. Linguistic Issues. 8. Tools for Architectural Design. 9. Education of Software Architects. Bibliography. Index.,1996,0,3738,204,32,83,158,251,232,221,262,225,256,250
908b54085603b3c6fa12a06220f8f5c5c921147b,"Accurate multiple alignments of 86 domains that occur in signaling proteins have been constructed and used to provide a Web-based tool (SMART: simple modular architecture research tool) that allows rapid identification and annotation of signaling domain sequences. The majority of signaling proteins are multidomain in character with a considerable variety of domain combinations known. Comparison with established databases showed that 25% of our domain set could not be deduced from SwissProt and 41% could not be annotated by Pfam. SMART is able to determine the modular architectures of single sequences or genomes; application to the entire yeast genome revealed that at least 6.7% of its genes contain one or more signaling domains, approximately 350 greater than previously annotated. The process of constructing SMART predicted (i) novel domain homologues in unexpected locations such as band 4.1-homologous domains in focal adhesion kinases; (ii) previously unknown domain families, including a citron-homology domain; (iii) putative functions of domain families after identification of additional family members, for example, a ubiquitin-binding role for ubiquitin-associated domains (UBA); (iv) cellular roles for proteins, such predicted DEATH domains in netrin receptors further implicating these molecules in axonal guidance; (v) signaling domains in known disease genes such as SPRY domains in both marenostrin/pyrin and Midline 1; (vi) domains in unexpected phylogenetic contexts such as diacylglycerol kinase homologues in yeast and bacteria; and (vii) likely protein misclassifications exemplified by a predicted pleckstrin homology domain in a Candida albicans protein, previously described as an integrin.",1998,95,3381,196,9,49,95,126,166,160,181,159,173,147
85c0c25c447de1a91dd94f6bed6fdfb9d5b024fd,"Concepts of basal ganglia organization have changed markedly over the past decade, due to significant advances in our understanding of the anatomy, physiology and pharmacology of these structures. Independent evidence from each of these fields has reinforced a growing perception that the functional architecture of the basal ganglia is essentially parallel in nature, regardless of the perspective from which these structures are viewed. This represents a significant departure from earlier concepts of basal ganglia organization, which generally emphasized the serial aspects of their connectivity. Current evidence suggests that the basal ganglia are organized into several structurally and functionally distinct 'circuits' that link cortex, basal ganglia and thalamus, with each circuit focused on a different portion of the frontal lobe. In this review, Garrett Alexander and Michael Crutcher, using the basal ganglia 'motor' circuit as the principal example, discuss recent evidence indicating that a parallel functional architecture may also be characteristic of the organization within each individual circuit.",1990,93,3879,144,1,20,98,64,77,69,147,121,102,130
56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7,"Abstract This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation.",1988,65,3484,154,53,84,127,68,111,125,165,84,89,93
2bb29b7d2b80c9be4ef3ee3bb82cc52b624b80da,"Photosynthesis uses light energy to drive the oxidation of water at an oxygen-evolving catalytic site within photosystem II (PSII). We report the structure of PSII of the cyanobacterium Thermosynechococcus elongatus at 3.5 angstrom resolution. We have assigned most of the amino acid residues of this 650-kilodalton dimeric multisubunit complex and refined the structure to reveal its molecular architecture. Consequently, we are able to describe details of the binding sites for cofactors and propose a structure of the oxygen-evolving center (OEC). The data strongly suggest that the OEC contains a cubane-like Mn3CaO4 cluster linked to a fourth Mn by a mono-μ-oxo bridge. The details of the surrounding coordination sphere of the metal cluster and the implications for a possible oxygen-evolving mechanism are discussed.",2004,62,2841,198,79,199,153,186,230,139,166,216,231,194
b7f625b7087600d1a67decd1b23c22dbe812229f,"A neural network architecture for the learning of recognition categories is derived. Real-time network dynamics are completely characterized through mathematical analysis and computer simulations. The architecture self-organizes and self-stabilizes its recognition codes in response to arbitrary orderings of arbitrarily many and arbitrarily complex binary input patterns. Top-down attentional and matching mechanisms are critical in self-stabilizing the code learning process. The architecture embodies a parallel search scheme which updates itself adaptively as the learning process unfolds. After learning self-stabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time does not grow as a function of code complexity. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. These invariant properties emerge in the form of learned critical feature patterns, or prototypes. The architecture possesses a context-sensitive self-scaling property which enables its emergent critical feature patterns to form. They detect and remember statistically predictive configurations of featural elements which are derived from the set of all input patterns that are ever experienced. Four types of attentional process—priming, gain control, vigilance, and intermodal competition—are mechanistically characterized. Top—down priming and gain control are needed for code matching and self-stabilization. Attentional vigilance determines how fine the learned categories will be. If vigilance increases due to an environmental disconfirmation, then the system automatically searches for and learns finer recognition categories. A new nonlinear matching law (the ⅔ Rule) and new nonlinear associative laws (the Weber Law Rule, the Associative Decay Rule, and the Template Learning Rule) are needed to achieve these properties. All the rules describe emergent properties of parallel network interactions. The architecture circumvents the noise, saturation, capacity, orthogonality, and linear predictability constraints that limit the codes which can be stably learned by alternative recognition models.",1988,67,2843,184,39,47,98,83,129,108,97,121,90,100
d3bd2d5f2dd7453bf668b25f35153ac8faccad3c,"Now available in paper, The Architecture of Cognition is a classic work that remains relevant to theory and research in cognitive science. The new version of Anderson's theory of cognitive architecture -- Adaptive Control of Thought (ACT*) -- is a theory of the basic principles of operation built into the cognitive system and is the main focus of the book. (http://books.google.fr/books?id=Uip3_g7zlAUC&printsec=frontcover&hl=fr#v=onepage&q&f=false)",1983,0,4310,43,18,37,60,119,127,168,129,157,185,176
af6a2cee8ecde3745b4237ff9a03dd7da39d905d,"From the Book: 
 
For many years, the three of us have been developing software using object oriented techniques. We started with object oriented programming languages, like C++, Smalltalk, and Eiffel. Soon we felt the need to describe our software at a higher level of abstraction. Even before the first object oriented analysis and design methods, like Coad/Yourdon and OMT, were published, we used our own invented bubbles and arrows diagrams. This naturally led to questions like ""What does this arrow mean?"" and ""What is the difference between this circle and that rectangle?"". We therefore rapidly decided to use the newly emerging methods to design and describe our software. During the years we found that we were spending more time on designing our models, than on writing code. The models helped us to cope with larger and more complex systems. Having a good model of the software available, made the process of writing code easier and in many cases even straightforward. 
 
In 1997 some of us got involved in defining the first standard for object oriented modeling called UML. This was a major milestone that stimulated the use of modeling in the software industry. When the OMG launched its initiative on Model Driven Architecture we felt that this was logically the next step to take. People try to get more and more value from their high level models, and the MDA approach supports these efforts. 
 
At that moment we realized that all these years we had naturally walked the path towards model driven development. Every bit of wisdom we acquired during our struggle with the systems we had to build, fitted in with this new idea of how to build software. It caused a feeling similar to an AHA-erlebnis: ""Yes, this is it,"" the same feeling we had years before when we first encountered the object-oriented way of thinking, and again when we first read the GOF book on design patterns. We feel that MDA could very well be the next major step forward in the way software is being developed. MDA brings the focus of software development to a higher level of abstraction, thereby raising the level of maturity of the IT industry. 
 
We are aware of the fact that the grand vision of MDA, which Richard Soley, the president of the OMG, presents so eloquently, is not yet a reality. However some parts of MDA can already be used today, while others are under development. With this book we want to give you insight in what MDA means and what you can achieve, both today and in the future. 
 
Anneke Kleppe, Jos Warmer, and Wim Bast 
Soest, the Netherlands 
January 2003",2003,0,2196,233,17,95,118,134,164,151,160,175,157,179
c461fc31ffd6928806109b0642ee15c00b8da950,"A variety of cell adhesion mechanisms underlie the way that cells are organized in tissues. Stable cell interactions are needed to maintain the structural integrity of tissues, and dynamic changes in cell adhesion participate in the morphogenesis of developing tissues. Stable interactions actually require active adhesion mechanisms that are very similar to those involved in tissue dynamics. Adhesion mechanisms are highly regulated during tissue morphogenesis and are intimately related to the processes of cell motility and cell migration. In particular, the cadherins and the integrins have been implicated in the control of cell movement. Cadherin mediated cell compaction and cellular rearrangements may be analogous to integrin-mediated cell spreading and motility on the ECM. Regulation of cell adhesion can occur at several levels, including affinity modulation, clustering, and coordinated interactions with the actin cytoskeleton. Structural studies have begun to provide a picture of how the binding properties of adhesion receptors themselves might be regulated. However, regulation of tissue morphogenesis requires complex interactions between the adhesion receptors, the cytoskeleton, and networks of signaling pathways. Signals generated locally by the adhesion receptors themselves are involved in the regulation of cell adhesion. These regulatory pathways are also influenced by extrinsic signals arising from the classic growth factor receptors. Furthermore, signals generated locally be adhesion junctions can interact with classic signal transduction pathways to help control cell growth and differentiation. This coupling between physical adhesion and developmental signaling provides a mechanism to tightly integrate physical aspects of tissue morphogenesis with cell growth and differentiation, a coordination that is essential to achieve the intricate patterns of cells in tissues.",1996,153,3332,91,27,109,186,216,183,208,154,180,164,151
c079d33824a53c0901d29229daedb98fe78be977,"Networked structures arise in a wide array of different contexts such as technological and transportation infrastructures, social phenomena, and biological systems. These highly interconnected systems have recently been the focus of a great deal of attention that has uncovered and characterized their topological complexity. Along with a complex topological structure, real networks display a large heterogeneity in the capacity and intensity of the connections. These features, however, have mainly not been considered in past studies where links are usually represented as binary states, i.e., either present or absent. Here, we study the scientific collaboration network and the world-wide air-transportation network, which are representative examples of social and large infrastructure systems, respectively. In both cases it is possible to assign to each edge of the graph a weight proportional to the intensity or capacity of the connections among the various elements of the network. We define appropriate metrics combining weighted and topological observables that enable us to characterize the complex statistical properties and heterogeneity of the actual strength of edges and vertices. This information allows us to investigate the correlations among weighted quantities and the underlying topological structure of the network. These results provide a better description of the hierarchies and organizational principles at the basis of the architecture of weighted networks.",2003,44,3082,152,0,32,49,73,129,106,125,111,176,164
cd0a0648572ccf2d350a8cb9a0bf9aacaa12942c,"Abstract The ultimate goal of work in cognitive architecture is to provide the foundation for a system capable of general intelligent behavior. That is, the goal is to provide the underlying structure that would enable a system to perform the full range of cognitive tasks, employ the full range of problem solving methods and representations appropriate for the tasks, and learn about all aspects of the tasks and its performance on them. In this article we present SOAR, an implemented proposal for such an architecture. We describe its organizational principles, the system as currently implemented, and demonstrations of its capabilities.",1987,84,2889,149,11,37,60,66,123,101,90,79,82,66
995a3b11cc8a4751d8e167abc4aa937abc934df0,"Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.",1989,27,2918,150,3,10,32,83,132,120,119,133,151,138
cf076f32e2d867cde74de61a1580ffa5d430e6e3,"With increasing size and complexity of the implementations of information systems, it is necessary to use some logical construct (or architecture) for defining and controlling the interfaces and the integration of all of the components of the system. This paper defines information systems architecture by creating a descriptive framework from disciplines quite independent of information systems, then by analogy specifies information systems architecture based upon the neutral, objective framework. Also, some preliminary conclusions about the implications of the resultant descriptive framework are drawn. The discussion is limited to architecture and does not include a strategic planning methodology.",1987,6,2703,174,0,2,6,10,10,14,10,29,26,26
a5eafb6c265e53da2a05606598a0d3480fca11af,"Intentions, an integral part of the mental state of an agent, play an important role in determining the behavior of rational agents as they seek to attain their goals. In this paper, a formalization of intentions based on a branching-time possible-worlds model is presented. It is shown how the formalism realizes many of the important elements of Bratman's theory of intention. In particular, the notion of intention developed here has equal status with the notions of belief and desire, and cannot be reduced to these concepts. This allows di erent types of rational agents to be modeled by imposing certain conditions on the persistence of an agent's beliefs, goals, and intentions. Finally, the formalism is compared with Bratman's theory of intention and Cohen and Levesque's formalization of intentions.",1997,20,2564,181,48,60,54,97,94,119,133,143,152,158
43b1be0743fac1d9a977450825f3448b09828534,"Technologies to measure whole-genome mRNA abundances and methods to organize and display such data are emerging as valuable tools for systems-level exploration of transcriptional regulatory networks. For instance, it has been shown that mRNA data from 118 genes, measured at several time points in the developing hindbrain of mice, can be hierarchically clustered into various patterns (or 'waves') whose members tend to participate in common processes. We have previously shown that hierarchical clustering can group together genes whose cis-regulatory elements are bound by the same proteins in vivo. Hierarchical clustering has also been used to organize genes into hierarchical dendograms on the basis of their expression across multiple growth conditions. The application of Fourier analysis to synchronized yeast mRNA expression data has identified cell-cycle periodic genes, many of which have expected cis-regulatory elements. Here we apply a systematic set of statistical algorithms, based on whole-genome mRNA data, partitional clustering and motif discovery, to identify transcriptional regulatory sub-networks in yeast—without any a priori knowledge of their structure or any assumptions about their dynamics. This approach uncovered new regulons (sets of co-regulated genes) and their putative cis-regulatory elements. We used statistical characterization of known regulons and motifs to derive criteria by which we infer the biological significance of newly discovered regulons and motifs. Our approach holds promise for the rapid elucidation of genetic network architecture in sequenced organisms in which little biology is known.",1999,27,2665,129,10,67,100,148,183,203,204,192,211,166
1f88427d7aa8225e47f946ac41a0667d7b69ac52,"In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (≫ 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",2009,55,2006,125,4,38,58,82,113,169,191,238,255,283
3c727e004aefcf2f7337e348cc4cfa2867486c2a,"A neural network architecture is introduced for incremental supervised learning of recognition categories and multidimensional maps in response to arbitrary sequences of analog or binary input vectors, which may represent fuzzy or crisp sets of features. The architecture, called fuzzy ARTMAP, achieves a synthesis of fuzzy logic and adaptive resonance theory (ART) neural networks by exploiting a close formal similarity between the computations of fuzzy subsethood and ART category choice, resonance, and learning. Four classes of simulation illustrated fuzzy ARTMAP performance in relation to benchmark backpropagation and generic algorithm systems. These simulations include finding points inside versus outside a circle, learning to tell two spirals apart, incremental approximation of a piecewise-continuous function, and a letter recognition database. The fuzzy ARTMAP system is also compared with Salzberg's NGE systems and with Simpson's FMMC system.",1992,17,2177,238,7,55,38,70,51,65,67,79,89,88
3cf945bfbc69107fd636de6fdfe751d3b06e4a58,"Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.",2000,198,2296,175,38,62,109,125,152,146,185,194,185,152
b3d1cbbddcef63b15fc1092fa89d09e4f29c118f,"Recognizing, that not all employees possess knowledge and skills that are of equal strategic importance, we draw on the resource-based view of the firm, human capital theory, and transaction cost economics to develop a human resource architecture of four different employment modes: internal development, acquisition, contracting, and alliance. We use this architecture to derive research questions for studying the relationships among employment modes, employment relationships, human resource configurations, and criteria for competitive advantage.",1999,84,2461,153,4,15,25,28,55,64,85,80,75,101
79e5947836d5e2845a3d390849c222f59106007c,"OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.",2000,80,2615,127,5,63,160,216,235,221,209,173,139,180
895860c6083736508d2541900cdf0960eb11592f,"The design, implementation, and capabilities of an extensible visualization system, UCSF Chimera, are discussed. Chimera is segmented into a core that provides basic services and visualization, and extensions that provide most higher level functionality. This architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features. Two unusual extensions are presented: Multiscale, which adds the ability to visualize large‐scale molecular assemblies such as viral coats, and Collaboratory, which allows researchers to share a Chimera session interactively despite being at separate locales. Other extensions include Multalign Viewer, for showing multiple sequence alignments and associated structures; ViewDock, for screening docked ligand orientations; Movie, for replaying molecular dynamics trajectories; and Volume Viewer, for display and analysis of volumetric data. A discussion of the usage of Chimera in real‐world situations is given, along with anticipated future directions. Chimera includes full user documentation, is free to academic and nonprofit users, and is available for Microsoft Windows, Linux, Apple Mac OS X, SGI IRIX, and HP Tru64 Unix from http://www.cgl.ucsf.edu/chimera/. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1605–1612, 2004",2004,70,28265,1711,0,0,0,0,1,0,0,0,0,0
d45eaee8b2e047306329e5dbfc954e6dd318ca1e,"This paper gives an overview of ROS, an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and briefly overview some of the available application software which uses ROS.",2009,10,7535,1468,13,90,194,363,487,623,680,800,903,947
6a17ebeeb80cd696bc83a288f1a77ddfc1467079,"Das Buch behandelt die Systemidentifizierung in dem theoretischen Bereich, der direkte Auswirkungen auf Verstaendnis und praktische Anwendung der verschiedenen Verfahren zur Identifizierung hat. Da ...",1987,0,20134,1860,0,0,0,0,0,0,0,1,0,0
915e96dd02d6ca3c989070791b02830fce6d6f63,"A new software suite, called Crystallography & NMR System (CNS), has been developed for macromolecular structure determination by X-ray crystallography or solution nuclear magnetic resonance (NMR) spectroscopy. In contrast to existing structure-determination programs, the architecture of CNS is highly flexible, allowing for extension to other structure-determination methods, such as electron microscopy and solid-state NMR spectroscopy. CNS has a hierarchical structure: a high-level hypertext markup language (HTML) user interface, task-oriented user input files, module files, a symbolic structure-determination language (CNS language), and low-level source code. Each layer is accessible to the user. The novice user may just use the HTML interface, while the more advanced user may use any of the other layers. The source code will be distributed, thus source-code modification is possible. The CNS language is sufficiently powerful and flexible that many new algorithms can be easily implemented in the CNS language without changes to the source code. The CNS language allows the user to perform operations on data structures, such as structure factors, electron-density maps, and atomic properties. The power of the CNS language has been demonstrated by the implementation of a comprehensive set of crystallographic procedures for phasing, density modification and refinement. User-friendly task-oriented input files are available for nearly all aspects of macromolecular structure determination by X-ray crystallography and solution NMR.",1998,61,15365,1217,0,0,1,0,1,0,17,743,1262,1225
7a6142cfa79cc01ceced5e144bd0e01a0f241a74,,2009,43,7888,2013,377,456,500,626,706,655,723,661,657,644
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17519,724,0,0,0,0,0,0,0,1,1,2
839c64b86d978baf5180381c975c0947eacdb7ba,Preface 1. Introduction Overview A Brief History of LMIs in Control Theory Notes on the Style of the Book Origin of the Book 2. Some Standard Problems Involving LMIs. Linear Matrix Inequalities Some Standard Problems Ellipsoid Algorithm Interior-Point Methods Strict and Nonstrict LMIs Miscellaneous Results on Matrix Inequalities Some LMI Problems with Analytic Solutions 3. Some Matrix Problems. Minimizing Condition Number by Scaling Minimizing Condition Number of a Positive-Definite Matrix Minimizing Norm by Scaling Rescaling a Matrix Positive-Definite Matrix Completion Problems Quadratic Approximation of a Polytopic Norm Ellipsoidal Approximation 4. Linear Differential Inclusions. Differential Inclusions Some Specific LDIs Nonlinear System Analysis via LDIs 5. Analysis of LDIs: State Properties. Quadratic Stability Invariant Ellipsoids 6. Analysis of LDIs: Input/Output Properties. Input-to-State Properties State-to-Output Properties Input-to-Output Properties 7. State-Feedback Synthesis for LDIs. Static State-Feedback Controllers State Properties Input-to-State Properties State-to-Output Properties Input-to-Output Properties Observer-Based Controllers for Nonlinear Systems 8. Lure and Multiplier Methods. Analysis of Lure Systems Integral Quadratic Constraints Multipliers for Systems with Unknown Parameters 9. Systems with Multiplicative Noise. Analysis of Systems with Multiplicative Noise State-Feedback Synthesis 10. Miscellaneous Problems. Optimization over an Affine Family of Linear Systems Analysis of Systems with LTI Perturbations Positive Orthant Stabilizability Linear Systems with Delays Interpolation Problems The Inverse Problem of Optimal Control System Realization Problems Multi-Criterion LQG Nonconvex Multi-Criterion Quadratic Problems Notation List of Acronyms Bibliography Index.,1998,173,11572,1282,0,0,0,0,1,3,33,407,473,566
0095b6bb7c92f5deeffa8a311b80f75e680325eb,"The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-line in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. >",1993,95,14339,1318,0,0,0,0,0,0,0,0,0,0
1c2040faf07594e60bd7dd339cf363a867416ef8,"Part I: Characteristics of Modern Power Systems. Introduction to the Power System Stability Problem. Part II: Synchronous Machine Theory and Modelling. Synchronous Machine Parameters. Synchronous Machine Representation in Stability Studies. AC Transmission. Power System Loads. Excitation in Stability Studies. Prime Mover and Energy Supply Systems. High-Voltage Direct-Current Transmission. Control of Active Power and Reactive Power. Part III: Small Signal Stability. Transient Stability. Voltage Stability. Subsynchronous Machine Representation in Stability Studies. AC Transmission. Power System Loads. Excitation in Stability Studies. Prime Mover and Energy Supply Systems, High-Voltage Direct-Current Transmission. Control of Active Power and Reactive Power. Part III: Small Signal Stability. Transient Stability. Voltage Stability. Subsynchronous Oscillations. Mid-Term and Long-Term Stability. Methods of Improving System Stability.",1994,12,11037,1366,0,0,0,0,0,0,0,0,0,1
7793cdaf86e9bd3663f965f779a040f0a3073803,"Myeloid-derived suppressor cells (MDSCs) are a heterogeneous population of cells that expand during cancer, inflammation and infection, and that have a remarkable ability to suppress T-cell responses. These cells constitute a unique component of the immune system that regulates immune responses in healthy individuals and in the context of various diseases. In this Review, we discuss the origin, mechanisms of expansion and suppressive functions of MDSCs, as well as the potential to target these cells for therapeutic benefit.",2009,149,5166,463,53,193,330,434,459,496,470,460,474,453
ecdd0f2d494ea181792ed0eb40900a5d2786f9c4,,2009,0,5977,1309,0,0,0,0,0,0,0,5,29,1564
5252e82f5f3b1d6633e2e4560653385af7323768,"An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS.",1996,62,10956,808,0,1,1,0,0,3,0,0,80,330
e04ecae6cba2bce685cff811906c17474c31a8a3,"SummaryThe NMRPipe system is a UNIX software environment of processing, graphics, and analysis tools designed to meet current routine and research-oriented multidimensional processing requirements, and to anticipate and accommodate future demands and developments. The system is based on UNIX pipes, which allow programs running simultaneously to exchange streams of data under user control. In an NMRPipe processing scheme, a stream of spectral data flows through a pipeline of processing programs, each of which performs one component of the overall scheme, such as Fourier transformation or linear prediction. Complete multidimensional processing schemes are constructed as simple UNIX shell scripts. The processing modules themselves maintain and exploit accurate records of data sizes, detection modes, and calibration information in all dimensions, so that schemes can be constructed without the need to explicitly define or anticipate data sizes or storage details of real and imaginary channels during processing. The asynchronous pipeline scheme provides other substantial advantages, including high flexibility, favorable processing speeds, choice of both all-in-memory and disk-bound processing, easy adaptation to different data formats, simpler software development and maintenance, and the ability to distribute processing tasks on multi-CPU computers and computer networks.",1995,99,12828,453,0,0,0,0,0,0,0,0,0,0
120c17ffdb8d9112d0820f12d94722e091b92f92,"Many plant-associated microbes are pathogens that impair plant growth and reproduction. Plants respond to infection using a two-branched innate immune system. The first branch recognizes and responds to molecules common to many classes of microbes, including non-pathogens. The second responds to pathogen virulence factors, either directly or through their effects on host targets. These plant immune systems, and the pathogen molecules to which they respond, provide extraordinary insights into molecular recognition, cell biology and evolution across biological kingdoms. A detailed understanding of plant immune function will underpin crop improvement for food, fibre and biofuels production.",2006,117,8983,809,5,155,248,390,401,477,585,621,716,701
92936ad88a5412bc48b86900da94b08fb7a3eefb,"Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.",2006,54,5336,411,9,32,82,182,336,447,490,595,631,613
593619c2a69391454eae1f5ebe75fb8fc7e77e9d,"The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.",1978,7,9541,499,7,15,24,13,37,43,34,48,50,75
4af77aafa93f810e403461e5ee911287aa16d76e,"A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks.",1986,20,9119,461,3,16,44,86,152,164,212,208,287,326
bb01353f818ca226b53433163893efc56c3df32d,"The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF)-based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable location-aware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy.",2000,36,8416,939,18,41,70,138,237,348,377,446,474,558
fbf58da8ebd072bb1fcb43683a2bf6490fe79c31,"Plasticity and functional polarization are hallmarks of the mononuclear phagocyte system. Here we review emerging key properties of different forms of macrophage activation and polarization (M1, M2a, M2b, M2c), which represent extremes of a continuum. In particular, recent evidence suggests that differential modulation of the chemokine system integrates polarized macrophages in pathways of resistance to, or promotion of, microbial pathogens and tumors, or immunoregulation, tissue repair and remodeling.",2004,94,4964,371,1,29,77,108,150,176,194,263,319,341
fee623ce8cb964a53cc1849007758aa5a32490b7,"A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanism--the mirror-neuron mechanism--that appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language.",2004,142,6792,408,10,82,166,256,351,428,450,483,570,552
a3223f1fa1718bf3913753cabda44d854392287b,,2002,0,8918,660,41,68,84,157,237,344,390,470,609,628
ab5716fcabcdf2a1359d7140913e140287190cf0,"The fourth edition of the World Health Organization (WHO) classification of tumours of the central nervous system, published in 2007, lists several new entities, including angiocentric glioma, papillary glioneuronal tumour, rosette-forming glioneuronal tumour of the fourth ventricle, papillary tumour of the pineal region, pituicytoma and spindle cell oncocytoma of the adenohypophysis. Histological variants were added if there was evidence of a different age distribution, location, genetic profile or clinical behaviour; these included pilomyxoid astrocytoma, anaplastic medulloblastoma and medulloblastoma with extensive nodularity. The WHO grading scheme and the sections on genetic profiles were updated and the rhabdoid tumour predisposition syndrome was added to the list of familial tumour syndromes typically involving the nervous system. As in the previous, 2000 edition of the WHO ‘Blue Book’, the classification is accompanied by a concise commentary on clinico-pathological characteristics of each tumour type. The 2007 WHO classification is based on the consensus of an international Working Group of 25 pathologists and geneticists, as well as contributions from more than 70 international experts overall, and is presented as the standard for the definition of brain tumours to the clinical oncology and cancer research communities world-wide.",2007,73,11361,223,0,0,0,5,582,806,872,967,1166,1130
cb4368eab957ded0ce8c587c4fda165a26bc5a22,"This paper introduces the ant colony system (ACS), a distributed algorithm that is applied to the traveling salesman problem (TSP). In the ACS, a set of cooperating agents called ants cooperate to find good solutions to TSPs. Ants cooperate using an indirect form of communication mediated by a pheromone they deposit on the edges of the TSP graph while building solutions. We study the ACS by running experiments to understand its operation. The results show that the ACS outperforms other nature-inspired algorithms such as simulated annealing and evolutionary computation, and we conclude comparing ACS-3-opt, a version of the ACS augmented with a local search procedure, to some of the best performing algorithms for symmetric and asymmetric TSPs.",1997,72,7445,724,14,26,44,49,65,111,98,186,209,299
b7ebcec93f35e52ef14a41c9c9d55fa132987473,"Abstract In the first of two papers on MAGMA , a new system for computational algebra, we present the MAGMA language, outline the design principles and theoretical background, and indicate its scope and use. Particular attention is given to the constructors for structures, maps, and sets.",1997,17,5714,644,9,11,30,38,57,53,61,78,83,121
4e9ec92a90c5d571d2f1d496f8df01f0a8f38596,"A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.",2008,80,4455,728,0,1,1,4,11,30,66,83,121,235
895ba2d6b9c6eae6d61c48e842a57d05e95b42c8,"Representing the first volume in the fourth edition series of the World Health Organization (WHO) Classification of Tumours, this book provides a welcome mix of old and new. Similar to prior versions, it opens with a summary of the recently revised WHO Classification of Tumours of the Central Nervous System (CNS), the remainder of the book being dedicated to a comprehensive yet succinct presentation of the most current knowledge relative to each individual tumor and familial tumor syndrome. The 73 contributing authors have likewise adopted a familiar standardized format with the following subheadings: definition, grading, incidence, age and sex distribution, localization, clinical features, neuroimaging, macroscopy, histopathology, proliferation, genetic susceptibility, genetics, histogenesis, and prognostic and predictive factors. Although a fair number of images have been recycled from previous editions, the majority is new and includes more than 400 full-color photomicrographs and macroscopic images, as well as numerous neuroimages, informative diagrams and charts. A number of tumor entities new to this version of the WHO Classification are explored in detail, including pilomyxoid astrocytoma, atypical choroid plexus papilloma, angiocentric glioma, extraventricular neurocytoma, papillary glioneuronal tumor, rosetteforming glioneuronal tumor of the fourth ventricle, papillary tumor of the pineal region, pituicytoma, and spindle cell oncocytoma of the adenohypophysis. Perhaps the most noticeable improvement comes by way of a voluminous expansion in the genetics sections of the majority of tumor categories. This update parallels the recent explosion of research utilizing high-resolution genome screening and other molecular techniques. The authors have done an outstanding job in distilling the information housed in over 2,500 cited references into a readerfriendly authoritative reference of CNS neoplasia. In summation, the current edition of the WHO Classification of Tumours of the Central Nervous System will serve as an indispensable textbook for all of those involved in the diagnosis and management of patients with tumors of the CNS, and will make a valuable addition to libraries in pathology, radiology, oncology, and neurosurgery departments.",2007,0,4321,324,14,112,212,263,387,435,428,442,495,388
62d36f23580ae0c822ebc7de69ae603d85441bfc,"The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the morphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.",1986,57,5014,476,7,4,9,14,27,28,33,35,30,54
88779104d045f4f319d17184368ee12431a0ce82,"Face perception, perhaps the most highly developed visual skill in humans, is mediated by a distributed neural system in humans that is comprised of multiple, bilateral regions. We propose a model for the organization of this system that emphasizes a distinction between the representation of invariant and changeable aspects of faces. The representation of invariant aspects of faces underlies the recognition of individuals, whereas the representation of changeable aspects of faces, such as eye gaze, expression, and lip movement, underlies the perception of information that facilitates social communication. The model is also hierarchical insofar as it is divided into a core system and an extended system. The core system is comprised of occipitotemporal regions in extrastriate visual cortex that mediate the visual analysis of faces. In the core system, the representation of invariant aspects is mediated more by the face-responsive region in the fusiform gyrus, whereas the representation of changeable aspects is mediated more by the face-responsive region in the superior temporal sulcus. The extended system is comprised of regions from neural systems for other cognitive functions that can be recruited to act in concert with the regions in the core system to extract meaning from faces.",2000,102,4222,417,5,36,55,77,85,89,124,165,168,230
634c9fde5f1c411e4487658ac738dcf18d98ea8d,"Information theory has recently been employed to specify more precisely than has hitherto been possible man's capacity in certain sensory, perceptual, and perceptual-motor functions (5, 10, 13, 15, 17, 18). The experiments reported in the present paper extend the theory to the human motor system. The applicability of only the basic concepts, amount of information, noise, channel capacity, and rate of information transmission, will be examined at this time. General familiarity with these concepts as formulated by recent writers (4, 11,20, 22) is assumed. Strictly speaking, we cannot study man's motor system at the behavioral level in isolation from its associated sensory mechanisms. We can only analyze the behavior of the entire receptor-neural-effector system. However, by asking 51 to make rapid and uniform responses that have been highly overlearned, and by holding all relevant stimulus conditions constant with the exception of those resulting from 5""s own movements, we can create an experimental situation in which it is reasonable to assume that performance is limited primarily by the capacity of the motor system. The motor system in the present case is defined as including the visual and proprioceptive feedback loops that permit S to monitor his own activity. The information capacity of the motor system is specified by its ability to produce consistently one class of movement from among several alternative movement classes. The greater the number of alternative classes, the greater is the information capacity of a particular type of response. Since measurable aspects of motor responses, such as their force, direction, and amplitude, are continuous variables, their information capacity is limited only by the amount of statistical variability, or noise, that is characteristic of repeated efforts to produce the same response. The information capacity of the motor Editor's Note. This article is a reprint of an original work published in 1954 in the Journal of Experimental Psychology, 47, 381391.",1954,28,7605,848,0,2,1,1,4,2,1,3,1,3
770b40c2c84bf7a3541b5c7fdd9fbd8842740156,A new total knee rating system has been developed by The Knee Society to provide an up-to-date more stringent evaluation form. The system is subdivided into a knee score that rates only the knee joint itself and a functional score that rates the patient's ability to walk and climb stairs. The dual rating system eliminates the problem of declining knee scores associated with patient infirmity.,1989,0,4341,368,0,0,3,6,12,9,17,32,43,47
571dd9820e128c44f6fb6d09f2d17eb021b2af8f,,1996,0,7975,439,1,23,94,173,271,342,432,534,618,610
b39b864606b8c0cf8e223e4d67554503ef33e010,"Ever since Richard Stone (1954) first estimated a system of demand equations derived explicitly from consumer theory, there has been a continuing search for alternative specifications and functional forms. Many models have been proposed, but perhaps the most important in current use, apart from the original linear expenditure system, are the Rotterdam model (see Henri Theil, 1965, 1976; Anton Barten) and the translog model (see Laurits Christensen, Dale Jorgenson, and Lawrence Lau; Jorgenson and Lau). Both of these models have been extensively estimated and have, in addition, been used to test the homogeneity and symmetry restrictions of demand theory. In this paper, we propose and estimate a new model which is of comparable generality to the Rotterdam and translog models but which has considerable advantages over both. Our model, which we call the Almost Ideal Demand System (AIDS), gives an arbitrary first-order approximation to any demand system; it satisfies the axioms of choice exactly; it aggregates perfectly over consumers without invoking parallel linear Engel curves; it has a functional form which is consistent with known household-budget data; it is simple to estimate, largely avoiding the need for non-linear estimation; and it can be used to test the restrictions of homogeneity and symmetry through linear restrictions on fixed parameters. Although many of these desirable properties are possessed by one or other of the Rotterdam or translog models, neither possesses all of them simultaneously. In Section I of the paper, we discuss the theoretical specification of the AIDS and justify the claims in the previous paragraph. In Section II, the model is estimated on postwar British data and we use our results to test the homogeneity and symmetry restrictions. Our results are consistent with earlier findings in that both sets of restrictions are decisively rejected. We also find that imposition of homogeneity generates positive serial correlation in the errors of those equations which reject the restrictions most strongly; this suggests that the now standard rejection of homogeneity in demand analysis may be due to insufficient attention to the dynamic aspects of consumer behavior. Finally, in Section III, we offer a summary and conclusions. We believe that the results of this paper suggest that the AIDS is to be recommended as a vehicle for testing, extending, and improving conventional demand analysis. This does not imply that the system, particularly in its simple static form, is to be regarded as a fully satisfactory explanation of consumers' behavior. Indeed, by proposing a demand system which is superior to its predecessors, we hope to be able to reveal more clearly the problems and potential solutions associated with the usual approach.",1980,27,4512,431,4,3,12,16,32,24,18,35,42,48
71b9b9c5d7fd73986af47a6515dcbab246f3aea5,"Despite multiple disparate prognostic risk analysis systems for evaluating clinical outcome for patients with myelodysplastic syndrome (MDS), imprecision persists with such analyses. To attempt to improve on these systems, an International MDS Risk Analysis Workshop combined cytogenetic, morphological, and clinical data from seven large previously reported risk-based studies that had generated prognostic systems. A global analysis was performed on these patients, and critical prognostic variables were re-evaluated to generate a consensus prognostic system, particularly using a more refined bone marrow (BM) cytogenetic classification. Univariate analysis indicated that the major variables having an impact on disease outcome for evolution to acute myeloid leukemia were cytogenetic abnormalities, percentage of BM myeloblasts, and number of cytopenias; for survival, in addition to the above, variables also included age and gender. Cytogenetic subgroups of outcome were as follows: ""good"" outcomes were normal, -Y alone, del(5q) alone, del(20q) alone; ""poor"" outcomes were complex (ie, > or = 3 abnormalities) or chromosome 7 anomalies; and ""intermediate"" outcomes were other abnormalities. Multivariate analysis combined these cytogenetic subgroups with percentage of BM blasts and number of cytopenias to generate a prognostic model. Weighting these variables by their statistical power separated patients into distinctive subgroups of risk for 25% of patients to undergo evolution to acute myeloid leukemia, with: low (31% of patients), 9.4 years; intermediate-1 (INT-1; 39%), 3.3 years; INT-2 (22%), 1.1 years; and high (8%), 0.2 year. These features also separated patients into similar distinctive risk groups for median survival: low, 5.7 years; INT-1, 3.5 years; INT-2, 1.2 years; and high, 0.4 year. Stratification for age further improved analysis of survival. Compared with prior risk-based classifications, this International Prognostic Scoring System provides an improved method for evaluating prognosis in MDS. This classification system should prove useful for more precise design and analysis of therapeutic trials in this disease.",1997,58,4016,364,10,43,60,80,90,110,92,121,143,170
89662b1305051451a9ab3ece083961921092a063,"Abstract Lack of user acceptance has long been an impediment to the success of new information systems. The present research addresses why users accept or reject information systems and how user acceptance is affected by system design features. The technology acceptance model (TAM) specifies the causal relationships between system design features, perceived usefulness, perceived ease of use, attitude toward using, and actual usage behavior. Attitude theory from psychology provides the rationale for hypothesized model relationships, and validated measures were used to operationalize model variables. A field study of 112 users regarding two end-user systems was conducted to test the hypothesized model. TAM fully mediated the effects of system characteristics on usage behavior, accounting for 36% of the variance in usage. Perhaps the most striking finding was that perceived usefulness was 50% more influential than ease of use in determining usage, underscoring the importance of incorporating the appropriate functional capabilities in new systems. Overall, TAM provides an informative representation of the mechanisms by which design choices influence user acceptance, and should therefore be helpful in applied contexts for forecasting and evaluating user acceptance of information technology. Implications for future research and practice are discussed.",1993,0,3838,453,2,2,9,10,18,14,24,26,27,51
7b532c5c43e4903b46d9ebff8be7e878ff3aad57,"New information regarding neuronal circuits that control food intake and their hormonal regulation has extended our understanding of energy homeostasis, the process whereby energy intake is matched to energy expenditure over time. The profound obesity that results in rodents (and in the rare human case as well) from mutation of key signalling molecules involved in this regulatory system highlights its importance to human health. Although each new signalling pathway discovered in the hypothalamus is a potential target for drug development in the treatment of obesity, the growing number of such signalling molecules indicates that food intake is controlled by a highly complex process. To better understand how energy homeostasis can be achieved, we describe a model that delineates the roles of individual hormonal and neuropeptide signalling pathways in the control of food intake and the means by which obesity can arise from inherited or acquired defects in their function.",2000,142,5981,317,39,151,224,225,291,279,313,330,309,333
20996231727b9d11d66ed092efef86054024214c,"Techniques to determine changing system complexity from data are evaluated. Convergence of a frequently used correlation dimension algorithm to a finite value does not necessarily imply an underlying deterministic model or chaos. Analysis of a recently developed family of formulas and statistics, approximate entropy (ApEn), suggests that ApEn can classify complex systems, given at least 1000 data values in diverse settings that include both deterministic chaotic and stochastic processes. The capability to discern changing complexity from such a relatively small amount of data holds promise for applications of ApEn in a variety of contexts.",1991,5,4509,351,1,4,6,7,13,17,24,48,33,41
2a16fe9680001fcade8b4ccf04ab458028dee80e,"Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.",2000,47,3894,366,12,30,117,204,278,379,370,368,320,312
e5616898a40b7c7356f7ea6bf49d16227b07abfe,,2003,6,5024,488,4,34,57,67,113,178,246,360,368,436
56c16d9e2a5270ba6b1d83271e2c10916591968d,"Publisher Summary This chapter presents a general theoretical framework of human memory and describes the results of a number of experiments designed to test specific models that can be derived from the overall theory. This general theoretical framework categorizes the memory system along two major dimensions. The first categorization distinguishes permanent, structural features of the system from control processes that can be readily modified or reprogrammed at the will of the subject. The second categorization divides memory into three structural components: the sensory register, the short-term store, and the long-term store. Incoming sensory information first enters the sensory register, where it resides for a very brief period of time, then decays and is lost. The short-term store is the subject's working memory; it receives selected inputs from the sensory register and also from long-term store. The chapter also discusses the control processes associated with the sensory register. The term control process refers to those processes that are not permanent features of memory, but are instead transient phenomena under the control of the subject; their appearance depends on several factors such as instructional set, the experimental task, and the past history of the subject.",1968,92,6111,322,12,32,61,55,47,66,79,71,54,64
418373ac211f44e50ad148e7e1368a51babfca01,"for competition that is based on information, their ability to exploit intangible assets has become far more decisive than their ability to invest in and manage physical assets. Several years ago, in recognition of this change, we introduced a concept we called the balanced scorecard. The balanced scorecard supplemented traditional fi nancial measures with criteria that measured performance from three additional perspectives – those of customers, internal business processes, and learning and growth. (See the exhibit “Translating Vision and Strategy: Four Perspectives.”) It therefore enabled companies to track fi nancial results while simultaneously monitoring progress in building the capabilities and acquiring the intangible assets they would need for future growth. The scorecard wasn’t Editor’s Note: In 1992, Robert S. Kaplan and David P. Norton’s concept of the balanced scorecard revolutionized conventional thinking about performance metrics. By going beyond traditional measures of fi nancial performance, the concept has given a generation of managers a better understanding of how their companies are really doing. These nonfi nancial metrics are so valuable mainly because they predict future fi nancial performance rather than simply report what’s already happened. This article, fi rst published in 1996, describes how the balanced scorecard can help senior managers systematically link current actions with tomorrow’s goals, focusing on that place where, in the words of the authors, “the rubber meets the sky.” Using the Balanced Scorecard as a Strategic Management System",1996,0,5313,443,9,32,38,63,79,98,94,128,132,170
0683fa33bb1ebb0b65b3bb6e8b0ea6272c3b5dce,"To address the need for a standardized system to classify the gross motor function of children with cerebral palsy, the authors developed a five‐level classification system analogous to the staging and grading systems used in medicine. Nominal group process and Delphi survey consensus methods were used to examine content validity and revise the classification system until consensus among 48 experts (physical therapists, occupational therapists, and developmental pediatricians with expertise in cerebral palsy) was achieved. Interrater reliability (k) was 0.55 for children less than 2 years of age and 0.75 for children 2 to 12 years of age. The classification system has application for clinical practice, research, teaching, and administration.",1997,15,4614,320,0,6,7,21,39,55,59,96,112,151
902be51a420f734b6ab033fe4cea69189a6297c4,"Introductory immunology textbook for medical students, advanced undergraduates, and graduate students.",1996,0,4560,398,11,35,55,73,83,109,132,155,176,184
eb51cb223fb17995085af86ac70f765077720504,"We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users.",2002,11,3209,423,11,56,80,112,163,230,229,278,284,282
3e7bf1c10181e243d5f4a69c4b845a2bdddf434a,"Enterprise systems present a new model of corporate computing. They allow companies to replace their existing information systems, which are often incompatible with one another, with a single, integrated system. By streamlining data flows throughout an organization, these commercial software packages, offered by vendors like SAP, promise dramatic gains in a company's efficiency and bottom line. It's no wonder that businesses are rushing to jump on the ES bandwagon. But while these systems offer tremendous rewards, the risks they carry are equally great. Not only are the systems expensive and difficult to implement, they can also tie the hands of managers. Unlike computer systems of the past, which were typically developed in-house with a company's specific requirements in mind, enterprise systems are off-the-shelf solutions. They impose their own logic on a company's strategy, culture, and organization, often forcing companies to change the way they do business. Managers would do well to heed the horror stories of failed implementations. FoxMeyer Drug, for example, claims that its system helped drive it into bankruptcy. Drawing on examples of both successful and unsuccessful ES projects, the author discusses the pros and cons of implementing an enterprise system, showing how a system can produce unintended and highly disruptive consequences. Because of an ES's profound business implications, he cautions against shifting responsibility for its adoption to technologists. Only a general manager will be able to mediate between the imperatives of the system and the imperatives of the business.",1998,0,3721,430,4,37,86,88,108,143,161,176,218,211
b1f3590c426f50d8320977f646479d4490e94f13,"Classification systems are necessary in order to provide a framework in which to scientifically study the etiology, pathogenesis, and treatment of diseases in an orderly fashion. In addition, such systems give clinicians a way to organize the health care needs of their patients. The last time scientists and clinicians in the field of periodontology and related areas agreed upon a classification system for periodontal diseases was in 1989 at the World Workshop in Clinical Periodontics. Subsequently, a simpler classification was agreed upon at the 1st European Workshop in Periodontology. These classification systems have been widely used by clinicians and research scientists throughout the world. Unfortunately, the 1989 classification had many shortcomings, including: (1) considerable overlap in disease categories, (2) absence of a gingival disease component, (3) inappropriate emphasis on age of onset of disease and rates of progression, and (4) inadequate or unclear classification criteria. The 1993 European classification lacked the detail necessary for adequate characterization of the broad spectrum of periodontal diseases encountered in clinical practice. The need for a revised classification system for periodontal diseases was emphasized during the 1996 World Workshop in Periodontics. In 1997 the American Academy of Periodontology responded to this need and formed a committee to plan and organize an international workshop to revise the classification system for periodontal diseases. The proceedings in this volume are the result of this reclassification effort. The process involved development by the Organizing Committee of an outline for a new classification and identification of individuals to write state-of-the-science reviews for each of the items on the outline. The reviewers were encouraged to depart from the preliminary outline if there were data to support any modifications. On October 30-November 2, 1999, the International Workshop for a Classification of Periodontal Diseases and Conditions was held and a new classification was agreed upon (Figure 1). This paper summarizes how the new classification for periodontal diseases and conditions presented in this volume differs from the classification system developed at the 1989 World Workshop in Clinical Periodontics. In addition, an analysis of the rationale is provided for each of the modifications and changes.",1999,33,4402,232,2,4,26,44,47,67,91,102,102,144
7c95d8fe969693dd71573dd803493d0869a915bd,"A model of the neuropsychology of anxiety is proposed. The model is based in the first instance upon an analysis of the behavioural effects of the antianxiety drugs (benzodiazepines, barbiturates, and alcohol) in animals. From such psychopharmacologi-cal experiments the concept of a “behavioural inhibition system” (BIS) has been developed. This system responds to novel stimuli or to those associated with punishment or nonreward by inhibiting ongoing behaviour and increasing arousal and attention to the environment. It is activity in the BIS that constitutes anxiety and that is reduced by antianxiety drugs. The effects of the antianxiety drugs in the brain also suggest hypotheses concerning the neural substrate of anxiety. Although the benzodiazepines and barbiturates facilitate the effects of γ-aminobutyrate, this is insufficient to explain their highly specific behavioural effects. Because of similarities between the behavioural effects of certain lesions and those of the antianxiety drugs, it is proposed that these drugs reduce anxiety by impairing the functioning of a widespread neural system including the septo-hippocampal system (SHS), the Papez circuit, the prefrontal cortex, and ascending monoaminergic and cholinergic pathways which innervate these forebrain structures. Analysis of the functions of this system (based on anatomical, physiological, and behavioural data) suggests that it acts as a comparator: it compares predicted to actual sensory events and activates the outputs of the BIS when there is a mismatch or when the predicted event is aversive. Suggestions are made as to the functions of particular pathways within this overall brain system. The resulting theory is applied to the symptoms and treatment of anxiety in man, its relations to depression, and the personality of individuals who are susceptible to anxiety or depression.",1982,369,4399,239,62,22,33,104,51,59,51,55,95,93
0bd6e70ab6c2d131bbda7ef24d552709e6b6455b,"This book provides a basic treatment of one of the most widely used operations research tools: discrete-event simulation. Prerequisites are calculus, probability theory, and elementary statistics. Contents, abridged: Introduction to discrete-event system simulation. Mathematical and statistical models. Random numbers. Analysis of simulation data. Index.",1995,0,4045,264,26,53,49,44,48,47,75,86,110,148
53a7cf6bf8568c660240c080125e55836d507098,"During the First International EEG Congress, London in 1947, it was recommended that Dr. Herbert H. Jasper study methods to standardize the placement of electrodes used in EEG (Jasper 1958). A report with recommendations was to be presented to the Second International Congress in Paris in 1949. The electrode placement systems in use at various centers were found to be similar, with only minor differences, although their designations, letters and numbers were entirely different. Dr. Jasper established some guidelines which would be established in recommending a speci®c system to the federation and these are listed below.",1999,4,6061,213,117,102,112,109,141,179,188,194,216,210
a6d38c2f78a12b92464ef95b89d0567e01262631,"Grey System theory was initiated in 1982 [7]. As far as information is concerned, the systems which lack information, such as structure message, operation mechanism and behaviour document, are referred to as Grey Systems. For example, the human body, agriculture, economy, etc., are Grey Systems. Usually, on the grounds of existing grey relations, grey elements, grey numbers (denoted by 8 ) one can identify which Grey System is, where ""grey"" means poor, incomplete, uncertain, etc. The goal of Grey Systeni and its applications is to bridge the gap existing between social science and natural science. Thus, one can say that the Grey System theory is interdisciplinary, cutting across a variety of specialized fields, and it is evident that Grey System theory stands the test of time since 1982. As the case stands, the developn~ent of the Grey System-as well as theoretical topic-is coupled with clear applications of the theory in assorted fields. The conccept of the Grey System, in its theory and successful application, is now well known in China. The application fields of the Grey System involve agriculture [23, 77-81, 911, ecology [59], economy [61, 102, 103, 1041, meteorology [58, 74,911, medicine [55, 891, history [63, 641, geography [I], industry [61, earthquake [73, 87, 881, geology [76, 1 191, hydrology [98, 1 121, irrigation strategy [261, military affairs, sports [116], traffic [67], management [30, 97, 1051, material science [82, 831, environment [ 1081, biological protection [69,70], judicial system [loo], etc. Projects which have been successfully completed with the Grey System theory and its applications are as follows: 1. Regional econonlic planning for several provinces in China; 2. To forecast yields of grain for some provinces in China:",1989,239,3896,279,1,1,0,0,0,1,4,4,8,11
5f8994ef56c66605c45c9a5e727c6cb1a3af28c1,ion “101” An Introduction for New Abstractors,2002,3,6577,254,104,130,183,231,287,256,277,328,330,354
9aa87ac3947fc59e58eb3eef2fad1b27709a74cc,"It is shown that the free energy of a volume V of an isotropic system of nonuniform composition or density is given by : NV∫V [f 0(c)+κ(▿c)2]dV, where NV is the number of molecules per unit volume, ▿c the composition or density gradient, f 0 the free energy per molecule of a homogeneous system, and κ a parameter which, in general, may be dependent on c and temperature, but for a regular solution is a constant which can be evaluated. This expression is used to determine the properties of a flat interface between two coexisting phases. In particular, we find that the thickness of the interface increases with increasing temperature and becomes infinite at the critical temperature Tc , and that at a temperature T just below Tc the interfacial free energy σ is proportional to (T c −T) 3 2 . The predicted interfacial free energy and its temperature dependence are found to be in agreement with existing experimental data. The possibility of using optical measurements of the interface thickness to provide an additional check of our treatment is briefly discussed.",1958,16,7573,211,1,4,4,4,7,6,2,4,9,8
9b88e58fa34269c31a0f8fe823f2bf96824fadc4,"This paper presents the form and validation results of APACHE II, a severity of disease classification system. APACHE II uses a point score based upon initial values of 12 routine physiologic measurements, age, and previous health status to provide a general measure of severity of disease. An increasing score (range 0 to 71) was closely correlated with the subsequent risk of hospital death for 5815 intensive care admissions from 13 hospitals. This relationship was also found for many common diseases.When APACHE II scores are combined with an accurate description of disease, they can prognostically stratify acutely ill patients and assist investigators comparing the success of new or differing forms of therapy. This scoring index can be used to evaluate the use of hospital resources and compare the efficacy of intensive care in different hospitals or over time.",1985,1,4958,230,1,1,1,7,5,12,7,8,17,14
a27087636a2708771f654fa175f406c63a9272be,"A description of the ab initio quantum chemistry package GAMESS is presented. Chemical systems containing atoms through radon can be treated with wave functions ranging from the simplest closed‐shell case up to a general MCSCF case, permitting calculations at the necessary level of sophistication. Emphasis is given to novel features of the program. The parallelization strategy used in the RHF, ROHF, UHF, and GVB sections of the program is described, and detailed speecup results are given. Parallel calculations can be run on ordinary workstations as well as dedicated parallel machines. © John Wiley & Sons, Inc.",1993,131,15511,135,0,0,0,0,0,0,0,0,0,0
1ed876aa17c32dfb8ca7b41ed506a44e976be6ed,"The framework of a national land use and land cover classification system is presented for use with remote sensor data. The classification system has been developed to meet the needs of Federal and State agencies for an up-to-date overview of land use and land cover throughout the country on a basis that is uniform in categorization at the more generalized first and second levels and that will be receptive to data from satellite and aircraft remote sensors. The pro-posed system uses the features of existing widely used classification systems that are amenable to data derived from re-mote sensing sources. It is intentionally left open-ended so that Federal, regional, State, and local agencies can have flexibility in developing more detailed land use classifications at the third and fourth levels in order to meet their particular needs and at the same time remain compatible with each other and the national system. Revision of the land use classification system as presented in US Geological Survey Circular 671 was undertaken in order to incorporate the results of extensive testing and review of the categorization and definitions.",1976,90,3910,226,2,9,8,7,8,13,13,11,8,23
35c0183e9940feb567b4417115be8460bc127cfa,"From the Publisher: 
An extensive revision of the author's highly successful text, this third edition of Linear System Theory and Design has been made more accessible to students from all related backgrounds. After introducing the fundamental properties of linear systems, the text discusses design using state equations and transfer functions. The two main objectives of the text are to: use simple and efficient methods to develop results and design procedures; enable students to employ the results to carry out design. Striking a balance between theory and applications, Linear System Theory and Design, 3/e, is ideal for use in advanced undergraduate/first-year graduate courses in linear systems and multivariable system design in electrical, mechanical, chemical, and aeronautical engineering departments. It assumes a working knowledge of linear algebra and the Laplace transform and an elementary knowledge of differential equations.",1995,0,3664,232,83,83,75,62,67,69,65,73,105,108
c3777a30ff0679b56f712324c93ad1b0ecb4ed46,"PROTEIN-protein interactions between two proteins have generally been studied using biochemical techniques such as crosslinking, co-immunoprecipitation and co-fractionation by chromatography. We have generated a novel genetic system to study these interactions by taking advantage of the properties of the GAL4 protein of the yeast Saccharomyces cerevisiae. This protein is a transcriptional activator required for the expression of genes encoding enzymes of galactose utilization1. It consists of two separable and functionally essential domains: an N-terminal domain which binds to specific DNA sequences (UASG); and a C-terminal domain containing acidic regions, which is necessary to activate transcription2,3. We have generated a system of two hybrid proteins containing parts of GAL4: the GAL4 DNA-binding domain fused to a protein 'X' and a GAL4 activating region fused to a protein 'Y'. If X and Y can form a protein-protein complex and reconstitute proximity of the GAL4 domains, tran-scription of a gene regulated by UASG occurs. We have tested this system using two yeast proteins that are known to interactSNF1 and SNF4. High transcriptional activity is obtained only when both hybrids are present in a cell. This system may be applicable as a general method to identify proteins that interact with a known protein by the use of a simple galactose selection.",1989,17,6131,199,0,5,5,23,50,104,196,254,330,332
007524794d49bebca5845722054e459a86d8b785,"Mobile users' data rate and quality of service are limited by the fact that, within the duration of any given call, they experience severe variations in signal attenuation, thereby necessitating the use of some type of diversity. In this two-part paper, we propose a new form of spatial diversity, in which diversity gains are achieved via the cooperation of mobile users. Part I describes the user cooperation strategy, while Part II (see ibid., p.1939-48) focuses on implementation issues and performance analysis. Results show that, even though the interuser channel is noisy, cooperation leads not only to an increase in capacity for both users but also to a more robust system, where users' achievable rates are less susceptible to channel variations.",2003,38,6553,177,18,86,204,323,469,599,707,784,644,600
0dcd97c4e2a91e6b8f8ea8ac2f5da393a49a96ab,"This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.",2000,34,4252,182,7,57,84,135,204,274,280,350,329,344
a5c333a7fbca00121d9221c107d2a82d3b582b71,"Solar photospheric and meteoritic CI chondrite abundance determinations for all elements are summarized and the best currently available photospheric abundances are selected. The meteoritic and solar abundances of a few elements (e.g., noble gases, beryllium, boron, phosphorous, sulfur) are discussed in detail. The photospheric abundances give mass fractions of hydrogen (X ¼ 0:7491), helium (Y ¼ 0:2377), and heavy elements (Z ¼ 0:0133), leading to Z=X ¼ 0:0177, which is lower than the widely used Z=X ¼ 0:0245 from previous compilations. Recent results from standard solar models considering helium and heavy-element settling imply that photospheric abundances and mass fractions are not equal to protosolar abundances (representative of solar system abundances). Protosolar elemental and isotopic abundances are derived from photospheric abundances by considering settling effects. Derived protosolar mass fractions are X0 ¼ 0:7110, Y0 ¼ 0:2741, and Z0 ¼ 0:0149. The solar system and photospheric abundance tables are used to compute self-consistent sets of condensation temperatures for all elements. Subject headings: astrochemistry — meteors, meteoroids — solar system: formation — Sun: abundances — Sun: photosphere",2003,96,3163,739,10,44,98,123,144,132,165,174,147,192
2b78a95be01f0d7f9faa592d81ce40e4330c1afc,"Second in a series of publications from the Institute of Medicine's Quality of Health Care in America project Today's health care providers have more research findings and more technology available to them than ever before. Yet recent reports have raised serious doubts about the quality of health care in America. Crossing the Quality Chasm makes an urgent call for fundamental change to close the quality gap. This book recommends a sweeping redesign of the American health care system and provides overarching principles for specific direction for policymakers, health care leaders, clinicians, regulators, purchasers, and others. In this comprehensive volume the committee offers: A set of performance expectations for the 21st century health care system. A set of 10 new rules to guide patient-clinician relationships. A suggested organizing framework to better align the incentives inherent in payment and accountability with improvements in quality. Key steps to promote evidence-based practice and strengthen clinical information systems. Analyzing health care organizations as complex systems, Crossing the Quality Chasm also documents the causes of the quality gap, identifies current practices that impede quality care, and explores how systems approaches can be used to implement change.",2001,0,13773,65,0,0,0,1,0,0,0,2,426,753
dc139f901c869f80b54b41f89d5b7f35c7dfa3c7,"Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >",1995,11,4293,179,9,88,183,237,295,272,248,281,241,240
94f1cbb8e88f524efe2a9034e25c1f83a6b2abf3,Illustration de trois fonctions principales qui sont predominantes dans l'etude de l'intervention de l'attention dans les processus cognitifs: 1) orientation vers des evenements sensoriels; 2) detection des signaux par processus focal; 3) maintenir la vigilance en etat d'alerte,1990,87,5550,190,11,29,93,90,111,131,115,154,159,156
983bee5c523b2b99f36a027772e67c1c2499ab31,"The adhesive interactions of cells with other cells and with the extracellular matrix are crucial to all developmental processes, but have a central role in the functions of the immune system throughout life. Three families of cell-surface molecules regulate the migration of lymphocytes and the interactions of activated cells during immune responses.",1990,158,6416,162,19,229,443,509,508,443,485,397,426,322
bcfe59f68a5b6883db9497374cba8cafacc355f4,"Never ever burnt out to improve your knowledge by reviewing book. Currently, we present you an exceptional reading e-book entitled International System For Human Cytogenetic Nomenclature panamabustickets.com Study Group has writer this book completely. So, just review them online in this click switch and even download them to allow you check out anywhere. Still confused ways to review? Find them and also make choice for data format in pdf, ppt, zip, word, rar, txt, and also kindle. i international legal protection of human rights ohchr 2 international legal protection of human rights in armed conflict this publication provides a thorough legal analysis and guidance to state authorities, human rights and humanitarian actors and others",1978,0,7421,128,0,7,19,25,43,47,41,74,83,122
13241a844c714549c173e239714ae020386172e3,"The authors describe a model of autobiographical memory in which memories are transitory mental constructions within a self-memory system (SMS). The SMS contains an autobiographical knowledge base and current goals of the working self. Within the SMS, control processes modulate access to the knowledge base by successively shaping cues used to activate autobiographical memory knowledge structures and, in this way, form specific memories. The relation of the knowledge base to active goals is reciprocal, and the knowledge base ""grounds"" the goals of the working self. It is shown how this model can be used to draw together a wide range of diverse data from cognitive, social, developmental, personality, clinical, and neuropsychological autobiographical memory research.",2000,307,3151,301,7,26,41,51,76,64,96,114,142,161
536f48a2755969d2bc269302ae2698f9496a4619,"Dendritic cells are a system of antigen presenting cells that function to initiate several immune responses such as the sensitization of MHC-restricted T cells, the rejection of organ transplants, and the formation of T-dependent antibodies. Dendritic cells are found in many nonlymphoid tissues but can migrate via the afferent lymph or the blood stream to the T-dependent areas of lymphoid organs. In skin, the immunostimulatory function of dendritic cells is enhanced by cytokines, especially GM-CSF. After foreign proteins are administered in situ, dendritic cells are a principal reservoir of immunogen. In vitro studies indicate that dendritic cells only process proteins for a short period of time, when the rate of synthesis of MHC products and content of acidic endocytic vesicles are high. Antigen processing is selectively dampened after a day in culture, but the capacity to stimulate responses to surface bound peptides and mitogens remains strong. Dendritic cells are motile, and efficiently cluster and activate T cells that are specific for stimuli on the cell surface. High levels of MHC class-I and -II products and several adhesins, such as ICAM-1 and LFA-3, likely contribute to these functions. Therefore dendritic cells are specialized to mediate several physiologic components of immunogenicity such as the acquisition of antigens in tissues, the migration to lymphoid organs, and the identification and activation of antigen-specific T cells. The function of these presenting cells in immunologic tolerance is just beginning to be studied.",1991,64,4908,141,4,43,106,98,169,206,283,339,327,335
17cc1d6be6ed398af518fef32ba4e37a389c9dbb,"A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.",1992,17,4254,171,6,19,29,25,31,25,43,75,72,102
3b6ea979601bbddf3cf36125d8c4e2432ec139a9,"Revisions in stage grouping of the TNM subsets (T=primary tumor, N=regional lymph nodes, M=distant metastasis) in the International System for Staging Lung Cancer have been adopted by the American Joint Committee on Cancer and the Union Internationale Contre le Cancer. These revisions were made to provide greater specificity for identifying patient groups with similar prognoses and treatment options with the least disruption of the present classification: T1N0M0, stage IA; T2N0M0, stage IB; T1N1M0, stage IIA; T2N1M0 and T3N0M0, stage IIB; and T3N1M0, T1N2M0, T2N2M0, T3N2M0, stage IIIA. The TNM subsets in stage IIIB-T4 any N M0, any T N3M0, and in stage IV-any T any N M1, remain the same. Analysis of a collected database representing all clinical, surgical-pathologic, and follow-up information for 5,319 patients treated for primary lung cancer confirmed the validity of the TNM and stage grouping classification schema.",1997,17,4590,166,4,37,100,184,219,257,282,316,325,339
fac9db01f3db66d937854101cdcad5f4d9c28013,"NF-kappa B is a ubiquitous transcription factor. Nevertheless, its properties seem to be most extensively exploited in cells of the immune system. Among these properties are NF-kappa B's rapid posttranslational activation in response to many pathogenic signals, its direct participation in cytoplasmic/nuclear signaling, and its potency to activate transcription of a great variety of genes encoding immunologically relevant proteins. In vertebrates, five distinct DNA binding subunits are currently known which might extensively heterodimerize, thereby forming complexes with distinct transcriptional activity, DNA sequence specificity, and cell type- and cell stage-specific distribution. The activity of DNA binding NF-kappa B dimers is tightly controlled by accessory proteins called I kappa B subunits of which there are also five different species currently known in vertebrates. I kappa B proteins inhibit DNA binding and prevent nuclear uptake of NF-kappa B complexes. An exception is the Bcl-3 protein which in addition can function as a transcription activating subunit in th nucleus. Other I kappa B proteins are rather involved in terminating NF-kappa B's activity in the nucleus. The intracellular events that lead to the inactivation of I kappa B, i.e. the activation of NF-kappa B, are complex. They involve phosphorylation and proteolytic reactions and seem to be controlled by the cells' redox status. Interference with the activation or activity of NF-kappa B may be beneficial in suppressing toxic/septic shock, graft-vs-host reactions, acute inflammatory reactions, acute phase response, and radiation damage. The inhibition of NF-kappa B activation by antioxidants and specific protease inhibitors may provide a pharmacological basis for interfering with these acute processes.",1994,52,4643,156,8,104,159,237,341,381,314,334,273,216
8e60816d0db257d2e20e88a5d63d73c2e21e716e,"Metallic, oxygen-deficient compounds in the Ba−La−Cu−O system, with the composition BaxLa5−xCu5O5(3−y) have been prepared in polycrystalline form. Samples withx=1 and 0.75,y>0, annealed below 900°C under reducing conditions, consist of three phases, one of them a perovskite-like mixed-valent copper compound. Upon cooling, the samples show a linear decrease in resistivity, then an approximately logarithmic increase, interpreted as a beginning of localization. Finally an abrupt decrease by up to three orders of magnitude occurs, reminiscent of the onset of percolative superconductivity. The highest onset temperature is observed in the 30 K range. It is markedly reduced by high current densities. Thus, it results partially from the percolative nature, bute possibly also from 2D superconducting fluctuations of double perovskite layers of one of the phases present.",1986,19,8937,64,0,569,840,654,503,414,320,261,224,115
27f02bf3e3fcd3435c3305c51c7491f73ae468e6,"It is shown that, particularly for terrestrial cellular telephony, the interference-suppression feature of CDMA (code division multiple access) can result in a many-fold increase in capacity over analog and even over competing digital techniques. A single-cell system, such as a hubbed satellite network, is addressed, and the basic expression for capacity is developed. The corresponding expressions for a multiple-cell system are derived. and the distribution on the number of users supportable per cell is determined. It is concluded that properly augmented and power-controlled multiple-cell CDMA promises a quantum increase in current cellular capacity. >",1991,4,3201,183,21,93,81,182,160,205,173,186,169,222
5f0a10c781c208e9da913b5d173d48b717ac0869,,1993,0,3420,199,17,21,20,27,37,38,33,83,70,84
e59f56e30f1eb7578a8e2ea1140c0765ddb1e024,"The decision support system for agrotechnology transfer (DSSAT) has been in use for the last 15 years by researchers worldwide. This package incorporates models of 16 different crops with software that facilitates the evaluation and application of the crop models for different purposes. Over the last few years, it has become increasingly difficult to maintain the DSSAT crop models, partly due to fact that there were different sets of computer code for different crops with little attention to software design at the level of crop models themselves. Thus, the DSSAT crop models have been re-designed and programmed to facilitate more efficient incorporation of new scientific advances, applications, documentation and maintenance. The basis for the new DSSAT cropping system model (CSM) design is a modular structure in which components separate along scientific discipline lines and are structured to allow easy replacement or addition of modules. It has one Soil module, a Crop Template module which can simulate different crops by defining species input files, an interface to add individual crop models if they have the same design and interface, a Weather module, and a module for dealing with competition for light and water among the soil, plants, and atmosphere. It is also designed for incorporation into various application packages, ranging from those that help researchers adapt and test the CSM to those that operate the DSSAT-CSM to simulate production over time and space for different purposes. In this paper, we describe this new DSSAT-CSM design as well as approaches used to model the primary scientific components (soil, crop, weather, and management). In addition, the paper describes data requirements and methods used for model evaluation. We provide an overview of the hundreds of published studies in which the DSSAT crop models have been used for various applications. The benefits of the new, re-designed DSSAT-CSM will provide considerable opportunities to its developers and others in the scientific community for greater cooperation in interdisciplinary research and in the application of knowledge to solve problems at field, farm, and higher levels.",2003,265,2965,270,11,23,29,50,62,78,79,123,125,161
c67818b2ce1410ba61f29e1f77412fe23c69f346,"Abstract Ant System, the first Ant Colony Optimization algorithm, showed to be a viable method for attacking hard combinatorial optimization problems. Yet, its performance, when compared to more fine-tuned algorithms, was rather poor for large instances of traditional benchmark problems like the Traveling Salesman Problem. To show that Ant Colony Optimization algorithms could be good alternatives to existing algorithms for hard combinatorial optimization problems, recent research in this area has mainly focused on the development of algorithmic variants which achieve better performance than Ant System. In this paper, we present MAX – MIN Ant System ( MM AS ), an Ant Colony Optimization algorithm derived from Ant System. MM AS differs from Ant System in several important aspects, whose usefulness we demonstrate by means of an experimental study. Additionally, we relate one of the characteristics specific to MM AS — that of using a greedier search than Ant System — to results from the search space analysis of the combinatorial optimization problems attacked in this paper. Our computational results on the Traveling Salesman Problem and the Quadratic Assignment Problem show that MM AS is currently among the best performing algorithms for these problems.",2000,156,2682,334,4,10,24,29,68,65,114,146,154,185
ca2949898b9a139f7b9129d414b3c1cafe915b61,"Abstract We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder's traffic transits. We give an overview of the system's design, which emphasizes high-speed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an `event engine' that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a `policy script interpreter' that interprets event handlers written in a specialized language used to express a site's security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.",1998,54,2798,258,5,19,23,39,57,78,108,146,175,159
d70e50a2cf1adb0a8de34e28de9cde267b931e26,"OBJECTIVE
This article defines stress and related concepts and reviews their historical development. The notion of a stress system as the effector of the stress syndrome is suggested, and its physiologic and pathophysiologic manifestations are described. A new perspective on human disease states associated with dysregulation of the stress system is provided.


DATA SOURCES
Published original articles from human and animal studies and selected reviews. Literature was surveyed utilizing MEDLINE and the Index Medicus.


STUDY SELECTION
Original articles from the basic science and human literature consisted entirely of controlled studies based on verified methodologies and, with the exception of the most recent studies, replicated by more than one laboratory. Many of the basic science and clinical studies had been conducted in our own laboratories and clinical research units. Reviews cited were written by acknowledged leaders in the fields of neurobiology, endocrinology, and behavior.


DATA EXTRACTION
Independent extraction and cross-referencing by the authors.


DATA SYNTHESIS
Stress and related concepts can be traced as far back as written science and medicine. The stress system coordinates the generalized stress response, which takes place when a stressor of any kind exceeds a threshold. The main components of the stress system are the corticotropin-releasing hormone and locus ceruleus-norepinephrine/autonomic systems and their peripheral effectors, the pituitary-adrenal axis, and the limbs of the autonomic system. Activation of the stress system leads to behavioral and peripheral changes that improve the ability of the organism to adjust homeostasis and increase its chances for survival. There has been an exponential increase in knowledge regarding the interactions among the components of the stress system and between the stress system and other brain elements involved in the regulation of emotion, cognitive function, and behavior, as well as with the axes responsible for reproduction, growth, and immunity. This new knowledge has allowed association of stress system dysfunction, characterized by sustained hyperactivity and/or hypoactivity, to various pathophysiologic states that cut across the traditional boundaries of medical disciplines. These include a range of psychiatric, endocrine, and inflammatory disorders and/or susceptibility to such disorders.


CONCLUSIONS
We hope that knowledge from apparently disparate fields of science and medicine integrated into a working theoretical framework will allow generation and testing of new hypotheses on the pathophysiology and diagnosis of, and therapy for, a variety of human illnesses reflecting systematic alterations in the principal effectors of the generalized stress response. We predict that pharmacologic agents capable of altering the central apparatus that governs the stress response will be useful in the treatment of many of these illnesses.",1992,130,3662,126,1,27,41,62,73,76,93,88,113,108
1ea0d689587486473b802ec166d7c052041421d7,"Vasculature O.U. Scremin, Cerebral Vascular System. Spinal Cord and Peripheral Nervous System C. Molander and G. Grant, Spinal Cord Cytoarchitecture. A. Ribeiro-da-Silva, Substantia Gelantinosa of Spinal Cord. G. Grant, Primary Afferent Projections to the Spinal Cord. D.J. Tracey, Ascending and Descending Pathways in the Spinal Cord. G. Gabella, Autonomic Nervous System. Brainstem and Cerebellum C.B. Saper, CentralAutonomic System. G. Holstege, The Basic, Somatic, and Emotional Components of the Motor System in Mammals. B.E. Jones, Reticular Formation: Cytoarchitecture, Transmitters, and Projections. A.J. Beitz, Periaqueductal Gray. G. Aston-Jones, M.T. Shipley, and R. Grzanna, The Locus Coeruleus, A5 and A7 Noradrenergic Cell Groups. J.H. Fallon and S.E. Loughlin, Substantia Nigra. J.B. Travers, Oromotor Nuclei. G. Holstege, B.F.M. Blok, and G.J. ter Horst, Brain Stem Systems Involved in the Blink Reflex, Feeding Mechanisms, and Micturition. T.J.H. Ruigrok and F. Cella, Precerebellar Nuclei and Red Nucleus. J. Voogd, Cerebellum. Forebrain R.B. Simerly, Anatomical Substrates of Hypothalamic Integration. W.E. Armstrong, Hypothalamic Supraoptic and Paraventricular Nuclei. B.J. Oldfield and M.J. McKinley, Circumventricular Organs. R.L. Jakab and C. Leranth, Septum. D.G. Amaral and M.P. Witter, Hippocampal Formation. G.F. Alheid, J.S. de Olmos, and C.A. Beltramino, Amygdala and Extended Amygdala. L. Heimer, D.S. Zahm, and G.F. Alheid, Basal Ganglia. J.L. Price, Thalamus. K. Zilles and A. Wree, Cortex: Areal and Laminar Structure. Sensory Systems D.J. Tracey and P.M.E. Waite, Somatosensory System. P.M.E. Waite and D.J. Tracey, Trigeminal Sensory System. W.D. Willis, K.N. Westlund, and S.M. Carlton, Pain. R. Norgren, Gustatory System. J.A. Rubertone, W.R. Mehler, and J. Voogd, The Vestibular Nuclear Complex. W.R. Webster, Auditory System. A.J. Sefton and B. Dreher, Visual System. M.T. Shipley, J.H. McLean, and M. Ennis, Olfactory System. Neurotransmitters G. Halliday, A. Harding, and G. Paxinos, Serotonin and Tachykinin Systems. S.E. Loughlin, F.M. Leslie, and J.H. Fallon, Endogenous Opioid Systems. L.L. Butcher, Cholinergic Neurons and Networks. O.P. Ottersen, O.P. Hjelle, K.K. Osen, and J.H. Laake, Amino Acid Transmitters. Development S.A. Bayer and J. Altman, Neurogenesis and Neuronal Migration. S.A. Bayer and J. Altman, Principles of Neurogenesis, Neuronal Migration, and Neural Circuit Formation. Subject Index.",1985,0,4358,106,0,2,23,20,20,29,33,35,26,24
8098c2189725d6a27407249376d1ae416fc31005,1 Engine Types and Their Operations 2 Engine Design and Operating Parameters 3 Thermochemistry of Fuel-Air Mixtures 4 Properties of Working Fluids 5 Ideal Models of Engine Cycles 6 Gas Exchange Processes 7 SI Engine Fuel Metering and Manifold Phenomena 8 Charge Motion within the Cylinder 9 Combustion in Ignition Engines 10 Combustion in Compression Ignition Engines 11 Pollutant Formation and Control 12 Engine Heat Transfer 13 Engine Friction and Lubrication 14 Modeling Real Engine Flow and Combustion Processes 15 Engine Operating Characteristics Appendixes,1988,0,14034,1013,0,0,0,0,0,0,0,0,0,0
f7216b952af5bb89b11039ea5a5ebc0edf2f45ca,,2001,0,2801,305,1,21,27,42,65,79,119,111,133,153
c54d65d53d7ada275d34e1a7c53643ff0c82eb93,"Abstract The increasing industrialization and motorization of the world has led to a steep rise for the demand of petroleum-based fuels. Petroleum-based fuels are obtained from limited reserves. These finite reserves are highly concentrated in certain regions of the world. Therefore, those countries not having these resources are facing energy/foreign exchange crisis, mainly due to the import of crude petroleum. Hence, it is necessary to look for alternative fuels which can be produced from resources available locally within the country such as alcohol, biodiesel, vegetable oils etc. This paper reviews the production, characterization and current statuses of vegetable oil and biodiesel as well as the experimental research work carried out in various countries. This paper touches upon well-to-wheel greenhouse gas emissions, well-to-wheel efficiencies, fuel versatility, infrastructure, availability, economics, engine performance and emissions, effect on wear, lubricating oil etc. Ethanol is also an attractive alternative fuel because it is a renewable bio-based resource and it is oxygenated, thereby providing the potential to reduce particulate emissions in compression-ignition engines. In this review, the properties and specifications of ethanol blended with diesel and gasoline fuel are also discussed. Special emphasis is placed on the factors critical to the potential commercial use of these blends. The effect of the fuel on engine performance and emissions (SI as well as compression ignition (CI) engines), and material compatibility is also considered. Biodiesel is methyl or ethyl ester of fatty acid made from virgin or used vegetable oils (both edible and non-edible) and animal fat. The main resources for biodiesel production can be non-edible oils obtained from plant species such as Jatropha curcas (Ratanjyot), Pongamia pinnata (Karanj), Calophyllum inophyllum (Nagchampa), Hevca brasiliensis (Rubber) etc. Biodiesel can be blended in any proportion with mineral diesel to create a biodiesel blend or can be used in its pure form. Just like petroleum diesel, biodiesel operates in compression ignition (diesel) engine, and essentially require very little or no engine modifications because biodiesel has properties similar to mineral diesel. It can be stored just like mineral diesel and hence does not require separate infrastructure. The use of biodiesel in conventional diesel engines result in substantial reduction in emission of unburned hydrocarbons, carbon monoxide and particulate. This review focuses on performance and emission of biodiesel in CI engines, combustion analysis, wear performance on long-term engine usage, and economic feasibility.",2007,97,2763,89,5,44,115,145,190,170,250,224,240,240
da65f4970806ab2724947247e8aa498203443f13,"Preface Preface to the Second Edition Preface to the First Edition 1: Introduction 2: Combustion and Thermochemistry 3: Introduction to Mass Transfer 4: Chemical Kinetics 5: Some Important Chemical Mechanisms 6: Coupling Chemical and Thermal Analyses of Reacting Systems 7: Simplifed Conversation Equations for Reacting Flows 8: Laminar Premixed Flames 9: Laminar Diffusion Flames 10: Droplet Evaporation and Burning 11: Introduction to Turbulent Flows 12: Turbulent Premixed Flames 13: Turbulent Nonpremixed Flames 14: Burning of Solids 15: Pollutant Emissions 16: Detonations Appendix A: Selected Thermodynamic Propertiesof Gases Comprising C-H-O-N System Appendix B: Fuel Properties Appendix C: Selected Properties of Air, Nitrogen, and Oxygen Appendix D: Diffusion Coefficients and Methodology for their Estimation Appendix E: Generalized Newton's Method for the Solution of Nonlinear Equations Appendix F: Computer Codes for Equilibrium Products of Hydrocarbon-Air Combustion",2000,0,2105,187,12,30,26,33,55,62,63,74,68,96
3e0725a32c83ee06419edafccacd6f3768ab5da7,"[1] We present a global tabulation of black carbon (BC) and primary organic carbon (OC) particles emitted from combustion. We include emissions from fossil fuels, biofuels, open biomass burning, and burning of urban waste. Previous ‘‘bottom-up’’ inventories of black and organic carbon have assigned emission factors on the basis of fuel type and economic sector alone. Because emission rates are highly dependent on combustion practice, we consider combinations of fuel, combustion type, and emission controls and their prevalence on a regional basis. Central estimates of global annual emissions are 8.0 Tg for black carbon and 33.9 Tg for organic carbon. These estimates are lower than previously published estimates by 25–35%. The present inventory is based on 1996 fuel-use data, updating previous estimates that have relied on consumption data from 1984. An offset between decreased emission factors and increased energy use since the base year of the previous inventory prevents the difference between this work and previous inventories from being greater. The contributions of fossil fuel, biofuel, and open burning are estimated as 38%, 20%, and 42%, respectively, for BC, and 7%, 19%, and 74%, respectively, for OC. We present a bottom-up estimate of uncertainties in source strength by combining uncertainties in particulate matter emission factors, emission characterization, and fuel use. The total uncertainties are about a factor of 2, with uncertainty ranges of 4.3–22 Tg/yr for BC and 17–77 Tg/yr for OC. Low-technology combustion contributes greatly to both the emissions and the uncertainties. Advances in emission characterization for small residential, industrial, and mobile sources and topdown analysis combining field measurements and transport modeling with iterative inventory development will be required to reduce the uncertainties further. INDEX TERMS: 0305 Atmospheric Composition and Structure: Aerosols and particles (0345, 4801); 0322 Atmospheric Composition and Structure: Constituent sources and sinks; 0345 Atmospheric Composition and Structure: Pollution—urban and regional (0305); 0360 Atmospheric Composition and Structure: Transmission and scattering of radiation; 0365 Atmospheric Composition and Structure: Troposphere—composition and chemistry; KEYWORDS: emission, black carbon, organic carbon, fossil fuel, biofuel, biomass burning",2004,394,2042,178,16,63,49,72,80,122,98,160,145,154
6019b33d51af92a113766ee73ba4d1eb53d9e50e,INTRODUCTION ....................................................................................................................................... 465 LIGNIN AS A SUBSTRATE ............................................................................................................................... 466 MICROBIOLOGY OF LIGNIN BIODEGRADATI ON ........................................................................... 468 Anaerobic Conditions ............................................................................................................................... 469 Aerobic Conditions ................................................................................................................................... 469 LIGNIN DEGRADATION BY WHITEROT FUNGI ............................................................................. 471 Physiology .......................................................................................................................................................... 472 Biochemistry ............................................................................................................................................ 475 Genetics ..............................................................................................................................................................486 Molecular Biology .................................................................................................................................... 489 CONCLUSIONS AND RECOMMENDATIONS ..................................................................................... 491 ENZYMATIC “COMBUSTION” ........................................................................................................................ 493,1987,0,2528,129,2,16,45,73,64,69,74,80,54,51
5fbfddb1efed4355b620f689e441619e3008d945,"Our current understanding of the mechanisms and rate parameters for the gas-phase reactions of nitrogen compounds that are applicable to combustion-generated air pollution is discussed and illustrated by comparison of results from detailed kinetics calculations with experimental data. In particular, the mechanisms and rate parameters for thermal and prompt NO formation, for fuel nitrogen conversion, for the Thermal De-NOx and RAPRENOx processes, and for NO2 and N2O formation and removal processes are considered. Sensitivity and rate-of-production analyses are applied in the calculations to determine which elementary reactions are of greatest importance in the nitrogen conversion process. Available information on the rate parameters for these important elementary reactions has been surveyed, and recommendations for the rate coefficients for these reactions are provided. The principal areas of uncertainty in nitrogen reaction mechanisms and rate parameters are outlined.",1989,94,2522,91,0,12,31,49,63,89,77,110,87,107
02d2616821d5a2fb99fa8de1f3dcc72e2fe9f3c3,"Principles of mathematical models as tools in engineering and science are discussed in relation to turbulent combustion modeling. A model is presented for the rate of combustion which takes into account the intermittent appearance of reacting species in turbulent flames. This model relates the rate of combustion to the rate of dissipation of eddies and expresses the rate of reaction by the mean concentration of a reacting specie, the turbulent kinetic energy and the rate of dissipation of this energy. The essential features of this model are that it does not call for predictions of fluctuations of reacting species and that it is applicable to premixed as well as diffusion flames. The combustion model is tested on both premixed and diffusion flames with good results. Special attention is given to soot formation and combustion in turbulent flames. Predictions are made for two C 2 H 2 turbulent diffusion flames by incorporating both the above combustion model and the model for the rate of soot formation developed by Tesner et al., as well as previous observations by Magnussen concerning the behavior of soot in turbulent flames. The predicted results are in close agreement with the experimental data. All predictions in the present paper have been made by modeling turbulence by the k -∈ model. Buoyancy is taken into consideration in the momentum equations. Effects of terms containing density fluctuations have not been included.",1977,6,2443,119,1,0,2,1,1,4,3,6,7,5
22a8709b338bf1eddcfebddc1c16982951d1f4b5,Review of Chemical Thermodynamics. Review of Chemical Kinetics. Conservation Equations for Multi--Component Reacting Systems. Rankine--Hugoniot Relations of Detonation and Deflagration Waves of Premixed Gases. Premixed Laminar Flames. Diffusion Flames and Combustion of a Single Liquid Fuel Droplet. Turbulent Flames. Turbulent Reacting Flows with Premixed Reactants. Chemically Reacting Boundary--Layer Flows. Ignition. Appendix. Index.,1986,0,1995,141,1,2,6,6,11,14,15,23,21,24
7be9620770607f4f08ec341438bdc52cff53f79b,,2009,0,1388,55,3,15,65,95,114,148,207,185,173,136
9a919541129b3dbb88dd3f8e0244779271701b54,"Properties of biomass relevant to combustion are briefly reviewed. The compositions of biomass among fuel types are variable, especially with respect to inorganic constituents important to the critical problems of fouling and slagging. Alkali and alkaline earth metals, in combination with other fuel elements such as silica and sulfur, and facilitated by the presence of chlorine, are responsible for many undesirable reactions in combustion furnaces and power boilers. Reductions in the concentrations of alkali metals and chlorine, created by leaching the elements from the fuel with water, yield remarkable improvements in ash fusion temperatures and confirm much of what is suggested regarding the nature of fouling by biomass fuels. Other influences of biomass composition are observed for the rates of combustion and pollutant emissions. Standardized engineering practices setting out protocols of analysis and interpretation may prove useful in reducing unfavorable impacts and industry costs, and further development is encouraged.",1998,40,1644,113,2,0,4,5,6,10,17,15,33,31
edb07725544317e529582309c7ce05cf9e0349c2,"In this article, the status of fat and oil derived diesel fuels with respect to fuel properties, engine performance, and emissions is reviewed. The fuels considered are primarily the methyl esters of fatty acids derived from a variety of vegetable oils and animal fats, and referred to as biodiesel. The major obstacle to widespread use of biodiesel is the high cost relative to petroleum. Economics of biodiesel production are discussed, and it is concluded that the price of the feedstock fat or oil is the major factor determining biodiesel price.Biodiesel is completely miscible with petroleum diesel fuel, and is generally tested as a blend. The use of biodiesel in neat or blended form has no effect on the energy based engine fuel economy. The lubricity of these fuels is superior to conventional diesel, and this property is imparted to blends at levels above 20 vol%. Emissions of PM can be reduced dramatically through use of biodiesel in engines that are not high lube oil emitters. Emissions of NOx increase significantly for both neat and blended fuels in both two- and four-stroke engines. The increase may be lower in newer, lower NOx emitting four-strokes, but additional data are needed to confirm this conclusion. A discussion of available data on unregulated air toxins is presented, and it is concluded that definitive studies have yet to be performed in this area. A detailed discussion of important biodiesel properties and recommendations for future research is presented. Among the most important recommendations is the need for all future studies to employ biodiesel of well-known composition and purity, and to report detailed analyses. The purity levels necessary for achieving adequate engine endurance, compatibility with coatings and elastomers, cold flow properties, stability, and emissions performance must be better defined.",1998,16,1819,55,0,4,12,9,8,26,18,44,44,84
07acf3c7e635c7970894192da585672e2bb51c42,"The awareness of the increase in greenhouse gas emissions has resulted in the development of new technologies with lower emissions and technologies that can accommodate capture and sequestration of carbon dioxide. For existing coal-fired combustion plants there are two main options for CO2 capture: removal of nitrogen from flue gases or removal of nitrogen from air before combustion to obtain a gas stream ready for geo-sequestration. In oxy-fuel combustion, fuel is combusted in pure oxygen rather than air. This technology recycles flue gas back into the furnace to control temperature and makeup the volume of the missing N2 to ensure there is sufficient gas to maintain the temperature and heat flux profiles in the boiler. A further advantage of the technology revealed in pilot-scale tests is substantially reduced NOx emissions. For coal-fired combustion, the technology was suggested in the eighties, however, recent developments have led to a renewed interest in the technology. This paper provides a comprehensive review of research that has been undertaken, gives the status of the technology development and assessments providing comparisons with other power generation options, and suggests research needs.",2005,42,1374,41,1,4,9,23,41,84,138,121,146,116
af10426046ca1410ed652a64ca6c1976ac1e519f,"This document serves as an update of the North American Society for Pediatric Gastroenterology, Hepatology, and Nutrition (NASPGHAN) and the European Society for Pediatric Gastroenterology, Hepatology, and Nutrition (ESPGHAN) 2009 clinical guidelines for the diagnosis and management of gastroesophageal reflux disease (GERD) in infants and children and is intended to be applied in daily practice and as a basis for clinical trials. Eight clinical questions addressing diagnostic, therapeutic and prognostic topics were formulated. A systematic literature search was performed from October 1, 2008 (if the question was addressed by 2009 guidelines) or from inception to June 1, 2015 using Embase, MEDLINE, the Cochrane Database of Systematic Reviews and the Cochrane Central Register of Controlled Clinical Trials. The approach of the Grading of Recommendations Assessment, Development and Evaluation (GRADE) was applied to define and prioritize outcomes. For therapeutic questions, the quality of evidence was also assessed using GRADE. Grading the quality of evidence for other questions was performed according to the Quality Assessment of Studies of Diagnostic Accuracy (QUADAS) and Quality in Prognostic Studies (QUIPS) tools. During a three-day consensus meeting, all recommendations were discussed and finalized. In cases where no randomized controlled trials (RCT; therapeutic questions) or diagnostic accuracy studies were available to support the recommendations, expert opinion was used. The group members voted on each recommendation, using the nominal voting technique.With this approach, recommendations regarding evaluation and management of infants and children with GERD to standardize and improve quality of care were formulated. Additionally, two algorithms were developed, one for infants < 12 months of age and the other for older infants and children.",2009,963,407,2,0,0,0,0,0,0,1,62,96,55
e8d197a8bc142117006fcc060eed75af52998dec,"The discovery of a series of genetic and serological markers associated with disease susceptibility and phenotype in inflammatory bowel disease has led to the prospect of an integrated classification system involving clinical, serological and genetic parameters. The Working Party has reviewed current clinical classification systems in Crohn's disease, ulcerative colitis and indeterminate colitis, and provided recommendations for clinical classification in practice. Progress with respect to integrating serological and genetic markers has been examined in detail, and the implications are discussed. While an integrated system is not proposed for clinical use at present, the introduction of a widely acceptable clinical subclassification is strongly advocated, which would allow detailed correlations among serotype, genotype and clinical phenotype to be examined and confirmed in independent cohorts of patients and, thereby, provide a vital foundation for future work.",2005,347,2653,81,3,30,80,114,119,155,156,152,172,201
a80b9786319a6e16a904f103d5daf965f4df3335,"This document is the first update of the American College of Gastroenterology (ACG) colorectal cancer (CRC) screening recommendations since 2000. The CRC screening tests are now grouped into cancer prevention tests and cancer detection tests. Colonoscopy every 10 years, beginning at age 50, remains the preferred CRC screening strategy. It is recognized that colonoscopy is not available in every clinical setting because of economic limitations. It is also realized that not all eligible persons are willing to undergo colonoscopy for screening purposes. In these cases, patients should be offered an alternative CRC prevention test (flexible sigmoidoscopy every 5–10 years, or a computed tomography (CT) colonography every 5 years) or a cancer detection test (fecal immunochemical test for blood, FIT).",2009,155,1314,17,21,88,110,150,153,163,135,133,120,77
126913d7e19bf80ea31f20172cd7629e8728a287,"Research on hepatic encephalopathy is hampered by the imprecise definition of this disabling complication of liver disease. Under this light, the Organisation Mondiale de Gastroentérologie commissioned a Working Party to reach a consensus in this area and to present it at the 11th World Congress of Gastroenterology in Vienna (1998). The Working Party continued its work thereafter and now present their final report. In summary, the Working Party has suggested a modification of current nomenclature for clinical diagnosis of hepatic encephalopathy; proposed guidelines for the performance of future clinical trials in hepatic encephalopathy; and felt the need for a large study to redefine neuropsychiatric abnormalities in liver disease, which would allow the diagnosis of minimal (subclinical) encephalopathy to be made on firm statistical grounds. In the interim, it proposes the use of a psychometric hepatic encephalopathy score, based on the result of 5 neuropsychologic tests. Finally, the need for a careful evaluation of the newer neuroimaging modalities for the diagnosis of hepatic encephalopathy was stressed.",2002,35,1822,42,6,32,30,50,50,60,78,79,112,123
01e81325095bfe4e9d371941314b9a0974532749,"Helicobacter pylori (H. pylori) remains a prevalent, worldwide, chronic infection. Though the prevalence of this infection appears to be decreasing in many parts of the world, H. pylori remains an important factor linked to the development of peptic ulcer disease, gastric malignanc and dyspeptic symptoms. Whether to test for H. pylori in patients with functional dyspepsia, gastroesophageal reflux disease (GERD), patients taking nonsteroidal antiinflammatory drugs, with iron deficiency anemia, or who are at greater risk of developing gastric cancer remains controversial. H. pylori can be diagnosed by endoscopic or nonendoscopic methods. A variety of factors including the need for endoscopy, pretest probability of infection, local availability, and an understanding of the performance characteristics and cost of the individual tests influences choice of evaluation in a given patient. Testing to prove eradication should be performed in patients who receive treatment of H. pylori for peptic ulcer disease, individuals with persistent dyspeptic symptoms despite the test-and-treat strategy, those with H. pylori-associated MALT lymphoma, and individuals who have undergone resection of early gastric cancer. Recent studies suggest that eradication rates achieved by first-line treatment with a proton pump inhibitor (PPI), clarithromycin, and amoxicillin have decreased to 70–85%, in part due to increasing clarithromycin resistance. Eradication rates may also be lower with 7 versus 14-day regimens. Bismuth-containing quadruple regimens for 7–14 days are another first-line treatment option. Sequential therapy for 10 days has shown promise in Europe but requires validation in North America. The most commonly used salvage regimen in patients with persistent H. pylori is bismuth quadruple therapy. Recent data suggest that a PPI, levofloxacin, and amoxicillin for 10 days is more effective and better tolerated than bismuth quadruple therapy for persistent H. pylori infection, though this needs to be validated in the United States.",2007,217,1255,50,9,45,66,90,102,114,125,128,128,138
8eb2a10b79dbbba7cdf2b0ffa7acf30e2952baa6,Discusses the performativity of economics and proposes theoretical directions to study it from a sociological perspective.,2006,92,998,115,3,11,17,36,56,51,70,81,96,80
8f966e607f2bbc89b1c68ce18eb503bf8d8aa59d,This literature review concerns the impact of immigration on the economy of the host country focusing on the experience of the United States. The emphasis is on the period from the 1970s to the 1990s. The author shows that research earlier in this period generally concluded that the economic effects of immigration were positive but that more recent research on later migrations have generally concluded that immigration may be having an adverse effect on the earnings of native unskilled workers and be placing an increased burden on welfare programs. The importance of such economic analysis for the formulation of appropriate migration policies is stressed.,1994,113,2491,196,2,13,33,47,55,60,60,79,93,111
b6d7e6a2763da443c3386edfe70bf46c07059da1,"There is now clear scientific evidence that emissions from economic activity, particularly the burning of fossil fuels for energy, are causing changes to the Earth´s climate. A sound understanding of the economics of climate change is needed in order to underpin an effective global response to this challenge. The Stern Review is an independent, rigourous and comprehensive analysis of the economic aspects of this crucial issue. It has been conducted by Sir Nicholas Stern, Head of the UK Government Economic Service, and a former Chief Economist of the World Bank. The Economics of Climate Change will be invaluable for all students of the economics and policy implications of climate change, and economists, scientists and policy makers involved in all aspects of climate change.",2007,0,8384,711,241,485,697,771,822,819,772,725,576,542
c04677651cb0545ccf080abeb0958fc907f70c4a,"THE new institutional economics is preoccupied with the origins, incidence, and ramifications of transaction costs. Indeed, if transaction costs are negligible, the organization of economic activity is irrelevant, since any advantages one mode of organization appears to hold over another will simply be eliminated by costless contracting. But despite the growing realization that transaction costs are central to the study of economics,' skeptics remain. Stanley Fischer's complaint is typical: ""Transaction costs have a well-deserved bad name as a theoretical device ... [partly] because there is a suspicion that almost anything can be rationalized by invoking suitably specified transaction costs.""2 Put differently, there are too many degrees of freedom; the concept wants for definition.",1979,1,8962,477,0,8,8,16,33,25,30,40,46,50
a6462e546ad432ddbe3e7d9e5854f029204f57e6,"This paper provides an introduction and “user guide” to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a “quasi-experimental” design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD. ( JEL C21, C31)",2009,202,3986,453,39,94,159,194,264,298,368,466,466,407
23908e4166066894e2ace9c76cb6a375d99ad8dc,"Economic geography in an era of global competition poses a paradox. In theory, location should no longer be a source of competitive advantage. Open global markets, rapid transportation, and high-speed communications should allow any company to source any thing from any place at any time. But in practice, Michael Porter demonstrates, location remains central to competition. Today's economic map of the world is characterized by what Porter calls clusters: critical masses in one place of linked industries and institutions--from suppliers to universities to government agencies--that enjoy unusual competitive success in a particular field. The most famous example are found in Silicon Valley and Hollywood, but clusters dot the world's landscape. Porter explains how clusters affect competition in three broad ways: first, by increasing the productivity of companies based in the area; second, by driving the direction and pace of innovation; and third, by stimulating the formation of new businesses within the cluster. Geographic, cultural, and institutional proximity provides companies with special access, closer relationships, better information, powerful incentives, and other advantages that are difficult to tap from a distance. The more complex, knowledge-based, and dynamic the world economy becomes, the more this is true. Competitive advantage lies increasingly in local things--knowledge, relationships, and motivation--that distant rivals cannot replicate. Porter challenges the conventional wisdom about how companies should be configured, how institutions such as universities can contribute to competitive success, and how governments can promote economic development and prosperity.",1998,0,8267,651,3,35,76,103,152,222,253,268,284,361
72910077a29caf411dbb03148997c72b47e65ab0,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",1993,109,6825,560,112,128,131,131,133,161,173,189,199,187
6732435ab6e0eb4e0d14662cf724f3be54807a35,"This paper considers how identity, a person's sense of self, affects economic outcomes. We incorporate the psychology and sociology of identity into an economic model of behavior. In the utility function we propose, identity is associated with different social categories and how people in these categories should behave. We then construct a simple game-theoretic model showing how identity can affect individual interactions. The paper adapts these models to gender discrimination in the workplace, the economics of poverty and social exclusion, and the household division of labor. In each case, the inclusion of identity substantively changes conclusions of previous economic analysis.",2000,144,4591,414,18,28,52,51,91,119,99,136,180,176
6c8409565460774b18eb020832343e647d094573,"Bringing together leaf trait data spanning 2,548 species and 175 sites we describe, for the first time at global scale, a universal spectrum of leaf economics consisting of key chemical, structural and physiological properties. The spectrum runs from quick to slow return on investments of nutrients and dry mass in leaves, and operates largely independently of growth form, plant functional type or biome. Categories along the spectrum would, in general, describe leaf economic variation at the global scale better than plant functional types, because functional types overlap substantially in their leaf traits. Overall, modulation of leaf traits and trait relationships by climate is surprisingly modest, although some striking and significant patterns can be seen. Reliable quantification of the leaf economics spectrum and its interaction with climate will prove valuable for modelling nutrient fluxes and vegetation boundaries under changing land-use and climate.",2004,69,5301,299,13,58,101,114,134,158,197,253,283,327
11040e39cb84397ac26f063058fefcb394da9400,"This paper examines the progressive development of the new institutional economics over the past quarter century. It begins by distinguishing four levels of social analysis, with special emphasis on the institutional environment and the institutions of governance. It then turns to some of the good ideas out of which the NIE works: the description of human actors, feasibility, firms as governance structures, and operationalization. Applications, including privatization, are briefly discussed. Its empirical successes, public policy applications, and other accomplishments notwithstanding, there is a vast amount of unfinished business.",2000,138,4906,329,9,36,78,90,111,137,114,176,201,239
35cb80cc9825527ce57f4f3c36c2862d73a0ee62,"Introduction General and theoretical issues in the economics of the performing arts: The economic structure of performing arts firms the nature of consumer demand for the performing arts structure, conduct and performance in the performing arts industry the positive theory of assistance empirical characteristics of the performing arts: characteristics of companies characteristics of audiences characteristics characteristics of performing arts industries the pattern of assistnace Public policy and the performing arts arguments for public assistnace market efficiency considerations arguments for public assistnace: equity and merit good considerations objectives of performing arts policy forms of assistance levels of assistance allocation of assistance alocation of assistance conclusion: some policy issues in persspective appendices notes list of references.",1979,0,232,7,0,0,1,1,3,0,1,4,4,3
674aae795b11a895e5fc2699a5b97fcb725cedfa,"The work cited by the Nobel committee was done jointly with Amos Tversky (1937-1996) during a long and unusually close collaboration. Together, we explored the psychology of intuitive beliefs and choices and examined their bounded rationality. Herbert A. Simon (1955, 1979) had proposed much earlier that decision makers should be viewed as boundedly rational, and had offered a model in which utility maximization was replaced by satisficing. Our research attempted to obtain a map of bounded rationality, by exploring the systematic biases that separate the beliefs that people have and the choices they make from the optimal beliefs and choices assumed in rational-agent models. The",2003,189,4247,301,4,45,62,84,139,153,181,176,219,227
30c40f8c16848fa14540ef47afe0cc8b381b1ff9,"People like to help those who are helping them and to hurt those who are hurting them. Outcomes rejecting such motivations are called fairness equilibria. Outcomes are mutual-max when each person maximizes the other's material payoffs, and mutual-min when each person minimizes the other's payoffs. It is shown that every mutual-max or mutual-min Nash equilibrium is a fairness equilibrium. If payoffs are small, fairness equilibria are roughly the set of mutual-max and mutual-min outcomes; if payoffs are large, fairness equilibria are roughly the set of Nash equilibria. Several economic examples are considered and possible welfare implications of fairness are explored. Copyright 1993 by American Economic Association.",1993,43,5061,298,2,6,4,23,19,37,48,104,99,136
c35be599aede0ee2ed502a2bb6e4d7ea592af36f,"The transaction cost approach to the study of economic organization regards the transaction as the basic unit of analysis and holds that an understanding of transaction cost economizing is central to the study of organizations. Applications of this approach require that transactions be dimensionalized and that alternative governance structures be described. Economizing is accomplished by assigning transactions to governance structures in a discriminating way. The approach applies both to the determination of efficient boundaries, as between firms and markets, and to the organization of internal transactions, including the design of employment relations. The approach is compared and contrasted with selected parts of the organization theory literature.",1981,60,5585,330,1,5,6,11,16,18,23,30,15,27
61d66e74e0d6973baf01ced1ddc27bc182c88bce,"The event study is an important research tool in economics and finance. The goal of an event study is to measure the effects of an economic event on the value of firms. Event study methods exploit the fact that, given rationality in the marketplace, the effects of an event will be reflected immediately in security prices. Thus the impact can be measured by examining security prices surrounding the event. In this paper event study methods are described including some of the potential complications. An example is included to illustrate the approach.",1997,41,3438,330,3,13,8,23,37,38,51,69,70,98
6cbb20a29588438b23d1545bec9736d69f450327,"What determines the size and form of redistributive programs, the extent and type of public goods provision, the burden of taxation across alternative tax bases, the size of government deficits, and the stance of monetary policy during the course of business and electoral cycles? A large and rapidly growing literature in political economics attempts to answer these questions. But so far there is little consensus on the answers and disagreement on the appropriate mode of analysis. Combining the best of three separate traditions—the theory of macroeconomic policy, public choice, and rational choice in political science—Torsten Persson and Guido Tabellini suggest a unified approach to the field. As in modern macroeconomics, individual citizens behave rationally, their preferences over economic outcomes inducing preferences over policy. As in public choice, the delegation of policy decisions to elected representatives may give rise to agency problems between voters and politicians. And, as in rational choice, political institutions shape the procedures for setting policy and electing politicians. The authors outline a common method of analysis, establish several new results, and identify the main outstanding problems.",2000,0,3757,216,15,49,94,154,169,184,193,204,261,256
d67f30652aa34fb99b5c42e67af3946f5de9afb0,,1985,10,5795,276,2,4,6,10,10,23,31,31,51,60
fc206e1723d6897dc8fcb6078efb05b9111b3b8e,"This classic text has introduced generations of students to the economic theory of consumer behaviour. Written by 2015 Nobel Laureate Angus Deaton and John Muellbauer, the book begins with a self-contained presentation of the basic theory and its use in applied econometrics. These early chapters also include elementary extensions of the theory to labour supply, durable goods, the consumption function, and rationing. The rest of the book is divided into three parts. In the first of these the authors discuss restrictions on choice and aggregation problems. The next part consists of chapters on consumer index numbers; household characteristics, demand, and household welfare comparisons; and social welfare and inequality. The last part extends the coverage of consumer behaviour to include the quality of goods and household production theory, labour supply and human capital theory, the consumption function and intertemporal choice, the demand for durable goods, and choice under uncertainty.",1980,0,3919,269,1,5,21,24,44,34,52,54,48,50
ff259e78776c738b94760ce73c044bfaf2a22f55,"This article is intended to enhance the position of stakeholder theory as an integrating theme for the business and society field. It offers an instrumental theory of stakeholder management based on a synthesis of the stakeholder concept, economic theory, behavioral science, and ethics. The core theory—that a subset of ethical principles (trust, trustworthiness, and cooperativeness) can result in significant competitive advantage—is supplemented by nine research propositions along with some research and policy implications.",1995,110,3278,283,2,7,23,26,38,31,48,42,51,55
6c3cd55660a48af4d9614c3f44ef69ce8eff0dfe,"Global climate change poses a threat to the well-being of humans and other living things through impacts on ecosystem functioning, biodiversity, capital productivity, and human health. Climate change economics attends to this issue by offering theoretical insights and empirical findings relevant to the design of policies to reduce, avoid, or adapt to climate change. This economic analysis has yielded new estimates of mitigation benefits, improved understanding of costs in the presence of various market distortions or imperfections, better tools for making policy choices under uncertainty, and alternate mechanisms for allowing flexibility in policy responses. These contributions have influenced the formulation and implementation of a range of climate change policies at the domestic and international levels.",1995,42,3086,362,0,4,0,5,6,4,6,6,2,7
d044dafc85a0b40237ae0ee40e76d00a98a62e29,"A systematic treatment of the economics of the modern firm, this book draws on the insights of a variety of areas in modern economics and other disciplines, but presents a coherent, consistent, innovative treatment of the central problems in organizations of motivating people and coordinating their activities.",1992,0,4464,177,2,23,35,55,88,99,117,146,180,189
d060e917cc7cdecb72807feea6432496c1c31a02,"The course to be followed by a motorist having a preselected destination is indicated by a plurality of markers set in the center of the path to be followed by the motorist, each set of markers having a different color to correspond to a different preselected destination.",1985,20,3398,244,2,4,15,17,20,15,30,35,31,28
23272b54be9ac651186bf19b2299f59873b0997c,"Policy makers view public sector-sponsored employment and training programs and other active labor market policies as tools for integrating the unemployed and economically disadvantaged into the work force. Few public sector programs have received such intensive scrutiny, and been subjected to so many different evaluation strategies. This chapter examines the impacts of active labor market policies, such as job training, job search assistance, and job subsidies, and the methods used to evaluate their effectiveness. Previous evaluations of policies in OECD countries indicate that these programs usually have at best a modest impact on participants' labor market prospects. But at the same time, they also indicate that there is considerable heterogeneity in the impact of these programs. For some groups, a compelling case can be made that these policies generate high rates of return, while for other groups these policies have had no impact and may have been harmful. Our discussion of the methods used to evaluate these policies has more general interest. We believe that the same issues arise generally in the social sciences and are no easier to address elsewhere. As a result, a major focus of this chapter is on the methodological lessons learned from evaluating these programs. One of the most important of these lessons is that there is no inherent method of choice for conducting program evaluations. The choice between experimental and non-experimental methods or among alternative econometric estimators should be guided by the underlying economic models, the available data, and the questions being addressed. Too much emphasis has been placed on formulating alternative econometric methods for correcting for selection bias and too little given to the quality of the underlying data. Although it is expensive, obtaining better data is the only way to solve the evaluation problem in a convincing way. However, better data are not synonymous with social experiments.",1999,199,3312,186,17,46,84,133,136,175,153,208,230,180
4f19d7f17067f3edb5ee3efaa161b2ad24876df4,"To harness the full power of computer technology, economists need to use a broad range of mathematical techniques. In this book, Kenneth Judd presents techniques from the numerical analysis and applied mathematics literatures and shows how to use them in economic analyses. The book is divided into five parts. Part I provides a general introduction. Part II presents basics from numerical analysis on R^n, including linear equations, iterative methods, optimization, nonlinear equations, approximation methods, numerical integration and differentiation, and Monte Carlo methods. Part III covers methods for dynamic problems, including finite difference methods, projection methods, and numerical dynamic programming. Part IV covers perturbation and asymptotic solution methods. Finally, Part V covers applications to dynamic equilibrium analysis, including solution methods for perfect foresight models and rational expectation models. A web site contains supplementary material including programs and answers to exercises.",1998,4,2896,319,9,30,58,83,91,102,133,143,177,160
4e77fd7c955f8fd70d9e1c6cb670fa1e38ad7157,"Part 1 Rise of science-related technology: introduction evolutionary theory in economics and technical change process innovations materials innovations product and system innovation paradigm change. Part 2 Innovations and the firms: the microeconomics of innovation success and failure, the role of marketing and user-producer networks innovation, size of firm, economies of scale and scope uncertainty, project evaluation and finance of innovation, management strategy and theory of the firm. Part 3 Macroeconomics of innovation - science, technology and economic growth globalization and multinational corporations underdevelopment and catching up. Part 4 Innovation and public policies: market failure and aspects of public support for innovation technical change, employment and skills environmental issues technological assessment. Appendix: measurement and definitions.",1975,0,4611,103,1,2,5,7,4,18,10,13,4,21
620351e3a30767e78c301a2cb193965b8268efc4,"This book, which comprises eight chapters, presents a comprehensive critical survey of the results and methods of laboratory experiments in economics. The first chapter provides an introduction to experimental economics as a whole, with the remaining chapters providing surveys by leading practitioners in areas of economics that have seen a concentration of experiments: public goods, coordination problems, bargaining, industrial organization, asset markets, auctions, and individual decision making.",1997,8,3846,67,41,75,75,115,118,186,202,219,189,236
ddbbf87a2a0ec74f9c62239ddf3abdb45afd9c7d,"N RECENT YEARS, public and professional interest in schools has been heightened by a spate of reports, many of them critical of current school policy.' These policy documents have added to persistent and long-standing concerns about the cost, effectiveness, and fairness of the current school structure, and have made schooling once again a serious public issue. As in the past, however, any renewed interest in education is likely to be short-lived, doomed to dissipate as frustration over the inability of policy to improve school practice sets in. This frustration about school policy relates directly to knowledge about the educational production process and in turn to underlying research on schools. Although the educational process has been extensively researched, clear policy prescriptions flowing from this research have been difficult to derive.2 There exists, however, a consistency to the research findings that does have an immediate application to school policy: Schools differ dramatically in ""quality,""",1986,109,3076,204,2,7,15,20,23,40,44,36,49,49
715254ac0ffa87857c00b91b1b4e6464a5c56ee6,The identification of sellers and the discovery of their prices is given as an example of the role of the search for information in economic life.,1961,0,3948,133,1,0,0,1,1,0,2,3,1,6
dfc06f7f7eb83d2c0e2a1d67ecb95b520a406843,"We examine the economics of financing small business in private equity and debt markets. Firms are viewed through a financial growth cycle paradigm in which different capital structures are optimal at different points in the cycle. We show the sources of small business finance, and how capital structure varies with firm size and age. The interconnectedness of small firm finance is discussed along with the impact of the macroeconomic environment. We also analyze a number of research and policy issues, review the literature, and suggest topics for future research.",1998,265,2216,302,1,8,12,47,44,40,65,55,73,79
e291333a380e1de8c758e1e14d2dccaf414899e0,"This paper summarizes evidence on the effects of early environments on child, adolescent, and adult achievement. Life cycle skill formation is a dynamic process in which early inputs strongly affect the productivity of later inputs.",2006,43,2795,127,12,29,68,96,114,122,173,213,193,211
a0c06ad17e0a731ead51d0853992f6b06fb3a4b5,1. The Original Affluent Society2. The Domestic Mode of Production: The Structure of Underproduction3. The Domestic Mode of Production: Intensification of Production4. The Spirit of the Gift5. On the Sociology of Primitive Exchange6. Exchange Value and the Diplomacy of Primitive Trade,1972,0,3546,132,0,5,4,8,10,23,20,24,28,22
c3b220081c6543f265edae74568e47dcf6f270c3,"This paper shows that reciprocity has powerful implications for many economic domains. It is an important determinant in the enforcement of contracts and social norms and enhances the possibilities of collective action greatly. Reciprocity may render the provision of explicit incentive inefficient because the incentives may crowd out voluntary co-operation. It strongly limits the effects to competition in markets with incomplete contracts and gives rise to noncompetitive wage differences. Finally, reciprocity it is also a strong force contributing to the existence of incomplete contracts.",2000,187,3173,149,32,57,75,101,96,126,146,167,184,166
050443122604df9197368e874bd80bc8180e6e31,Manufacturing is undergoing a revolution. The mass production model is being replaced by a vision of a flexible multiproduct firm that emphasizes quality and speedy response to market conditions while utilizing technologically advanced equipment and new forms of organization. The authors' optimizing model of the firm generates many of the observed patterns that mark modern manufacturing. Central to the authors' results is a method of handling optimization and comparative statics problems that requires neither differentiability nor convexity. Copyright 1990 by American Economic Association.,1990,9,2847,136,4,12,10,24,28,32,45,45,60,54
3ee2a3694161433364273b60dadd5b061b5caa9b,"Undoubtedly one of the highlights of the 1999 Conference was the plenary session in which Professors David Held and Mahdi Elmandjra came together to discuss the theme of ‘“Globalization”: Democracy and Diversity’. The Conference also witnessed the launch of Global Transformations (Polity Press, 1999), at which David Held was joined by two of his three coauthors, Professor Anthony McGrew and Dr Jonathan Perraton. Global Transformations is the product of almost a decade’s work by a research team (based at the Open University and supported by the ESRC) who have produced what James. N. Rosenau has called ‘the definitive work on globalization’. It is a study which not only synthesises an extraordinary amount of information from research on globalization in a range of social science disciplines, but also makes its own distinctive contribution to our understanding of the complex range of forces which are reshaping the world order. We are delighted to be able to reproduce here an ‘executive summary’ of Global Transformations that summarises the major findings of this 500-page survey in just six thousand words.",1999,1,2768,111,8,43,76,123,179,160,157,160,163,172
f8efc9c04fcc543943552f5e168b36318cea3e1b,"Wood performs several essential functions in plants, including mechanically supporting aboveground tissue, storing water and other resources, and transporting sap. Woody tissues are likely to face physiological, structural and defensive trade-offs. How a plant optimizes among these competing functions can have major ecological implications, which have been under-appreciated by ecologists compared to the focus they have given to leaf function. To draw together our current understanding of wood function, we identify and collate data on the major wood functional traits, including the largest wood density database to date (8412 taxa), mechanical strength measures and anatomical features, as well as clade-specific features such as secondary chemistry. We then show how wood traits are related to one another, highlighting functional trade-offs, and to ecological and demographic plant features (growth form, growth rate, latitude, ecological setting). We suggest that, similar to the manifold that tree species leaf traits cluster around the 'leaf economics spectrum', a similar 'wood economics spectrum' may be defined. We then discuss the biogeography, evolution and biogeochemistry of the spectrum, and conclude by pointing out the major gaps in our current knowledge of wood functional traits.",2009,120,2046,166,9,45,67,110,108,146,158,212,201,245
7fbc2485837aa460ce48b90d3869a5d2f06ae306,"This study analyzes organization of economic activity within and between markets and hierarchies. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains. Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration. Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition. The analysis explains why primary work groups of the peer group and simple hierarchy types arise. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such as bounded rationality and opportunism, occur. Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The analysis attempts better to assess the employment relation in circumstances where workers acquire, during the course of the employment, significant job-specific skills and knowledge. The study compares alternative labor-contracting modes and demonstrates that collective organization is helpful in enhancing the acquisition of idiosyncratic knowledge and skills by the work force. The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies. Discusses the market structure in relation to technical and organizational innovation. The study proposes a systems approach to the innovation process. Its purpose is to permit the realization of the distinctive advantages of both small and large firms which apply at different stages of the innovation process. The analysis also examines the relation of organizational innovation to technological innovation. (AT)",1975,0,2890,162,0,1,1,0,3,2,5,4,4,3
2a0fc90991ab74e18be81208eafd1d9c3d81a428,"Abstract Science policy issues have recently joined technology issues in being acknowledged to have strategic importance for national ‘competitiveness’ and ‘economic security’. The economics literature addressed specifically to science and its interdependences with technological progress has been quite narrowly focused and has lacked an overarching conceptual framework to guide empirical studies and public policy discussions in this area. The emerging ‘new economics of science’, described by this paper, offers a way to remedy these deficiencies. It makes use of insights from the theory of games of incomplete information to synthesize the classic approach of Arrow and Nelson in examining the implications of the characteristics of information for allocative efficiency in research activities, on the one hand, with the functionalist analysis of institutional structures, reward systems and behavioral norms of ‘open science’ communities-associated with the sociology of science in the tradition of Merton-on the other. An analysis is presented of the gross features of the institutions and norms distinguishing open science from other modes of organizing scientific research, which shows that the collegiate reputation-based reward system functions rather well in satisfying the requirement of social efficiency in increasing the stock of reliable knowledge. At a more fine-grain level of examination, however, the detailed workings of the system based on the pursuit of priority are found to cause numerous inefficiencies in the allocation of basic and applied science resources, both within given fields and programs and across time. Another major conclusion, arrived at in the context of examining policy measures and institutional reforms proposed to promote knowledge transfers between university-based open science and commercial R&D, is that there are no economic forces that operate automatically to maintain dynamic efficiency in the interactions of these two (organizational) spheres. Ill-considered institutional experiments, which destroy their distinctive features if undertaken on a sufficient scale, may turn out to be very costly in terms of long-term economic performance.",1994,80,2333,149,5,6,19,12,24,34,36,45,52,71
91cb3b78f73b6fd1717bb3e31544f056a8f4f4a9,"IN RECENT years has not felt his gorge rise upon learning the staggeringly high salary of a shortstop, a movie star, an opera singer? A basketball player on a losing team earns $1.2 million; an author sells the paperback rights to his book for $800,000; a television interviewer switches networks and signs a contract calling for her to receive an annual income of just under $2 million. And the gorge continues to rise. The spectacle of people doing work that doesn't always seem overweighted with significance for annual (and, in the case of rock singers, sometimes nightly) sums of money that figure to exceed what you and I may earn in our lifetimes this, as they say nowadays, does not give off good vibes. What's going on here? What we are talking about, of course, is the phenomenon of superstars, wherein relatively small numbers of people earn enormous amounts of money and seem to dominate the fields in which they are engaged. This phenomenon appears to be increasingly important in the modern world certainly, with the breakdown of economic privacy, it is an increasingly visible phenomenon. The very word superstar implies inflation in our most precious currency, language; to be a star would have been sufficient in my youth. Yet we appear to be stuck with the term. As for the phenomenon itself, viewed from the standpoint of an economist, it may not be as puzzling as it at first glimpse seems. The first thing to be said in this connection is that certain economic activities admit extreme concentration of both personal reward and market size among a handful of participants. Every economic activity supports considerable diversity of talent and significant inequality in the personal distribution of rewards. Activities where superstars are found differ from those in which most of us make our livings by supporting much less diversity and much more inequality in the distribution of earnings. The bulk of earnings goes to relatively small numbers of practitioners typically, the few regarded as among the best in their fields. Similar distributions of earnings in the industrial sector would ultimately come to the attention of the Federal Trade Commission or the",1981,17,2465,176,0,2,4,5,2,4,2,3,0,5
47b40e819edf53c09e9ec7c4f735fb5cec72a713,"This article surveys the economic analysis of public enforcement of law – the use of public agents (inspectors, tax auditors, police, prosecutors) to detect and to sanction violators of legal rules. We first discuss the basic elements of the theory: the probability of imposition of sanctions, the magnitude and form of sanctions (fines, imprisonment), and the rule of liability. We then examine a variety of extensions, including the costs of imposing fines, mistakes, marginal deterrence, settlement, selfreporting, repeat offences, and incapacitation.",2007,59,1977,132,68,124,137,172,182,155,186,147,118,113
9906a97c7d2431ea447eef3ec769ef625de2c338,"With climate change as prototype example, this paper analyzes the implications of structural uncertainty for the economics of low-probability, high-impact catastrophes. Even when updated by Bayesian learning, uncertain structural parameters induce a critical tail fattening of posterior-predictive distributions. Such fattened tails have strong implications for situations, like climate change, where a catastrophe is theoretically possible because prior knowledge cannot place sufficiently narrow bounds on overall damages. This paper shows that the economic consequences of fat-tailed structural uncertainty (along with unsureness about high-temperature damages) can readily outweigh the effects of discounting in climate-change policy analysis.",2009,52,1535,118,100,120,149,125,143,151,111,114,101,87
614625b071af366b441fc15408850c04fd304459,"Plant design and economics for chemical engineers , Plant design and economics for chemical engineers , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1958,0,2821,192,0,1,0,0,2,0,0,0,1,1
09d6a448068fbfa44712993ec689f38ea47bf4a9,TABLE OF CONTENTS PAGE Executive Summary i-xxvii Preface & Acknowledgements,2006,83,1710,185,9,100,163,164,149,159,143,139,138,109
896172e1d3a5d8a06ce6ef0187f53f1cf3803755,Commercial production of intracellular microalgal metabolites requires the following: (1) large-scale monoseptic production of the appropriate microalgal biomass; (2) recovery of the biomass from a relatively dilute broth; (3) extraction of the metabolite from the biomass; and (4) purification of the crude extract. This review examines the options available for recovery of the biomass and the intracellular metabolites from the biomass. Economics of monoseptic production of microalgae in photobioreactors and the downstream recovery of metabolites are discussed using eicosapentaenoic acid (EPA) recovery as a representative case study.,2003,94,1948,136,5,1,8,13,17,27,30,69,117,125
d55b23466d157f238673f9dfb70f334e2de93891,"As firms switch from defined‐benefit plans to defined‐contribution plans, employees bear more responsibility for making decisions about how much to save. The employees who fail to join the plan or who participate at a very low level appear to be saving at less than the predicted life cycle savings rates. Behavioral explanations for this behavior stress bounded rationality and self‐control and suggest that at least some of the low‐saving households are making a mistake and would welcome aid in making decisions about their saving. In this paper, we propose such a prescriptive savings program, called Save More Tomorrow™ (hereafter, the SMarT program). The essence of the program is straightforward: people commit in advance to allocating a portion of their future salary increases toward retirement savings. We report evidence on the first three implementations of the SMarT program. Our key findings, from the first implementation, which has been in place for four annual raises, are as follows: (1) a high proportion (78 percent) of those offered the plan joined, (2) the vast majority of those enrolled in the SMarT plan (80 percent) remained in it through the fourth pay raise, and (3) the average saving rates for SMarT program participants increased from 3.5 percent to 13.6 percent over the course of 40 months. The results suggest that behavioral economics can be used to design effective prescriptive programs for important economic decisions.",2004,59,2280,67,34,47,72,68,75,99,112,143,129,124
47e32579b853d2647e6e9a70ef6ff317cd38b7d3,"How much and how fast should the globe reduce greenhouse-gas emissions? How should nations balance the costs of the reductions against the damages and dangers of climate change? This question has been addressed by the recent ""Stern Review on the Economics of Climate Change,"" which answers these questions clearly and unambiguously. We need urgent, sharp, and immediate reductions in greenhouse-gas emissions. An analysis of the ""Stern Review"" finds that these recommendations depend decisively on the assumption of a near-zero social discount rate. The Review's unambiguous conclusions about the need for extreme immediate action will not survive the substitution of discounting assumptions that are consistent with today's market place.",2006,49,2147,57,15,171,209,241,225,252,235,222,185,145
dac864eeb1b5de388e96a6fab4c15cf3d6d7ba5b,,1963,0,3147,22,2,1,4,5,7,2,5,14,5,11
98fe960b3f5354a987e33e62ab1de060a42ec9ec,"Economic theorists traditionally banish discussions of information to footnotes. Serious consideration of costs of communication, imperfect knowledge, and the like would, it is believed, complicate without informing. This paper, which analyzes competitive markets in which the characteristics of the commodities exchanged are not fully known to at least one of the parties to the transaction, suggests that this comforting myth is false. Some of the most important conclusions of economic theory are not robust to considerations of imperfect information.",1976,21,2530,91,1,7,8,11,7,15,23,24,26,22
a3d2dcfed78248ac3944c6b00ef97ed36807a1c1,"The advantages and disadvantages of expanding the standard economic model by more realistic behavioral assumptions have received much attention. The issue raised in this article is whether it is useful to complicate-or perhaps to enrichthe model of the profit-seeking firm by considering the preferences that people have for being treated fairly and for treating others fairly. The absence of considerations of fairness and loyalty from standard economic theory is one of the most striking contrasts between this body of theory and other social sciences-and also between economic theory and lay intuitions about human behavior. Actions in many domains commonly conform to standards of decency that are more restrictive than the legal ones: the institutions of tipping and lost-and-found offices rest on expectations of such actions. Nevertheless, the standard microeconomic model of the profitmaximizing firm assigns essentially no role to The traditional assumption that fairness is irrelevant to economic analysis is questioned. Even profit-maximizing firms will have an incentive to act in a manner that is perceived as fair if the individuals with whom they deal are willing to resist unfair transactions and punish unfair firms at some cost to themselves. Three experiments demonstrated that willingness to enforce fairness is common. Community standards for actions affecting customers, tenants, and employees were studied in telephone surveys. The rules of fairness, some of which are not obvious, help explain some anomalous market phenomena. * The research for this paper was supported by the Department of Fisheries and Oceans Canada. Kahneman and Thaler were also supported, respectively, by the U.S. Office of Naval Research and by the Alfred P. Sloan Foundation. Conversations with J. Brander, R. Frank, and A. Tversky were very helpful. We also thank Leslie McPherson and Daniel Treisman for their assistance. The paper presented at the conference and commented on by the discussants included a detailed report of study 3, which is only summarized here. It did not contain study 1, which was incomplete at the time. Daniel Kahneman is now in the Department of Psychology, University of California, Berkeley 94720.",1986,22,2185,98,4,0,8,5,13,14,17,14,44,16
83a569a41faaa1819d718559d89cd2b2e2e1c50c,"Milton Friedman's 1953 essay 'The methodology of positive economics' remains the most cited, influential, and controversial piece of methodological writing in twentieth-century economics. Since its appearance, the essay has shaped the image of economics as a scientific discipline, both within and outside of the academy. At the same time, there has been an ongoing controversy over the proper interpretation and normative evaluation of the essay. Perceptions have been sharply divided, with some viewing economics as a scientific success thanks to its adherence to Friedman's principles, others taking it as a failure for the same reason. In this book, a team of world-renowned experts in the methodology of economics cast new light on Friedman's methodological arguments and practices from a variety of perspectives. It provides the 21st century reader with an invaluable assessment of the impact and contemporary significance of Friedman's seminal work.",2009,0,1378,110,96,100,100,89,102,93,99,98,94,104
ed3ddf12573aebe49008dbb6c6f2fe2fe42756ca,"R ECENTLY, orbiting evidence of unAmerican technological competition has focused attention on the role played by scientific research in our political economy. Since Sputnik it has become almost trite to argue that we are not spending as much on basic scientific research as we should. But, though dollar figures have been suggested, they have not been based on economic analysis of what is meant by ""as much as we should."" And, once that question is raised, another immediately comes to mind. Economists often argue that opportunities for private profit draw resources where society most desires them. Why, therefore, does not basic research draw more resources through privateprofit opportunity, if, in fact, we are not spending as much on basic scientific research as is ""socially desirable""? In order to answer some of these questions, it seems useful to examine the simple economics of basic research. How much are we spending on basic research? How much should we be spending? Under what conditions will these figures tend to be different? Is basic research marked by these conditions? If so, what can we do to eliminate or reduce the discrepancy? How much are we spending on basic research? In 1953, the latest date for which relatively sophisticated estimates are available, total expenditure on research and development was about $5.4 billion. Of that total, much more than half was for engineering development, much less than half for scientific research. Even less of the total, about $435 million in 1953, was spent on ""basic research."" All evidence indicates that since 1953 expenditure on research and development has increased markedly; $10 billion seems a reasonable estimate for 1957. Expenditure on basic research has also increased at a rapid rate, perhaps at a faster rate than total research and development expenditure. But basic-research expenditure today is probably under $1 billion, less than one-quarter of 1 per cent of gross national product.' How much should we spend on basic research? Replacing the Xi of the familiar literature on welfare economics with ""basic research"" provides the theoretical answer. From a given expenditure on science we may expect a given flow, over time, of benefits that would not have been created had none of our resources been directed to basic research. This flow of benefits (properly discounted) may be defined as the social value of a given expenditure on basic research. However, if we allocate a given quantity of resources to science, this implies that we are not allocating these resources to other activities and, hence, that we are depriving ourselves of a flow of future benefits that we could have obtained had we directed these resources elsewhere. The discounted flow of bene-",1959,0,2683,123,0,0,0,0,0,0,3,0,1,1
4740121953761c263ddfc31d10daa2b941e446b4,"While innovation processes toward sustainable development (eco-innovations) have received increasing attention during the past years, theoretical and methodological approaches to analyze these processes are poorly developed. Against this background, the term eco-innovation is introduced in this paper addressing explicitly three kinds of changes towards sustainable development: technological, social and institutional innovation. Secondly, the potential contribution of neoclassical and (co-)evolutionary approaches from environmental and innovation economics to eco-innovation research is discussed. Three peculiarities of eco-innovation are identified: the double externality problem, the regulatory push:pull effect and the increasing importance of social and institutional innovation. While the first two are widely ignored in innovation economics, the third is at the least not elaborated appropriately. The consideration of these peculiarities may help to overcome market failure by establishing a specific eco-innovation policy and to avoid a ‘technology bias’ through a broader understanding of innovation. Finally, perspectives for a specific contribution of ecological economics to eco-innovation research are drawn. It is argued that methodological pluralism as established in ecological economics would be very beneficial for eco-innovation research. A theoretical framework integrating elements from both neoclassical and evolutionary approaches should be pursued in order to consider the complexity of factors influencing innovation decisions as well as the specific role of regulatory instruments. And the experience gathered in ecological economics integrating ecological, social and economic aspects of sustainable development is highly useful for opening up innovation research to social and institutional changes. © 2000 Elsevier Science B.V. All rights reserved.",2000,39,1779,154,0,5,3,14,13,15,15,27,39,43
c9cc18de3c1269b24151785b29b8cab7ac39474b,This paper explores the interface between personality psychology and economics. We examine the predictive power of personality and the stability of personality traits over the life cycle. We develop simple analytical frameworks for interpreting the evidence in personality psychology and suggest promising avenues for future research.,2008,739,1391,135,50,56,84,99,109,128,157,132,147,167
71e8e2a9f0d164962dc6caca4e0ba8cc8fbca57b,"Contemplation of the world’s disappearing supplies of minerals, forests, and other exhaustible assets has led to demands for regulation of their exploitation. The feeling that these products are now too cheap for the good of future generations, that they are being selfishly exploited at too rapid a rate, and that in consequence of their excessive cheapness they are being produced and consumed wastefully has given rise to the conservation movement. The method ordinarily proposed to stop the wholesale devastation of irreplaceable natural resources, or of natural resources replaceable only with difficulty and long delay, is to forbid production at certain times and in certain regions or to hamper production by insisting that obsolete and inefficient methods be continued. The prohibitions against oil and mineral development and cutting timber on certain government lands have this justification, as have also closed seasons for fish and game and statutes forbidding certain highly efficient means of catching fish. Taxation would be a more economic method than publicly ordained inefficiency in the case of purely commercial activities such as mining and fishing for profit, if not also for sport fishing. However, the opposition of those who are making the profits, with the apathy of everyone else, is usually sufficient to prevent the diversion into the public treasury of any considerable part of the proceeds of the exploitation of natural resources.",1991,11,1820,194,6,13,12,12,11,13,16,20,14,30
cf9ecfbbd0095687c4cfbbbfa0546914e651b109,"PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",1998,8,2940,181,0,4,15,9,17,26,28,35,38,46
7c72b917a38b09e6d3ab19d28a4344ba54edb6ae,"Part I. Exact String Matching: The Fundamental String Problem: 1. Exact matching: fundamental preprocessing and first algorithms 2. Exact matching: classical comparison-based methods 3. Exact matching: a deeper look at classical methods 4. Semi-numerical string matching Part II. Suffix Trees and their Uses: 5. Introduction to suffix trees 6. Linear time construction of suffix trees 7. First applications of suffix trees 8. Constant time lowest common ancestor retrieval 9. More applications of suffix trees Part III. Inexact Matching, Sequence Alignment and Dynamic Programming: 10. The importance of (sub)sequence comparison in molecular biology 11. Core string edits, alignments and dynamic programming 12. Refining core string edits and alignments 13. Extending the core problems 14. Multiple string comparison: the Holy Grail 15. Sequence database and their uses: the motherlode Part IV. Currents, Cousins and Cameos: 16. Maps, mapping, sequencing and superstrings 17. Strings and evolutionary trees 18. Three short topics 19. Models of genome-level mutations.",1997,0,3106,176,4,21,32,49,77,110,130,160,175,204
05913f4b504aa1fb1e638cdd0848d94bf5eb43da,"Probability and Statistics with Reliability, Queuing and Computer Science Applications, Second Edition, offers a comprehensive introduction to probabiliby, stochastic processes, and statistics for students of computer science, electrical and computer engineering, and applied mathematics. Its wealth of practical examples and up-to-date information makes it an excellent resource for practitioners as well.",1984,6,2913,113,2,4,21,19,21,30,50,35,49,54
b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",1990,0,2877,2,18,46,73,112,105,146,181,165,161,130
9c42e606b18c992b91908626abc46ed92f39848e,"Ubiquitous computing is the method of enhancing computer use by making many computers available throughout the physical environment, but making them effectively invisible to the user. Since we started this work at Xerox PARC in 1988, a number of researchers around the world have begun to work in the ubiquitous computing framework. This paper explains what is new and different about the computer science in ubiquitous computing. It starts with a brief overview of ubiquitous computing, and then elaborates through a series of examples drawn from various subdisciplines of computer science: hardware components (e.g. chips), network protocols, interaction substrates (e.g. software for screens and pens), applications, privacy, and computational methods. Ubiquitous computing offers a framework for new and exciting research across the spectrum of computer science.",1993,37,2553,93,10,19,42,44,35,57,52,68,98,98
d62594672d6f765b44827b1066328e96e1e58f7e,"The aim of this study was to assess the learning effectiveness and motivational appeal of a computer game for learning computer memory concepts, which was designed according to the curricular objectives and the subject matter of the Greek high school Computer Science (CS) curriculum, as compared to a similar application, encompassing identical learning objectives and content but lacking the gaming aspect. The study also investigated potential gender differences in the game's learning effectiveness and motivational appeal. The sample was 88 students, who were randomly assigned to two groups, one of which used the gaming application (Group A, N=47) and the other one the non-gaming one (Group B, N=41). A Computer Memory Knowledge Test (CMKT) was used as the pretest and posttest. Students were also observed during the interventions. Furthermore, after the interventions, students' views on the application they had used were elicited through a feedback questionnaire. Data analyses showed that the gaming approach was both more effective in promoting students' knowledge of computer memory concepts and more motivational than the non-gaming approach. Despite boys' greater involvement with, liking of and experience in computer gaming, and their greater initial computer memory knowledge, the learning gains that boys and girls achieved through the use of the game did not differ significantly, and the game was found to be equally motivational for boys and girls. The results suggest that within high school CS, educational computer games can be exploited as effective and motivational learning environments, regardless of students' gender.",2009,33,1342,90,12,31,61,85,121,136,130,135,153,133
86b05bc7e953e683fa839ad01d6100a8f99558df,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",1991,0,2502,113,5,15,20,26,27,27,24,43,37,42
af465996da89a302fae95c2fe22e54d2b79e4ac3,"Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine—not just the hardware, but the programmed, living machine—is the organism we study.",1976,14,2420,71,1,8,7,4,6,5,10,5,9,8
300f6e755d2e1fd1e3c42804c724f988b7b0e1a1,,2000,340,3951,260,44,55,107,116,130,168,171,237,240,224
2fc911242f20333452d13ab39d678d017ef74852,,2006,0,1291,47,12,43,65,91,127,121,141,144,86,103
e9abcc403b216184bb3ba92989f56f70691408c1,"People can make decisions to join a group based solely on exposure to that group's physical environment. Four studies demonstrate that the gender difference in interest in computer science is influenced by exposure to environments associated with computer scientists. In Study 1, simply changing the objects in a computer science classroom from those considered stereotypical of computer science (e.g., Star Trek poster, video games) to objects not considered stereotypical of computer science (e.g., nature poster, phone books) was sufficient to boost female undergraduates' interest in computer science to the level of their male peers. Further investigation revealed that the stereotypical broadcast a masculine stereotype that discouraged women's sense of ambient belonging and subsequent interest in the environment (Studies 2, 3, and 4) but had no similar effect on men (Studies 3, 4). This masculine stereotype prevented women's interest from developing even in environments entirely populated by other women (Study 2). Objects can thus come to broadcast stereotypes of a group, which in turn can deter people who do not identify with these stereotypes from joining that group.",2009,88,849,48,1,8,27,27,44,54,72,52,81,107
88e642a5918e09480a2a59af4acc07c328368dd8,"For courses in Computer/Network Security. In recent years, the need for education in computer security and related topics has grown dramatically -- and is essential for anyone studying Computer Science or Computer Engineering. This is the only text available to provide integrated, comprehensive, up-to-date coverage of the broad range of topics in this subject. In addition to an extensive pedagogical program, the book provides unparalleled support for both research and modeling projects, giving students a broader perspective. The Text and Academic Authors Association have named Computer Security: Principles and Practice the winner of the Textbook Excellence Award for the best Computer Science textbook of 2008. Visit Stallings Companion Website at http://williamstallings.com/CompSec/CompSec1e.html for student and instructor resources and his Computer Science Student Resource site http://williamstallings.com/StudentSupport.html Password protected instructor resources can be accessed here by clicking on the Resources Tab to view downloadable files. (Registration required) Supplements Include: *Power Point Lecture Slides*Instructor's Manual*Author maintained website .",2007,0,191,14,0,0,0,0,0,0,0,0,0,1
39aa55cb38ad5711718692493dd5e77f3b79ff74,Algorithms on strings trees and sequences puter. suffix tree. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences by dan gusfield. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences guide books. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences. algorithms on strings trees and sequences ??. pdf download algorithms on strings trees and sequences. dan gusfield algorithms on strings trees and sequences pdf. algorithms on strings trees and sequences puter. 9780521585194 algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. pdf algorithms on strings trees and sequences puter. dan gusfield. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences by gusfield. 26os algorithms on strings trees and sequences. algorithms on strings trees and sequences 97 edition. algorithms on strings trees and sequences puter. algorithms on strings coursera. algorithms on strings trees and sequences puter. 0521585198 algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. algorithms on string trees and sequences libre. algorithms on strings trees and sequences puter science. algorithms on strings 9781107670990 puter science. pdf download algorithms on strings trees and sequences. rebinatorics the mit press. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. linear time construction of suffix trees chapter 6. algorithms on strings trees and sequences puter. buy algorithms on strings trees and sequences puter. seminumerical string matching chapter 4 algorithms on. substring. read free algorithms on strings trees and sequences. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. algorithms on strings trees and sequences puter. customer reviews algorithms on strings trees. longest mon substring problem,1997,0,1311,164,3,5,7,22,32,40,54,50,48,60
f861d8e4bcda72cbd821454ffed2d20be8ff5e85,"Examples are discussed to show the differences among discriminant analysis, logistic regression, and multiple regression. Chapter 6, “Multivariate Analysis of Variance,” presents advantages of multivariate analysis of variance (MANOVA) over univariate analysis of variance (ANOVA), discusses assumptions of MANOVA, and assesses validations of MANOVA assumptions and model estimation. The authors also discuss post hoc tests of MANOVA and multivariate analysis of covariance. Chapter 7, “Conjoint Analysis,” explains what conjoint analysis does and how it is different from other multivariate techniques. Guidelines of selecting attributes, models, and methods of data collection are presented. Chapter 8, “Cluster Analysis,” studies objectives, roles, and limitations of cluster analysis. Two basic concepts: similarity and distance are discussed. The authors also discuss details of five most popular hierarchical algorithms (singlelinkage, complete-linkage, average-linkage, centroid method, Ward’s method) and three nonhierarchical algorithms (the sequential threshold method, the parallel threshold method, and the optimizing procedure). Profiles of clusters and guidelines for cluster validation are studied as well. Chapter 9, “Multidimensional Scaling and Correspondence Analysis,” introduces two interdependence techniques to display the relationships in the data. The book describes clearly and intuitively the differences between the two techniques and how these two techniques are performed. Chapters 10–12 cover topics in SEM. Chapter 10, “Structural Equation Modeling: An Introduction,” introduces SEM and related concepts such as exogenous, endogenous constructs, and so on, points out the differences between SEM and other multivariate techniques, overviews the decision process of SEM. Chapter 11, “Confirmatory Factor Analysis,” explains the differences between exploratory and confirmatory factor analysis, discusses how to construct, validate, and assess the goodness of fit of a measurement model in SEM by confirmatory factor analysis. Chapter 12, “Testing a Structural Model,” presents some methods of SEM in examining the relationships between latent constructs. The book is an excellent book for people in management and marketing. For the Technometrics audience, this book does not have much flavor of physical, chemical, and engineering sciences. For example, partial least squares, a very popular method in Chemometrics, is discussed but not as detailed as other techniques in the book. Furthermore, due to the amount of materials covered in the book, it might be inappropriate for someone who is new to multivariate analysis.",2007,0,886,94,40,43,56,50,52,67,70,61,60,60
42a709536d5c1f8540cb221de1d869129f8a0b2c,mathematical logic for computer science 2nd edition PDF logic in computer science solution manual PDF logic in computer science huth ryan solutions PDF handbook of logic in computer science volume 2 background computational structures PDF symbolic rewriting techniques progress in computer science and applied logic PDF logic mathematics and computer science modern foundations with practical applications PDF automated reasoning and mathematics essays in memory of william w mccune lecture notes in computer science PDF,2007,58,793,99,55,49,73,65,58,59,56,55,57,42
f1553e2e546a430e401afdfc1a2e5fc7a5a009f9,"The collection of TCS issues is about 1 meter high, 17,000 pages long and it contains 1100 papers. When in 1974 Einar Fredriksson and myself started talking about the creation of a journal dedicated to Theoretical Computer Science we were very far from even dreaming that it could take such an extension within twelve years. We were also a bit shy: what could such a journal, very theoretical indeed and hard to read, be useful to, and who would read it? Fortunately, some people encouraged us and indeed helped us a lot, Mike Paterson who was at that time President of EATCS and who accepted to become Associate Editor, Albert Meyer who was a very active editor at the beginning, Arto Salomaa, who was to become President of EATCS shortly afterwards. Indeed, I should mention all the first members of the Editorial Board, for TCS would never have come to existence without them. Theoretical Computer Science is not a clearly defined discipline with neat borderlines: it is more a state of mind, the conviction that the observed computation phenomena can be formally described and analysed as any physical phenomenon; the conviction that such a formal description helps to understand these phenomena and to master them in order to design better algorithms, better computers, better systems. Our fundamental activity is not to prove theorems in strange mathematical theories, it is to model a complicated reality and in this respect it has to be compared with theoretical physics or what we call in French “Mecanique rationnelle”. This comparison can be pursued rather far, for we also use all possible mathematical concepts and methods and when we do not find appropriate ones in traditional mathematics we create them. The aim is quite clear: using the compact and unambiguous language of mathematics brings to life concepts and methods which will be useful to all designers, builders and users of computer systems, exactly in the same way as matrix calculus or Fourier series and transforms are useful to all engineers and technicians in the electric and electronic industry. And when one thinks about the amount of time it took to build the mathematical theory of matrices and to polish and simplify it up to the state in which it could be taught to all future engineers and become a tool in daily use, we can be extremely satisfied by the development of Theoretical Computer Science. It is true that concepts and methods which were still vague and unclear when TCS was created became essential tools for all industrial designers and manufacturers, in algorithmics, in semantics, in automata theory and control, etc. . . . Certainly, TCS can be proud to have contributed to this development. Coming back to what I was saying a few minutes ago, this contribution was made possible only by the miraculous fact that the first members of the Editorial Board were sharing the same conviction about the necessity of Theoretical Computer Science",1988,0,1324,0,5,8,12,21,44,40,41,31,56,43
9d370d05a3f159ef58655124be6355a6a9bfa59b,"AbstractEmbeddedsystemsareofgrowingimportanceinindustry. Forexample,inatoday'svehicleahugenumberofembeddedandcommunicating systems can be found. Exhaustive testing of such systems is a requirement, because changes after deliveryand use are expensive and sometimes even impossible. In this paper we propose the use of qualitative models, which are anabstraction of quantitative physical models, for test case generation and test execution. In particular, we show how Simulinkmodels from which control programs are automatically extracted can be tested with respect to qualitative models. SinceSimulink models are heavily used in industry, the approach is of practical interest.Keywords: conformance testing, hybrid systems, qualitative reasoning, qrioconf, Garp3 1 Introduction In industry and especially in the automotive industry Simulink is often used to implementcontrol programs for various purposes. One reason is that those models can be directlyconverted into C code, which runs on the vehicle's electronic control units (ECUs). As aconsequence Simulink models have to be tested thoroughly. This holds in particular forsafety critical systems. In order to meet the safety and quality criteria of such modelsautomated test case generation and more specically model-based testing is of specicinterest but has hardly been explored. In order to ll this gap we present an approach thatmakes use of qualitative models for model-based test cases generation.Qualitative models represent basically cause-effect relationships and constraints onmodel variables. They can be seen as an abstraction of the usually implemented quan-titative differential equation models when using Simulink or other modeling languages.Hence, Simulink models are a renement of qualitative models. This is in contrast to theuse of other means for representing models in this domain like hybrid automata, whichshares basically the same abstraction level with Simulink models.Inordertoallowforusingqualitativemodelsformodel-basedtestingwehavetospecifythe equality relation between the specication and the implementation. For this purpose",2009,117,461,11,89,37,46,22,26,24,24,10,11,5
a5e8fdef0bfb5b41138fb79e611781cfb7a0b305,"The Computer Science Unplugged project provides ways to expose students to ideas from Computer Science without having to use computers. This has a number of applications, including outreach, school curriculum support, and clubs. The “Unplugged” project, based at Canterbury University, uses activities, games, magic tricks and competitions to show children the kind of thinking that is expected of a computer scientist. All of the activities are available free of charge at csunplugged.org. The project has recently enjoyed widespread adoption internationally, and substantial industry support. It is recommended in the ACM K-12 curriculum, and has been translated into 12 languages. As well as simply providing teaching resources, there is a very active program developing and evaluating new formats and activities. This includes adaptations of the kinaesthetic activities in virtual worlds; integration with other outreach tools such as the Alice language, adaptation for use by students in large classrooms, and videos to help teachers and presenters understand how to use the material. This paper will explore why this approach has become popular, and describe developments and adaptations that are being used for outreach and teaching around New Zealand, as well as internationally.",2009,17,270,14,0,0,6,6,12,10,18,18,21,35
e597f4c9997255e9e42205cfa5073316697bb07f,This paper describes an exploratory study to identify which environmental and student factors best predict intention to persist in the computer science major. The findings can be used to make decisions about initiatives for increasing retention. Eight indices of student characteristics and perceptions were developed using the research-based Student Experience of the Major Survey: student-student interaction; student-faculty interaction; collaborative learning opportunities; pace/workload/prior experience with programming; teaching assistants; classroom climate/pedagogy; meaningful assignments; and racism/sexism. A linear regression revealed that student-student interaction was the most powerful predictor of students' intention to persist in the major beyond the introductory course. Other factors predicting intention to persist were pace/workload/prior experience and male gender. The findings suggest that computer science departments interested in increasing retention of students set structured expectations for student-student interaction in ways that integrate peer involvement as a mainstream activity rather than making it optional or extracurricular. They also suggest departments find ways to manage programming experience gaps in CS1.,2009,21,167,14,0,5,13,17,16,12,13,14,14,17
aa3b0866887011ddabd4072927ba3645aedba477,"These instructions give you guidelines for preparing papers for the journal IAENG International Journal of Computer Science. Use this document as a template if you are using LaTeX. Motion tracking and object recognition often use cameras that are mounted in motion platforms like pantilt units, linear tables and even robots. Tracking can be automated by visually servoing the platform’s degrees-of-freedom (DOF) thus keeping the camera’s point-of-view directed at the target. Tracking quick moving targets often demands faster bandwidth platforms. However biology suggests a redundant approach where DOF, like the eye and head, cooperate to direct vision systems and overcome joint limits. This paper illustrates the effectiveness of this concept using a robot-mounted camera.",2009,6,158,0,23,15,8,12,7,7,4,0,0,0
1166487b2d1f4c19a79998506ac6dbb5a83f6431,"A large data set consisting of about 1000 normal mode periods, 500 summary travel time observations, 100 normal mode Q values, mass and moment of inertia have been inverted to obtain the radial distribution of elastic properties, Q values and density in the Earth's interior. The data set was supplemented with a special study of 12 years of ISC phase data which yielded an additional 1.75 × 10^6 travel time observations for P and S waves. In order to obtain satisfactory agreement with the entire data set we were required to take into account anelastic dispersion. The introduction of transverse isotropy into the outer 220 km of the mantle was required in order to satisfy the shorter period fundamental toroidal and spheroidal modes. This anisotropy also improved the fit of the larger data set. The horizontal and vertical velocities in the upper mantle differ by 2–4%, both for P and S waves. The mantle below 220 km is not required to be anisotropic. Mantle Rayleigh waves are surprisingly sensitive to compressional velocity in the upper mantle. High S_n velocities, low P_n velocities and a pronounced low-velocity zone are features of most global inversion models that are suppressed when anisotropy is allowed for in the inversion. 
 
The Preliminary Reference Earth Model, PREM, and auxiliary tables showing fits to the data are presented.",1981,37,8209,765,5,26,44,55,45,61,61,56,79,79
b3d8f8bc662c044771be612b94753e0d0e44597e,"Abstract Compositional models of the Earth are critically dependent on three main sources of information: the seismic profile of the Earth and its interpretation, comparisons between primitive meteorites and the solar nebula composition, and chemical and petrological models of peridotite-basalt melting relationships. Whereas a family of compositional models for the Earth are permissible based on these methods, the model that is most consistent with the seismological and geodynamic structure of the Earth comprises an upper and lower mantle of similar composition, an FeNi core having between 5% and 15% of a low-atomic-weight element, and a mantle which, when compared to CI carbonaceous chondrites, is depleted in Mg and Si relative to the refractory lithophile elements. The absolute and relative abundances of the refractory elements in carbonaceous, ordinary, and enstatite chondritic meteorites are compared. The bulk composition of an average CI carbonaceous chondrite is defined from previous compilations and from the refractory element compositions of different groups of chondrites. The absolute uncertainties in their refractory element compositions are evaluated by comparing ratios of these elements. These data are then used to evaluate existing models of the composition of the Silicate Earth. The systematic behavior of major and trace elements during differentiation of the mantle is used to constrain the Silicate Earth composition. Seemingly fertile peridotites have experienced a previous melting event that must be accounted for when developing these models. The approach taken here avoids unnecessary assumptions inherent in several existing models, and results in an internally consistent Silicate Earth composition having chondritic proportions of the refractory lithophile elements at ∼ 2.75 times that in CI carbonaceous chondrites. Element ratios in peridotites, komatiites, basalts and various crustal rocks are used to assess the abundances of both non-lithophile and non-refractory elements in the Silicate Earth. These data provide insights into the accretion processes of the Earth, the chemical evolution of the Earth's mantle, the effect of core formation, and indicate negligible exchange between the core and mantle throughout the geologic record (the last 3.5 Ga). The composition of the Earth's core is poorly constrained beyond its major constituents (i.e. an FeNi alloy). Density contrasts between the inner and outer core boundary are used to suggest the presence (∼ 10 ± 5%) of a light element or a combination of elements (e.g., O, S, Si) in the outer core. The core is the dominant repository of siderophile elements in the Earth. The limits of our understanding of the core's composition (including the light-element component) depend on models of core formation and the class of chondritic meteorites we have chosen when constructing models of the bulk Earth's composition. The Earth has a bulk Fe Al of ∼ 20 ± 2, established by assuming that the Earth's budget of Al is stored entirely within the Silicate Earth and Fe is partitioned between the Silicate Earth (∼ 14%) and the core (∼ 86%). Chondritic meteorites display a range of Fe Al ratios, with many having a value close to 20. A comparison of the bulk composition of the Earth and chondritic meteorites reveals both similarities and differences, with the Earth being more strongly depleted in the more volatile elements. There is no group of meteorites that has a bulk composition matching that of the Earth's.",1995,192,8184,488,6,19,36,37,41,72,82,98,117,138
d13a04844e4a781e5180987118f732d93aa9f398,"We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances.",2000,58,3941,505,1,2,12,28,49,75,97,140,163,185
e415038575b84918414f8973acff6a399a8b66f2,"Human alteration of Earth is substantial and growing. Between one-third and one-half of the land surface has been transformed by human action; the carbon dioxide concentration in the atmosphere has increased by nearly 30 percent since the beginning of the Industrial Revolution; more atmospheric nitrogen is fixed by humanity than by all natural terrestrial sources combined; more than half of all accessible surface fresh water is put to use by humanity; and about one-quarter of the bird species on Earth have been driven to extinction. By these and other standards, it is clear that we live on a human-dominated planet.",1997,154,8531,257,7,46,80,127,173,140,194,220,271,285
80900fb85e2f29953b7d70fb59fa0e092cf41afe,"T tapestry of life on Earth is unraveling as humans increasingly dominate and transform natural ecosystems. Scarce resources and dwindling time force conservationists to target their actions to stem the loss of biodiversity— a pragmatic approach, given the highly uneven distribution of species and threats (Soulé and Kohm 1989, Olson and Dinerstein 1998, Mace et al. 2000, Myers et al. 2000). Unfortunately, the ability to focus strategically is hindered by the absence of a global biodiversity map with sufficient biogeographic resolution to accurately reflect the complex distribution of the Earth’s natural communities. Without such a map, many distinctive biotas remain unrecognized. In this article, we address the disparity in resolution between maps currently available for global conservation planning and the reality of the Earth’s intricate patterns of life. We have developed a detailed map of the terrestrial ecoregions of the world that is better suited to identify areas of outstanding biodiversity and representative communities (Noss 1992). We define ecoregions as relatively large units of land containing a distinct assemblage of natural communities and species, with boundaries that approximate the original extent of natural communities prior to major land-use change. Our ecoregion map offers features that enhance its utility for conservation planning at global and regional scales: comprehensive coverage, a classification framework that builds on existing biogeographic knowledge, and a detailed level of biogeographic resolution. Ecoregions reflect the distributions of a broad range of fauna and flora across the entire planet, from the vast Sahara Desert to the diminutive Clipperton Island (eastern Pacific Ocean). They are classified within a system familiar to all biologists—biogeographic realms and biomes. Ecoregions, representing distinct biotas (Dasmann 1973, 1974, Udvardy 1975), are nested within the biomes and realms and, together, these provide a framework for comparisons among units and the identification of representative habitats and species assemblages. Although our ecoregions are intended primarily as units for conservation action, they are built on the foundations of classical biogeography and reflect extensive collaboration with over 1000 biogeographers, taxonomists, conservation biologists, and ecologists from around the world. Consequently, ecoregions are likely to reflect the distribution of species and communities more accurately than do units based on global and regional models derived from gross biophysical features, such as rainfall and temperature (Holdridge 1967, Walter and Box 1976, Schulz 1995, Bailey 1998), vegetation structure (UNESCO 1969, deLaubenfels 1975, Schmidthüsen 1976), or",2001,61,5521,341,0,9,14,45,51,74,89,122,179,182
9a09cc05643e2a84bc018b193502ea670a097f95,This paper presents a table of abundances of the elements in the various major units of the Earth's lithic crust with a documentation of the sources and a discussion of the choice of units and data.,1961,106,4172,442,3,7,8,12,16,17,22,29,33,18
2cc7be3d5161e865807e13de7975c9d77fbd2815,"After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2012 APA, all rights reserved)",1994,62,3719,326,0,26,60,70,87,91,92,94,105,83
9bca4426abcece2c01bb4750a46a73de1122af1e,"Frantz Fanon's seminal work on the trauma of colonization, ""The Wretched of the Earth"" made him the leading anti-colonialist thinker of the twentieth century. This ""Penguin Modern Classics"" edition is translated from the French by Constance Farrington, with an introduction by Jean-Paul Sartre. Written at the height of the Algerian war for independence from French colonial rule and first published in 1961, Frantz Fanon's classic text has provided inspiration for anti-colonial movements ever since, analysing the role of class, race, national culture and violence in the struggle for freedom. With power and anger, Fanon makes clear the economic and psychological degradation inflicted by imperialism. It was Fanon, himself a psychotherapist, who exposed the connection between colonial war and mental disease, who showed how the fight for freedom must be combined with building a national culture, and who showed the way ahead, through revolutionary violence, to socialism. Many of the great calls to arms from the era of decolonization are now of purely historical interest, yet this passionate analysis of the relations between the great powers and the 'Third World' is just as illuminating about the world we live in today. Frantz Fanon (1925-61) was a Martinique-born French author essayist, psychoanalyst, and revolutionary. Fanon was a supporter of the Algerian struggle for independence from French rule, and became a member of the Algerian National Liberation Front. He was perhaps the preeminent thinker of the 20th century on the issue of decolonization and the psychopathology of colonization. His works have inspired anti-colonial liberation movements for more than four decades. If you enjoyed ""The Wretched of the Earth"", you might like Edward Said's ""Orientalism"", also available in ""Penguin Modern Classics"". ""In clear language, in words that can only have been written in the cool heat of rage, he showed us the internal theatre of racism"". (""Independent"").",1961,0,6797,180,0,1,0,0,0,1,3,6,8,14
002adc9ec1a44eecac61770c7b7471db615ca862,"Review: Our Ecological Footprint: reducing human impact on the Earth. By Mathis Wackernagel and William Rees Reviewed by Gene Bazan Center for Sustainability, Pennsylvania State University Wackernagel, Mathis and William Rees. Our Ecological Footprint: reducing human impact on the Earth. Philadelphia, PA: New Society Publishers, 1996. 160 pp. US $14.94 paper ISBN: 0-86571-312-X. Partially recycled, acid-free paper using soy-based ink. If the earth's inhabitants were to live at the standard of the U.S., we would require three planet Earths to support us. Many of us have heard or read something like this before. Our Ecological Footprint provides a graphically compelling and quantitatively rigorous way for us to engage in the worldwide sustainability debate: Ecological Footprint analysis. Through this analysis we can determine the consequences of our behavior, and proposed solutions, at any level: individual, household, community, nation, or world. Ecological Footprint analysis measures the aggregate land area required for a given population to exist in a sustainable manner. Wackernagel and Rees note that at 11 acres per person, the U.S. has the highest per capita footprint and suggest that this number should be closer to 6 acres per person. Further, the U.S. faces an 80% ecological deficit, which means we are borrowing from our grandchildren's legacy, and expropriating land from elsewhere in the world. By contrast, each European requires around 5 acres; however, Europeans face higher ecological deficits because they have smaller land areas. Unlike other approaches, which focus on the depletion of non-renewables such as fossil fuel and minerals, Ecological Footprint analysis asserts that the road to sustainability must be paved with sustainable practices. Thus, our use of fossil fuel must have as a compensatory sink the acres of woodlot required to sequester the carbon from our combustion of fossil fuel (in our cars, home heating, etc.) or, alternatively, the acres of fields required to grow biofuel. For example, in comparing our daily commute by car, bus or bicycle, and considering all land requirements (e.g., manufacturing land to produce",1997,0,4016,179,24,45,61,72,97,91,110,112,142,171
0ed6131f38ae61d949e2e5c2ad53ef950b8a889d,,2005,0,2931,230,60,71,80,93,111,109,130,134,166,192
3e88fbfee7844394bc96f62b36bed5a0396ba669,"▪ Abstract The 100 kyr quasiperiodic variation of continental ice cover, which has been a persistent feature of climate system evolution throughout the most recent 900 kyr of Earth history, has occurred as a consequence of changes in the seasonal insolation regime forced by the influence of gravitational n-body effects in the Solar System on the geometry of Earth's orbit around the Sun. The impacts of the changing surface ice load upon both Earth's shape and gravitational field, as well as upon sea-level history, have come to be measurable using a variety of geological and geophysical techniques. These observations are invertible to obtain useful information on both the internal viscoelastic structure of the solid Earth and on the detailed spatiotemporal characteristics of glaciation history. This review focuses upon the most recent advances that have been achieved in each of these areas, advances that have proven to be central to the construction of the refined model of the global process of glacial isos...",2004,93,2352,405,5,32,63,74,85,110,143,150,198,174
bc375d8a65e8311e1dce87b4d0bd36ca7067dec6,"The average chemical compositions of the continental crust and the oceanic crust (represented by MORB), normalized to primitive mantle values and plotted as functions of the apparent bulk partition coefficient of each element, form surprisingly simple, complementary concentration patterns. In the continental crust, the maximum concentrations are on the order of 50 to 100 times the primitive-mantle values, and these are attained by the most highly incompatible elements Cs, Rb, Ba, and Th. In the average oceanic crust, the maximum concentrations are only about 10 times the primitive mantle values, and they are attained by the moderately incompatible elements Na, Ti, Zr, Hf, Y and the intermediate to heavy REE. This relationship is explained by a simple, two-stage model of extracting first continental and then oceanic crust from the initially primitive mantle. This model reproduces the characteristic concentration maximum in MORB. It yields quantitative constraints about the effective aggregate melt fractions extracted during both stages. These amount to about 1.5% for the continental crust and about 8-10% for the oceanic crust. The comparatively low degrees of melting inferred for average MORB are consistent with the correlation of Na20 concentration with depth of extrusion [1], and with the normalized concentrations of Ca, Sc, and AI (= 3) in MORB, which are much lower than those of Zr, Hf, and the HREE ( = 10). Ca, A1 and Sc are compatible with clinopyroxene and are preferentially retained in the residual mantle by this mineral. This is possible only if the aggregate melt fraction is low enough for the clinopyroxene not to be consumed. A sequence of increasing compatibility of lithophile elements may be defined in two independent ways: (1) the order of decreasing normalized concentrations in the continental crust; or (2) by concentration correlations in oceanic basalts. The results are surprisingly similar except for Nb, Ta, and Pb, which yield inconsistent bulk partition coefficients as well as anomalous concentrations and standard deviations. The anomalies can be explained if Nb and Ta have relatively large partition coefficients during continental crust production and smaller coefficients during oceanic crust production. In contrast, Pb has a very small coefficient during continental crust production and a larger coefficient during oceanic crust production. This is the reason why these elements are useful in geochemical discrimination diagrams for distinguishing MORB and OIB on the one hand from island arc and most intracontinental volcanics on the other. The results are consistent with the crust-mantle differentiation model proposed previously [2]. Nb and Ta are preferentially retained and enriched in the residual mantle during formation of continental crust. After separation of the bulk of the continental crust, the residual portion of the mantle was rehomogenized, and the present-day internal heterogeneities between MORB and OIB sources were generated subsequently by processes involving only oceanic crust and mantle. During this second stage, Nb and Ta are highly incompatible, and their abundances are anomalously high in both OIB and MORB. The anomalous behavior of Pb causes the so-called ""lead paradox"", namely the elevated U/Pb and Th/Pb ratios (inferred from Pb isotopes) in the present-day, depleted mantle, even though U and Th are more incompatible than Pb in oceanic basalts. This is explained if Pb is in fact more incompatible than U and Th during formation of the continental crust, and less incompatible than U and Th during formation of oceanic crust.",1988,66,2862,246,0,10,21,37,36,43,55,71,58,63
8a391585b69e7dde9d950d7673505b49ee50fa72,"We present here a new solution for the astronomical computation of the insolation quantities on Earth spanning from -250 Myr to 250 Myr. This solution has been improved with respect to La93 (Laskar et al. [CITE]) by using a direct integration of the gravitational equations for the orbital motion, and by improving the dissipative contributions, in particular in the evolution of the Earth–Moon System. The orbital solution has been used for the calibration of the Neogene period (Lourens et al.  [CITE]), and is expected to be used for age calibrations of paleoclimatic data over 40 to 50 Myr, eventually over the full Palaeogene period (65 Myr) with caution. Beyond this time span, the chaotic evolution of the orbits prevents a precise determination of the Earth's motion. However, the most regular components of the orbital solution could still be used over a much longer time span, which is why we provide here the solution over 250 Myr. Over this time interval, the most striking feature of the obliquity solution, apart from a secular global increase due to tidal dissipation, is a strong decrease of about 0.38 degree in the next few millions of years, due to the crossing of the  resonance (Laskar et al. [CITE]). For the calibration of the Mesozoic time scale (about 65 to 250 Myr), we propose to use the term of largest amplitude in the eccentricity, related to , with a fixed frequency of /yr, corresponding to a period of 405 000 yr. The uncertainty of this time scale over 100 Myr should be about , and  over the full Mesozoic era.",2004,142,2514,308,3,21,36,50,71,77,112,101,159,183
da7a70a97733eaaee04bf0c253314f29ed10aa53,"1. Remote Sensing of the Environment 2. Electromagnetic Radiation Principles 3. History of Aerial Photography and Aerial Platforms 4. Aerial Photography - Vantage Point, Cameras, Filters, and Film 5. Elements of Visual Image Interpretation 6. Photogrammetry 7. Multispectral Remote Sensing Systems 8. Thermal Infrared Remote Sensing 9. Active and Passive Microwave Remote Sensing 10. LIDAR Remote Sensing (new) 11. Remote Sensing of Vegetation 12. Remote Sensing of Water 13. Remote Sensing the Urban Landscape 14. Remote Sensing of Soils, Minerals, and Geomorphology 15. In situ Spectral Reflectance Measurement (new) Index Appendix A-Sources of Remote Sensing Information",2000,1,2549,304,7,14,21,35,78,71,93,100,125,134
51c7e0cc2db1792e40456086308697e14674bbdb,"1) Three indices of global climate have been monitored in the record of the past 450,000 years in Southern Hemisphere ocean-floor sediments. 2) Over the frequency range 10–4 to 10–5 cycle per year, climatic variance of these records is concentrated in three discrete spectral peaks at periods of 23,000, 42,000, and approximately 100,000 years. These peaks correspond to the dominant periods of the earth's solar orbit, and contain respectively about 10, 25, and 50 percent of the climatic variance. 3) The 42,000-year climatic component has the same period as variations in the obliquity of the earth's axis and retains a constant phase relationship with it. 4) The 23,000-year portion of the variance displays the same periods (about 23,000 and 19,000 years) as the quasi-periodic precession index. 5) The dominant, 100,000-year climatic [See table in the PDF file] component has an average period close to, and is in phase with, orbital eccentricity. Unlike the correlations between climate and the higher-frequency orbital variations (which can be explained on the assumption that the climate system responds linearly to orbital forcing), an explanation of the correlation between climate and eccentricity probably requires an assumption of nonlinearity. 6) It is concluded that changes in the earth's orbital geometry are the fundamental cause of the succession of Quaternary ice ages. 7) A model of future climate based on the observed orbital-climate relationships, but ignoring anthropogenic effects, predicts that the long-term trend over the next sevem thousand years is toward extensive Northern Hemisphere glaciation.",1976,93,3323,147,2,25,32,43,33,55,62,38,66,46
1bbb55000baeca8fc209b148f7acf5f3a187f880,"Magnetic and electric dipole transitions between levels of the 4fx configuration perturbed by a static crystalline field are treated. The expression obtained for the pure‐electronic electric‐dipole transition probability involves matrix elements of an even‐order unit tensor between the two 4fx states involved in the transition. The contributions to the transition probability from interactions, via the crystalline field, with the nd 94fx−1, 4fx−1 nd, 4fx−1 ng configurations are shown to add linearly, in such a manner as to multiply each odd k crystal‐field parameter Ak q by a constant. If ``J mixing'' in the 4fx configuration is neglected ΔJ between the upper and lower 4fx levels is restricted to six units or less. If ``L mixing'' is neglected then ΔL is also restricted to six units or less. Application is made to the fluorescence spectra of PrCl3 and EuCl3. Many of the missing and weak transitions are explained.",1962,18,5839,38,0,4,5,6,6,7,11,8,5,9
438a67c80a3777d4e32587150e5166a2591faefe,,1962,0,5774,28,0,3,4,6,13,7,13,11,9,10
77aac4345e1d998fe1075f66eb66b754f161fc7a,"PART I: INTRODUCTION 1. Making Sense of Earth's Politics: A Discourse Approach PART II: GLOBAL LIMITS AND THEIR DENIAL 2. Looming Tragedy: Survivalism 3. Growth Forever: The Promethean Response PART III: SOLVING ENVIRONMENTAL PROBLEMS 4. Leave it to the Experts: Administrative Rationalism 5. Leave it to the People: Democratic Pragmatism 6. Leave it to the Market: Economic Rationalism PART IV: THE QUEST FOR SUSTAINABILITY 7. Environmentally Benign Growth: Sustainable Development 8. Industrial Society and Beyond: Ecological Modernization PART V: GREEN RADICALISM 9. Save the World through New Consciousness: Green Romanticism 10. Save the World through New Politics: Green Rationalism PART VI: CONCLUSION 11. Ecological Democracy Bibliography, Index",1997,10,2371,218,0,7,13,38,38,48,48,56,69,74
a6bc4a16d203b6c7c69fc7420eaf13fa59a8218c,"The term “tipping point” commonly refers to a critical threshold at which a tiny perturbation can qualitatively alter the state or development of a system. Here we introduce the term “tipping element” to describe large-scale components of the Earth system that may pass a tipping point. We critically evaluate potential policy-relevant tipping elements in the climate system under anthropogenic forcing, drawing on the pertinent literature and a recent international workshop to compile a short list, and we assess where their tipping points lie. An expert elicitation is used to help rank their sensitivity to global warming and the uncertainty about the underlying physical mechanisms. Then we explain how, in principle, early warning systems could be established to detect the proximity of some tipping points.",2008,202,2585,105,55,143,177,213,190,211,179,199,174,163
6e7c3b4b16680a837517b62195fff92e3441eee3,"SUMMARY 
New empirical traveltime curves for the major seismic phases have been derived from the catalogues of the International Seismological Centre by relocating events by using P readings, depth phases and the iasp91 traveltimes, and then re-associating phase picks. A smoothed set of traveltime tables is extracted by a robust procedure which gives estimates of the variance of the traveltimes for each phase branch. This set of smoothed empirical times is then used to construct a range of radial velocity profiles, which are assessed against a number of different measures of the level of fit between the empirical times and the predictions of the models. These measures are constructed from weighted sums of L2 misfits for individual phases. The weights are chosen to provide a measure of the probable reliability of the picks for the different phases. 
 
A preferred model, ak135, is proposed which gives a significantly better fit to a broad range of phases than is provided by the iasp91 and sp6 models. The differences in velocity between ak135 and these models are generally quite small except at the boundary of the inner core, where reduced velocity gradients are needed to achieve satisfactory performance for PKP differential time data. 
 
The potential resolution of velocity structure has been assessed with the aid of a non-linear search procedure in which 5000 models have been generated in bounds about ak135. Msfit calculations are performed for each of the phases in the empirical traveltime sets, and the models are then sorted using different overall measures of misfit. The best 100 models for each criterion are displayed in a model density plot which indicates the consistency of the different models. The interaction of information from different phases can be analysed by comparing the different misfit measures. Structure in the mantle is well resolved except at the base, and ak135 provides a good representation of core velocities.",1995,21,2500,198,3,9,11,24,28,45,43,49,44,57
9ee5f4ae37016d3524cc45ed04ed457fa017c0a9,"Verification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always nonunique. Models can be confirmed by the demonstration of agreement between observation and prediction, but confirmation is inherently partial. Complete confirmation is logically precluded by the fallacy of affirming the consequent and by incomplete access to natural phenomena. Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.",1994,36,2895,104,16,46,56,58,59,71,62,89,79,88
9c9777f503a21b454742d2414f68f2843f473c8c,,1989,0,2456,199,0,0,5,18,12,27,20,22,18,24
d21775ae13918db5dc81d346110a1d68c48ae036,,1984,131,2817,114,1,1,3,11,10,11,9,11,13,15
fb280fc782f87f73c24f4b6a8379c6a70deb1732,"With humans having an increasing impact on the planet, the interactions between the nitrogen cycle, the carbon cycle and climate are expected to become an increasingly important determinant of the Earth system.",2008,32,2184,78,32,55,79,115,147,158,171,164,176,181
7e7827bdf386be1dbf4da362d93be2078c6a163b,"Geophysical applications of radar interferometry to measure changes in the Earth's surface have exploded in the early 1990s. This new geodetic technique calculates the interference pattern caused by the difference in phase between two images acquired by a spaceborne synthetic aperture radar at two distinct times. The resulting interferogram is a contour map of the change in distance between the ground and the radar instrument. These maps provide an unsurpassed spatial sampling density (∼100 pixels km−2), a competitive precision (∼1 cm), and a useful observation cadence (1 pass month−1). They record movements in the crust, perturbations in the atmosphere, dielectric modifications in the soil, and relief in the topography. They are also sensitive to technical effects, such as relative variations in the radar's trajectory or variations in its frequency standard. We describe how all these phenomena contribute to an interferogram. Then a practical summary explains the techniques for calculating and manipulating interferograms from various radar instruments, including the four satellites currently in orbit: ERS-1, ERS-2, JERS-1, and RADARSAT. The next chapter suggests some guidelines for interpreting an interferogram as a geophysical measurement: respecting the limits of the technique, assessing its uncertainty, recognizing artifacts, and discriminating different types of signal. We then review the geophysical applications published to date, most of which study deformation related to earthquakes, volcanoes, and glaciers using ERS-1 data. We also show examples of monitoring natural hazards and environmental alterations related to landslides, subsidence, and agriculture. In addition, we consider subtler geophysical signals such as postseismic relaxation, tidal loading of coastal areas, and interseismic strain accumulation. We conclude with our perspectives on the future of radar interferometry. The objective of the review is for the reader to develop the physical understanding necessary to calculate an interferogram and the geophysical intuition necessary to interpret it.",1998,222,2175,128,1,19,41,37,61,62,67,65,73,66
a4eadc385e10ce2ce26c65826accbfd1da14ce0f,"Negative carbon isotope anomalies in carbonate rocks bracketing Neoproterozoic glacial deposits in Namibia, combined with estimates of thermal subsidence history, suggest that biological productivity in the surface ocean collapsed for millions of years. This collapse can be explained by a global glaciation (that is, a snowball Earth), which ended abruptly when subaerial volcanic outgassing raised atmospheric carbon dioxide to about 350 times the modern level. The rapid termination would have resulted in a warming of the snowball Earth to extreme greenhouse conditions. The transfer of atmospheric carbon dioxide to the ocean would result in the rapid precipitation of calcium carbonate in warm surface waters, producing the cap carbonate rocks observed globally.",1998,96,1985,135,3,27,41,39,76,85,77,75,71,115
085216c86d31e987fd245f009e40c52bf198f7a2,,2000,0,2291,98,1,2,9,4,15,32,59,74,85,145
b5139854a9a957eab50f709e9bd49a5e276d4925,"Burn, Baby, Burn Wildfires can have dramatic and devastating effects on landscapes and human structures and are important agents in environmental transformation. Their impacts on nonanthropocentric aspects of the environment, such as ecosystems, biodiversity, carbon reserves, and climate, are often overlooked. Bowman et al. (p. 481) review what is known and what is needed to develop a holistic understanding of the role of fire in the Earth system, particularly in view of the pervasive impact of fires and the likelihood that they will become increasingly difficult to control as climate changes. Fire is a worldwide phenomenon that appears in the geological record soon after the appearance of terrestrial plants. Fire influences global ecosystem patterns and processes, including vegetation distribution and structure, the carbon cycle, and climate. Although humans and fire have always coexisted, our capacity to manage fire remains imperfect and may become more difficult in the future as climate change alters fire regimes. This risk is difficult to assess, however, because fires are still poorly represented in global models. Here, we discuss some of the most important issues involved in developing a better understanding of the role of fire in the Earth system.",2009,395,1898,29,18,66,105,132,148,140,177,157,158,180
c93c2bd7c97b95dc7f76fe95741b6127bffc6fb5,"RECENTLY, Birch1 reported data on the density and composition of the mantle and core. He wrote: “The resulting densities in the lower mantle are found to be in good agreement with shock-wave measurements on rocks having FeO contents in the range 10 ± 2% by weight … except for iron oxide, the chemical composition of the mantle is indeterminate. The density of the outer core is lower than that of iron by about 10%”.",1966,4,2454,186,0,1,0,1,3,1,2,3,3,3
2657328a88494d63860db327ce9500c1bf9b0604,"An update is provided on the Earth's global annual mean energy budget in the light of new observations and analyses. In 1997, Kiehl and Trenberth provided a review of past estimates and performed a number of radiative computations to better establish the role of clouds and various greenhouse gases in the overall radiative energy flows, with top-of-atmosphere (TOA) values constrained by Earth Radiation Budget Experiment values from 1985 to 1989, when the TOA values were approximately in balance. The Clouds and the Earth's Radiant Energy System (CERES) measurements from March 2000 to May 2004 are used at TOA but adjusted to an estimated imbalance from the enhanced greenhouse effect of 0.9 W m−2. Revised estimates of surface turbulent fluxes are made based on various sources. The partitioning of solar radiation in the atmosphere is based in part on the International Satellite Cloud Climatology Project (ISCCP) FD computations that utilize the global ISCCP cloud data every 3 h, and also accounts for increased ...",2009,86,1286,106,19,56,97,91,117,100,121,111,124,116
85df5ea239743dfc769771bca04fdef0af805ac3,"Virtually all nonequilibrium electron transfers on Earth are driven by a set of nanobiological machines composed largely of multimeric protein complexes associated with a small number of prosthetic groups. These machines evolved exclusively in microbes early in our planet's history yet, despite their antiquity, are highly conserved. Hence, although there is enormous genetic diversity in nature, there remains a relatively stable set of core genes coding for the major redox reactions essential for life and biogeochemical cycles. These genes created and coevolved with biogeochemical cycles and were passed from microbe to microbe primarily by horizontal gene transfer. A major challenge in the coming decades is to understand how these machines evolved, how they work, and the processes that control their activity on both molecular and planetary scales.",2008,55,1854,59,9,32,44,81,90,100,130,124,168,171
0cf16c3770d1ce1b9b1b4d875741ba51252882f7,"Abstract Ti–Ni-based alloys are quite attractive functional materials not only as practical shape memory alloys with high strength and ductility but also as those exhibiting unique physical properties such as pre-transformation behaviors, which are enriched by various martensitic transformations. The paper starts from phase diagram, structures of martensites, mechanisms of martensitic transformations, premartensitic behavior, mechanism of shape memory and superelastic effects etc., and covers most of the fundamental issues related with the alloys, which include not only martensitic transformations but also diffusional transformations, since the latter greatly affect the former, and are useful to improve shape memory characteristics. Thus the alloy system will serve as an excellent case study of physical metallurgy, as is the case for steels where all kinds of phase transformations are utilized to improve the physical properties. In short this review is intended to give a self-consistent and logical account of key issues on Ti–Ni based alloys from physical metallurgy viewpoint on an up-to-date basis.",2005,345,2951,99,6,37,60,102,100,110,141,134,195,206
c7ca77863c289ecb4bcfdd7e329d90f792d707be,"Physical Metallurgy Principles is intended for use in an introductory course in physical metallurgy and is designed for all engineering students at the junior or senior level. The approach is largely theoretical, but covers all aspects of physical metallurgy and behavior of metals and alloys. The treatment used in this textbook is in harmony with a more fundamental approach to engineering education.",1972,0,2195,50,5,3,3,1,7,9,7,7,8,13
eb4ec95f5ea8aa0b7e54b3b0975494999b113595,"Preface. 1. Introduction. 1.1 Ni-base Alloy Classification. 1.2 History of Nickel and Ni-base Alloys. 1.3 Corrosion Resistance. 1.4 Nickel Alloy Production. 2. Alloying Additions, Phase Diagrams, and Phase Stability. 2.1 Introduction. 2.2 General Influence of Alloying Additions. 2.3 Phase Diagrams for Solid-Solution Alloys. 2.4 Phase Diagrams for Precipitation Hardened Alloys--gamma' Formers. 2.5 Phase Diagrams for Precipitation-Hardened Alloys--gamma"" Formers. 2.6 Calculated Phase Stability Diagrams. 2.7 PHACOMP Phase Stability Calculations. 3. Solid-Solution Strengthened Ni-base Alloys. 3.1 Standard Alloys and Consumables. 3.2 Physical Metallurgy and Mechanical Properties. 3.3 Welding Metallurgy. 3.4 Mechanical Properties of Weldments. 3.5 Weldability. 3.6 Corrosion Resistance. 3.7 Case Studies. 4. Precipitation Strengthened Ni-base Alloys. 4.1 Standard Alloys and Consumables. 4.2 Physical Metallurgy and Mechanical Properties. 4.3 Welding Metallurgy. 4.4 Mechanical Properties of Weldments. 4.5 Weldability. 5. Oxide Dispersion Strengthened Alloys and Nickel Aluminides. 5.1 Oxide Dispersion Strengthened Alloys. 5.2 Nickel Aluminide Alloys. 6. Repair Welding of Ni-base Alloys. 6.1 Solid-Solution Strengthened Alloys. 6.2 Precipitation Strengthened Alloys. 6.3 Single Crystal Superalloys. 7. Dissimilar Welding. 7.1 Application of Dissimilar Welds. 7.2 Influence of Process Parameters on Fusion Zone Composition. 7.3 Carbon, Low Alloys and Stainless Steels. 7.4 Postweld Heat Treatment Cracking in Stainless Steels Welded with Ni-base Filler Metals. 7.5 Super Austenitic Stainless Steels. 7.6 Dissimilar Welds in Ni-base Alloys - Effect on Corrosion Resistance. 7.7 9%Ni Steels. 7.8 Super Duplex Stainless Steels. 7.9 Case Studies. 8. Weldability Testing. 8.1 Introduction. 8.2 The Varestraint Test. 8.3 Modified Cast Pin Tear Test. 8.4 The Sigmajig Test. 8.5 The Hot Ductility Test. 8.6 The Strain-to-Fracture Test. 8.7 Other Weldability Tests. Appendix A Composition of Wrought and Cast Nickel-Base Alloys. Appendix B Composition of Nickel and Nickel Alloy Consumables. Appendix C Corrosion Acceptance Testing Methods. Appendix D Etching Techniques for Ni-base Alloys and Welds. Author Index. Subject Index.",2009,66,699,60,0,3,18,18,47,54,53,71,68,85
2524a882d157961d70b6106459aed3bb442b74c1,"Comprehensive information for the American aluminium industry Collective effort of 53 recognized experts on aluminium and aluminium alloys Joint venture by world renowned authorities-the Aluminium Association Inc. and American Society for Metals. The completely updated source of information on aluminium industry as a whole rather than its individual contributors. this book is an opportunity to gain from The knowledge of the experts working for prestigious companies such as Alcoa, Reynolds Metals Co., Alcan International Ltd., Kaiser Aluminium & Chemical Corp., Martin Marietta Laboratories and Anaconda Aluminium Co. It took four years of diligent work to complete this comprehensive successor to the classic volume, Aluminium, published by ASM in 1967. Contents: Properties of Pure Aluminum Constitution of Alloys Microstructure of Alloys Work Hardening Recovery, Recrystalization and Growth Metallurgy of Heat Treatment and General Principles of Precipitation Hardening Effects of Alloying Elements and Impurities on Properties Corrosion Behaviour Properties of Commercial Casting Alloys Properties of Commercial Wrought Alloys Aluminum Powder and Powder Metallurgy Products.",1984,0,1561,73,0,0,2,7,6,5,3,10,13,14
34058cc7431331f5af8cd63ac3c9df14765c7eac,Preface. 1. Introduction. 2. Phase Diagrams. 3. Alloying Elements and Constitution Diagrams. 4. Martensitic Stainless Steels. 5. Ferritic Stainless Steels. 6. Austenitic Stainless Steels. 7. Duplex Stainless Steels. 8. Precipitation-Hardening Stainless Steels. 9. Dissimilar Welding of Stainless Steels. 10. Weldability Testing. Appendix 1: Nominal Compositions of Stainless Steels. Appendix 2: Etching Techniques for Stainless Steel Welds. Author Index. Subject Index.,2005,0,1025,77,1,3,13,28,27,34,60,56,74,77
d8133fb8e17ed2a5809e03cc43c7914ea49f2b0d,"This practical reference provides thorough and systematic coverage on both basic metallurgy and the practical engineering aspects of metallic material selection and application. Contents includes: Practical information on the engineering properties and applications of steels, cast irons, nonferrous alloys, and metal matrix composites. Concise overviews and practical implications of metallic structure, imperfections, deformation, and phase transformations Process metallurgy of solidification and casting, recovery, recrystallization and grain growth, precipitation hardening Mechanical deformation during processing and in-service properties of fatigue, fracture, and creep. Physical properties and corrosion.",2008,0,567,50,0,1,4,12,16,42,52,57,74,68
eafe94460769dc1dca747eb9d9c9490a6d8ec12f,"Abstract The generation of zinc and zinc alloy coatings on steel is one of the commercially most important processing techniques used to protect steel components exposed to corrosive environments. From a technological standpoint, the principles of galvanizing have remained unchanged since this coating came into use over 200 years ago. However, because of new applications in the automotive and construction industry, a considerable amount of research has recently occurred on all aspects of the galvanizing process and on new types of Zn coatings. This review will discuss the metallurgy of zinc-coated steel from a scientific standpoint to develop relationships to practical applications. Hot-dip zinc coating methods, i.e. batch and continuous processes, will first be reviewed along with Fe–Zn phase equilibria and kinetics. Commercially, the addition of aluminum to the zinc bath results in three important types of coatings, galvanized, galfan and galvalume, and produces complex reactions at the coating/substrate interface. Fe–Zn–Al equilibrium will be reviewed in the light of recent studies of solubility and inhibition layer formation and breakdown. The effect of steel substrate composition on these reactions will also be critically analyzed. The overlay coating formation, or the coating alloy, is specifically chosen for its desired properties. The morphology of the galvanize, galfan and galvalume coating overlays will be reviewed, as well as the effect of heat treatment to produce a galvanneal coating. Finally, the effect of the microstructures of these coatings on the important properties of corrosion, formability, weldability and paintability will be discussed.",2000,64,1046,39,0,2,2,6,13,25,32,48,46,40
5c1af053ef5d859221eab1d0e56418b04d8f9249,"Skripta Fizikalna metalurgija I je sažeti prikaz znanosti o materijalima u kojoj se na znanstvenim i inženjerskim principima tumaci kristalna građa metala, dizajniranje legura i mikrostrukture, te odnos između strukture metala i njegovih mehanickih i fizickih svojstava.",2009,8,395,29,21,25,30,32,25,28,40,25,24,35
abbe5dee41d472e0468ae30e7bc6dc7d53284b2e,"This volume provides a substantial background to microalloyed steels with a wide selection of applications, some of which are very recent. A well-illustrated practical guide, this book acts as a useful source of data and a concise account of the theoretical aspects of the subject. Both academic institutions and the world-wide steel industry will find it indispensable.",1997,0,958,75,0,6,4,10,18,15,16,22,39,32
3e29d4c182490ea94be52b5593f7d218d4782358,"1. Stress and strain 2. Plasticity 3. Strain hardening 4. Plastic instability 5. Temperature and strain-rate dependence 6. Work balance 7. Slab analysis and friction 8. Friction and lubrication 9. Upper-bound analysis 10. Slip-line field analysis 11. Deformation zone geometry 12. Formability 13. Bending 14. Plastic anisotropy 15. Cupping, redrawing and ironing 16. Forming limit diagrams 17. Stamping 18. Hydroforming 19. Other sheet forming operations 20. Formability tests 21. Sheet metal properties.",1993,13,1086,42,9,12,9,14,15,15,9,16,20,17
6b09d20de5ccfcf15453d0f953147fd1e9079587,,1984,0,1147,69,0,0,3,0,2,5,9,8,7,9
49cdf16489ee008649a00037cb22041a7bcd96ec,"CRC handbook of chemistry and physics , CRC handbook of chemistry and physics , کتابخانه مرکزی دانشگاه علوم پزشکی تهران",1990,0,17372,971,0,0,0,0,0,0,0,0,0,0
958b8af460128a4c79e14f39329ca89e2e1294ad,1 The Atmosphere. 2 Atmospheric Trace Constituents. 3 Chemical Kinetics. 4 Atmospheric Radiation and Photochemistry. 5 Chemistry of the Stratosphere. 6 Chemistry of the Troposphere. 7 Chemistry of the Atmospheric Aqueous Phase. 8 Properties of the Atmospheric Aerosol. 9 Dynamics of Single Aerosol Particles. 10 Thermodynamics of Aerosols. 11 Nucleation. 12 Mass Transfer Aspects of Atmospheric Chemistry. 13 Dynamics of Aerosol Populations. 14 Organic Atmospheric Aerosols. 15 Interaction of Aerosols with Radiation. 16 Meteorology of the Local Scale. 17 Cloud Physics. 18 Atmospheric Diffusion. 19 Dry Deposition. 20 Wet Deposition. 21 General Circulation of the Atmosphere. 22 Global Cycles: Sulfur and Carbon. 23 Climate and Chemical Composition of the Atmosphere. 24 Aerosols and Climate. 25 Atmospheric Chemical Transport Models. 26 Statistical Models.,1997,0,12094,1517,0,0,0,0,0,0,0,0,1,1
c5d7b0fafdd72f57f2b0bfdd0ce3608a2528b666,,1953,0,14694,345,0,0,0,0,0,0,0,0,0,0
8d8e334076d16ba6cb157ae72f751276c1c1c78c,,1992,0,9911,258,76,100,123,380,418,368,299,257,320,280
05b9e01b36be8f688cdbd7131b89920fa46ac048,"Chemical Thermodynamics and Kinetics Acid-Base Dissolved Carbon Dioxide Atmosphere-Water Interactions Metal Ions in Aqueous Solution Aspects of Coordination Chemistry Precipitation and Dissolution Oxidation and Reduction Equilibria the Solid-Solution Interface Trace Metals: Cycling, Regulation and Biological Role Kinetics and Redox Processes Photochemical Processes Kinetics at the Solid-Water Interface Adsorption Dissolution of Minerals Nucleation and Crystal Growth Particle-Particle Interaction Colloids Coagulation and Filtration Regulation of the Chemical Composition of Natural Waters (Examples) Thermodynamic Data.",1970,0,5780,534,0,0,0,0,0,0,0,0,0,0
d0dcf7ebec5d09df2e1d9a899b72cdd77d6a7eb6,"Capillarity. The Nature and Thermodynamics of Liquid Interfaces. Surface Films on Liquid Substrates. Electrical Aspects of Surface Chemistry. Long--Range Forces. Surfaces of Solids. Surfaces of Solids: Microscopy and Spectroscopy. The Formation of a New Phase--Nucleation and Crystal Growth. The Solid--Liquid Interface--Contact Angle. The Solid--Liquid Interface--Adsorption from Solution. Frication, Lubrication, and Adhesion. Wetting, Flotation, and Detergency. Emulsions, Foams, and Aerosols. Macromolecular Surface Films, Charged Films, and Langmuir--Blodgett Layers. The Solid--Gas Interface--General Considerations. Adsorption of Gases and Vapors on Solids. Chemisorption and Catalysis. Index.",1960,22,9772,259,2,2,7,13,17,12,10,19,28,28
5366ae191ffd171dd8a5053b5356abee031ffaa4,Preface. Acknowledgments. Introduction. Hydrolysis and Condensation I: Nonsilicates. Hydrolysis and Condensation II: Silicates. Particulate Sols and Gels. Gelation. Aging of Gels. Theory of Deformation and Flow in Gels. Drying. Structural Evolution during Consolidation. Surface Chemistry and Chemical Modification. Sintering. Comparison of Gel-Derived and Conventional Ceramics. Film Formation. Applications. Index.,1990,0,7797,338,3,16,46,31,85,89,100,145,140,129
b258961fa2f5b4d3171133e23c2c56765bd5b179,"This introduction to modern soil chemistry describes chemical processes in soils in terms of established principles of inorganic, organic, and physical chemistry. The text provides an understanding of the structure of the solid mineral and organic materials from which soils are formed, and explains such important processes as cation exchange, chemisorption and physical absorption of organic and inorganic ions and molecules, soil acidification and weathering, oxidation-reduction reactions, and development of soil alkalinity and swelling properties. Environmental rather than agricultural topics are emphasized, with individual chapters on such pollutants as heavy metals, trace elements, and inorganic chemicals.",1994,0,6422,344,29,37,60,73,67,82,120,160,163,226
58419e1daabad5115a0564e461d574d6500a53cb,"Partial table of contents: Organic Matter in Soils: Pools, Distribution, Transformations, and Function. Extraction, Fractionation, and General Chemical Composition of Soil Organic Matter. Organic Forms of Soil Nitrogen. Native Fixed Ammonium and Chemical Reactions of Organic Matter with Ammonia and Nitrite. Organic Phosphorus and Sulfur Compounds. Soil Carbohydrates. Soil Lipids. Biochemistry of the Formation of Humic Substances. Reactive Functional Groups. Structural Components of Humic and Fulvic Acids as Revealed by Degradation Methods. Characterization of Soil Organic Matter by NMR Spectroscopy and Analytical Pyrolysis. Structural Basis of Humic Substances. Spectroscopic Approaches. Colloidal Properties of Humic Substances. Electrochemical and Ion-Exchange Properties of Humic Substances. Organic Matter Reactions Involving Pesticides in Soil. Index.",1982,0,5420,425,0,1,10,23,14,11,26,30,35,37
a844f4ce5d00e8f62ba3a756c7112ed207e3b605,"N G van Kampen 1981 Amsterdam: North-Holland xiv + 419 pp price Dfl 180 This is a book which, at a lower price, could be expected to become an essential part of the library of every physical scientist concerned with problems involving fluctuations and stochastic processes, as well as those who just enjoy a beautifully written book. It provides an extensive graduate-level introduction which is clear, cautious, interesting and readable.",1983,0,4262,674,2,0,4,3,4,6,4,5,4,6
88e23044396b7c63e831ad0195b6184ea3a12097,"Preface. Part I: Introduction. 1. General Topic and Overview. 2. An Introduction to Environmental Organic Chemicals. Part II: Equilibrium Partitioning Between Gaseous, Liquid, and Solid Phases. 3. Partitioning: Molecular Interactions and Thermodynamics. 4. Vapor Pressure. 5. Activity Coefficient and Solubility in Water. 6. Air-Organic Solvent and Air-Water Partitioning. 7. Organic Liquid-Water Partitioning. 8. Organic Acids and Bases: Acidity Constant and Partitioning Behavior. 9. Sorption I: General Introduction and Sorption Processes Involving Organic Matter. 10. Sorption II: Partitioning to Living Media - Bioaccumulation and Baseline Toxicity. 11. Sorption III: Sorption Processes Involving Inorganic Surfaces. Part III: Transformation Processes. 12. Thermodynamics and Kinetics of Transformation Reactions. 13. Chemical Transformations I: Hydrolysis and Reactions Involving Other Nucleophilic Species. 14. Chemical Transformations II: Redox Reactions. 15. Direct Photolysis. 16. Indirect Photolysis: Reactions with Photooxidants in Natural Waters and in the Atmosphere. 17. Biological Transformations. Part IV: Modeling Tools: Transport and Reaction. 18. Transport by Random Motion. 19. Transport Through Boundaries. 20. Air-Water Exchange. 21. Box Models. 22. Models in Space and Time. Part V: Environmental Systems and Case Studies. 23. Ponds, Lakes, and Oceans. 24. Rivers. 25. Groundwater. Appendix. Bibliography. Index (Subject Index, Compound Index, List of Illustrative Examples).",1993,0,4139,346,0,19,34,53,68,101,118,117,137,147
e0b46e54a742c077006418a7e132e8105371829f,"Lipid peroxidation often occurs in response to oxidative stress, and a great diversity of aldehydes are formed when lipid hydroperoxides break down in biological systems. Some of these aldehydes are highly reactive and may be considered as second toxic messengers which disseminate and augment initial free radical events. The aldehydes most intensively studied so far are 4-hydroxynonenal, 4-hydroxyhexenal, and malonaldehyde. The purpose of this review is to provide a comprehensive summary on the chemical properties of these aldehydes, the mechanisms of their formation and their occurrence in biological systems and methods for their determination. We will also review the reactions of 4-hydroxyalkenals and malonaldehyde with biomolecules (amino acids, proteins, nucleic acid bases), their metabolism in isolated cells and excretion in whole animals, as well as the many types of biological activities described so far, including cytotoxicity, genotoxicity, chemotactic activity, and effects on cell proliferation and gene expression. Structurally related compounds, such as acrolein, crotonaldehyde, and other 2-alkenals are also briefly discussed, since they have some properties in common with 4-hydroxyalkenals.",1991,327,6117,307,1,20,37,56,66,82,123,129,168,179
0966e4fd4910e1bd1be8f3d941bcb955ee01bfae,"For more than a quarter century, Cotton and Wilkinson's Advanced Inorganic Chemistry has been the source that students and professional chemists have turned to for the background needed to understand current research literature in inorganic chemistry and aspects of organometallic chemistry. Like its predecessors, this updated Sixth Edition is organized around the periodic table of elements and provides a systematic treatment of the chemistry of all chemical elements and their compounds. It incorporates important recent developments with an emphasis on advances in the interpretation of structure, bonding, and reactivity.From the reviews of the Fifth Edition:* ""The first place to go when seeking general information about the chemistry of a particular element, especially when up-to-date, authoritative information is desired."" -Journal of the American Chemical Society.* ""Every student with a serious interest in inorganic chemistry should have [this book]."" -Journal of Chemical Education.* ""A mine of information . . . an invaluable guide."" -Nature.* ""The standard by which all other inorganic chemistry books are judged.""-Nouveau Journal de Chimie.* ""A masterly overview of the chemistry of the elements.""-The Times of London Higher Education Supplement.* ""A bonanza of information on important results and developments which could otherwise easily be overlooked in the general deluge of publications."" -Angewandte Chemie.",1999,0,8764,223,272,354,325,352,367,421,399,444,453,457
83283e6359fedf11b27203e9b11dbf370c8b82e7,"The phenomenon of growth, decline and death—aging—has been the source of considerable speculation (1, 8, 10). This cycle seems to be a more or less direct function of the metabolic rate and this in turn depends on the species (animal or plant) on which are superimposed the factors of heredity and the effects of the stresses and strains of life—which alter the metabolic activity. The universality of this phenomenon suggests that the reactions which cause it are basically the same in all living things. Viewing this process in the light of present day free radical and radiation chemistry and of radiobiology, it seems possible that one factor in aging may be related to deleterious side attacks of free radicals (which are normally produced in the course of cellular metabolism) on cell constituents.* Irradiation of living things induces mutation, cancer, and aging (9). Inasmuch as these also arise spontaneously in nature, it is natural to inquire if the processes might not be similar. It is believed that one mechanism of irradiation effect is through liberation of OH and HO 2 radicals (12). There is evidence, although indirect, that these two highly active free radicals are produced normally in living systems. In the first place, free radicals are present in living cells; this was recently demonstrated in vivo by a paramagnetic resonance absorption method (3). Further, it was shown that the concentration of free radicals increased with increasing metabolic activity in conformity with the postulates set forth some years ago that free radicals were involved in biologic oxidation-reduction reactions (11, 13). Are some of these free radicals OH and/or HO2, or radicals of a similar high order of reactivity, and where might they arise in the cell? The most likely source of OH and HO2 radicals, at least in the animal cell, would be the interaction of the respiratory enzymes involved",1956,30,7556,245,0,2,1,1,2,1,3,2,0,1
ea77a4ba7d6cab863de3e8c5e11fcfc850d51c02,"Lange's Handbook has served as a leading source of reliable chemical information and data for chemists, engineers, and other technical specialists since l934. This encyclopedic work includes authoritative coverage of atomic and molecular structure, organic chemistry (revised), inorganic, analytical, and electro- chemistry, mathematics as applied to chemistry, and more. It also includes nomenclature consistent with recommendations of the IUPAC Commission rules.",1978,0,7140,269,12,9,6,17,11,14,16,15,26,24
72e9061fb2c8eaf6b34a9094d2de4fc737294bc3,"This review summarizes the multifaceted aspects of antioxidants and the basic kinetic models of inhibited autoxidation and analyzes the chemical principles of antioxidant capacity assays. Depending upon the reactions involved, these assays can roughly be classified into two types: assays based on hydrogen atom transfer (HAT) reactions and assays based on electron transfer (ET). The majority of HAT-based assays apply a competitive reaction scheme, in which antioxidant and substrate compete for thermally generated peroxyl radicals through the decomposition of azo compounds. These assays include inhibition of induced low-density lipoprotein autoxidation, oxygen radical absorbance capacity (ORAC), total radical trapping antioxidant parameter (TRAP), and crocin bleaching assays. ET-based assays measure the capacity of an antioxidant in the reduction of an oxidant, which changes color when reduced. The degree of color change is correlated with the sample's antioxidant concentrations. ET-based assays include the total phenols assay by Folin-Ciocalteu reagent (FCR), Trolox equivalence antioxidant capacity (TEAC), ferric ion reducing antioxidant power (FRAP), ""total antioxidant potential"" assay using a Cu(II) complex as an oxidant, and DPPH. In addition, other assays intended to measure a sample's scavenging capacity of biologically relevant oxidants such as singlet oxygen, superoxide anion, peroxynitrite, and hydroxyl radical are also summarized. On the basis of this analysis, it is suggested that the total phenols assay by FCR be used to quantify an antioxidant's reducing capacity and the ORAC assay to quantify peroxyl radical scavenging capacity. To comprehensively study different aspects of antioxidants, validated and specific assays are needed in addition to these two commonly accepted assays.",2005,91,4977,235,8,36,104,141,186,246,306,375,449,437
a0c3e1d0353f00fd9df6931af82010f2172168cf,Preface to the first edition. Preface to the second edition. Abbreviated references. I. Stochastic variables. II. Random events. III. Stochastic processes. IV. Markov processes. V. The master equation. VI. One-step processes. VII. Chemical reactions. VIII. The Fokker-Planck equation. IX. The Langevin approach. X. The expansion of the master equation. XI. The diffusion type. XII. First-passage problems. XIII. Unstable systems. XIV. Fluctuations in continuous systems. XV. The statistics of jump events. XVI. Stochastic differential equations. XVII. Stochastic behavior of quantum systems.,1981,0,6448,245,0,3,21,57,55,82,86,82,85,62
7f2dd4901da55d83c01475829cf9a3a7feebca2b,"Origin of the elements, isotopes and atomic weights Chemical periodicity and the periodic table Hydrogen Lithium, sodium, potassium, rubidium, caesium and francium Beryllium, magnesium, calcium, strontium, barium and radium Boron Aluminium, gallium, indium and thallium Carbon Silicon Germanium, tin and lead Nitrogen Phosphorus Arsenic, antimony and bismuth Oxygen Sulfur Selenium, tellurium and polonium The halogens: fluorine, chlorine, bromine, iodine and astatine The noble gases: helium, neon, argon, krypton, xenon, and radon Coordination and organometallic compounds Scandium, yttrium, lanthanum and actinium Titanium, zirconium and hafnium Vanadium, niobium and tantalum Chromium, molybdenum and tungsten Manganese, technetium and rhenium Iron, ruthenium and osmium Cobalt, rhodium and iridium Nickel, palladium, and platinum Copper, silver and gold Zinc, cadmium and mercury The lanthanide elements The actinideand transactinide elements (Z=90-112).",1984,0,6149,234,0,2,13,20,30,32,43,38,50,68
e8e87da268c28da50d67c7e028cbedb29d66939c,"A process for the preparation of a peroxycarboxylic acid of general formula (I), wherein R is an organic residue, in particular a linear or branched alkyled group, an aryl group or an alkyl aryl group each of which is optionally substituted with one or more groups, the process comprising treating a carboxylic acid of the general formula R-COOH, wherein Rhas the meaning indicated above, with hydrogen peroxide or a precursor thereof in the presence of an enzyme catalyst is described. The enzyme catalyst is preferably a hydrolase, such as a protease or a lipase. Also, a process for the oxidation of organic compounds with the peroxycarboxylic acids thus prepared is described.",1999,1,4619,246,14,39,37,43,52,50,49,63,60,85
d4686dec40085e3f5415c21b162ec06efdece47c,,1973,0,32448,17,0,0,0,0,0,0,0,0,0,0
594ee853ff36c1d26d1d2e067d53a97531dc5dea,,1973,0,6569,202,57,60,63,69,71,75,75,89,78,99
af1d52565f3ed8823471af0d9d5a9a1396d0ebad,"INTRODUCTION SOLUTE-SOLVENT INTERACTIONS Solutions Intermolecular Forces Solvation Preferential Solvation Micellar Solvation (Solubilization) Ionization and Dissociation CLASSIFICATION OF SOLVENTS Classification of Solvents According to Chemical Constitution Classification of Solvents Using Physical Constants Classification of Solvents in Terms of Acid-Base Behaviour Classification of Solvents in Terms of Specific Solute/Solvent Interactions Classification of Solvents Using Multivariate Statistical Methods SOLVENT EFFECTS ON THE POSITION OF HOMOGENEOUS CHEMICAL EQUILIBRIA General Remarks Solvent Effects on Acid/Base Equilibria Solvent Effects on Tautomeric Equilibria Solvent Effects on other Equilibria SOLVENT EFFECTS ON THE RATES OF HOMOGENEOUS CHEMICAL REACTIONS General Remarks Gas-Phase Reactivities Qualitative Theory of Solvent Effects on Reaction Rates Quantitative Theories of Solvent Effects on Reaction Rates Specific Solvation Effects on Reaction Rates SOLVENT EFFECTS ON THE ABSORPTION SPECTRA OF ORGANIC COMPOUNDS General Remarks Solvent Effects on UV/Vis Spectra Solvent Effects on Infrared Spectra Solvent Effects on Electron Spin Resonance Spectra Solvent Effects on Nuclear Magnetic Resonance Spectra EMPIRICAL PARAMETERS OF SOLVENT POLARITY Linear Gibbs Energy Relationships Empirical Parameters of Solvent Polarity from Equilibrium Measurements Empirical Parameters of Solvent Polarity from Kinetic Measurements Empirical Parameters of Solvent Polarity from Spectroscopic Measurements Empirical Parameters of Solvent Polarity from Other Measurements Interrelation and Application of Solvent Polarity Parameters Multiparameter Approaches SOLVENTS AND GREEN CHEMISTRY Green Chemistry Reduction of Solvent Use Green Solvent Selection Non-Traditional Solvents Outlook APPENDIX: PROPERTIES, PURIFICATION, AND USE OF ORGANIC SOLVENTS Physical Properties Purification of Organic Solvents Spectroscopic Solvents Solvents as Reaction Media Solvents for Recrystallization Solvents for Extraction and Partitioning (Distribution) Solvents for Adsorption Chromatography Solvents for Acid/Base Titrations in Non-Aqueous Media Solvents for Electrochemistry Toxicity of Organic Solvents",1988,0,5152,206,1,2,15,19,31,36,53,69,76,94
923ac1ebb1d3182967715d3bdbd3b5fb86772973,"A review about the application of response surface methodology (RSM) in the optimization of analytical methods is presented. The theoretical principles of RSM and steps for its application are described to introduce readers to this multivariate statistical technique. Symmetrical experimental designs (three-level factorial, Box-Behnken, central composite, and Doehlert designs) are compared in terms of characteristics and efficiency. Furthermore, recent references of their uses in analytical chemistry are presented. Multiple response optimization applying desirability functions in RSM and the use of artificial neural networks for modeling are also discussed.",2008,106,3670,137,0,20,39,79,114,147,243,286,339,417
fc00cf7afd7b3fc4a6fad4c4ba8bc20b64ae5c07,"Nanocrystals are fundamental to modern science and technology. Mastery over the shape of a nanocrystal enables control of its properties and enhancement of its usefulness for a given application. Our aim is to present a comprehensive review of current research activities that center on the shape-controlled synthesis of metal nanocrystals. We begin with a brief introduction to nucleation and growth within the context of metal nanocrystal synthesis, followed by a discussion of the possible shapes that a metal nanocrystal might take under different conditions. We then focus on a variety of experimental parameters that have been explored to manipulate the nucleation and growth of metal nanocrystals in solution-phase syntheses in an effort to generate specific shapes. We then elaborate on these approaches by selecting examples in which there is already reasonable understanding for the observed shape control or at least the protocols have proven to be reproducible and controllable. Finally, we highlight a number of applications that have been enabled and/or enhanced by the shape-controlled synthesis of metal nanocrystals. We conclude this article with personal perspectives on the directions toward which future research in this field might take.",2009,564,4356,34,38,139,265,377,437,455,469,444,400,361
6bc29d62b0a0fd3e37f37d6740156c95b619c23c,1. Introduction 2. What is Green Chemistry? 3. Tools of Green Chemistry 4. Principles of Green Chemistry 5. Evaluating the Impacts of Chemistry 6. Evaluating Feedstocks and Starting Materials 7. Evaluating Reaction Types 8. Evaluation of Methods to Design Safer Chemicals 10. Future Trends in Green Chemistry,1969,0,5259,126,2,1,0,2,0,0,0,0,0,0
1928849b67f62a33272d048b53fd4b0db92e95ce,Colloid and surface chemistry - scope and variables sedimentation and diffusion and their equilibrium solution thermodynamics - osmotic and Donnan equilibria the rheology of dispersions static and dynamic light scattering and other radiation scattering surface tension and contact angle - application to pure substances adsorption from solution and monolayer formation colloidal structures in surfactant solutions - association colloids adsorption at gas-solid interfaces van der Waals forces the electrical double layer and double-layer interactions electrophoresis and other electrokinetic phenomena electrostatic and polymer-induced colloid stability appendix A - examples of expansions encountered in this book appendix B - units - CGS-SI interconversions appendix C - statistics of discrete and continuous distributions of data appendix D - list of worked-out examples.,1977,0,4060,151,0,0,3,4,7,10,6,18,14,22
fd7644269fc661be7b9ff95aeecbd1c9a8367981,1.0. Introduction 4044 2.0. Biomass Chemistry and Growth Rates 4047 2.1. Lignocellulose and Starch-Based Plants 4047 2.2. Triglyceride-Producing Plants 4049 2.3. Algae 4050 2.4. Terpenes and Rubber-Producing Plants 4052 3.0. Biomass Gasification 4052 3.1. Gasification Chemistry 4052 3.2. Gasification Reactors 4054 3.3. Supercritical Gasification 4054 3.4. Solar Gasification 4055 3.5. Gas Conditioning 4055 4.0. Syn-Gas Utilization 4056 4.1. Hydrogen Production by Water−Gas Shift Reaction 4056,2006,240,6208,107,4,38,83,158,220,378,452,536,631,585
8a54948904ebbdb40aaf12b8e79aed5a3ef09ef3,,1995,0,3035,270,11,27,28,46,58,49,38,70,74,80
1c916242556db3a72c9f70778967bae54a045e19,"Although density functional theory is widely used in the computational chemistry community, the most popular density functional, B3LYP, has some serious shortcomings: (i) it is better for main-group chemistry than for transition metals; (ii) it systematically underestimates reaction barrier heights; (iii) it is inaccurate for interactions dominated by medium-range correlation energy, such as van der Waals attraction, aromatic-aromatic stacking, and alkane isomerization energies. We have developed a variety of databases for testing and designing new density functionals. We used these data to design new density functionals, called M06-class (and, earlier, M05-class) functionals, for which we enforced some fundamental exact constraints such as the uniform-electron-gas limit and the absence of self-correlation energy. Our M06-class functionals depend on spin-up and spin-down electron densities (i.e., spin densities), spin density gradients, spin kinetic energy densities, and, for nonlocal (also called hybrid) functionals, Hartree-Fock exchange. We have developed four new functionals that overcome the above-mentioned difficulties: (a) M06, a hybrid meta functional, is a functional with good accuracy ""across-the-board"" for transition metals, main group thermochemistry, medium-range correlation energy, and barrier heights; (b) M06-2X, another hybrid meta functional, is not good for transition metals but has excellent performance for main group chemistry, predicts accurate valence and Rydberg electronic excitation energies, and is an excellent functional for aromatic-aromatic stacking interactions; (c) M06-L is not as accurate as M06 for barrier heights but is the most accurate functional for transition metals and is the only local functional (no Hartree-Fock exchange) with better across-the-board average performance than B3LYP; this is very important because only local functionals are affordable for many demanding applications on very large systems; (d) M06-HF has good performance for valence, Rydberg, and charge transfer excited states with minimal sacrifice of ground-state accuracy. In this Account, we compared the performance of the M06-class functionals and one M05-class functional (M05-2X) to that of some popular functionals for diverse databases and their performance on several difficult cases. The tests include barrier heights, conformational energy, and the trend in bond dissociation energies of Grubbs' ruthenium catalysts for olefin metathesis. Based on these tests, we recommend (1) the M06-2X, BMK, and M05-2X functionals for main-group thermochemistry and kinetics, (2) M06-2X and M06 for systems where main-group thermochemistry, kinetics, and noncovalent interactions are all important, (3) M06-L and M06 for transition metal thermochemistry, (4) M06 for problems involving multireference rearrangements or reactions where both organic and transition-metal bonds are formed or broken, (5) M06-2X, M05-2X, M06-HF, M06, and M06-L for the study of noncovalent interactions, (6) M06-HF when the use of full Hartree-Fock exchange is important, for example, to avoid the error of self-interaction at long-range, (7) M06-L when a local functional is required, because a local functional has much lower cost for large systems.",2008,82,4638,14,41,127,148,283,399,397,457,423,411,427
b5f957cc2364b149ea14d2d21b43059e1a3e0c7b,,1996,0,4277,153,31,53,88,104,141,145,138,182,192,206
d385ec8e9e16775dab0d7def0c1ad24829add413,"We report a method to form multifunctional polymer coatings through simple dip-coating of objects in an aqueous solution of dopamine. Inspired by the composition of adhesive proteins in mussels, we used dopamine self-polymerization to form thin, surface-adherent polydopamine films onto a wide range of inorganic and organic materials, including noble metals, oxides, polymers, semiconductors, and ceramics. Secondary reactions can be used to create a variety of ad-layers, including self-assembled monolayers through deposition of long-chain molecular building blocks, metal films by electroless metallization, and bioinert and bioactive surfaces via grafting of macromolecules.",2007,33,6926,110,1,21,52,88,142,220,360,534,604,673
95ff3f28e7fc3fdfe025e6d6cd49b9d98e85b3db,1. Introduction 2. Archetypes of the weak hydrogen bond 3. Other weak and non-conventional hydrogen bonds 4. The weak hydrogen bond in supramolecular chemistry 5. The weak hydrogen bond in biological structures 6. Conclusions Appendix,1999,0,3703,137,8,25,76,114,127,170,192,198,216,229
818b9bdf99ad1273e9e223b7d284dbf0126965a7,Environmental Soil Chemistry: An Overview: Evolution of Soil Chemistry. The Modern Environmental Movement. Contaminants in Waters and Soils. Case Study of Pollution of Soils and Waters. Soil Decontamination. Inorganic Soil Components: Pauling's Rules. Primary Soil Minerals. Secondary Soil Minerals. Specific Surface of Soil Minerals. Surface Charge of Soil Minerals. Identification of Minerals by X-Ray Diffraction Analyses. Use of Clay Minerals to Retain Organic Contaminants. Chemistry of Soil Organic Matter: Effects of Soil Formation Factors on SOM Contents. Composition of SOM. Fractionation of SOM. SOM Structure. Functional Groups and Charge Characteristics. Humic Substance-Metal Interactions. SOM-Clay Complexes. Retention of Pesticides and Other Organic Substances by Humic Substances. Soil Solution-Solid Phase Equilibria: Measurement of the Soil Solution. Speciation of the Soil Solution. Ion Activity and Activity Coefficients. Dissolution and Solubility Processes. Sorption Phenomena on Soils: Introduction and Terminology. Surface Functional Groups. Surface Complexes. Adsorption Isotherms. Equilibrium-Based Adsorption Models. Surface Precipitation. Sorption of Metal Cations. Sorption of Anions. Points of Zero Charge. Desorption. Use of Spectroscopic and Microscopic Methods in Determining Mechanisms for Sorption-Desorption Phenomena. Ion Exchange Processes: Characteristics of Ion Exchange. Cation Exchange Equilibrium Constants and Selectivity Coefficients. Thermodynamics of Ion Exchange. Relationship between Thermodynamics and Kinetics of Ion Exchange. Kinetics of Soil Chemical Processes: Rate-Limiting Steps and Time Scales of Soil Chemical Reactions. Rate Laws. Determination of Reacti,1995,0,2714,231,0,2,7,13,31,31,43,49,65,78
f30eb940e601c7fd80e10619f174e3ed74503700,Introduction. Errors in Classical Analysis - Statistics of Repeated Measurements. Significance Tests. Quality Control and Sampling. Errors in Instrumental Analysis. Regression and Correlation. Non-parametric and Robust Methods. Experimental Design. Optimization and Pattern Recognition. Solutions to Exercises. Appendix 1: Summary of Statistical Tests. Appendix 2: Statistical Tests.,1993,0,3411,132,32,50,109,113,124,100,131,150,140,152
5fca0eafcfe8cea91fd640b40cfaeb667781898d,"Polyphenols constitute one of the most numerous and ubiquitous groups of plant metabolites and are an integral part of both human and animal diets. Ranging from simple phenolic molecules to highly polymerized compounds with molecular weights of greater than 30,000 Da, the occurrence of this complex group of substances in plant foods is extremely variable. Polyphenols traditionally have been considered antinutrients by animal nutritionists, because of the adverse effect of tannins, one type of polyphenol, on protein digestibility. However, recent interest in food phenolics has increased greatly, owing to their antioxidant capacity (free radical scavenging and metal chelating activities) and their possible beneficial implications in human health, such as in the treatment and prevention of cancer, cardiovascular disease, and other pathologies. Much of the literature refers to a single group of plant phenolics, the flavonoids. This review offers an overview of the nutritional effects of the main groups of polyphenolic compounds, including their metabolism, effects on nutrient bioavailability, and antioxidant activity, as well as a brief description of the chemistry of polyphenols and their occurrence in plant foods.",2009,118,3696,166,147,175,220,231,273,279,270,243,268,227
fdc354d08984f0b36be818b8a287a767cad727ed,Air Pollutants Effects of Air Pollution Sources of Pollutants in Combustion Processes Gas-Phase Atmospheric Chemistry Aqueous-Phase Atmospheric Chemistry Mass Transfer Aspects of Atmospheric Chemistry Properties of Aerosols Dynamics of Single Aerosol Particles Thermodynamics of Aerosols and Nucleation Theory Dynamics of Aerosol Population Air Pollution Meteorology Micrometeorology Atmospheric Diffusion Theories The Gaussian Plume Equation The Atmospheric Diffusion Equation and Air Quality Models Atmospheric Removal Processes and Residence Times Air Pollution Statistics Acid Rain Index.,1986,0,2652,272,1,15,21,31,51,56,52,74,70,77
7d7e9fa2b79dbaace65755a4e86d1ed3f1695cf4,"Examination of nature's favorite molecules reveals a striking preference for making carbon-heteroatom bonds over carbon-carbon bonds-surely no surprise given that carbon dioxide is nature's starting material and that most reactions are performed in water. Nucleic acids, proteins, and polysaccharides are condensation polymers of small subunits stitched together by carbon-heteroatom bonds. Even the 35 or so building blocks from which these crucial molecules are made each contain, at most, six contiguous C-C bonds, except for the three aromatic amino acids. Taking our cue from nature's approach, we address here the development of a set of powerful, highly reliable, and selective reactions for the rapid synthesis of useful new compounds and combinatorial libraries through heteroatom links (C-X-C), an approach we call ""click chemistry"". Click chemistry is at once defined, enabled, and constrained by a handful of nearly perfect ""spring-loaded"" reactions. The stringent criteria for a process to earn click chemistry status are described along with examples of the molecular frameworks that are easily made using this spartan, but powerful, synthetic strategy.",2001,17,5784,43,1,2,2,8,13,72,129,236,314,411
58aae2c2c9b059124f7168af4611059d6c4fa224,"The interest in nanoscale materials stems from the fact that new properties are acquired at this length scale and, equally important, that these properties * To whom correspondence should be addressed. Phone, 404-8940292; fax, 404-894-0294; e-mail, mostafa.el-sayed@ chemistry.gatech.edu. † Case Western Reserve UniversitysMillis 2258. ‡ Phone, 216-368-5918; fax, 216-368-3006; e-mail, burda@case.edu. § Georgia Institute of Technology. 1025 Chem. Rev. 2005, 105, 1025−1102",2005,34,6092,30,22,127,212,276,316,382,536,580,597,602
b7c89b0247fc7009ab51dd0873b5f6c3b6ec144e,"Electron-transfer reactions between ions and 
molecules in solution have been the subject of 
considerable experimental study during the past 
three decades. Experimental results have also been 
obtained on related phenomena, such as reactions 
between ions or molecules and electrodes, charge-transfer 
spectra, photoelectric emission spectra of 
ionic solutions, chemiluminescent electron transfers, 
electron transfer through frozen media, and 
electron transfer through thin hydrocarbon-like 
films on electrodes.",1985,276,5705,66,3,23,48,73,55,75,85,95,119,155
b299eb6f6cf52ed9ef2a6e055503491cffe95887,"A recent visit to one of BP’s European technology centers produced a request to develop statistics training for the people in the analytical lab. Of course, I first searched to see what books were available. I have always liked the book by Caulcott and Boddy (1983), but it is rather dated. The most recent review in Technometrics of a similar book was that for Meloun et al. (1992) (see LaLonde 1995). There also was a review of the book by Meier and Zünd (1993) (see Hillyer 1995). A second edition of that book was published in 2000. It is a large book, nearly 500 pages, with a hefty price, $126.00 from Amazon.com. This more modest-sized and lower-priced book actually would be a reasonable starter book for a typical analytical chemist. This book was first published in 1984, but it has had suitable updating. This edition uses Excel and MINITAB for statistical computing. It also is the first of the editions to incorporate chemometrics topics. The book now has had a new final chapter added to introduce the multivariate methods of chemometrics. The 11 different tools that are presented run the gamut from principal components and cluster analyses to partial least squares regression and neural networks. The topics are all discussed without any use of matrix algebra and little use of equations. As for the other seven chapters, they start with the basics of “Statistics for Repeated Measurements” in Chapter 2 and progress through significance tests, measurement quality, calibrations using regression methods, nonparametric methods, robust methods, experimental design, and optimization. There are excellent discussions of a number of important indigenous topics relevant to the analytical laboratory, including lognormal distributions, propagation of errors, components of variability, collaborative trials, limits of detection, comparisons from calibration curves, standard additions, and cusum charts. The authors, apparently a husband and wife pairing of an analytical chemist and a statistician, have an excellent understanding of the technical level appropriate for the people in the lab. There are relatively few equations but a considerable number of plots and displays of results. Hopefully there will soon be a report for the second edition of the text of Meier and Zünd (1993), but it is hard to imagine a much better book than this one for use as the first course with a group of analytical chemists. There are excellent sets of references, both from the statistics and analytical chemistry literature, that provide sources for further study by anyone so inclined.",2004,4,2228,312,17,32,44,45,52,69,92,116,153,155
6442a59bd6310a9bb3c1d64e3a0f3bda68110d47,,1971,0,4992,71,34,29,25,33,34,38,41,37,34,57
e7a75eb11f62ed650004b8776fc0266e8b764ddc,"A fully coupled ‘‘online’’ Weather Research and Forecasting/Chemistry (WRF/Chem) model has been developed. The air quality component of the model is fully consistent with the meteorological component; both components use the same transport scheme (mass and scalar preserving), the same grid (horizontal and vertical components), and the same physics schemes for subgrid-scale transport. The components also use the same timestep, hence no temporal interpolation is needed. The chemistry package consists of dry deposition (‘‘flux-resistance’’ method), biogenic emission as in [Simpson et al., 1995. Journal of Geophysical Research 100D, 22875–22890; Guenther et al., 1994. Atmospheric Environment 28, 1197–1210], the chemical mechanism from RADM2, a complex photolysis scheme (Madronich scheme coupled with hydrometeors), and a state of the art aerosol module (MADE/SORGAM aerosol parameterization). The WRF/Chem model is statistically evaluated and compared to MM5/Chem and to detailed photochemical data collected during the summer 2002 NEAQS field study. It is shown that the WRF/Chem model is statistically better skilled in forecasting O3 than MM5/Chem, with no appreciable differences between models in terms of bias with the observations. Furthermore, the WRF/Chem model consistently exhibits better skill at forecasting the O3 precursors CO and NOy at all of the surface sites. However, the WRF/Chem model biases of these precursors and of other gas-phase species are persistently higher than for MM5/Chem, and are most often biased high compared to observations. Finally, we show that the impact of other basic model assumptions on these same statistics can be much larger than the differences caused by model differences. An example showing the sensitivity of various statistical measures with respect to the treatment of biogenic volatile organic compounds emissions illustrates this impact. r 2005 Elsevier Ltd. All rights reserved.",2005,70,2340,275,6,20,17,44,57,65,102,124,118,148
06ddd16277152af1e09f0986955cd123564e666c,"CHEC III is organized in 15 Volumes and closely follows the organization used in the previous edition: Volumes 1 and 2: Cover respectively three- and four-membered heterocycles, together with all fused systems containing a three- or four-membered heterocyclic ring. Volume 3: Five-membered rings with one heteroatom together with their benzo- and other carbocyclic-fused derivatives. Volumes 4, 5 and 6: Cover five-membered rings with two heteroatoms, and three or more heteroatoms, respectively, each with their fused carbocyclic compounds. Volumes 7, 8 and 9: Dedicated to six-membered rings with one, two, and more than two heteroatoms, respectively, again with the corresponding fused carbocylic compounds. Volumes 10, 11 and 12: Cover systems containing at least two directly fused heterocyclic five- and/or six-membered rings: of these Volume 10 deals with bi-heterocyclic rings without a ring junction heteroatom, and Volume 11 deals with 5:5 and 5:6 fused rings systems with at least one ring junction nitrogen, while Volume 12 is devoted to all other systems of five and/or six-membered fused or spiro heterocyclic rings with ring junction heteroatoms. Volumes 13 and 14: Seven-membered and larger heterocyclic rings including all their fused derivatives (except those containing three- or four-membered heterocyclic rings which are included in Volume 1 and 2, respectively). Volume 15: Author, ring and subject indexes.",1996,0,6905,44,127,150,145,186,180,176,230,217,242,266
4137c7569c18fd130dab7e500a8c2292cade4bab,"Department of Materials Science, University of Patras, 26504 Rio Patras, Greece, Theoretical and Physical Chemistry Institute, National Hellenic Research Foundation, 48 Vass. Constantinou Avenue, 116 35 Athens, Greece, Institut de Biologie Moleculaire et Cellulaire, UPR9021 CNRS, Immunologie et Chimie Therapeutiques, 67084 Strasbourg, France, and Dipartimento di Scienze Farmaceutiche, Universita di Trieste, Piazzale Europa 1, 34127 Trieste, Italy",2006,0,3464,29,22,121,180,245,304,308,291,340,288,242
33e6e21938caa82babe26ca7bb4667603165c822,"There are now a wide variety of packages for electronic structure calculations, each of which differs in the algorithms implemented and the output format. Many computational chemistry algorithms are only available to users of a particular package despite being generally applicable to the results of calculations by any package. Here we present cclib, a platform for the development of package‐independent computational chemistry algorithms. Files from several versions of multiple electronic structure packages are automatically detected, parsed, and the extracted information converted to a standard internal representation. A number of population analysis algorithms have been implemented as a proof of principle. In addition, cclib is currently used as an input filter for two GUI applications that analyze output files: PyMOlyze and GaussSum. © 2007 Wiley Periodicals, Inc. J Comput Chem, 2008",2008,44,3411,20,5,19,53,94,144,193,261,298,334,376
930f6041f86b233c2d4ea0f61f8f16322ef26a01,"We present the theoretical and technical foundations of the Amsterdam Density Functional (ADF) program with a survey of the characteristics of the code (numerical integration, density fitting for the Coulomb potential, and STO basis functions). Recent developments enhance the efficiency of ADF (e.g., parallelization, near order‐N scaling, QM/MM) and its functionality (e.g., NMR chemical shifts, COSMO solvent effects, ZORA relativistic method, excitation energies, frequency‐dependent (hyper)polarizabilities, atomic VDD charges). In the Applications section we discuss the physical model of the electronic structure and the chemical bond, i.e., the Kohn–Sham molecular orbital (MO) theory, and illustrate the power of the Kohn–Sham MO model in conjunction with the ADF‐typical fragment approach to quantitatively understand and predict chemical phenomena. We review the “Activation‐strain TS interaction” (ATS) model of chemical reactivity as a conceptual framework for understanding how activation barriers of various types of (competing) reaction mechanisms arise and how they may be controlled, for example, in organic chemistry or homogeneous catalysis. Finally, we include a brief discussion of exemplary applications in the field of biochemistry (structure and bonding of DNA) and of time‐dependent density functional theory (TDDFT) to indicate how this development further reinforces the ADF tools for the analysis of chemical phenomena. © 2001 John Wiley & Sons, Inc. J Comput Chem 22: 931–967, 2001",2001,410,6597,35,8,36,62,86,129,163,157,207,266,240
d907adcb91056cf87ddb1560874403d203a47057,"Flavonoids are a class of secondary plant phenolics with significant antioxidant and chelating properties. In the human diet, they are most concentrated in fruits, vegetables, wines, teas and cocoa. Their cardioprotective effects stem from the ability to inhibit lipid peroxidation, chelate redox-active metals, and attenuate other processes involving reactive oxygen species. Flavonoids occur in foods primarily as glycosides and polymers that are degraded to variable extents in the digestive tract. Although metabolism of these compounds remains elusive, enteric absorption occurs sufficiently to reduce plasma indices of oxidant status. The propensity of a flavonoid to inhibit free-radical mediated events is governed by its chemical structure. Since these compounds are based on the flavan nucleus, the number, positions, and types of substitutions influence radical scavenging and chelating activity. The diversity and multiple mechanisms of flavonoid action, together with the numerous methods of initiation, detection and measurement of oxidative processes in vitro and in vivo offer plausible explanations for existing discrepancies in structure-activity relationships. Despite some inconsistent lines of evidence, several structure-activity relationships are well established in vitro. Multiple hydroxyl groups confer upon the molecule substantial antioxidant, chelating and prooxidant activity. Methoxy groups introduce unfavorable steric effects and increase lipophilicity and membrane partitioning. A double bond and carbonyl function in the heterocycle or polymerization of the nuclear structure increases activity by affording a more stable flavonoid radical through conjugation and electron delocalization. Further investigation of the metabolism of these phytochemicals is justified to extend structure-activity relationships (SAR) to preventive and therapeutic nutritional strategies.",2002,128,3239,111,0,10,35,43,57,79,103,112,140,162
82f3ad50889fc32467403fb433bea129ba6cd5b0,It has become evident that fluorinated compounds have a remarkable record in medicinal chemistry and will play a continuing role in providing lead compounds for therapeutic applications. This tutorial review provides a sampling of renowned fluorinated drugs and their mode of action with a discussion clarifying the role and impact of fluorine substitution on drug potency.,2008,124,3699,0,28,45,88,136,205,278,379,418,429,434
6cb70f250fdedd1a33b2456e00baec3ba97669c2,,1961,0,3568,113,0,4,9,18,16,22,41,51,42,47
5514043c3c7db5345ccee514789aff243470eeaf,,1984,0,4455,18,29,60,36,42,58,62,66,55,63,64
e35f0fb701522eef2595f034813e401fa8b599e7,,1988,0,4272,22,9,13,29,37,28,40,40,137,145,171
f4ac82d9ca3627d7b676e40a7f933514dd4bb71f,"Wood Chemistry, Fundamentals and Applications, Second Edition, examines the basic principles of wood chemistry and its potential applications to pulping and papermaking, wood and wood waste utilization, pulping by-products for production of chemicals and energy, and biomass conversion.",1981,0,2253,234,1,1,6,8,8,5,5,8,13,12
386aa493b0208c236979fb091018d0fee2da293b,"The present status of knowledge of the gas-phase reactions of inorganic O x ,H O x and NO x species and of selected classes of volatile organic compounds (VOCs) [alkanes, alkenes, aromatic hydrocarbons, oxygen-containing VOCs and nitrogen-containing VOCs] and their degradation products in the troposphere is discussed. There is now a good qualitative and, in a number of areas, quantitative understanding of the tropospheric chemistry of NO x and VOCs involved in the photochemical formation of ozone. During the past ""ve years much progress has been made in elucidating the reactions of alkoxy radicals, the mechanisms of the gas-phase reactions of O 3 with alkenes, and the mechanisms and products of the OH radical-initiated reactions of aromatic hydrocarbons, and further progress is expected. However, there are still areas of uncertainty which impact the ability to accurately model the formation of ozone in urban, rural and regional areas, and these include a need for: rate constants and mechanisms of the reactions of organic peroxy (RO 0 2 ) radicals with NO, NO 3 radicals, HO 2 radicals and other RO 0 2 radicals; organic nitrate yields from the reactions of RO 0 2 radicals with NO, preferably as a function of temperature and pressure; the reaction rates of alkoxy radicals for decomposition, isomerization, and reaction with O 2 , especially for alkoxy radicals other than those formed from alkanes and alkenes; the detailed mechanisms of the reactions of O 3 with alkenes and VOCs containing ’C""C( bonds; the mechanisms and products of the reactions of OH-aromatic adducts with O 2 and NO 2 ; the tropospheric chemistry of many oxygenated VOCs formed as ""rst-generation products of VOC photooxidations; and a quantitative understanding of the reaction sequences leading to products which gas/particle partition and lead to secondary organic aerosol formation. ( 2000 Elsevier Science Ltd. All rights reserved.",2000,251,2326,200,4,27,38,57,50,64,90,77,86,92
d9e4c02b5b6d29dc99a58ea03f58c0dbbb8b621e,Volume 1 - Molecular Recognition: Receptors for Cationic Guests. Volume 2 - Molecular Recognition: Receptors for Molecular Guests. Volume 3 - Cyclodextrins. Volume 4 - Supramolecular Reactivity and Transport: Bioorganic Systems. Volume 5 - Supramolecular Reactivity and Transport: Bioinorganic Systems. Volume 6 - Solid-State Supramolecular Chemistry: Crystal Engineering. Volume 7 - Solid-State Supramolecular Chemistry: Two- and Three-Dimensional Inorganic Networks. Volume 8 - Physical Methods in Supramolecular Chemistry. Volume 9 - Templating Self-Assembly and Self-Organization. Voulme 10 - Supramolecular Technology. Volume 11 - Cumulative Index.,1996,0,3705,7,18,83,155,202,220,241,235,205,268,264
218962e071957909ce0182779ec6865640294984,,1998,0,3342,40,1,16,33,49,54,68,73,117,108,131
787cd29e9b984f5ce7978705bf02f4a57c4419b6,1. The Chemical Composition of Soils 2. Soil Minerals 3. Soil Humus 4. The Soil Solution 5. Mineral Stability and Weathering 6. Oxidation-Reduction Reactions 7. Soil Particle Surface Charge 8. Soil Adsorption Phenomena 9. Exchangeable Ions 10. Colloidal Phenomena 11. Soil Acidity 12. Soil Salinity,2008,0,2194,161,99,93,108,113,110,129,123,138,127,120
9c4fb83f9c95c677ca5fa7e478de438b733a002d,"The identification of genetically homogeneous groups of individuals is a long standing issue in population genetics. A recent Bayesian algorithm implemented in the software structure allows the identification of such groups. However, the ability of this algorithm to detect the true number of clusters (K) in a sample of individuals when patterns of dispersal among populations are not homogeneous has not been tested. The goal of this study is to carry out such tests, using various dispersal scenarios from data generated with an individual‐based model. We found that in most cases the estimated ‘log probability of data’ does not provide a correct estimation of the number of clusters, K. However, using an ad hoc statistic ΔK based on the rate of change in the log probability of data between successive K values, we found that structure accurately detects the uppermost hierarchical level of structure for the scenarios we tested. As might be expected, the results are sensitive to the type of genetic marker used (AFLP vs. microsatellite), the number of loci scored, the number of populations sampled, and the number of individuals typed in each sample.",2005,52,16245,1729,0,0,0,0,0,0,0,0,0,7
4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3,"First, the span of absolute judgment and the span of immediate memory impose severe limitations on the amount of information that we are able to receive, process, and remember. By organizing the stimulus input simultaneously into several dimensions and successively into a sequence or chunks, we manage to break (or at least stretch) this informational bottleneck. Second, the process of recoding is a very important one in human psychology and deserves much more explicit attention than it has received. In particular, the kind of linguistic recoding that people do seems to me to be the very lifeblood of the thought processes. Recoding procedures are a constant concern to clinicians, social psychologists, linguists, and anthropologists and yet, probably because recoding is less accessible to experimental manipulation than nonsense syllables or T mazes, the traditional experimental psychologist has contributed little or nothing to their analysis. Nevertheless, experimental techniques can be used, methods of recoding can be specified, behavioral indicants can be found. And I anticipate that we will find a very orderly set of relations describing what now seems an uncharted wilderness of individual differences. Third, the concepts and measures provided by the theory of information provide a quantitative way of getting at some of these questions. The theory provides us with a yardstick for calibrating our stimulus materials and for measuring the performance of our subjects. In the interests of communication I have suppressed the technical details of information measurement and have tried to express the ideas in more familiar terms; I hope this paraphrase will not lead you to think they are not useful in research. Informational concepts have already proved valuable in the study of discrimination and of language; they promise a great deal in the study of learning and memory; and it has even been proposed that they can be useful in the study of concept formation. A lot of questions that seemed fruitless twenty or thirty years ago may now be worth another look. In fact, I feel that my story here must stop just as it begins to get really interesting. And finally, what about the magical number seven? What about the seven wonders of the world, the seven seas, the seven deadly sins, the seven daughters of Atlas in the Pleiades, the seven ages of man, the seven levels of hell, the seven primary colors, the seven notes of the musical scale, and the seven days of the week? What about the seven-point rating scale, the seven categories for absolute judgment, the seven objects in the span of attention, and the seven digits in the span of immediate memory? For the present I propose to withhold judgment. Perhaps there is something deep and profound behind all these sevens, something just calling out for us to discover it. But I suspect that it is only a pernicious, Pythagorean coincidence.",1956,24,21278,766,0,0,0,0,0,0,0,0,0,0
81f8fb4361b2bfa0159d5165641b87d30a9701f6,"The magnitudes of the systematic biases involved in sample heterozygosity and sample genetic distances are evaluated, and formulae for obtaining unbiased estimates of average heterozygosity and genetic distance are developed. It is also shown that the number of individuals to be used for estimating average heterozygosity can be very small if a large number of loci are studied and the average heterozygosity is low. The number of individuals to be used for estimating genetic distance can also be very small if the genetic distance is large and the average heterozygosity of the two species compared is low.",1978,3,10867,1708,0,0,3,1,2,3,3,5,4,4
7dd73816b9e1d5079ef23d7a45ffc7ae42507464,"Examining the pattern of nucleotide substitution for the control region of mitochondrial DNA (mtDNA) in humans and chimpanzees, we developed a new mathematical method for estimating the number of transitional and transversional substitutions per site, as well as the total number of nucleotide substitutions. In this method, excess transitions, unequal nucleotide frequencies, and variation of substitution rate among different sites are all taken into account. Application of this method to human and chimpanzee data suggested that the transition/transversion ratio for the entire control region was approximately 15 and nearly the same for the two species. The 95% confidence interval of the age of the common ancestral mtDNA was estimated to be 80,000-480,000 years in humans and 0.57-2.72 Myr in common chimpanzees.",1993,30,9145,1153,5,16,29,37,76,76,95,111,166,186
379df72de684003963f11427c97490a8c2d2a593,,1966,19,11623,679,0,0,0,0,0,0,1,0,0,0
439672067967d3501b0d201bdb50a60886459b00,"Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population. Despite mixture models' usefulness in practice, one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deciding on the number of classes in a study population. This article presents the results of a simulation study that examines the performance of likelihood-based tests and the traditionally used Information Criterion (ICs) used for determining the number of classes in mixture modeling. We look at the performance of these tests and indexes for 3 types of mixture models: latent class analysis (LCA), a factor mixture model (FMA), and a growth mixture models (GMM). We evaluate the ability of the tests and indexes to correctly identify the number of classes at three different sample sizes (n = 200, 500, 1,000). Whereas the Bayesian Information Criterion performed the best of the ICs, the bootstrap likelihood ratio test proved to be a very consistent indicator of classes across all of the models considered.",2007,47,6187,417,27,53,88,131,188,268,311,388,444,569
c8f359b3967ddef8e6d7f6ad58213a543d33ea22,"Miller (1956) summarized evidence that people can remember about seven chunks in short-term memory (STM) tasks. However, that number was meant more as a rough estimate and a rhetorical device than as a real capacity limit. Others have since suggested that there is a more precise capacity limit, but that it is only three to five chunks. The present target article brings together a wide variety of data on capacity limits suggesting that the smaller capacity limit is real. Capacity limits will be useful in analyses of information processing only if the boundary conditions for observing them can be carefully described. Four basic conditions in which chunks can be identified and capacity limits can accordingly be observed are: (1) when information overload limits chunks to individual stimulus items, (2) when other steps are taken specifically to block the recoding of stimulus items into larger chunks, (3) in performance discontinuities caused by the capacity limit, and (4) in various indirect effects of the capacity limit. Under these conditions, rehearsal and long-term memory cannot be used to combine stimulus items into chunks of an unknown size; nor can storage mechanisms that are not capacity-limited, such as sensory memory, allow the capacity-limited storage mechanism to be refilled during recall. A single, central capacity limit averaging about four chunks is implicated along with other, noncapacity-limited sources. The pure STM capacity limit expressed in chunks is distinguished from compound STM limits obtained when the number of separately held chunks is unclear. Reasons why pure capacity estimates fall within a narrow range are discussed and a capacity limit for the focus of attention is proposed.",2001,484,5275,534,24,48,70,96,129,143,173,216,260,253
db966ed68e8e435885b67012ec96c20662e6de9b,"A direct numerical simulation of a turbulent channel flow is performed. The unsteady Navier-Stokes equations are solved numerically at a Reynolds number of 3300, based on thc mean centreline velocity and channel half-width, with about 4 x los grid points (192 x 129 x 160 in 2, y, 2). All essential turbulence scales are resolved on the computational grid and no subgrid model is used. A large number of turbulence statistics are computed and compared with the existing experimental data at comparable Reynolds numbers. Agreements as well as discrepancies are discussed in detail. Particular attention is given to the behaviour of turbulence correlations near the wall. In addition, a number of statistical correlations which are complementary to the existing experimental data are reported for the first time.",1987,58,4457,383,13,15,38,39,66,48,83,74,81,94
df719e6e99b07912c33645e174a8aa7ff0ace68b,"AIM: To estimate the prevalence of glaucoma among people worldwide. METHODS: Available published data on glaucoma prevalence were reviewed to determine the relation of open angle and angle closure glaucoma with age in people of European, African, and Asian origin. A comparison was made with estimated world population data for the year 2000. RESULTS: The number of people with primary glaucoma in the world by the year 2000 is estimated at nearly 66.8 million, with 6.7 million suffering from bilateral blindness. In developed countries, fewer than 50% of those with glaucoma are aware of their disease. In the developing world, the rate of known disease is even lower. CONCLUSIONS: Glaucoma is the second leading cause of vision loss in the world. Improved methods of screening and therapy for glaucoma are urgently needed.",1996,59,5358,294,0,11,27,37,49,62,67,104,78,104
1efc86d78b94931fefdfd47614299f6c6c17980e,"It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares “capitalization” on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted.",1965,8,5806,346,1,3,4,4,1,4,3,2,5,2
d745f239021ce1ff976eb9bfe0e5b788c12304c6,"Abstract The distribution is obtained for the number of segregating sites observed in a sample from a population which is subject to recurring, new, mutations but not subject to recombination. After allowance is made for the different effective population sizes, the results apply approximately to three population models, due to Wright, Burrows and Cockerham, and Moran. Included as extreme special cases are the distributions of the number of segregating sites in the whole population and of the number of heterozygous sites in a diploid individual. Some results of Fisher, Haldane, Kimura, and Ewens concerning the means of the distributions for different models are confirmed, but the variances, and the distributions themselves, are new.",1975,36,3693,391,0,2,1,1,3,1,4,6,2,4
2454e846733f0f0d6ae98c489a632a7199d07ed6,"Preface 1. Monte Carlo methods and Quasi-Monte Carlo methods 2. Quasi-Monte Carlo methods for numerical integration 3. Low-discrepancy point sets and sequences 4. Nets and (t,s)-sequences 5. Lattice rules for numerical integration 6. Quasi- Monte Carlo methods for optimization 7. Random numbers and pseudorandom numbers 8. Nonlinear congruential pseudorandom numbers 9. Shift-Register pseudorandom numbers 10. Pseudorandom vector generation Appendix A. Finite fields and linear recurring sequences Appendix B. Continued fractions Bibliography Index.",1992,0,3654,361,4,17,30,59,71,79,110,75,82,101
6fac7b336a33916919050e17543ca8f4d1cf9ee4,"Low Reynolds number flow theory finds wide application in such diverse fields as sedimentation, fluidization, particle-size classification, dust and mist collection, filtration, centrifugation, polymer and suspension rheology, flow through porous media, colloid science, aerosol and hydrosal technology, lubrication theory, blood flow, Brownian motion, geophysics, meteorology, and a host of other disciplines. This text provides a comprehensive and detailed account of the physical and mathematical principles underlying such phenomena, heretofore available only in the original literature.",1965,0,4307,286,0,5,6,8,7,17,16,17,23,22
4249e1d658670c2f3253ecf384f31029acbaddeb,"Copy number variation (CNV) of DNA sequences is functionally significant but has yet to be fully ascertained. We have constructed a first-generation CNV map of the human genome through the study of 270 individuals from four populations with ancestry in Europe, Africa or Asia (the HapMap collection). DNA from these individuals was screened for CNV using two complementary technologies: single-nucleotide polymorphism (SNP) genotyping arrays, and clone-based comparative genomic hybridization. A total of 1,447 copy number variable regions (CNVRs), which can encompass overlapping or adjacent gains or losses, covering 360 megabases (12% of the genome) were identified in these populations. These CNVRs contained hundreds of genes, disease loci, functional elements and segmental duplications. Notably, the CNVRs encompassed more nucleotide content per genome than SNPs, underscoring the importance of CNV in genetic diversity and evolution. The data obtained delineate linkage disequilibrium patterns for many CNVs, and reveal marked variation in copy number among populations. We also demonstrate the utility of this resource for genetic disease studies.",2006,82,4121,185,13,328,453,509,460,351,340,310,238,208
cf7d72b1c0f495cf36a1d9a37565ca7f1ae07cca,"On applique la methode d'Efron (1981, 1982) a la construction d'intervalles de confiance bases sur des distributions du bootstrap",1984,23,3450,381,0,0,0,2,2,1,0,3,4,1
89c8179cce5887300a8b588c86cfd3e6db0b2801,"We propose a method (the ‘gap statistic’) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.",2000,20,4411,220,1,11,24,49,47,55,81,111,125,148
2f5ec0fcdee047e21c3e6987a2e17f642d128574,"This is the first volume of a two-volume textbook which evolved from a course (Mathematics 160) offered at the California Institute of Technology during the last 25 years. It provides an introduction to analytic number 
theory suitable for undergraduates with some background in advanced calculus, but with no previous knowledge of number theory. Actually, a great deal of the book requires no calculus at all and could profitably be studied by sophisticated high school students. 
 
Number theory is such a vast and rich field that a one-year course cannot do justice to all its parts. The choice of topics included here is intended to provide some variety and some depth. Problems which have fascinated generations of professional and amateur mathematicians are discussed 
together with some of the techniques for solving them. 
 
One of the goals of this course has been to nurture the intrinsic interest that many young mathematics students seem to have in number theory and to open some doors for them to the current periodical literature. It has been 
gratifying to note that many of the students who have taken this course during the past 25 years have become professional mathematicians, and some have made notable contributions of their own to number theory. To all of 
them this book is dedicated.",1976,0,3255,301,1,2,7,5,10,7,10,8,9,7
61f7a2593b323f3ecdf1a556d83c614a9995cb08,"Abstract : This paper discusses some aspects of selecting and testing random and pseudorandom number generators. The outputs of such generators may he used in many cryptographic applications, such as the generation of key material. Generators suitable for use in cryptographic applications may need to meet stronger requirements than for other applications. In particular, their outputs must he unpredictable in the absence of knowledge of the inputs. Some criteria for characterizing and selecting appropriate generators are discussed in this document. The subject of statistical testing and its relation to cryptanalysis is also discussed, and some recommended statistical tests are provided. These tests may he useful as a first step in determining whether or not a generator is suitable for a particular cryptographic application. The design and cryptanalysis of generators is outside the scope of this paper.",2000,52,3195,356,1,5,10,17,29,42,53,58,77,119
098d5792ffa43e9885f9fc644ffdd7b6a59b0922,"A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 2<supscrpt>19937</supscrpt> −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to <italic>v</italic> bits accuracy for 1 ≤ <italic>v</italic> ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity <italic>O(p<supscrpt>2</supscrpt>)</italic> where  <italic>p</italic> is the degree of the polynomial.
We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.",1998,62,5342,203,9,13,33,27,61,86,119,178,196,216
856e2c985825d055ec619a4ac9bf408182fd35a8,"Popular statistical software packages do not have the proper procedures for determining the number of components in factor and principal components analyses. Parallel analysis and Velicer’s minimum average partial (MAP) test are validated procedures, recommended widely by statisticians. However, many researchers continue to use alternative, simpler, but flawed procedures, such as the eigenvaluesgreater-than-one rule. Use of the proper procedures might be increased if these procedures could be conducted within familiar software environments. This paper describes brief and efficient programs for using SPSS and SAS to conduct parallel analyses and the MAP test.",2000,24,3220,219,3,0,2,10,20,44,62,92,107,109
afc8da96925a24c778533ec5c9d02ca8db5f06b2,"In this paper we develop some econometric theory for factor models of large dimensions. The focus is the determination of the number of factors, which is an unresolved issue in the rapidly growing literature on multifactor models. We propose some panel C(p) criteria and show that the number of factors can be consistently estimated using the criteria. The theory is developed under the framework of large cross-sections (N) and large time dimensions (T). No restriction is imposed on the relation between N and T. Simulations show that the proposed criteria yield almost precise estimates of the number of factors for configurations of the panel data encountered in practice.",2000,51,2871,606,3,8,21,36,50,73,70,89,98,120
b49caa46aa223d5443846adb65ccf2d0ceb9a729,"Aim: To estimate the number of people with open angle (OAG) and angle closure glaucoma (ACG) in 2010 and 2020. Methods: A review of published data with use of prevalence models. Data from population based studies of age specific prevalence of OAG and ACG that satisfied standard definitions were used to construct prevalence models for OAG and ACG by age, sex, and ethnicity, weighting data proportional to sample size of each study. Models were combined with UN world population projections for 2010 and 2020 to derive the estimated number with glaucoma. Results: There will be 60.5 million people with OAG and ACG in 2010, increasing to 79.6 million by 2020, and of these, 74% will have OAG. Women will comprise 55% of OAG, 70% of ACG, and 59% of all glaucoma in 2010. Asians will represent 47% of those with glaucoma and 87% of those with ACG. Bilateral blindness will be present in 4.5 million people with OAG and 3.9 million people with ACG in 2010, rising to 5.9 and 5.3 million people in 2020, respectively. Conclusions: Glaucoma is the second leading cause of blindness worldwide, disproportionately affecting women and Asians.",2006,63,5668,116,24,108,153,159,184,264,301,377,468,515
55f73ca18b8dd75eb0365e449321766e6860c23b,"The optimum routing of a fleet of trucks of varying capacities from a central depot to a number of delivery points may require a selection from a very large number of possible routes, if the number of delivery points is also large. This paper, after considering certain theoretical aspects of the problem, develops an iterative procedure that enables the rapid selection of an optimum or near-optimum route. It has been programmed for a digital computer but is also suitable for hand computation.",1964,1,3564,263,0,0,0,2,5,7,0,5,11,2
58bdeda2c4be279c060034c3eeef495e9d7c7799,"A high number of tree species, low density of adults of each species, and long distances between conspecific adults are characteristic of many low-land tropical forest habitats. I propose that these three traits, in large part, are the result of the action of predators on seeds and seedlings. A model is presented that allows detailed examination of the effect of different predators, dispersal agents, seed-crop sizes, etc. on these three traits. In short, any event that increases the efficiency of the predators at eating seeds and seedlings of a given tree species may lead to a reduction in population density of the adults of that species and/or to increased distance between new adults and their parents. Either event will lead to more space in the habitat for other species of trees, and therefore higher total number of tree species, provided seed sources are available over evolutionary time. As one moves from the wet lowland tropics to the dry tropics or temperate zones, the seed and seedling predators in a habitat are hypothesized to be progressively less efficient at keeping one or a few tree species from monopolizing the habitat through competitive superiority. This lowered efficiency of the predators is brought about by the increased severity and unpredictability of the physical environment, which in turn leads to regular or erratic escape of large seed or seedling cohorts from the predators.",1970,72,4029,187,0,8,11,10,28,23,23,29,19,21
8c362d0fd11e593bd27bd5b655c9cc04969a753f,"This article considers forecasting a single time series when there are many predictors (N) and time series observations (T). When the data follow an approximate factor model, the predictors can be summarized by a small number of indexes, which we estimate using principal components. Feasible forecasts are shown to be asymptotically efficient in the sense that the difference between the feasible forecasts and the infeasible forecasts constructed using the actual values of the factors converges in probability to 0 as both N and T grow large. The estimated factors are shown to be consistent, even in the presence of time variation in the factor model.",2002,30,2653,349,5,16,20,39,51,65,88,124,102,155
1ac6ffb033a1e2d704279f3a496a4b919ffcbb25,"In systematic searches for embryonic lethal mutants of Drosophila melanogaster we have identified 15 loci which when mutated alter the segmental pattern of the larva. These loci probably represent the majority of such genes in Drosophila. The phenotypes of the mutant embryos indicate that the process of segmentation involves at least three levels of spatial organization: the entire egg as developmental unit, a repeat unit with the length of two segments, and the individual segment.",1980,30,3953,124,3,12,13,10,25,46,53,53,80,67
c01d227077dae98b82c68d9871e8d28dc24a0887,"A description of 148 algorithms fundamental to number-theoretic computations, in particular for computations related to algebraic number theory, elliptic curves, primality testing and factoring. The first seven chapters guide readers to the heart of current research in computational algebraic number theory, including recent algorithms for computing class groups and units, as well as elliptic curve computations, while the last three chapters survey factoring and primality testing methods, including a detailed description of the number field sieve algorithm. The whole is rounded off with a description of available computer packages and some useful tables, backed by numerous exercises. Written by an authority in the field, and one with great practical and teaching experience, this is certain to become the standard and indispensable reference on the subject.",1993,229,2775,210,3,13,27,41,64,80,66,97,76,94
15f37ff89f68747a3f4cb34cb6097ba79b201f92,"We performed a Monte Carlo study to evaluate the effect of the number of events per variable (EPV) analyzed in logistic regression analysis. The simulations were based on data from a cardiac trial of 673 patients in which 252 deaths occurred and seven variables were cogent predictors of mortality; the number of events per predictive variable was (252/7 =) 36 for the full sample. For the simulations, at values of EPV = 2, 5, 10, 15, 20, and 25, we randomly generated 500 samples of the 673 patients, chosen with replacement, according to a logistic model derived from the full sample. Simulation results for the regression coefficients for each variable in each group of 500 samples were compared for bias, precision, and significance testing against the results of the model fitted to the original sample. For EPV values of 10 or greater, no major problems occurred. For EPV values less than 10, however, the regression coefficients were biased in both positive and negative directions; the large sample variance estimates from the logistic model both overestimated and underestimated the sample variance of the regression coefficients; the 90% confidence limits about the estimated values did not have proper coverage; the Wald statistic was conservative under the null hypothesis; and paradoxical associations (significance in the wrong direction) were increased. Although other factors (such as the total number of events, or sample size) may influence the validity of the logistic model, our findings indicate that low EPV can lead to major problems.",1996,19,5489,101,0,2,4,7,16,32,27,36,54,47
6befd7e6d780a3247d169ecaf858d720d7311c6c,"A stereological method for obtaining estimates of the total number of neurons in five major subdivisions of the rat hippocampus is described. The new method, the optical fractionator, combines two recent developments in stereology: a three‐dimensional probe for counting neuronal nuclei, the optical disector, and a systematic uniform sampling scheme, the fractionator. The optical disector results in unbiased estimates of neuron number, i.e., estimates that are free of assumptions about neuron size and shape, are unaffected by lost caps and over‐projection, and approach the true number of neurons in an unlimited manner as the number of samples is increased. The fractionator involves sampling a known fraction of a structural component. In the case of neuron number, a zero dimensional quantity, it provides estimates that are unaffected by shrinkage before, during, and after processing of the tissue. Because the fractionator involves systematic sampling, it also results in highly efficient estimates. Typically only 100–200 neurons must be counted in an animal to obtain a precision that is compatible with experimental studies. The methodology is compared with those used in earlier works involving estimates of neuron number in the rat hippocampus and a number of new stereological methods that have particular relevance to the quantitative study of the structure of the nervous system are briefly described in an appendix.",1991,33,2904,223,2,7,14,14,13,23,34,32,62,72
3972dbc40fd2733214028a5d6e9ab961d7c5c41b,"Abstract A new k-ϵ eddy viscosity model, which consists of a new model dissipation rate equation and a new realizable eddy viscosity formulation, is proposed in this paper. The new model dissipation rate equation is based on the dynamic equation of the mean-square vorticity fluctuation at large turbulent Reynolds number. The new eddy viscosity formulation is based on the realizability constraints; the positivity of normal Reynolds stresses and the Schwarz' inequality for turbulent shear stresses. We find that the present model with a set of unified model coefficients can perform well for a variety of flows. The flows that are examined include: (i) rotating homogeneous shear flows; (ii) boundary-free shear flows including a mixing layer, planar and round jets; (iii) a channel flow, and flat plate boundary layers with and without a pressure gradient; and (iv) backward facing step separated flows. The model predictions are compared with available experimental data. The results from the standard k-ϵ eddy viscosity model are also included for comparison. It is shown that the present model is a significant improvement over the standard k-ϵ eddy viscosity model.",1995,23,3854,102,5,7,4,8,8,7,11,33,39,51
e46ac802d7207e0e51b5333456a3f46519c2f92d,"All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. Regle generale, les methodes numeriques enregistrent des lignes avec beaucoup plus de donnees qu'il n'est necessaire a la reproduction graphique precise ou a la recherche par ordinateur. L'auteur presente deux algorithmes pour reduire le nombre de points necessaires pour representer la ligne et produire des caricatures si desire, et les compare aux methodes les plus prometteuses suggerees jusqu'ici. La reduction de la ligne constituera une partie importante de la generalisation automatique.",1973,5,3642,163,0,0,1,2,3,1,3,0,4,2
9d46c57d778b9a15634e9c42b3dbc8c23088ea43,,1989,0,4196,90,0,5,16,28,59,60,113,153,206,265
d7d385f45c096082812deb1623e5af2c2915b4a9,"Despite its popularity for general clustering, K-means suuers three major shortcomings; it scales poorly computationally, the number of clusters K has to be supplied by the user, and the search is prone to local minima. We propose solutions for the rst two problems, and a partial remedy for the third. Building on prior work for algorithmic acceleration that is not based on approximation, we introduce a new algorithm that eeciently, searches the space of cluster locations and number of clusters to optimize the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC) measure. The innovations include two new ways of exploiting cached suucient statistics and a new very eecient test that in one K-means sweep selects the most promising subset of classes for reenement. This gives rise to a fast, statistically founded algorithm that outputs both the number of classes and their parameters. Experiments show this technique reveals the true number of classes in the underlying distribution , and that it is much faster than repeatedly using accelerated K-means for different values of K.",2000,17,2456,214,4,11,17,14,37,49,68,93,88,108
f57819360c6e00d0ebf0a3d2d41ff79c2bf14d80,"Editor’s note: This is a reprint (slightly edited) of a paper of the same title that appeared in the book Physics and Our World: A Symposium in Honor of Victor F. Weisskopf, published by the American Institute of Physics (1976). The personal tone of the original talk has been preserved in the paper, which was itself a slightly edited transcript of a tape. The figures reproduce transparencies used in the talk. The demonstration involved a tall rectangular transparent vessel of corn syrup, projected by an overhead projector turned on its side. Some essential hand waving could not be reproduced.",1977,0,3269,159,1,2,0,4,5,4,8,9,10,10
9d0ace7494b1b0075cdec51e684f0de4974efce7,"We examined the number of tropical cyclones and cyclone days as well as tropical cyclone intensity over the past 35 years, in an environment of increasing sea surface temperature. A large increase was seen in the number and proportion of hurricanes reaching categories 4 and 5. The largest increase occurred in the North Pacific, Indian, and Southwest Pacific Oceans, and the smallest percentage increase occurred in the North Atlantic Ocean. These increases have taken place while the number of cyclones and cyclone days has decreased in all basins except the North Atlantic during the past decade.",2005,81,2850,114,15,141,207,216,176,226,179,200,193,176
29f456c100d70eea88805aa6d5091df3f4338200,Introduction Arithmetic functions Elementary theory of prime numbers Characters Summation formulas Classical analytic theory of $L$-functions Elementary sieve methods Bilinear forms and the large sieve Exponential sums The Dirichlet polynomials Zero-density estimates Sums over finite fields Character sums Sums over primes Holomorphic modular forms Spectral theory of automorphic forms Sums of Kloosterman sums Primes in arithmetic progressions The least prime in an arithmetic progression The Goldbach problem The circle method Equidistribution Imaginary quadratic fields Effective bounds for the class number The critical zeros of the Riemann zeta function The spacing of zeros of the Riemann zeta-function Central values of $L$-functions Bibliography Index.,2004,388,2643,174,23,37,61,80,109,117,169,161,179,157
a3b7e80260891dcd3844b1835df8dee3a1cd67c7,"L'A. propose quelques observations concernant l'ouvrage de Stanislas Dehaene The number sense. How the mind creates mathematics (1997) qui explore tous les aspects de la relation entre les hommes et les nombres : la numerosite chez les autres animaux, la numerosite et le calcul simple chez les bebes, l'histoire de l'expression du nombre dans le langage, l'histoire de la notation du nombre, le circuit neuronal necessaire pour faire de l'arithmetique et du calcul, la localisation dans le cerveau, l'ordre mathematique de l'univers, etc ... L'A. examine ici en particulier les questions portant sur la relation entre les nombres et le langage dans une perspective cognitive, puis explique ce que Dehaene entend par le sens du nombre en caracterisant les mathematiques comme une formalisation progressive de nos intuitions sur les ensembles, le nombre, l'espace, le temps et la logique",1998,0,2213,227,4,14,21,28,32,48,41,66,55,68
0c1c4b3b78e54381e1a1011843ffa1924af2da83,,1986,58,2865,151,2,32,11,26,21,25,24,34,21,36
fddf2adb6ab085bc6231e72ba39904d7d2af57af,,1980,0,2501,223,0,2,0,0,0,1,0,0,0,1
f442aca9c8a754a9ef75ef68d2bd436080609d17,"Did evolution endow the human brain with a predisposition to represent and acquire knowledge about numbers? Although the parietal lobe has been suggested as a potential substrate for a domain-specific representation of quantities, it is also engaged in verbal, spatial, and attentional functions that may contribute to calculation. To clarify the organisation of number-related processes in the parietal lobe, we examine the three-dimensional intersection of fMRI activations during various numerical tasks, and also review the corresponding neuropsychological evidence. On this basis, we propose a tentative tripartite organisation. The horizontal segment of the intraparietal sulcus (HIPS) appears as a plausible candidate for domain specificity: It is systematically activated whenever numbers are manipulated, independently of number notation, and with increasing activation as the task puts greater emphasis on quantity processing. Depending on task demands, we speculate that this core quantity system, analogous to an internal “number line,” can be supplemented by two other circuits. A left angular gyrus area, in connection with other left-hemispheric perisylvian areas, supports the manipulation of numbers in verbal form. Finally, a bilateral posterior superior parietal system supports attentional orientation on the mental number line, just like on any other spatial dimension.",2003,129,2160,210,9,35,54,76,80,107,137,119,146,131
22b4fc0bd039037cc903eb1c41fe851066289e0a,"BackgroundThe integrity of RNA molecules is of paramount importance for experiments that try to reflect the snapshot of gene expression at the moment of RNA extraction. Until recently, there has been no reliable standard for estimating the integrity of RNA samples and the ratio of 28S:18S ribosomal RNA, the common measure for this purpose, has been shown to be inconsistent. The advent of microcapillary electrophoretic RNA separation provides the basis for an automated high-throughput approach, in order to estimate the integrity of RNA samples in an unambiguous way.MethodsA method is introduced that automatically selects features from signal measurements and constructs regression models based on a Bayesian learning technique. Feature spaces of different dimensionality are compared in the Bayesian framework, which allows selecting a final feature combination corresponding to models with high posterior probability.ResultsThis approach is applied to a large collection of electrophoretic RNA measurements recorded with an Agilent 2100 bioanalyzer to extract an algorithm that describes RNA integrity. The resulting algorithm is a user-independent, automated and reliable procedure for standardization of RNA quality control that allows the calculation of an RNA integrity number (RIN).ConclusionOur results show the importance of taking characteristics of several regions of the recorded electropherogram into account in order to get a robust and reliable prediction of RNA integrity, especially if compared to traditional methods.",2006,23,2215,198,18,40,82,105,129,161,181,192,177,190
0308b4fdab1e08bc9492285edce9afd35f3dab0b,"We demonstrate that, under a theorem proposed by Vuong, the likelihood ratio statistic based on the Kullback-Leibler information criterion of the null hypothesis that a random sample is drawn from a k 0 -component normal mixture distribution against the alternative hypothesis that the sample is drawn from a k 1 -component normal mixture distribution is asymptotically distributed as a weighted sum of independent chi-squared random variables with one degree of freedom, under general regularity conditions. We report simulation studies of two cases where we are testing a single normal versus a two-component normal mixture and a two-component normal mixture versus a three-component normal mixture. An empirical adjustment to the likelihood ratio statistic is proposed that appears to improve the rate of convergence to the limiting distribution.",2001,0,2953,96,1,2,7,18,19,28,58,53,73,106
2c6fe95cdcb4aeca4bd6c0099fefa867fca27652,"Nine experiments of timed odd-even judgments examined how parity and number magnitude are accessed from Arabic and verbal numerals. With Arabic numerals, Ss used the rightmost digit to access a store of semantic number knowledge. Verbal numerals went through an additional stage of transcoding to base 10. Magnitude information was automatically accessed from Arabic numerals. Large numbers preferentially elicited a rightward response, and small numbers a leftward response. The Spatial-Numerical Association of Response Codes (SNARC) effect depended only on relative number magnitude and was weaker or absent with letters or verbal numerals. Direction did not vary with handedness or hemispheric dominance but was linked to the direction of writing, as it faded or even reversed in right-to-left writing Iranian Ss",1993,44,2131,240,0,1,4,3,5,4,13,6,12,10
39990d887387b14f6d8e6918b37c6f8ea93bf8ff,"We tested the hypothesis that de novo copy number variation (CNV) is associated with autism spectrum disorders (ASDs). We performed comparative genomic hybridization (CGH) on the genomic DNA of patients and unaffected subjects to detect copy number variants not present in their respective parents. Candidate genomic regions were validated by higher-resolution CGH, paternity testing, cytogenetics, fluorescence in situ hybridization, and microsatellite genotyping. Confirmed de novo CNVs were significantly associated with autism (P = 0.0005). Such CNVs were identified in 12 out of 118 (10%) of patients with sporadic autism, in 2 out of 77 (3%) of patients with an affected first-degree relative, and in 2 out of 196 (1%) of controls. Most de novo CNVs were smaller than microscopic resolution. Affected genomic regions were highly heterogeneous and included mutations of single genes. These findings establish de novo germline mutation as a more significant risk factor for ASD than previously recognized.",2007,31,2615,81,49,170,273,255,248,227,245,183,191,152
37223160fd148c3956f866f27e4e7d4198a64d4f,,1980,0,2578,117,0,0,0,0,0,0,0,1,1,0
4fd5f6b7c036c7522d82f48e7cfd384e38328d6f,I: Algebraic Integers.- II: The Theory of Valuations.- III: Riemann-Roch Theory.- IV: Abstract Class Field Theory.- V: Local Class Field Theory.- VI: Global Class Field Theory.- VII: Zeta Functions and L-series.,1999,7,2632,114,46,70,60,80,80,104,100,94,104,114
02b28f3b71138a06e40dbd614abf8568420ae183,"We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% for the combination of all features.",2008,20,1565,238,1,11,28,30,35,53,75,87,109,152
9ceedeb5e2ebc39a08c70e2d42d69bca0e064bc1,"A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hierarchical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the performance of some of the criteria may be data dependent.",1985,56,2759,108,4,2,14,10,10,16,10,18,27,37
874cc3d7f38afed3543f961b49511aead2c91457,"DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modification of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set.",2004,29,2095,183,2,25,41,100,86,117,156,149,149,148
5eca27f33bc6b051b3e7e9ac84b9bc41f4c0e13b,"Abstract— Recent studies provide increasing evidence that postnatal neovascularization involves bone marrow-derived circulating endothelial progenitor cells (EPCs). The regulation of EPCs in patients with coronary artery disease (CAD) is unclear at present. Therefore, we determined the number and functional activity of EPCs in 45 patients with CAD and 15 healthy volunteers. The numbers of isolated EPCs and circulating CD34/kinase insert domain receptor (KDR)-positive precursor cells were significantly reduced in patients with CAD by ≈40% and 48%, respectively. To determine the influence of atherosclerotic risk factors, a risk factor score including age, sex, hypertension, diabetes, smoking, positive family history of CAD, and LDL cholesterol levels was used. The number of risk factors was significantly correlated with a reduction of EPC levels (R =−0.394, P =0.002) and CD34-/KDR-positive cells (R =−0.537, P <0.001). Analysis of the individual risk factors demonstrated that smokers had significantly reduced levels of EPCs (P <0.001) and CD34-/KDR-positive cells (P =0.003). Moreover, a positive family history of CAD was associated with reduced CD34-/KDR-positive cells (P =0.011). Most importantly, EPCs isolated from patients with CAD also revealed an impaired migratory response, which was inversely correlated with the number of risk factors (R =−0.484, P =0.002). By multivariate analysis, hypertension was identified as a major independent predictor for impaired EPC migration (P =0.043). The present study demonstrates that patients with CAD revealed reduced levels and functional impairment of EPCs, which correlated with risk factors for CAD. Given the important role of EPCs for neovascularization of ischemic tissue, the decrease of EPC numbers and activity may contribute to impaired vascularization in patients with CAD. The full text of this article is available at http://www.circresaha.org.",2001,36,2384,121,0,22,53,102,122,140,190,177,168,239
281939819f7ca10ea68d66913bbe2de34852d63a,"The extent to which large duplications and deletions contribute to human genetic variation and diversity is unknown. Here, we show that large-scale copy number polymorphisms (CNPs) (about 100 kilobases and greater) contribute substantially to genomic variation between normal humans. Representational oligonucleotide microarray analysis of 20 individuals revealed a total of 221 copy number differences representing 76 unique CNPs. On average, individuals differed by 11 CNPs, and the average length of a CNP interval was 465 kilobases. We observed copy number variation of 70 different genes within CNP intervals, including genes involved in neurological function, regulation of cell growth, regulation of metabolism, and several genes known to be associated with disease.",2004,84,2520,81,14,156,220,229,224,253,229,171,169,162
6ce7876c0b2acb52b88610ce9cfe21d239c28922,"New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.",1997,121,1937,222,12,37,38,52,66,59,54,87,77,109
25f5eb7f3075ea69fbd6fbf6db60d3e342b23cf2,"The qEffectiveq Number of Parties: qA Measure with Application to West Europeq Laakso, Markku;Taagepera, Rein Comparative Political Studies; Apr 1, 1979; 12, 1; Proouest pg. 3 “EFFECTIVE” NUMBER OF PARTIES A Measure with Application to West Europe MARKKU LAAKSO University of Helsinki REIN TAAGEPERA University of California, Irvine I s a large number of parties bound to destabilize a political system (Duverger, 1954) or is it not (e.g., Lijphart, 1968; Nilson, 1974)? Before this question can be answered, the number of parties must be operationally deﬁned in a way that takes into account their relative size. Such a number is also needed if one wants to detect trends toward fewer or more numerous parties over time, or the effects of a proposed change in electoral rules. This article presents ways to calculate this important political variable, calculates it for I42 post-1944 elections in 15 West European countries, and analyzes its possible effect on stability. We often talk of two-party and multiparty systems. We further dis- tinguish three~ or four—party systems in some countries, and even talk (e. g., Blondel, 1969: 535) of a two-and-a-half-party system whenthere is a third party of marginal size. Mexico could be viewed as a one-and-a- half-party system because the PR1 is so much larger than all other parties. Rather than take the number of all existing parties, including even the very smallest, one visibly has a need for a number that takes into account their relative size. We will call this number the “effective number of parties,” using the word “effectiveq somewhat in the sense pressure group literature uses it when talking about “effective access” (Truman, 195]: 506), but even more in the operational sense physicists give it when they talk about effective current (Richards et al., 1960: 594), COMPARATIVE POLITICAL STUDIES. Vol. I2 No. I. April 1979 3-27 © I979 Sage Publications. Inc. Copyright (c) 2000 Bell & Howell Information and Learning Company Copyright (c) Sage Inc.",1979,26,2653,73,0,2,4,0,2,1,1,2,4,1
4457bde4d1698a8a678d04841d3b455558d7f19c,"The relationship between the energy expended per offspring, fitness of offspring, and parental fitness is presented in a two-dimensional graphical model. The validity of the model in determining an optimal parental strategy is demonstrated analytically. The model applies under various conditions of parental care and sibling care for the offspring but is most useful for species that produce numerous small offspring which are given no parental care.",1974,9,2664,105,0,1,8,7,15,9,11,13,19,7
53ddafe17cfe0bcbf0932e454cf6fcc627c68380,"Companies spend lots of time and money on complex tools to assess customer satisfaction. But they're measuring the wrong thing. The best predictor of top-line growth can usually be captured in a single survey question: Would you recommend this company to a friend? This finding is based on two years of research in which a variety of survey questions were tested by linking the responses with actual customer behavior--purchasing patterns and referrals--and ultimately with company growth. Surprisingly, the most effective question wasn't about customer satisfaction or even loyalty per se. In most of the industries studied, the percentage of customers enthusiastic enough about a company to refer it to a friend or colleague directly correlated with growth rates among competitors. Willingness to talk up a company or product to friends, family, and colleagues is one of the best indicators of loyalty because of the customer's sacrifice in making the recommendation. When customers act as references, they do more than indicate they've received good economic value from a company; they put their own reputations on the line. And they will risk their reputations only if they feel intense loyalty. The findings point to a new, simpler approach to customer research, one directly linked to a company's results. By substituting a single question--blunt tool though it may appear to be--for the complex black box of the customer satisfaction survey, companies can actually put consumer survey results to use and focus employees on the task of stimulating growth.",2003,0,2083,171,1,9,22,32,60,72,68,91,107,114
d07b6d71653fd96659939544214a30714ff7fbe0,"Bridging the gap between elementary number theory and the systematic study of advanced topics, A Classical Introduction to Modern Number Theory is a well-developed and accessible text that requires only a familiarity with basic abstract algebra. Historical development is stressed throughout, along with wide-ranging coverage of significant results with comparatively elementary proofs, some of them new. An extensive bibliography and many challenging exercises are also included. This second edition has been corrected and contains two new chapters which provide a complete proof of the Mordell-Weil theorem for elliptic curves over the rational numbers, and an overview of recent progress on the arithmetic of elliptic curves.",1982,0,2503,110,2,2,4,7,10,10,10,16,19,15
4463f383a43b63c9f4bd9bf3b332f166ec79f3ec,"Gene dosage variations occur in many diseases. In cancer, deletions and copy number increases contribute to alterations in the expression of tumour-suppressor genes and oncogenes, respectively. Developmental abnormalities, such as Down, Prader Willi, Angelman and Cri du Chat syndromes, result from gain or loss of one copy of a chromosome or chromosomal region. Thus, detection and mapping of copy number abnormalities provide an approach for associating aberrations with disease phenotype and for localizing critical genes. Comparative genomic hybridization(CGH) was developed for genome-wide analysis of DNA sequence copy number in a single experiment. In CGH, differentially labelled total genomic DNA from a 'test' and a 'reference' cell population are cohybridized to normal metaphase chromosomes, using blocking DNA to suppress signals from repetitive sequences. The resulting ratio of the fluorescence intensities at a location on the 'cytogenetic map', provided by the chromosomes, is approximately proportional to the ratio of the copy numbers of the corresponding DNA sequences in the test and reference genomes. CGH has been broadly applied to human and mouse malignancies. The use of metaphase chromosomes, however, limits detection of events involving small regions (of less than 20 Mb) of the genome, resolution of closely spaced aberrations and linking ratio changes to genomic/genetic markers. Therefore, more laborious locus-by-locus techniques have been required for higher resolution studies. Hybridization to an array of mapped sequences instead of metaphase chromosomes could overcome the limitations of conventional CGH (ref. 6) if adequate performance could be achieved. Copy number would be related to the test/reference fluorescence ratio on the array targets, and genomic resolution could be determined by the map distance between the targets, or by the length of the cloned DNA segments. We describe here our implementation of array CGH. We demonstrate its ability to measure copy number with high precision in the human genome, and to analyse clinical specimens by obtaining new information on chromosome 20 aberrations in breast cancer.",1998,38,2316,82,2,25,42,63,87,113,162,185,206,211
a1c9051a645df0009820cc588004798429409a95,"A three‐dimensional counting rule and its integral test system, the disector, for obtaining unbiased estimates of the number of arbitrary particles in a specimen is presented. Used in combination with ordinary and recently developed stereological methods unbiased estimates of various mean particle sizes and the variance of particle volume are obtainable on sets of two parallel sections with a known separation. The same principle allows the unbiased estimation of the distribution of individual particle volumes in sets of serial sections.",1984,26,2384,88,1,8,11,14,29,31,48,60,62,80
ff4e4cb688b88b9975345bdbd3ab9dc67de81efa,"This book is a comprehensive study of cooperation among the advanced capitalist countries. Can cooperation persist without the dominance of a single power, such as the United States after World War II? To answer this pressing question, Robert Keohane analyzes the institutions, or ""international regimes,"" through which cooperation has taken place in the world political economy and describes the evolution of these regimes as American hegemony has eroded. Refuting the idea that the decline of hegemony makes cooperation impossible, he views international regimes not as weak substitutes for world government but as devices for facilitating decentralized cooperation among egoistic actors. In the preface the author addresses the issue of cooperation after the end of the Soviet empire and with the renewed dominance of the United States, in security matters, as well as recent scholarship on cooperation.",1984,0,4642,183,0,5,14,25,35,33,39,29,45,41
d997c12b1873b4dbcace542b93d1db75771eeb58,"The word Economy, or OEconomy, is derived from oikos, a house, and nomos, law, and meant originally only the wise and legitimate government of the house for the common good of the whole family. The meaning of the term was then extended to the government of that great family, the State. To distinguish these two senses of the word, the latter is called general or political economy, and the former domestic or particular economy. The first only is discussed in the present discourse.",2004,0,102,0,10,4,5,3,1,7,3,5,6,7
b219b0aee86dc7eab7f609e46e64a80a5d2d7470,Studies the impact of competitive import licenses on the economy. Value of rents associated with import licenses; Relationship between welfare cost of quantitative restrictions and tariff equivalents; Impact of wage legislation on equilibrium levels of unemployment. (Из Ebsco),1974,7,5093,146,1,5,5,1,3,6,8,11,18,20
f8928221d290a9cdd84d1de52e121373bc836caa,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",2001,12,2888,347,11,27,40,50,90,103,119,165,171,203
def0bfe9a0904afe303aa3e4fce0544da1d0b77f,"AbstractThe same rule which regulates the relative value of commodities in one country does not regulate the relative value of the commodities exchanged between two or more countries.Under a system of perfectly free commerce, each country naturally devotes its capital and labor to such employments as are most beneficial to each. This pursuit of individual advantage is admirably connected with the universal good of the whole. By stimulating industry, by rewarding ingenuity, and by using most efficaciously the peculiar powers bestowed by nature, it distributes labor most effectively and most economically: while, by increasing the general mass of productions, it diffuses general benefit, and binds together, by one common tie of interest and intercourse, the universal society of nations throughout the civilised world. It is this principle which determines that wine shall be made in France and Portugal, that corn sell be grown in America and Poland, and that hardware and other goods shall be manufactured in England…",1891,3,6001,146,0,0,0,0,0,0,0,0,0,0
c58b533f62714102ac0d4d3c11908d536db8cacb,This sociological classic is updated with a new preface by the authors looking at developments in the study of urban planning during the twenty-year life of this influential work.,1987,0,3157,120,0,5,14,15,28,25,25,39,31,42
e096761e7683e0c7f269dad6425c51ee6e5ef9e0,"Эта популярная книга, выдержавшая множество изданий, представляет собой резкую марксистскую критику ведущих (mainstream) масс медиа т.н. «демократических» стран, прежде всего США. В первой главе авторы предлагают теоретическую модель, называемую моделью пропаганды, согласно которой даже в демократических странах, где нет официальной цензуры и СМИ не принадлежат государству, функционирование масс медиа подчиняется механизмам «пропаганды», с той разницей, что они более скрыты. Авторы выделяют пять таких механизмов, или фильтров: (1) структура собственности СМИ и их ориентация на прибыль, (2) зависимость от рекламодателей; (3) зависимость от источников новостей; (4) «flak» - негативная обратная связь и неформальное давление; (5) антикоммунизм как общая идеологическая рамка. Остальные шесть глав представляют собой яркие иллюстрации модели пропаганды в действии: авторы подробно рассматривают такие примеры как освещение американскими СМИ выборов в Латинской Америке, войн во Вьетнаме, Лаосе и Камбодже и др. Авторы приходят к выводу, что Американские СМИ являются мощным идеологическим оружием, однако есть и силы, противостоящие их влиянию.",1988,18,2682,90,0,2,12,7,18,17,21,26,30,49
03d8a1a70f4e1e935b949bb12934047c5114a75b,"Three perspectives on the efficacy of social capital have been explored in the public health literature. A ""social support"" perspective argues that informal networks are central to objective and subjective welfare; an ""inequality"" thesis posits that widening economic disparities have eroded citizens' sense of social justice and inclusion, which in turn has led to heightened anxiety and compromised rising life expectancies; a ""political economy"" approach sees the primary determinant of poor health outcomes as the socially and politically mediated exclusion from material resources. A more comprehensive but grounded theory of social capital is presented that develops a distinction between bonding, bridging, and linking social capital. It is argued that this framework helps to reconcile these three perspectives, incorporating a broader reading of history, politics, and the empirical evidence regarding the mechanisms connecting types of network structure and state-society relations to public health outcomes.",2004,275,1816,109,10,26,59,58,73,64,78,75,107,105
31eb4d91d6c2d7182b8dfa080efa0ce5a12787d3,"Gayle S. Rubin, Associate Professor of Anthropology, Women’s Studies and Comparative Literature at the University of Michigan, made her first impact on feminist and gender theory in 1975 with the publication of her groundbreaking essay ‘The traffic in women: Notes on the “political economy” of sex’, in which she introduced the term ‘sex/gender system’ as a corrective to what she saw as the conceptual limitations of the word ‘patriarchy’ for theorising gender and sexuality. When ‘Thinking sex: Notes for a radical theory of the politics of sexuality’ (a ‘protoqueer’ text that became foundational to queer studies) was published in 1984, she had already established her reputation as a fearless and often controversial pioneering theorist of the politics of sexuality and an activist on behalf of sexual minorities, which brought her into open conflict with some sister feminists – notably those spearheading the anti-pornography lobby. As the title of this timely reader of Rubin’s work suggests, her essays are deviations from norm and doctrine in their sustained and reasoned refusal of convention, including those givens of feminism, and in their centring of the idea and practice of sexual deviance as intellectual concerns that are always, inextricably, both personal and political. (Rubin’s explicit appropriation of the terminology of 19th-century sexology is significant and the lineage of the term ‘deviant’ is comprehensively engaged in her essay ‘Studying sexual subcultures’). Her stated objective in ‘Thinking sex’ to ‘contribute to the pressing task of creating an accurate, humane, and genuinely liberatory body of thought about sexuality’ (p. 145) underpins the trajectory of the career mapped in her key essays and subsequent reflections on them in afterwords, postscripts and an interview conducted by Judith Butler included in this reader, which, in its entirety, stands as a necessary reminder of the role feminism should play ‘as a progressive, visionary force in the domain of sexuality’ (p. 275), especially when confronted by the cooption of its discourse by right-wing agendas. Reflecting on the retrospective imperatives of writing the introduction to Deviations, Rubin uses geology – a ‘recreational obsession’ of hers (p. 2) – as an analogy to situate the essays as ‘artifacts of very particular circumstances’ or ‘different matrices’ which ‘manifest a consistent lineage of theoretically interconnected interests’. The aptness of the analogy is increasingly evident as one reads through the sequence of essays – each ‘something like a piece of amber that preserves’ a particular cultural moment and place, to use Rubin’s description of ‘The traffic in women’ (p. 12) – and her later reviews of them in which additional contextualisation deepens one’s understanding of their significance, both then and now. She returns to this analogy in her final essay in the collection,",1975,19,2207,57,1,3,2,3,3,3,7,3,11,8
45a26329992de29ac40cf21698b1475f222608d9,"THE contents of the following pages can hardly meet with ready acceptance among those who regard the Science of Political Economy as having already acquired a nearly perfect form. I believe it is generally supposed that Adam Smith laid the foundations of this science; that Malthus, Anderson, and Senior added important doctrines; that Ricardo systematised the whole; and, finally, that Mr. J. S. Mill filled in the details and completely expounded this branch of knowledge. Mr. Mill appears to have had a similar notion; for he distinctly asserts that there was nothing; in the Laws of Value which remained for himself or any future writer to clear up. Doubtless it is difficult to help feeling that opinions adopted and confirmed by such eminent men have much weight of probability in their favour. Yet, in the other sciences this weight of authority has not been allowed to restrict the free examination of new opinions and theories; and it has often been ultimately proved that authority was on the wrong side.",1965,0,1706,30,1,1,3,3,1,1,1,3,3,1
fef218793641cf6a072efbcbe9c9585c6f01f1c3,"This book provides a comprehensive account of the structure, conduct, and performance of the centrally planned economies of Eastern Europe, the USSR, Communist China and the Marxist LDCs, looking at 26 nations in all. The author focuses on reform, perhaps the most important issue facing countries such as the USSR, Poland, Hungary, and China. Bureaucracy, soft budget constraints, markets, and the nature of the socialist state are the central issues that arise in the course of reforming a socialist economy. The first half of the book deals with 'classical socialism' and provides a theoretical summary of the main features of a now closed period of history. The second half deals with the processes of reform and concludes that the reform of classical socialist systems is doomed to failure as they are unable to renew themselves internally.",1993,0,1725,99,9,14,33,41,36,31,58,65,65,75
7f15ca64dc46114491694d80e15ca3169be8ad94,"The determinants of government responsiveness to its citizens is a key issue in political economy. Here we develop a model based on the solution of political agency problems. Having a more informed and politically active electorate strengthens incentives for governments to be responsive. This suggests that there is a role both for democratic institutions and the mass media in ensuring that the preferences of citizens are reflected in policy. The ideas behind the model are tested on panel data from India. We show that public food distribution and calamity relief expenditure are greater, controlling for shocks, where governments face greater electoral accountability and where newspaper circulation is highest.",2000,67,1555,95,2,8,20,28,36,50,45,72,70,70
db96e173025a8d786cedcac5bcc559417348c43b,"How does a state's natural resource wealth influence its economic development? For the past fifty years, versions of this question have been explored by both economists and political scientists. New research suggests that resource wealth tends to harm economic growth, yet there is little agreement on why this occurs. This article reviews a wide range of recent attempts in both economics and political science to explain the ""resource curse."" It suggests that much has been learned about the economic problems of resource exporters but less is known about their political problems. The disparity between strong findings on economic matters and weak findings on political ones partly reflects the failure of political scientists to carefully test their own theories.",1999,118,1641,84,1,3,11,15,18,38,48,55,60,68
7fd70d6c87ecabfcadea9f8f95840cf9bec79a25,"This article provides a statistical analysis of core contentions of the ‘varieties of capitalism’ perspective on comparative capitalism. The authors construct indices to assess whether patterns of co-ordination in the OECD economies conform to the predictions of the theory and compare the correspondence of institutions across subspheres of the political economy. They test whether institutional complementarities occur across these subspheres by estimating the impact of complementarities in labour relations and corporate governance on growth rates. To assess the durability of varieties of capitalism, they report on the extent of institutional change in the 1980s and 1990s. Powerful interaction effects across institutions in the subspheres of the political economy must be considered if assessments of the economic impact of institutional reform in any one sphere are to be accurate.",2009,159,842,57,38,55,55,71,76,62,82,86,40,61
3ff2e1a87971afa6c7a6de947c61a70086a3a64e,"POLITICS AND JOURNALISM HERMAN. EDWARD S. and NOAM CHOMSKY, Manufacturing Consent: The Political Economy of the Mass Media. New York: Pantheon Books, 1988. 412 pp., $24.95 cloth, $14.95 paper. In an important work of scholarship, Herman and Chomsky offer a ""propaganda model"" of the manner in which the media serve as a system-supportive institution by inculcating and reinforcing the economic, social and political agenda of the elite. Herman, a professor of finance at the University of Pennsylvania, and Chomsky, a professor in the department of linguistics and philosophy at the Massachusetts Institute of Technology, draw extensively on their previous research to offer a well-documented critique of the myth that the mass media serve as an ""independent"" social institution. At the crux of Herman and Chomsky's treatise is their propaganda model, consisting of five ""news filters"" which determine the nature and content of news. The first filter can best be characterized as ""concentration of ownership."" Through analysis of financial data in proxy statements, trade publications and annual reports, Herman and Chomsky provide compelling evidence that media organizations have a central stake in the maintenance of the social and economic status quo. The second news filter, heavy reliance on advertising revenue, is an extension of the first, for the ability to attract audiences with buying power attracts advertisers which, in turn, leads to greater concentration of wealth, power and media ownership. An extension of the work of several media sociologists, especially Mark Fishman, Gave Tuchman and Herbert Gans, the third filter the media's reliance on news ""beats"" and official sources in both the governmental and corporate realms. The section describing this filter also includes an excellent account of governmental public-information operations which strive to supply much of the ""official"" and legitimized information the media crave. The fourth and fifth frames are predominantly ideological in nature, and refer, respectively, to feedback mechanisms which delimit the media's occasional forays into criticism of the system, and the media's blind acceptance of the ""national religion"" of anticommunism. Both filters, Herman and Chomsky argue, are testament to the media's general reluctance to dispute the priorities, agenda and interests promulgated by government, corporations and even conservative interest groups. The confluence of these various forces and filters, according to the model, results in the media's selection of a dominant frame for each issue and concomitant selection of facts to fit each frame. The same can be said for the authors' description of the model itself; and that it is important to acknowledge the assumptions underlying this work. First, the main argument of the book is that the media are subject to many systemic influences that shape news content. In contrast, the underlying premise of the book appears that media ought to be independent of influence. This sentiment is best articulated in the last line of the book, following the author's various prescriptions for reform: ""Only to the extent that such developments succeed can we hope to see media that are free and independent. …",1989,0,1630,103,2,1,4,4,4,4,5,6,11,9
9c428b32aacc43a3192178dbd189460bb12c527c,1 COMPETING VIEWS OF GOVERNMENT The issues This book Background Economic Policy Making Political Economy Incentives and Selection in Politics Concluding Comments 2 THE ANATOMY OF GOVERNMENT FAILURE Introduction Three Notions of Government Failure The Basic Model Government Failure Democratic Political Failures A Dynamic Model Government Failure in the Dynamic Model Responses to Political Failure Concluding Comments 3 POLITICAL AGENCY AND ACCOUNTABILITY Introduction Elements of Political Agency Models The Baseline Model Extensions Discussion Concluding Comments 4 POLITICAL AGENCY AND PUBLIC FINANCE Introduction The Model Three Scenarios Implications Restraining Government Debt and Deficits Governments versus NGOs Competence Conclusions,2006,0,1160,145,13,31,49,70,68,82,98,94,98,98
de2d5bb79a30f238a7f89086b37dbe61a6c2adbb,"One of the most important developments over the past three decades has been the spread of liberal economic ideas and policies throughout the world. These policies have affected the lives of millions of people, yet our most sophisticated political economy models do not adequately capture influences on these policy choices. Evidence suggests that the adoption of liberal economic practices is highly clustered both temporally and spatially. We hypothesize that this clustering might be due to processes of policy diffusion. We think of diffusion as resulting from one of two broad sets of forces: one in which mounting adoptions of a policy alter the benefits of adopting for others and another in which adoptions provide policy relevant information about the benefits of adopting. We develop arguments within these broad classes of mechanisms, construct appropriate measures of the relevant concepts, and test their effects on liberalization and restriction of the current account, the capital account, and the exchange rate regime. Our findings suggest that domestic models of foreign economic policy making are insufficient. The evidence shows that policy transitions are influenced by international economic competition as well as the policies of a country's sociocultural peers. We interpret the latter influence as a form of channeled learning reflecting governments' search for appropriate models for economic policy.",2004,155,1317,49,13,37,46,69,80,80,75,82,83,98
933b226a52016e6a07da21d11141cb887ab4a7a6,"The first edition of Unequal Democracy was an instant classic, shattering illusions about American democracy and spurring scholarly and popular interest in the political causes and consequences of escalating economic inequality. This revised and expanded edition includes two new chapters on the political economy of the Obama era. One presents the Great Recession as a “stress test” of the American political system by analyzing the 2008 election and the impact of Barack Obama’s “New New Deal” on the economic fortunes of the rich, middle class, and poor. The other assesses the politics of inequality in the wake of the Occupy Wall Street movement, the 2012 election, and the partisan gridlock of Obama’s second term. Larry Bartels offers a sobering account of the barriers to change posed by partisan ideologies and the political power of the wealthy. He also provides new analyses of tax policy, partisan differences in economic performance, the struggle to raise the minimum wage, and inequalities in congressional representation. President Obama identified inequality as “the defining challenge of our time.” Unequal Democracy is the definitive account of how and why our political system has failed to rise to that challenge. Now more than ever, this is a book every American needs to read.",2008,0,1020,66,12,30,62,66,76,79,87,96,126,87
dea85ee5fbd8a5de04e12aeaac1df216b85a2872,"Numerous theoretical frameworks have been developed to explain the gap between the possession of environmental knowledge and environmental awareness, and displaying pro-environmental behavior. Although many hundreds of studies have been undertaken, no definitive explanation has yet been found. Our article describes a few of the most influential and commonly used analytical frameworks: early US linear progression models; altruism, empathy and prosocial behavior models; and finally, sociological models. All of the models we discuss (and many of the ones we do not such as economic models, psychological models that look at behavior in general, social marketing models and that have become known as deliberative and inclusionary processes or procedures (DIPS)) have some validity in certain circumstances. This indicates that the question of what shapes pro-environmental behavior is such a complex one that it cannot be visualized through one single framework or diagram. We then analyze the factors that have been found to have some influence, positive or negative, on pro-environmental behavior such as demographic factors, external factors (e.g. institutional, economic, social and cultural) and internal factors (e.g. motivation, pro-environmental knowledge, awareness, values, attitudes, emotion, locus of control, responsibilities and priorities). Although we point out that developing a model that tries to incorporate all factors might neither be feasible nor useful, we feel that it can help illuminate this complex field. Accordingly, we propose our own model based on the work of Fliegenschnee and Schelakovsky (1998) who were influenced by Fietkau and Kessel (1981).",2002,72,5276,368,9,11,14,29,31,45,65,86,134,180
af2b7af47ff23290f75a48ba2fd1ad1a9aca1dde,"We explored genomic expression patterns in the yeast Saccharomyces cerevisiae responding to diverse environmental transitions. DNA microarrays were used to measure changes in transcript levels over time for almost every yeast gene, as cells responded to temperature shocks, hydrogen peroxide, the superoxide-generating drug menadione, the sulfhydryl-oxidizing agent diamide, the disulfide-reducing agent dithiothreitol, hyper- and hypo-osmotic shock, amino acid starvation, nitrogen source depletion, and progression into stationary phase. A large set of genes (approximately 900) showed a similar drastic response to almost all of these environmental changes. Additional features of the genomic responses were specialized for specific conditions. Promoter analysis and subsequent characterization of the responses of mutant strains implicated the transcription factors Yap1p, as well as Msn2p and Msn4p, in mediating specific features of the transcriptional response, while the identification of novel sequence elements provided clues to novel regulators. Physiological themes in the genomic responses to specific environmental stresses provided insights into the effects of those stresses on the cell.",2000,72,4751,525,2,55,150,168,272,257,282,303,276,301
d89cc8f2ecfc6c0ed2480bb4c3604b0578a3c332,"This article develops a conceptual framework for advancing theories of environmentally significant individual behavior and reports on the attempts of the author’s research group and others to develop such a theory. It discusses definitions of environmentally significant behavior; classifies the behaviors and their causes; assesses theories of environmentalism, focusing especially on value-belief-norm theory; evaluates the relationship between environmental concern and behavior; and summarizes evidence on the factors that determine environmentally significant behaviors and that can effectively alter them. The article concludes by presenting some major propositions supported by available research and some principles for guiding future research and informing the design of behavioral programs for environmental protection. Recent developments in theory and research give hope for building the understanding needed to effectively alter human behaviors that contribute to environmental problems. This article develops a conceptual framework for the theory of environmentally significant individual behavior, reports on developments toward such a theory, and addresses five issues critical to building a theory that can inform efforts to promote proenvironmental behavior.",2000,80,5074,618,0,11,11,23,18,34,50,67,102,126
80f77b586274c499222cf14c9841b90c0b43ef3b,"Presentation de l'editeur : This report aims to assess the full impact of the livestock sector on environmental problems, along with potential technical and policy approaches to mitigation. The assessment takes into account direct impacts, along with the impacts of feed crop agriculture required for livestock production. The livestock sector emerges as one of the top two or three most significant contributors to the most serious environmental problems, at every scale from local to global. The findings of this report suggest that it should be a major policy focus when dealing with problems of land degradation, climate change and air pollution, water shortage and water pollution, and loss of biodiversity. Livestock's contribution to environmental problems is on a massive scale and its potential contribution to their solution is equally large. The impact is so significant that it needs to be addressed with urgency. Major reductions in impact could be achieved at reasonable cost",2006,0,3839,369,2,51,115,198,222,296,363,341,355,304
0475cbed8c8fa313f69d6c673502308cde35c9d1,"Environmental psychology, though a fast-growing field, is one of the most difficult to fit into the confines of scientific inquiry. Measuring such subjective data as reactions to color, heat, light, and sound would seem to be an almost impossible task; indeed, until now there has been no theory around which the research in this field could be organized. This volume represents a preliminary effort to identify the relevant variables involved and fit them into a systematic framework. Furthermore, it presents extensive sets of measures for investigating the theory and implementing it in a variety of everyday environments.Basically, the framework outlined here proposes that environmental stimuli are linked to behavioral responses by the primary emotional responses of arousal, pleasure, and dominance. By considering the impact of the environment on these basic emotional responses, the effects of diverse stimulus components within or across sense modalities can be readily compared. An additional concept, information rate, is used to compare the effects of different environments, each with stimulation in many sense modalities. In the final chapters the authors present a series of hypotheses which relate the emotional response variables to a diversity of behaviors such as physical approach, performance, affiliation, and verbally or nonverbally expressed preference.",1974,0,4937,492,1,5,16,12,9,15,21,14,11,15
66caad2da024a0b9cdc5708b5e7448919fdbb423,"Invading alien species in the United States cause major environmental damages and losses adding up to almost $120 billion per year. There are approximately 50,000 foreign species and the number is increasing. About 42% of the species on the Threatened or Endangered species lists are at risk primarily because of alien-invasive species. D 2004 Elsevier B.V. All rights reserved.",2005,188,4207,303,19,42,111,138,196,229,264,284,292,372
b258961fa2f5b4d3171133e23c2c56765bd5b179,"This introduction to modern soil chemistry describes chemical processes in soils in terms of established principles of inorganic, organic, and physical chemistry. The text provides an understanding of the structure of the solid mineral and organic materials from which soils are formed, and explains such important processes as cation exchange, chemisorption and physical absorption of organic and inorganic ions and molecules, soil acidification and weathering, oxidation-reduction reactions, and development of soil alkalinity and swelling properties. Environmental rather than agricultural topics are emphasized, with individual chapters on such pollutants as heavy metals, trace elements, and inorganic chemicals.",1994,0,6422,344,29,37,60,73,67,82,120,160,163,226
88e23044396b7c63e831ad0195b6184ea3a12097,"Preface. Part I: Introduction. 1. General Topic and Overview. 2. An Introduction to Environmental Organic Chemicals. Part II: Equilibrium Partitioning Between Gaseous, Liquid, and Solid Phases. 3. Partitioning: Molecular Interactions and Thermodynamics. 4. Vapor Pressure. 5. Activity Coefficient and Solubility in Water. 6. Air-Organic Solvent and Air-Water Partitioning. 7. Organic Liquid-Water Partitioning. 8. Organic Acids and Bases: Acidity Constant and Partitioning Behavior. 9. Sorption I: General Introduction and Sorption Processes Involving Organic Matter. 10. Sorption II: Partitioning to Living Media - Bioaccumulation and Baseline Toxicity. 11. Sorption III: Sorption Processes Involving Inorganic Surfaces. Part III: Transformation Processes. 12. Thermodynamics and Kinetics of Transformation Reactions. 13. Chemical Transformations I: Hydrolysis and Reactions Involving Other Nucleophilic Species. 14. Chemical Transformations II: Redox Reactions. 15. Direct Photolysis. 16. Indirect Photolysis: Reactions with Photooxidants in Natural Waters and in the Atmosphere. 17. Biological Transformations. Part IV: Modeling Tools: Transport and Reaction. 18. Transport by Random Motion. 19. Transport Through Boundaries. 20. Air-Water Exchange. 21. Box Models. 22. Models in Space and Time. Part V: Environmental Systems and Case Studies. 23. Ponds, Lakes, and Oceans. 24. Rivers. 25. Groundwater. Appendix. Bibliography. Index (Subject Index, Compound Index, List of Illustrative Examples).",1993,0,4139,346,0,19,34,53,68,101,118,117,137,147
338427238b13cbc325d4f6a778d2a71a44557e79,"We have applied “whole-genome shotgun sequencing” to microbial populations collected en masse on tangential flow and impact filters from seawater samples collected from the Sargasso Sea near Bermuda. A total of 1.045 billion base pairs of nonredundant sequence was generated, annotated, and analyzed to elucidate the gene content, diversity, and relative abundance of the organisms within these environmental samples. These data are estimated to derive from at least 1800 genomic species based on sequence relatedness, including 148 previously unknown bacterial phylotypes. We have identified over 1.2 million previously unknown genes represented in these samples, including more than 782 new rhodopsin-like photoreceptors. Variation in species present and stoichiometry suggests substantial oceanic microbial diversity.",2004,72,4122,283,100,262,283,309,318,261,300,322,292,253
765547fb369699e3f11d15cad00f9cad0dce9f63,"The civilian, commercial, and defense sectors of most advanced industrialized nations are faced with a tremendous set of environmental problems related to the remediation of hazardous wastes, contaminated groundwaters, and the control of toxic air contaminants. For example, the slow pace of hazardous waste remediation at military installations around the world is causing a serious delay in conversion of many of these facilities to civilian uses. Over the last 10 years problems related to hazardous waste remediation have emerged as a high national and international priority.",1995,259,15534,150,0,0,0,0,0,0,0,0,0,0
4e10837b0689c10207f050d7dd686015108cdbdb,"A lightweight, compact, portable pull-testing device is disclosed which is especially adapted for testing the load capabilities of installed, relatively small earth anchors such as those utilized with hold-down equipment for mobile homes or the like. The device includes a tripod-mounted, vertically adjustable, hydraulically actuated piston and cylinder assembly securable to the upper protruding end of an earth anchor shaft, with generally upright tripod support legs positioned closely adjacent the protruding shaft end to permit use of the device by a single operator in crowded areas such as beneath a mobile home where space is at a premium. Novel creep indicator apparatus is also disclosed for the accurate measurement of anchor creepage independent of any settling of the device itself into the earth which may occur during pull testing.",1997,0,3944,384,16,23,56,87,92,113,111,108,145,163
272259021e636cf90c80af3b77420f13f6c07454,"The Environmental Isotopes Environmental Isotopes in Hydrogeology Stable Isotopes: Standards and Measurement Isotope Ratio Mass Spectrometry Radioisotopes Isotope Fractionation Isotope Fractionation (a), Enrichment (e), and Separation (D) Tracing the Hydrological Cycle Craig's Meteoric Relationship in Global Fresh Waters Partitioning of Isotopes Through the Hydrological Cycle Condensation, Precipitation, and the Meteoric Water Line A Closer Look at Rayleigh Distillation Effects of Extreme Evaporation Precipitation The T - d18O Correlation in Precipitation Local Effects on T - d18O Ice Cores and Paleotemperature Groundwater Recharge in Temperate Climates Recharge in Arid Regions Recharge from River-Connected Aquifers Hydrograph Separation in Catchment Studies Groundwater Mixing Tracing the Carbon Cycle Evolution of Carbon in Groundwaters Carbonate Geochemistry Carbon-13 in the Carbonate System Dissolved Organic Carbon Methane in Groundwaters Isotopic Composition of Carbonates Chapter 6. Groundwater Quality Sulphate, Sulphide and the Sulphur Cycle Nitrogen Cycles in Rural Watersheds The ""Fuhrberger Feld"" Study Source of Chloride Salinity Landfill Leachates Degredation of Chloro-organics and Hydrocarbon Sensitivity of Groundwater to Contamination Summary of Isotopes in Contaminant Hydrology Identifying and Dating Modern Groundwaters The ""Age"" of Groundwater Stable Isotopes Tritium in Precipitation Dating Groundwaters with Tritium Groundwater Dating with 3H -3He Chlorofluorocarbons (CFCs) Thermonuclear 36Cl Detecting Modern Groundwaters with 85Kr Submodern Groundwater Age Dating Old Groundwaters Stable Isotopes and Paleogroundwaters Groundwater Dating with Radiocarbon Correction for Carbonate Dissolution Some Additional Complications to 14C Dating 14C Dating with Dissolved Organic Carbon (DOC) Case Studies for 14C dating with DOC and DIC Chlorine-36 and Very Old Groundwater The Uranium Decay Series Water-Rock Interaction Mechanisms of Isotope Exchange High Temperature Systems Low Temperature Water-Rock Interaction Strontium Isotopes in Water and Rock Isotope Exchange in Gas-Water Reactions High pH Groundwaters-The Effect of Cement Reactions Field Methods for Sampling Groundwater Water in the Unsaturated Zone Precipitation Gases Geochemistry References Index Each chapter has Problems sections.",1997,0,3217,575,0,7,6,27,39,44,59,83,95,102
87c5d47514aea45bb366380f8dfd79bebb0dbb6c,"Drawing on the resource-based view of the firm, we posited that environmental performance and economic performance are positively linked and that industry growth moderates the relationship, with the returns to environmental performance higher in high-growth industries. We tested these hypotheses with an analysis of 243 Finns over two years, using independently developed environmental ratings. Results indicate that “it pays to be green” and that this relationship strengthens with industry growth. We conclude by highlighting the study's academic and managerial implications, making special reference to the social issues in management literature.",1997,123,4024,244,3,7,23,33,37,42,43,62,84,96
e843be77f652aaa8d379c39f68322d47167ef1c4,"This book is basically an account of research undertaken by the author and his colleagues at the Technical University of Denmark and at the Institute for Environmental Research, Kansas State University. Although the data in the literature on thermal comfort are extensive, they are disjointed Other CABI sites ",1972,0,3659,410,0,1,3,1,1,3,5,5,2,4
adff7973cbfefdf139758e2bc0f1b5fd8ad2fa3b,"In this review, a wide array of bioaccumulation markers and biomarkers, used to demonstrate exposure to and effects of environmental contaminants, has been discussed in relation to their feasibility in environmental risk assessment (ERA). Fish bioaccumulation markers may be applied in order to elucidate the aquatic behavior of environmental contaminants, as bioconcentrators to identify certain substances with low water levels and to assess exposure of aquatic organisms. Since it is virtually impossible to predict the fate of xenobiotic substances with simple partitioning models, the complexity of bioaccumulation should be considered, including toxicokinetics, metabolism, biota-sediment accumulation factors (BSAFs), organ-specific bioaccumulation and bound residues. Since it remains hard to accurately predict bioaccumulation in fish, even with highly sophisticated models, analyses of tissue levels are required. The most promising fish bioaccumulation markers are body burdens of persistent organic pollutants, like PCBs and DDTs. Since PCDD and PCDF levels in fish tissues are very low as compared with the sediment levels, their value as bioaccumulation markers remains questionable. Easily biodegradable compounds, such as PAHs and chlorinated phenols, do not tend to accumulate in fish tissues in quantities that reflect the exposure. Semipermeable membrane devices (SPMDs) have been successfully used to mimic bioaccumulation of hydrophobic organic substances in aquatic organisms. In order to assess exposure to or effects of environmental pollutants on aquatic ecosystems, the following suite of fish biomarkers may be examined: biotransformation enzymes (phase I and II), oxidative stress parameters, biotransformation products, stress proteins, metallothioneins (MTs), MXR proteins, hematological parameters, immunological parameters, reproductive and endocrine parameters, genotoxic parameters, neuromuscular parameters, physiological, histological and morphological parameters. All fish biomarkers are evaluated for their potential use in ERA programs, based upon six criteria that have been proposed in the present paper. This evaluation demonstrates that phase I enzymes (e.g. hepatic EROD and CYP1A), biotransformation products (e.g. biliary PAH metabolites), reproductive parameters (e.g. plasma VTG) and genotoxic parameters (e.g. hepatic DNA adducts) are currently the most valuable fish biomarkers for ERA. The use of biomonitoring methods in the control strategies for chemical pollution has several advantages over chemical monitoring. Many of the biological measurements form the only way of integrating effects on a large number of individual and interactive processes in aquatic organisms. Moreover, biological and biochemical effects may link the bioavailability of the compounds of interest with their concentration at target organs and intrinsic toxicity. The limitations of biomonitoring, such as confounding factors that are not related to pollution, should be carefully considered when interpreting biomarker data. Based upon this overview there is little doubt that measurements of bioaccumulation and biomarker responses in fish from contaminated sites offer great promises for providing information that can contribute to environmental monitoring programs designed for various aspects of ERA.",2003,652,4008,205,10,32,58,107,116,148,159,183,248,271
5a1eaf45ae2bdbf07f0647466939a87108882cfe,"The complex and dynamic nature of environmental problems requires flexible and transparent decision-making that embraces a diversity of knowledges and values. For this reason, stakeholder participation in environmental decision-making has been increasingly sought and embedded into national and international policy. Although many benefits have been claimed for participation, disillusionment has grown amongst practitioners and stakeholders who have felt let down when these claims are not realised. This review first traces the development of participatory approaches in different disciplinary and geographical contexts, and reviews typologies that can be used to categorise and select participatory methods. It then reviews evidence for normative and pragmatic benefits of participation, and evaluates limitations and drawbacks. Although few of the claims that are made have been tested, there is evidence that stakeholder participation can enhance the quality of environmental decisions by considering more comprehensive information inputs. However, the quality of decisions made through stakeholder participation is strongly dependant on the nature of the process leading to them. Eight features of best practice participation are then identified from a Grounded Theory Analysis of the literature. These features emphasise the need to replace a ‘‘tool-kit’’ approach, which emphasises selecting the relevant tools for the job, with an approach that emphasises participation as a process. It is argued that stakeholder participation needs to be underpinned by a philosophy that emphasises empowerment, equity, trust and learning. Where relevant, participation should be considered as early as possible and throughout the process, representing relevant stakeholders systematically. The process needs to have clear objectives from the outset, and should not overlook the need for highly skilled facilitation. Local and scientific knowledges can be integrated to provide a more comprehensive understanding of complex and dynamic socio-ecological systems and processes. Such knowledge can also be used to evaluate the appropriateness of potential technical and local solutions to environmental problems. Finally, it is argued that to overcome many of its limitations, stakeholder participation must be institutionalised, creating organisational cultures that can facilitate processes where goals are negotiated and outcomes are necessarily uncertain. In this light, participatory processes may seem very risky, but there is growing evidence that if well designed, these perceived risks may be well worth taking. The review concludes by identifying future research needs.",2008,197,3092,296,5,28,74,89,143,210,235,284,353,355
e69e904ec6339303464dd2ef11a3d16f160eb73e,"Responses of plants to environmental stresses , Responses of plants to environmental stresses , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1973,0,4773,187,13,16,27,30,36,29,44,57,45,46
054d5a1646f2747a94216aeec354d29afda5c396,"Abstract Air quality forecasters, emergency responders, aviation interests, government agencies, and the atmospheric research community are among those who require access to tools to analyze and predict the transport and dispersion of pollutants in the atmosphere. Because of this need, the unique web-based Real-time Environmental Applications and Display sYstem (READY) has been under continuous development since 1997 to provide access to a suite of tools for producing air parcel trajectory and dispersion model results and displaying meteorological data. READY provides a “quasi-operational” portal to run the HYSPLIT atmospheric transport and dispersion model and interpret its results. Typical user applications include modeling the release of hazardous pollutants and volcanic ash, forest fire and prescribed burn smoke forecasting, poor air quality events, and various climatological studies. In addition, READY provides the user with quick access to meteorological data interpolated to the location of interest, helping in the interpretation of the HYSPLIT model results.",1995,9,1052,61,0,0,0,0,0,0,1,0,0,0
4cab880a62df06a300814ace9fb542643dac1863,"New therapeutic targets for noncognitive reductions in energy intake, absorption, or storage are crucial given the worldwide epidemic of obesity. The gut microbial community (microbiota) is essential for processing dietary polysaccharides. We found that conventionalization of adult germ-free (GF) C57BL/6 mice with a normal microbiota harvested from the distal intestine (cecum) of conventionally raised animals produces a 60% increase in body fat content and insulin resistance within 14 days despite reduced food intake. Studies of GF and conventionalized mice revealed that the microbiota promotes absorption of monosaccharides from the gut lumen, with resulting induction of de novo hepatic lipogenesis. Fasting-induced adipocyte factor (Fiaf), a member of the angiopoietin-like family of proteins, is selectively suppressed in the intestinal epithelium of normal mice by conventionalization. Analysis of GF and conventionalized, normal and Fiaf knockout mice established that Fiaf is a circulating lipoprotein lipase inhibitor and that its suppression is essential for the microbiota-induced deposition of triglycerides in adipocytes. Studies of Rag1-/- animals indicate that these host responses do not require mature lymphocytes. Our findings suggest that the gut microbiota is an important environmental factor that affects energy harvest from the diet and energy storage in the host. Data deposition: The sequences reported in this paper have been deposited in the GenBank database (accession nos. AY 667702--AY 668946).",2004,58,4666,206,1,36,51,75,100,126,197,200,292,312
c68001e5f2923ab5f1012ef5ae20523a67be422f,"Cells of a multicellular organism are genetically homogeneous but structurally and functionally heterogeneous owing to the differential expression of genes. Many of these differences in gene expression arise during development and are subsequently retained through mitosis. Stable alterations of this kind are said to be 'epigenetic', because they are heritable in the short term but do not involve mutations of the DNA itself. Research over the past few years has focused on two molecular mechanisms that mediate epigenetic phenomena: DNA methylation and histone modifications. Here, we review advances in the understanding of the mechanism and role of DNA methylation in biological processes. Epigenetic effects by means of DNA methylation have an important role in development but can also arise stochastically as animals age. Identification of proteins that mediate these effects has provided insight into this complex process and diseases that occur when it is perturbed. External influences on epigenetic processes are seen in the effects of diet on long-term diseases such as cancer. Thus, epigenetic mechanisms seem to allow an organism to respond to the environment through changes in gene expression. The extent to which environmental effects can provoke epigenetic responses represents an exciting area of future research.",2003,208,5436,176,40,172,178,210,203,226,205,288,314,348
7e4e5065f64fffe868640f05a9e7ed296cda4e0f,"County-level socioeconomic and demographic data were used to construct an index of social vulnerability to environmental hazards, called the Social Vulnerability Index (SoVI) for the United States based on 1990 data. Copyright (c) 2003 by the Southwestern Social Science Association.",2003,37,2994,249,2,4,10,20,33,52,76,76,120,144
ba9e794246fdcb2b7370ee9b1b531342a2fb3f6f,Twenty-two decision groups in three manufacturing and three research and development organizations are studied to identify the characteristics of the environment that contribute to decision unit members experiencing uncertainty in decision making.' Two dimensions of the environment are identified. The simple-complex dimension is defined as the number of factors taken into consideration in decision making. The static-dynamic dimension is viewed as the degree to which these factors in the decision unit's environment remain basically the same over time or are in a continual process of change. Results indicate that individuals in decision units with dynamic-complex environments experience the greatest amount of uncertainty in decision making. The data also indicate that the static-dynamic dimension of the environment is a more important contributor to uncertainty than the simple-complex dimension.,1972,20,3400,235,0,4,7,15,8,14,16,27,27,27
6a30c9ca7c46ad67b2dde6df1d5d087e88293dbe,,1992,0,5183,131,4,5,3,16,18,17,14,21,17,30
d571963a597742e6aa80f43db9446517501b3e20,"During the next 50 years, which is likely to be the final period of rapid agricultural expansion, demand for food by a wealthier and 50% larger global population will be a major driver of global environmental change. Should past dependences of the global environmental impacts of agriculture on human population and consumption continue, 109 hectares of natural ecosystems would be converted to agriculture by 2050. This would be accompanied by 2.4- to 2.7-fold increases in nitrogen- and phosphorus-driven eutrophication of terrestrial, freshwater, and near-shore marine ecosystems, and comparable increases in pesticide use. This eutrophication and habitat destruction would cause unprecedented ecosystem simplification, loss of ecosystem services, and species extinctions. Significant scientific advances and regulatory, technological, and policy changes are needed to control the environmental impacts of agricultural expansion.",2001,54,3378,152,14,57,51,56,63,89,121,124,120,170
dd6393c5a44d3638bf322817a472ef3f3d3f099e,"Environmental quality strongly depends on human behaviour patterns. We review the contribution and the potential of environmental psychology for understanding and promoting pro-environmental behaviour. A general framework is proposed, comprising: (1) identification of the behaviour to be changed, (2) examination of the main factors underlying this behaviour, (3) design and application of interventions to change behaviour to reduce environmental impact, and (4) evaluation of the effects of interventions. We discuss how environmental psychologists empirically studied these four topics, identify apparent shortcomings so far, and indicate major issues for future research.",2009,120,2718,181,4,23,53,83,144,185,212,267,282,352
3835607a4afc36f88f1f257ef8a480f996c8e8b5,"This paper serves as an introduction to this special issue of Accounting, Auditing & Accountability Journal; an issue which embraces themes associated with social and environmental reporting (SAR) and its role in maintaining or creating organisational legitimacy. In an effort to place this research in context the paper begins by making reference to contemporary trends occurring in social and environmental accounting research generally, and this is then followed by an overview of some of the many research questions which are currently being addressed in the area. Understanding motivations for disclosure is shown to be one of the issues attracting considerable research attention, and the desire to legitimise an organisation’s operations is in turn shown to be one of the many possible motivations. The role of legitimacy theory in explaining managers’ decisions is then discussed and it is emphasised that legitimacy theory, as it is currently used, must still be considered to be a relatively under‐developed theory of managerial behaviour. Nevertheless, it is argued that the theory provides useful insights. Finally, the paper indicates how the other papers in this issue of AAAJ contribute to the ongoing development of legitimacy theory in SAR research.",2002,106,2591,281,1,12,12,29,30,61,75,83,107,113
818b9bdf99ad1273e9e223b7d284dbf0126965a7,Environmental Soil Chemistry: An Overview: Evolution of Soil Chemistry. The Modern Environmental Movement. Contaminants in Waters and Soils. Case Study of Pollution of Soils and Waters. Soil Decontamination. Inorganic Soil Components: Pauling's Rules. Primary Soil Minerals. Secondary Soil Minerals. Specific Surface of Soil Minerals. Surface Charge of Soil Minerals. Identification of Minerals by X-Ray Diffraction Analyses. Use of Clay Minerals to Retain Organic Contaminants. Chemistry of Soil Organic Matter: Effects of Soil Formation Factors on SOM Contents. Composition of SOM. Fractionation of SOM. SOM Structure. Functional Groups and Charge Characteristics. Humic Substance-Metal Interactions. SOM-Clay Complexes. Retention of Pesticides and Other Organic Substances by Humic Substances. Soil Solution-Solid Phase Equilibria: Measurement of the Soil Solution. Speciation of the Soil Solution. Ion Activity and Activity Coefficients. Dissolution and Solubility Processes. Sorption Phenomena on Soils: Introduction and Terminology. Surface Functional Groups. Surface Complexes. Adsorption Isotherms. Equilibrium-Based Adsorption Models. Surface Precipitation. Sorption of Metal Cations. Sorption of Anions. Points of Zero Charge. Desorption. Use of Spectroscopic and Microscopic Methods in Determining Mechanisms for Sorption-Desorption Phenomena. Ion Exchange Processes: Characteristics of Ion Exchange. Cation Exchange Equilibrium Constants and Selectivity Coefficients. Thermodynamics of Ion Exchange. Relationship between Thermodynamics and Kinetics of Ion Exchange. Kinetics of Soil Chemical Processes: Rate-Limiting Steps and Time Scales of Soil Chemical Reactions. Rate Laws. Determination of Reacti,1995,0,2714,231,0,2,7,13,31,31,43,49,65,78
1cb1946049da4cff926028e4821d7cdb25d68f41,"Prentice Hall, 1997. Book Condition: New. Brand New, Unread Copy in Perfect Condition. A+ Customer Service! Summary: 1. Thermochemical Principles. 2. Chemical Kinetics. 3. Aqueous Complexes. 4. Activity Coefficients of Dissolved Species. 5. Acids and Bases. 6. Carbonate Chemistry. 7. Chemical Weathering. 8. General Controls on Natural Water Chemistry. 9. The Geochemistry of Clay Minerals. 10. Adsorption-Desorption Reactions. 11. Oxidation-Reduction Concepts. 12. Iron and Sulfur Geochemistry. 13. Actinides and Their Daughter and Fission Products. Geochemical Computer Models. References. Index.",1997,0,2359,343,1,10,35,52,47,58,79,113,103,141
3c50b229b9a59de264df893f6444d515f2546ca1,"Negative environmental consequences of fossil fuels and concerns about petroleum supplies have spurred the search for renewable transportation biofuels. To be a viable alternative, a biofuel should provide a net energy gain, have environmental benefits, be economically competitive, and be producible in large quantities without reducing food supplies. We use these criteria to evaluate, through life-cycle accounting, ethanol from corn grain and biodiesel from soybeans. Ethanol yields 25% more energy than the energy invested in its production, whereas biodiesel yields 93% more. Compared with ethanol, biodiesel releases just 1.0%, 8.3%, and 13% of the agricultural nitrogen, phosphorus, and pesticide pollutants, respectively, per net energy gain. Relative to the fossil fuels they displace, greenhouse gas emissions are reduced 12% by the production and combustion of ethanol and 41% by biodiesel. Biodiesel also releases less air pollutants per net energy gain than ethanol. These advantages of biodiesel over ethanol come from lower agricultural inputs and more efficient conversion of feedstocks to fuel. Neither biofuel can replace much petroleum without impacting food supplies. Even dedicating all U.S. corn and soybean production to biofuels would meet only 12% of gasoline demand and 6% of diesel demand. Until recent increases in petroleum prices, high production costs made biofuels unprofitable without subsidies. Biodiesel provides sufficient environmental advantages to merit subsidy. Transportation biofuels such as synfuel hydrocarbons or cellulosic ethanol, if produced from low-input biomass grown on agriculturally marginal land or from waste biomass, could provide much greater supplies and environmental benefits than food-based biofuels.",2006,55,2705,157,12,99,161,211,224,219,260,267,219,189
80c4bdb7aee2e5dde6dcdc9801a615aa82d49c7f,"Although the incidence and prevalence of ulcerative colitis and Crohn's disease are beginning to stabilize in high-incidence areas such as northern Europe and North America, they continue to rise in low-incidence areas such as southern Europe, Asia, and much of the developing world. As many as 1.4 million persons in the United States and 2.2 million persons in Europe suffer from these diseases. Previously noted racial and ethnic differences seem to be narrowing. Differences in incidence across age, time, and geographic region suggest that environmental factors significantly modify the expression of Crohn's disease and ulcerative colitis. The strongest environmental factors identified are cigarette smoking and appendectomy. Whether other factors such as diet, oral contraceptives, perinatal/childhood infections, or atypical mycobacterial infections play a role in expression of inflammatory bowel disease remains unclear. Additional epidemiologic studies to define better the burden of illness, explore the mechanism of association with environmental factors, and identify new risk factors are needed.",2004,188,2833,171,8,47,99,122,145,163,183,190,201,234
df0d3949fe7b40ada202ce2a1f82f6a4d94ffb42,"[Extract] Aproximately 50,000 nonindigenous (non-native) species are estimated to have been introduced to the United States. Some of these are beneficial; for example, species introduced as food crops (e.g., corn, wheat, and rice) and as livestock (e.g., cattle and poultry) now provide more than 98% of the US food system, at a value of approximately $800 billion per year (USBC 1998). Other exotic species have been introduced for landscape restoration, biological pest control, sport, pets, and food processing, also with significant benefits. Some nonindigenous species, however, have caused major economic losses in agriculture, forestry, and several other segments of the US economy, in addition to harming the environment. One study reported that 79 exotic species had caused approximately $97 billion in damages during the period 1906–1991 (OTA 1993).",2000,99,2809,157,21,38,50,81,135,148,195,209,179,200
f9b8a854f0d5633d08810f3a73634791cd21cb33,1. Introduction 2. Temperature 3. Water Vapor and Other Gases 4. Liquid Water in Organisms and their Environment 5. Wind 6. Heat and Mass Transport 7. Conductances for Heat and Mass Transport 8. Heat Flow in the Soil 9. Water Flow in Soil 10. Radiation Basics 11. Radiation Fluxes in Natural Environments 12. Animals and Their Environment 13. Humans and Their Environment 14. Plants and Plant Communities 15. The Light Environment of Plant Canopies Appendix,1977,0,2790,208,3,2,5,9,12,5,10,20,17,11
bdb2a4f197ba6da046ac8c41789fb53b1ba06c38,"Research on exploration and exploitation is burgeoning, yet our understanding of the antecedents and consequences of both activities remains rather unclear. We advance the growing body of literature by focusing on the apparent differences of exploration and exploitation and examining implications for using formal (i.e., centralization and formalization) and informal (i.e., connectedness) coordination mechanisms. This study further examines how environmental aspects (i.e., dynamism and competitiveness) moderate the effectiveness of exploratory and exploitative innovation. Results indicate that centralization negatively affects exploratory innovation, whereas formalization positively influences exploitative innovation. Interestingly, connectedness within units appears to be an important antecedent of both exploratory and exploitative innovation. Furthermore, our findings reveal that pursuing exploratory innovation is more effective in dynamic environments, whereas pursuing exploitative innovation is more beneficial to a unit's financial performance in more competitive environments. Through this richer explanation and empirical assessment, we contribute to a greater clarity and better understanding of how ambidextrous organizations coordinate the development of exploratory and exploitative innovation in organizational units and successfully respond to multiple environmental conditions.",2006,94,2408,184,4,18,28,56,82,104,162,207,188,194
96184048fd0b6e11a4d86b7dc989ad224444f77e,"To study the potential effects of increased biofuel use, we evaluated six representative analyses of fuel ethanol. Studies that reported negative net energy incorrectly ignored coproducts and used some obsolete data. All studies indicated that current corn ethanol technologies are much less petroleum-intensive than gasoline but have greenhouse gas emissions similar to those of gasoline. However, many important environmental effects of biofuel production are poorly understood. New metrics that measure specific resource inputs are developed, but further research into environmental metrics is needed. Nonetheless, it is already clear that large-scale use of ethanol for fuel will almost certainly require cellulosic technology.",2006,83,2781,111,45,152,256,261,324,268,256,242,198,182
77aac4345e1d998fe1075f66eb66b754f161fc7a,"PART I: INTRODUCTION 1. Making Sense of Earth's Politics: A Discourse Approach PART II: GLOBAL LIMITS AND THEIR DENIAL 2. Looming Tragedy: Survivalism 3. Growth Forever: The Promethean Response PART III: SOLVING ENVIRONMENTAL PROBLEMS 4. Leave it to the Experts: Administrative Rationalism 5. Leave it to the People: Democratic Pragmatism 6. Leave it to the Market: Economic Rationalism PART IV: THE QUEST FOR SUSTAINABILITY 7. Environmentally Benign Growth: Sustainable Development 8. Industrial Society and Beyond: Ecological Modernization PART V: GREEN RADICALISM 9. Save the World through New Consciousness: Green Romanticism 10. Save the World through New Politics: Green Rationalism PART VI: CONCLUSION 11. Ecological Democracy Bibliography, Index",1997,10,2371,218,0,7,13,38,38,48,48,56,69,74
4852aa90e69435580a9185c80c383ca625f2acaa,"Summary. — This paper presents a critical history of the environmental Kuznets curve (EKC). The EKC proposes that indicators of environmental degradation first rise, and then fall with increasing income per capita. Recent evidence shows however, that developing countries are addressing environmental issues, sometimes adopting developed country standards with a short time lag and sometimes performing better than some wealthy countries, and that the EKC results have a very flimsy statistical foundation. A new generation of decomposition and efficient frontier models can help disentangle the true relations between development and the environment and may lead to the demise of the classic EKC. � 2004 Elsevier Ltd. All rights reserved.",2004,147,2617,160,12,44,51,57,75,117,102,133,144,163
8c786179646af5b7d2d6cc65e5957f959b5e6b86,"Abstract The goal of the present paper is a replication as well as an extension of the Hines et al. [(1986/87). Analysis and synthesis of research on responsible environmental behaviour: A meta-analysis. Journal of Environmental Education, 18, 1–8] meta-analysis on psycho-social determinants of pro-environmental behaviour. Based on information from a total of 57 samples the present meta-analysis finds mean correlations between psycho-social variables and pro-environmental behaviour similar to those reported by Hines et al. In a second step, the matrix of pooled correlations is used for a structural equation modelling (SEM) test of theoretically postulated structural relations between eight determinants of pro-environmental behaviour (so-called Meta-analytic SEM (MASEM)). MASEM results confirm that pro-environmental behavioural intention mediate the impact of all other psycho-social variables on pro-environmental behaviour (27% explained variance). Results also confirm that besides attitude and behavioural control personal moral norm is a third predictor of pro-environmental behavioural intention (52% explained variance). The MASEM also indicates that problem awareness is an important but indirect determinant of pro-environmental intention. Its impact seems to be mediated by moral and social norms, guilt and attribution processes.",2007,91,2222,144,5,24,26,46,75,96,141,174,194,209
5f6504b270e8fdd81095cb00f7ae4379be8a883c,"Because human activities impact the timing, location, and degree of pollutant exposure, they play a key role in explaining exposure variation. This fact has motivated the collection of activity pattern data for their specific use in exposure assessments. The largest of these recent efforts is the National Human Activity Pattern Survey (NHAPS), a 2-year probability-based telephone survey ( n=9386) of exposure-related human activities in the United States (U.S.) sponsored by the U.S. Environmental Protection Agency (EPA). The primary purpose of NHAPS was to provide comprehensive and current exposure information over broad geographical and temporal scales, particularly for use in probabilistic population exposure models. NHAPS was conducted on a virtually daily basis from late September 1992 through September 1994 by the University of Maryland's Survey Research Center using a computer-assisted telephone interview instrument (CATI) to collect 24-h retrospective diaries and answers to a number of personal and exposure-related questions from each respondent. The resulting diary records contain beginning and ending times for each distinct combination of location and activity occurring on the diary day (i.e., each microenvironment). Between 340 and 1713 respondents of all ages were interviewed in each of the 10 EPA regions across the 48 contiguous states. Interviews were completed in 63% of the households contacted. NHAPS respondents reported spending an average of 87% of their time in enclosed buildings and about 6% of their time in enclosed vehicles. These proportions are fairly constant across the various regions of the U.S. and Canada and for the California population between the late 1980s, when the California Air Resources Board (CARB) sponsored a state-wide activity pattern study, and the mid-1990s, when NHAPS was conducted. However, the number of people exposed to environmental tobacco smoke (ETS) in California seems to have decreased over the same time period, where exposure is determined by the reported time spent with a smoker. In both California and the entire nation, the most time spent exposed to ETS was reported to take place in residential locations.",2001,44,2738,88,1,12,14,22,25,39,38,39,57,69
3b40d8d5e372b267f1e3591be1a9926b696baeeb,"Engineered nanomaterials (ENM) are already used in many products and consequently released into environmental compartments. In this study, we calculated predicted environmental concentrations (PEC) based on a probabilistic material flow analysis from a life-cycle perspective of ENM-containing products. We modeled nano-TiO(2), nano-ZnO, nano-Ag, carbon nanotubes (CNT), and fullerenes for the U.S., Europe and Switzerland. The environmental concentrations were calculated as probabilistic density functions and were compared to data from ecotoxicological studies. The simulated modes (most frequent values) range from 0.003 ng L(-1) (fullerenes) to 21 ng L(-1) (nano-TiO(2)) for surface waters and from 4 ng L(-1) (fullerenes) to 4 microg L(-1) (nano-TiO(2)) for sewage treatment effluents. For Europe and the U.S., the annual increase of ENMs on sludge-treated soil ranges from 1 ng kg(-1) for fullerenes to 89 microg kg(-1) for nano-TiO(2). The results of this study indicate that risks to aquatic organisms may currently emanate from nano-Ag, nano-TiO(2), and nano-ZnO in sewage treatment effluents for all considered regions and for nano-Ag in surface waters. For the other environmental compartments for which ecotoxicological data were available, no risks to organisms are presently expected.",2009,28,2059,169,0,27,96,142,200,230,253,222,214,192
2136b49c43e57de0a562c760e4e1d31e9ae42070,"Evolutionary-biological reasoning suggests that individuals should be differentially susceptible to environmental influences, with some people being not just more vulnerable than others to the negative effects of adversity, as the prevailing diathesis-stress view of psychopathology (and of many environmental influences) maintains, but also disproportionately susceptible to the beneficial effects of supportive and enriching experiences (or just the absence of adversity). Evidence consistent with the proposition that individuals differ in plasticity is reviewed. The authors document multiple instances in which (a) phenotypic temperamental characteristics, (b) endophenotypic attributes, and (c) specific genes function less like ""vulnerability factors"" and more like ""plasticity factors,"" thereby rendering some individuals more malleable or susceptible than others to both negative and positive environmental influences. Discussion focuses upon limits of the evidence, statistical criteria for distinguishing differential susceptibility from diathesis stress, potential mechanisms of influence, and unknowns in the differential-susceptibility equation.",2009,162,2182,140,1,26,94,137,186,199,233,207,236,217
e13a8198b7f30d3caad56c331da90df866c0a003,"Abstract The Environmental Kuznets Curve (EKC) hypothesis postulates an inverted-U-shaped relationship between different pollutants and per capita income, i.e., environmental pressure increases up to a certain level as income goes up; after that, it decreases. An EKC actually reveals how a technically specified measurement of environmental quality changes as the fortunes of a country change. A sizeable literature on EKC has grown in recent period. The common point of all the studies is the assertion that the environmental quality deteriorates at the early stages of economic development/growth and subsequently improves at the later stages. In other words, environmental pressure increases faster than income at early stages of development and slows down relative to GDP growth at higher income levels. This paper reviews some theoretical developments and empirical studies dealing with EKC phenomenon. Possible explanations for this EKC are seen in (i) the progress of economic development, from clean agrarian economy to polluting industrial economy to clean service economy; (ii) tendency of people with higher income having higher preference for environmental quality, etc. Evidence of the existence of the EKC has been questioned from several corners. Only some air quality indicators, especially local pollutants, show the evidence of an EKC. However, an EKC is empirically observed, till there is no agreement in the literature on the income level at which environmental degradation starts declining. This paper provides an overview of the EKC literature, background history, conceptual insights, policy and the conceptual and methodological critique.",2004,195,2405,174,2,11,27,35,43,80,81,91,113,144
1238bbc823870f8ceea30111abe61c02d11cba0f,"Soil erosion is a major environmental threat to the sustainability and productive capacity of agriculture. During the last 40 years, nearly one-third of the world's arable land has been lost by erosion and continues to be lost at a rate of more than 10 million hectares per year. With the addition of a quarter of a million people each day, the world population's food demand is increasing at a time when per capita food productivity is beginning to decline.",1995,102,2470,137,11,14,35,43,38,54,49,51,77,49
2e297a88bd7a3bbf3d6da348f532261959897ae3,"Takes as its departure point the criticism of Guthrie and Parker by Arnold and the Tinker et al. critique of Gray et al. Following an extensive review of the corporate social reporting literature, its major theoretical preoccupations and empirical conclusions, attempts to re‐examine the theoretical tensions that exist between “classical” political economy interpretations of social disclosure and those from more “bourgeois” perspectives. Argues that political economy, legitimacy theory and stakeholder theory need not be competitor theories but may, if analysed appropriately, be seen as alternative and mutually enriching theories from alternative levels of resolution. Offers evidence from 13 years of social disclosure by UK companies and attempts to interpret this from different levels of resolution. There is little doubt that social disclosure practice has changed dramatically in the period. The theoretical perspectives prove to offer different, but mutually enhancing, interpretations of these phenomena.",1995,101,2611,167,1,5,12,7,17,28,28,20,31,47
03e90ff5605748cb3d4e7f8a621953796783c704,"Abstract Despite the wealth of information which exists concerning environmental behavior, it is not known which variable or variables appear to be most influential in motivating individuals to take responsible environmental action. A meta-analysis of environmental behavior research was undertaken in an attempt to determine this. An exhaustive search of the empirically based environmental behavior research conducted over the past decade yielded a substantial number of studies representative of a broad academic base. The characteristics and findings of these studies served as the data for the meta-analysis. As a result of the meta-analysis, the following variables were found to be associated with responsible environmental behavior: knowledge of issues, knowledge of action strategies, locus of control, attitudes, verbal commitment, and an individual's sense of responsibility. A model of predictors of environmental behavior is proposed.",1987,25,2706,118,0,3,4,7,6,10,16,12,26,29
16b125d47f922cb7f80ee17fb457cfc6d1af4545,"This book is on the various methods of environmental impact assessment as a guide to design of new environmental development and management projects. This approach surveys the features of the environment likely to be affected by the developments under consideration, analyses the information collected, tries to predict the impact of these developments and lays down guidelines or rules for their management. 
 
This book is concerned with practical problems, e.g. development in Canada, the management of fisheries, pest control, etc. It is devoted to a general understanding of environmental systems through methods that have worked in the real world with its many uncertainties. It does not reject the concept of environmental impact analysis but rather stresses the need for fundamental understanding of the structure and dynamics of ecosystems.",2005,0,2660,83,121,135,153,147,155,156,184,165,177,187
e05d2ae8159f0ed22ea4826b19cc12a2d053a501,"Associations between modifiable exposures and disease seen in observational epidemiology are sometimes confounded and thus misleading, despite our best efforts to improve the design and analysis of studies. Mendelian randomization-the random assortment of genes from parents to offspring that occurs during gamete formation and conception-provides one method for assessing the causal nature of some environmental exposures. The association between a disease and a polymorphism that mimics the biological link between a proposed exposure and disease is not generally susceptible to the reverse causation or confounding that may distort interpretations of conventional observational studies. Several examples where the phenotypic effects of polymorphisms are well documented provide encouraging evidence of the explanatory power of Mendelian randomization and are described. The limitations of the approach include confounding by polymorphisms in linkage disequilibrium with the polymorphism under study, that polymorphisms may have several phenotypic effects associated with disease, the lack of suitable polymorphisms for studying modifiable exposures of interest, and canalization-the buffering of the effects of genetic variation during development. Nevertheless, Mendelian randomization provides new opportunities to test causality and demonstrates how investment in the human genome project may contribute to understanding and preventing the adverse effects on human health of modifiable exposures.",2003,192,2849,61,14,48,61,61,90,94,82,103,79,105
6109f1e47d2af5300b16c5563184fc85109c038f,"Two field experiments examined the effectiveness of signs requesting hotel guests' participation in an environmental conservation program. Appeals employing descriptive norms (e.g., ""the majority of guests reuse their towels"") proved superior to a traditional appeal widely used by hotels that focused solely on environmental protection. Moreover, normative appeals were most effective when describing group behavior that occurred in the setting that most closely matched individuals' immediate situational circumstances (e.g., ""the majority of guests in this room reuse their towels""), which we refer to as provincial norms. Theoretical and practical implications for managing proenvironmental efforts are discussed. (c) 2008 by JOURNAL OF CONSUMER RESEARCH, Inc..",2008,57,2279,103,4,38,56,113,128,144,209,181,213,247
94dfa185d0802956490cef714cf2612f446d1a64,1 Basics of Microbiology 2 Stoichiometry and Bacterial Energetics 3 Microbial Kinetics 4 Biofilm Kinetics 5 Reactors 6 Complex Systems 7 The Activated Sludge Process 8 Lagoons 9 Aerobic Biofilm Process 10 Nitrification 11 Denitrification 12 Phosphorus Removal 13 Drinking Water Treatment 14 Anaerobic Treatment by Methanogenesis 15 Detoxification of Hazardous Chemicals 16 Bioremediation,2000,0,2123,193,10,15,23,34,77,68,87,110,106,97
2b4a2a5d1b5673db6ffe1e8cd0eb0356ffebc035,"The Environmental Protection Agency (EPA) provides access to information on a variety of topics related to the environment and strives to inform citizens of health risks. The EPA also has an extensive library network that consists of 26 libraries throughout the United States, which provide access to a plethora of information to EPA employees, scientists, and researchers. The EPA implemented a reorganization project to digitize their materials so they would be more accessible to a wider range of users, but this plan was drastically accelerated when the EPA was threatened with a budget cut. It chose to close and reduce the hours and services of some of their libraries. As a result, the agency was accused of denying users the “right to know” by making information unavailable, not providing an adequate strategic plan, and discarding vital materials. This case study explores the background of the digitization project, the practices implemented, and the critiques of the project.",1982,37,2575,113,6,4,6,3,11,4,5,7,4,8
2e1c547a2e7f8683923c84420d4a94f7cf9f5533,"Recent progress in the design and application of artificial cellular microenvironments and nanoenvironments has revealed the extraordinary ability of cells to adjust their cytoskeletal organization, and hence their shape and motility, to minute changes in their immediate surroundings. Integrin-based adhesion complexes, which are tightly associated with the actin cytoskeleton, comprise the cellular machinery that recognizes not only the biochemical diversity of the extracellular neighbourhood, but also its physical and topographical characteristics, such as pliability, dimensionality and ligand spacing. Here, we discuss the mechanisms of such environmental sensing, based on the finely tuned crosstalk between the assembly of one type of integrin-based adhesion complex, namely focal adhesions, and the forces that are at work in the associated cytoskeletal network owing to actin polymerization and actomyosin contraction.",2009,161,2135,82,39,136,196,184,208,216,183,198,205,154
9ea494770ed507b7d4a68ab71de663db65101ff4,"Assessing how environmental changes affect the distribution and dynamics of vegetation and animal populations is becoming increasingly important for terrestrial ecologists to enable better predictions of the effects of global warming, biodiversity reduction or habitat degradation. The ability to predict ecological responses has often been hampered by our rather limited understanding of trophic interactions. Indeed, it has proven difficult to discern direct and indirect effects of environmental change on animal populations owing to limited information about vegetation at large temporal and spatial scales. The rapidly increasing use of the Normalized Difference Vegetation Index (NDVI) in ecological studies has recently changed this situation. Here, we review the use of the NDVI in recent ecological studies and outline its possible key role in future research of environmental change in an ecosystem context.",2005,90,2181,152,3,22,48,68,73,74,96,113,139,148
1f1a7e89e9ec80817960407613a7ea814f82c3bf,,1980,0,3386,0,3,5,7,13,16,7,16,11,21,21
ffcdc8979f106383e04dde9a97e75a3781264325,"Reports the results of a study on the social and environmental disclosure practices of New Zealand companies. In addition to providing an up‐to‐date description of such practices, and placing them in an international context, examines some potential determinants of these practices. Replicates and extends a recent US study (Patten, 1991). Makes improvements on Patten’s study by measuring the amount of disclosure as a continuous variable using both page amount and the number of sentences. The results indicate both measures are highly correlated. Consistent with Patten (1991) and other studies, reports that both company size and industry are significantly associated with social and environmental disclosures, and that profitability (both current and lagged) is not. In addition to these variables, provides some tentative evidence that New Zealand companies with dual (overseas) stock exchange listings are greater disclosers of social and environmental information.",1996,29,2034,199,1,4,3,11,15,13,17,24,29,32
ca15dd2b1cc9db86ee1a7c8d209e42ff5fd70da2,"Epidemiological evidence increasingly suggests that environmental exposures early in development have a role in susceptibility to disease in later life. In addition, some of these environmental effects seem to be passed on through subsequent generations. Epigenetic modifications provide a plausible link between the environment and alterations in gene expression that might lead to disease phenotypes. An increasing body of evidence from animal studies supports the role of environmental epigenetics in disease susceptibility. Furthermore, recent studies have demonstrated for the first time that heritable environmentally induced epigenetic modifications underlie reversible transgenerational alterations in phenotype. Methods are now becoming available to investigate the relevance of these phenomena to human disease.",2007,138,2191,47,21,106,127,149,203,210,194,220,178,174
05d89da56bf7547950604d164dfd24ccf5ac69d9,"This is the fourth edition of an established and successful reference for plant scientists. The author has taken into consideration extensive reviews performed by colleagues and students who have touted this book as the ultimate reference for research and learning. The original structure and philosophy of the book continue in this new edition, providing a genuine synthesis of modern physicochemical and physiological thinking, while entirely updating the detailed content. Key concepts in plant physiology are developed with the use of chemistry, physics, and mathematics fundamentals. The figures and illustrations have been improved and the list of references has been expanded to reflect the author's continuing commitment to providing the most valuable learning tool in the field. This revision will ensure the reputation of Park Nobel's work as a leader in the field.It includes more than 40 per cent new coverage. It incorporates student-recommended changes from the previous edition. It also includes: five brand new equations and four new tables, with updates to 24 equations and six tables; and, 30 new figures added with more than three-quarters of figures and legends improved . It is organized so that a student has easy access to locate any biophysical phenomenon in which he or she is interested. It features: per-chapter key equation tables; problems with solutions presented in the back of the book; appendices with conversion factors, constants/coefficients, abbreviations and symbols.",1991,4,2144,229,6,10,19,38,36,37,42,43,48,61
9c8678d9c6cb9c477ad82eec655f778865e60006,"Previous empirical evidence provides mixed results on the relationship between corporate environmental performance and the level of environmental disclosures. We revisit this relation by testing competing predictions from economics based and socio-political theories of voluntary disclosure using a more rigorous research design. In particular, we improve on the prior literature by focusing on purely discretionary environmental disclosures and by developing a content analysis index based on the Global Reporting Initiative sustainability reporting guidelines to assess the extent of discretionary disclosures in environmental and social responsibility reports. This index better captures firm disclosures related to its commitment to protect the environment than the indices employed by prior studies. Using a sample of 191 firms from the five most polluting industries in the US, we find a positive association between environmental performance and the level of discretionary environmental disclosures. The result is consistent with the predictions of the economics disclosure theory but inconsistent with the negative association predicted by socio-political theories. Nevertheless, we show that socio-political theories explain patterns in the data (""legitimization"") that cannot be explained by economics disclosure theories.",2008,63,1872,197,7,34,63,70,102,106,125,138,167,171
2001275e7a9c8d7e3ccc1c4d2b2543d5a8936292,2007 Guidelines for the Management of Arterial Hypertension : The Task Force for the Management of Arterial Hypertension of the European Society of Hypertension (ESH) and of the European Society of Cardiology (ESC).,2007,1505,5102,221,0,1,24,5,396,460,493,705,705,731
b58c40da4051134954fccf0ab765b314f58ff18b,,1996,123,13796,241,0,0,1,1,0,0,0,0,0,0
a4017f4928c8aae4ae2ecb37e86238b5af7b1e6c,2007 Guidelines for the Management of Arterial Hypertension : The Task Force for the Management of Arterial Hypertension of the European Society of Hypertension (ESH) and of the European Society of Cardiology (ESC).,2007,893,8676,83,275,585,750,747,660,718,793,875,774,639
ec85e9b8ba314c9e8c583631ed61bfd109ffbefb,"Cardiac imaging is an integral part of the evaluation of patients with all forms of heart disease. Unfortunately, each imaging modality, including nuclear cardiology, echocardiography, cardiovascular magnetic resonance imaging, cardiac computed tomography, coronary angiography, and cardiac positron emission tomography, has adopted its own separate and sometimes markedly differing nomenclature, as well as methods of orientation and segmentation of the heart. The lack of common nomenclature and views has resulted in difficulties in optimal patient management, communication between modalities, interpretation of results, and combined research. Attempts by several subspecialty organizations in the past have improved but not resolved these terminology issues. To ultimately resolve these differences, a remarkable committee was convened: The American Heart Association Writing Group on Myocardial Segmentation and Registration for Cardiac Imaging. This writing group was composed of members from the following organizations: the American Society of Echocardiography, the American Society of Nuclear Cardiology, the North American Society of Cardiac Imaging, the Society for Cardiac Angiography and Interventions, and the Society of Cardiovascular Magnetic Resonance. Their task was a real challenge: to come to an agreement upon all aspects of nomenclature and anatomic descriptions of the heart. Their efforts have resulted in agreements for the following issues: orientation of the heart, naming of cardiac planes, number of segments of the left ventricle, common displays, and finally, regional nomenclature and locations. This extraordinary effort is being published in this issue of the Journal of Nuclear Cardiology as well as in Circulation, Catheterization and Cardiovascular Intervention, The International Journal of Cardiovascular Imaging, Journal of Cardiovascular Magnetic Resonance, and Journal of the American Society of Echocardiography. The virtually simultaneous publication of this statement in many journals is a real tribute to the importance of this document. However, the ultimate test of the success of this writing group will be the widespread acceptance of these tools to better communication amongst modalities, ultimately resulting in better patient care. Gary V. Heller, MD, PhD President, American Society of Nuclear Cardiology",2002,12,3546,190,3,26,60,85,110,146,173,223,203,218
8d51c5f7e40c5d8d4767c7fc4990648c6f0b5853,"Guidelines and Expert Consensus Documents aim to present management recommendations based on all of the relevant evidence on a particular subject in order to help physicians select the best possible management strategies for the individual patient suffering from a specific condition, taking into account the impact on outcome and also the risk–benefit ratio of a particular diagnostic or therapeutic procedure. Numerous studies have demonstrated that patient outcomes improve when guideline recommendations, based on the rigorous assessment of evidence-based research, are applied in clinical practice.

A great number of Guidelines and Expert Consensus Documents have been issued in recent years by the European Society of Cardiology (ESC) and also by other organizations or related societies. The profusion of documents can put at stake the authority and credibility of guidelines, particularly if discrepancies appear between different documents on the same issue, as this can lead to confusion in the minds of physicians. In order to avoid these pitfalls, the ESC and other organizations have issued recommendations for formulating and issuing Guidelines and Expert Consensus Documents. The ESC recommendations for guidelines production can be found on the ESC website.1 It is beyond the scope of this preamble to recall all but the basic rules.

In brief, the ESC appoints experts in the field to carry out a comprehensive review of the literature, with a view to making a critical evaluation of the use of diagnostic and therapeutic procedures and assessing the risk–benefit ratio of the therapies recommended for management and/or prevention of a given condition. Estimates of expected health outcomes are included, where data exist. The strength of evidence for or against particular procedures or treatments is weighed according to predefined scales for grading recommendations and levels of evidence, as outlined in what follows.

The Task Force members of the writing panels, …",2006,317,3524,103,3,39,132,208,283,298,356,436,425,453
44ed95d53ec3563e8287626ea69a2876fcf4fd83,,2004,188,7448,76,160,317,431,497,546,474,527,508,488,513
016affda2fb90ddf94a48b5b67b06c55259c4ebb,"This document was developed by a consensus conference initiated by Kristian Thygesen, MD, and Joseph S. Alpert, MD, after formal approval by Lars Rydén, MD, President of the European Society of Cardiology (ESC), and Arthur Garson, MD, President of the American College of Cardiology (ACC). All of the participants were selected for their expertise in the field they represented, with approximately one-half of the participants selected from each organization. Participants were instructed to review the scientific evidence in their area of expertise and to attend the consensus conference with prepared remarks. The first draft of the document was prepared during the consensus conference itself. Sources of funding appear in Appendix A. The recommendations made in this document represent the attitudes and opinions of the participants at the time of the conference, and these recommendations were revised subsequently. The conclusions reached will undoubtedly need to be revised as new scientific evidence becomes available. This document has been reviewed by members of the ESC Committee for Scientific and Clinical Initiatives and by members of the Board of the ESC who approved the document on April 15, 2000.*",2000,69,3679,55,4,58,128,170,216,272,291,307,242,208
2fa8bf2cbb77fab5803e4ba6081017a22bad1a16,"ESC Committee for Practice Guidelines (CPG), Silvia G. Priori (Chairperson) (Italy), Jean-Jacques Blanc (France), Andrzej Budaj (Poland), John Camm (UK), Veronica Dean (France), Jaap Deckers (The Netherlands), Kenneth Dickstein (Norway), John Lekakis (Greece), Keith McGregor (France), Marco Metra (Italy), Joao Morais (Portugal), Ady Osterspey (Germany), Juan Tamargo (Spain), Jose Luis Zamorano (Spain)  Document Reviewers, Marco Metra (CPG Review Coordinator) (Italy), Michael Bohm (Germany), Alain Cohen-Solal (France), Martin Cowie (UK), Ulf Dahlstrom (Sweden), Kenneth Dickstein (Norway), Gerasimos S. Filippatos (Greece), Edoardo Gronda (Italy), Richard Hobbs (UK), John K. Kjekshus (Norway), John McMurray (UK), Lars Ryden (Sweden), Gianfranco Sinagra (Italy), Juan Tamargo (Spain), Michal Tendera (Poland), Dirk van Veldhuisen (The Netherlands), Faiez Zannad (France)

Guidelines and Expert Consensus Documents aim to present all the relevant evidence on a particular issue in order to help physicians to weigh the benefits and risks of a particular diagnostic or therapeutic procedure. They should be helpful in everyday clinical decision-making.

A great number of Guidelines and Expert Consensus Documents have been issued in recent years by the European Society of Cardiology (ESC) and by different organizations and other related societies. This profusion can put at stake the authority and validity of guidelines, which can only be guaranteed if they have been developed by an unquestionable decision-making process. This is one of the reasons why the ESC and others have issued recommendations for formulating and issuing Guidelines and Expert Consensus Documents.

In spite of the fact that standards for issuing good quality Guidelines and Expert Consensus Documents are well defined, recent surveys of Guidelines and Expert Consensus Documents published in peer-reviewed journals between 1985 and 1998 have shown that methodological standards were not complied with in the vast majority of cases. It is therefore of great importance that guidelines and recommendations are presented in formats that are …",2005,194,5758,102,127,325,416,452,466,399,449,458,682,712
a20a63b283ac8c044823cbc538e7729eca35ab8f,"Nuclear cardiology, echocardiography, cardiovascular magnetic resonance (CMR), cardiac computed tomography (CT), positron emission computed tomography (PET), and coronary angiography are imaging modalities that have been used to measure myocardial perfusion, left ventricular function, and coronary anatomy for clinical management and research. Although there are technical differences between these modalities, all of them image the myocardium and the adjacent cavity. However, the orientation of the heart, angle selection for cardiac planes, number of segments, slice display and thickness, nomenclature for segments, and assignment of segments to coronary arterial territories have evolved independently within each field. This evolution has been based on the inherent strengths and weaknesses of the technique and the practical clinical application of these modalities as they are used for patient management. This independent evolution has resulted in a lack of standardization and has made accurate intra- and cross-modality comparisons for clinical patient management and research very difficult, if not, at times, impossible.

Attempts to standardize these options for all cardiac imaging modalities should be based on the sound principles that have evolved from cardiac anatomy and clinical needs.1–3⇓⇓ Selection of standardized methods must be based on the following criteria:





An earlier special report from the American Heart Association, American College of Cardiology, and Society of Nuclear Medicine4 defined standards for plane selection and display orientation for serial …",2002,14,3306,13,6,28,65,108,142,174,154,188,195,199
6a29b1e2a193f72b56feee4e9100275006117ce4,"Non-thrombotic PE does not represent a distinct clinical syndrome. It may be due to a variety of embolic materials and result in a wide spectrum of clinical presentations, making the diagnosis difficult. With the exception of severe air and fat embolism, the haemodynamic consequences of non-thrombotic emboli are usually mild. Treatment is mostly supportive but may differ according to the type of embolic material and clinical severity.",2008,441,2709,174,5,102,178,209,274,252,320,312,251,245
ef66d43779cc8e6c050f841229be9dcc0cb2552f,"Classifications of heart muscle diseases have proved to be exceedingly complex and in many respects contradictory. Indeed, the precise language used to describe these diseases is profoundly important. A new contemporary and rigorous classification of cardiomyopathies (with definitions) is proposed here. This reference document affords an important framework and measure of clarity to this heterogeneous group of diseases. Of particular note, the present classification scheme recognizes the rapid evolution of molecular genetics in cardiology, as well as the introduction of several recently described diseases, and is unique in that it incorporates ion channelopathies as a primary cardiomyopathy.",2006,26,2642,69,15,85,109,138,184,203,184,222,205,208
223fcc19b392d2e83ff0be826862cd49708c66db,"The American College of Cardiology (ACC)/American Heart Association (AHA) Task Force on Practice Guidelines was formed to make recommendations regarding the diagnosis and treatment of patients with known or suspected cardiovascular disease. Coronary artery disease (CAD) is the leading cause of death in the United States. Unstable angina (UA) and the closely related condition non–ST-segment elevation myocardial infarction (NSTEMI) are very common manifestations of this disease. These life-threatening disorders are a major cause of emergency medical care and hospitalizations in the United States. In 1996, the National Center for Health Statistics reported 1 433 000 hospitalizations for UA or NSTEMI. In recognition of the importance of the management of this common entity and of the rapid advances in the management of this condition, the need to revise guidelines published by the Agency for Health Care Policy and Research (AHCPR) and the National Heart, Lung and Blood Institute in 1994 was evident. This Task Force therefore formed the current committee to develop guidelines for the management of UA and NSTEMI. The present guidelines supersede the 1994 guidelines.

The customary ACC/AHA classifications I, II, and III summarize both the evidence and expert opinion and provide final recommendations for both patient evaluation and therapy:

Class I: Conditions for which there is evidence and/or general agreement that a given procedure or treatment is useful and effective .

Class II: Conditions for which there is conflicting evidence and/or a divergence of opinion about the usefulness/efficacy of a procedure or treatment. 

Class IIa: Weight of evidence/opinion is in favor of usefulness/efficacy. 

Class IIb: Usefulness/efficacy is less well established by evidence/opinion. 

Class III: Conditions for which there is evidence and/or general agreement that the procedure/treatment is not useful/effective and in some cases may be harmful. 

The weight of the evidence was ranked highest (A) if the data …",2002,650,2826,59,117,190,226,268,264,174,162,145,125,119
5e409725d53f259e051a1ad7872da336b80a8c94,"Definition of MI. Criteria for acute, evolving or recent MI. Either one of the following criteria satisfies the diagnosis for an acute, evolving or recent MI: 1) Typical rise and gradual fall (troponin) or more rapid rise and fall (CK-MB) of biochemical markers of myocardial necrosis with at least one of the following: a) ischemic symptoms; b) development of pathologic Qwaves on the ECG; c) ECG changes indicative of ischemia (ST segment elevation or depression); or d) coronary artery intervention (e.g., coronary angioplasty). 2) Pathologic findings of an acute MI. Criteria for established MI. Any one of the following criteria satisfies the diagnosis for established MI: 1) Development of new pathologic Q waves on serial ECGs. The patient may or may not remember previous symptoms. Biochemical markers of myocardial necrosis may have normalized, depending on the length of time that has passed since the infarct developed. 2) Pathologic findings of a healed or healing MI.",2000,70,2824,101,1,40,72,111,132,193,218,208,228,221
ffde91f2a0cd6263ff55dd7c5d71b033fbde85ff,"It is important that the medical profession plays a significant role in critically evaluating the use of diagnostic procedures and therapies as they are introduced and tested in the detection, management, or prevention of disease states. Rigorous and expert analysis of the available data documenting absolute and relative benefits and risks of those procedures and therapies can produce helpful guidelines that improve the effectiveness of care, optimize patient outcomes, and favorably affect the overall cost of care by focusing resources on the most effective strategies.

The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of such guidelines in the area of cardiovascular disease since 1980. The ACC/AHA Task Force on Practice Guidelines, whose charge is to develop, update, or revise practice guidelines for important cardiovascular diseases and procedures, directs this effort. The Task Force is pleased to have this guideline developed in conjunction with the European Society of Cardiology (ESC). Writing committees are charged with the task of performing an assessment of the evidence and acting as an independent group of authors to develop or update written recommendations for clinical practice.

Experts in the subject under consideration have been selected from all 3 organizations to examine subject-specific data and write guidelines. The process includes additional representatives from other medical practitioner and specialty groups when appropriate. Writing committees are specifically charged to perform a formal literature review, weigh the strength of evidence for or against a particular treatment or procedure, and include estimates of expected health outcomes where data exist. Patient-specific modifiers, comorbidities, and issues of patient preference that might influence the choice of particular tests or therapies are considered as well as frequency of follow-up and cost effectiveness. When available, information from studies on cost will be considered; however, review …",2006,1216,2495,81,6,99,198,210,220,223,227,244,263,209
1487335adcfb75344d99544e133559ec4620acd8,"For several years the European Society of Hypertension (ESH) and the European Society of Cardiology (ESC) decided not to produce their own guidelines on the diagnosis and treatment of hypertension but to endorse the guidelines on hypertension issued by the World Health Organization (WHO) and International Society of Hypertension (ISH)1,2 with some adaptation to reflect the situation in Europe. However, in 2003 the decision was taken to publish ESH/ESC specific guidelines3 based on the fact that, because the WHO/ISH Guidelines address countries widely varying in the extent of their health care and availability of economic resource, they contain diagnostic and therapeutic recommendations that may be not totally appropriate for European countries. In Europe care provisions may often allow a more in-depth diagnostic assessment of cardiovascular risk and organ damage of hypertensive individuals as well as a wider choice of antihypertensive treatment.

The 2003 ESH/ESC Guidelines3 were well received by the clinical world and have been the most widely quoted paper in the medical literature in the last two years.4 However, since 2003 considerable additional evidence on important issues related to diagnostic and treatment approaches to hypertension has become available and therefore updating of the previous guidelines has been found advisable.

In preparing the new guidelines the Committee established by the ESH and ESC has agreed to adhere to the principles informing the 2003 Guidelines, namely 1) to try to offer the best available and most balanced recommendation to all health care providers involved in the management of hypertension, 2) to address this aim again by an extensive and critical review of the data accompanied by a series of boxes where specific recommendations are given, as well as by a concise set of practice recommendations to be published soon thereafter as already done in 2003; …",2007,1247,2394,69,27,141,199,229,246,278,320,208,129,122
f6dbc98c59690ebe8527c8e5e2aee49948736bf8,,1996,8,3015,62,9,29,51,54,99,97,100,95,123,142
aa5b8fdf71cc360dd9c99bbc2712564065a9d176,"Abstract Titanium and titanium alloys are excellent candidates for aerospace applications owing to their high strength to weight ratio and excellent corrosion resistance. Titanium usage is, however, strongly limited by its higher cost relative to competing materials, primarily aluminum alloys and steels. Hence the advantages of using titanium must be balanced against this added cost. The titanium alloys used for aerospace applications, some of the characteristics of these alloys, the rationale for utilizing them, and some specific applications of different types of actual usage, and constraints, are discussed as an expansion of previous reviews of β alloy applications. [1,2]",1996,7,1694,27,0,1,4,0,3,8,3,8,8,18
