paperId,abstract,year,referenceCount,citationCount,influentialCitationCount,year0_citation_count,year1_citation_count,year2_citation_count,year3_citation_count,year4_citation_count,year5_citation_count,year6_citation_count,year7_citation_count,year8_citation_count,year9_citation_count
55a63c2a72325a86de9a17814fb6243c132ac19a,"The problem of translating the theory of economic choice behavior into concrete models suitable for analyzing housing location is discussed. The analysis is based on the premise that the classical, economically rational consumer will choose a residential location by weighing the attributes of each available alternative and by selecting the alternative that maximizes utility. The assumption of independence in the commonly used multinomial logit model of choice is relaxed to permit a structure of perceived similarities among alternatives. In this analysis, choice is described by a multinomial logit model for aggregates of similar alternatives. Also discussed are methods for controlling the size of data collection and estimation tasks by sampling alternatives from the full set of alternatives. /Author/",1977,16,3010,247,0,1,4,6,9,11,14,15,28,19
096f17d9dc7437828ea3128c8463fdf41f19d577,This paper conceives of residential segregation as a multidimensional phenomenon varying along 5 distinct axes of measurement: evenness exposure concentration centralization and clustering. 20 indices of segregation are surveyed and related conceptually to 1 of the 5 dimensions. Using data from a large set of US metropolitan areas the indices are intercorrelated and factor analyzed. Orthogonal and oblique rotations produce pattern matrices consistent with the postulated dimensional structure. Based on the factor analyses and other information 1 index was chosen to represent each of the 5 dimensions and these selections were confirmed with a principal components analysis. The paper recommends adopting these indices as standard indicators in future studies of segregation. (authors),1988,59,2639,216,1,5,6,7,12,10,7,10,14,13
fbd9460a027b20e9ea1943d927eb6e9baf89282f,"Racial residential segregation is a fundamental cause of racial disparities in health. The physical separation of the races by enforced residence in certain areas is an institutional mechanism of racism that was designed to protect whites from social interaction with blacks. Despite the absence of supportive legal statutes, the degree of residential segregation remains extremely high for most African Americans in the United States. The authors review evidence that suggests that segregation is a primary cause of racial differences in socioeconomic status (SES) by determining access to education and employment opportunities. SES in turn remains a fundamental cause of racial differences in health. Segregation also creates conditions inimical to health in the social and physical environment. The authors conclude that effective efforts to eliminate racial disparities in health must seriously confront segregation and its pervasive consequences.",2001,121,1982,100,2,4,19,23,40,50,54,67,81,99
b26e0fa7b363f42e31bbde61356f5fcf24d509fd,"There is a growing interest in reducing energy consumption and the associated greenhouse gas emissions in every sector of the economy. The residential sector is a substantial consumer of energy in every country, and therefore a focus for energy consumption efforts. Since the energy consumption characteristics of the residential sector are complex and inter-related, comprehensive models are needed to assess the technoeconomic impacts of adopting energy efficiency and renewable energy technologies suitable for residential applications. The aim of this paper is to provide an up-to-date review of the various modeling techniques used for modeling residential sector energy consumption. Two distinct approaches are identified: top-down and bottom-up. The top-down approach treats the residential sector as an energy sink and is not concerned with individual end-uses. It utilizes historic aggregate energy values and regresses the energy consumption of the housing stock as a function of top-level variables such as macroeconomic indicators (e.g. gross domestic product, unemployment, and inflation), energy price, and general climate. The bottom-up approach extrapolates the estimated energy consumption of a representative set of individual houses to regional and national levels, and consists of two distinct methodologies: the statistical method and the engineering method. Each technique relies on different levels of input information, different calculation or simulation techniques, and provides results with different applicability. A critical review of each technique, focusing on the strengths, shortcomings and purposes, is provided along with a review of models reported in the literature.",2009,86,1570,84,5,26,53,88,90,115,155,167,200,192
51e640475b7811610c705378025330aa55944954,"The stress-threshold model (Wolpert, 1965; Brown and Moore, 1970) assumes that people do not consider moving unless they experience residential stress. This paper develops a similar model of residential mobility in which residential satisfaction acts as an intervening variable between individual and residence variables and mobility. The model is tested with data from a panel study of Rhode Island residents. The results indicate that residential satisfaction at the first interview is related to the wish to move and to mobility in the year following the interview. Individual and residence characteristics such as age of head duration of residence, home ownership, and room crowding are shown to affect mobility through their effect on residential satisfaction.",1974,31,391,24,0,0,1,2,1,1,0,0,0,0
2bacf1c33fdaa3a28c3b3eb6cbad94dfe7ed59a1,"The author considers the possibility that there is not, in fact, much hidden mass in galaxies and galaxy systems. If a certain modified version of the Newtonian dynamics is used to describe the motion of bodies in a gravitational field (of a galaxy, say), the observational results are reproduced with no need to assume hidden mass in appreciable quantities. Various characteristics of galaxies result with no further assumptions. The basis of the modification is the assumption that in the limit of small acceleration a very low a0, the acceleration of a particle at distance r from a mass M satisfies approximately a2/a0 a MGr-2, where a0 is a constant of the dimensions of an acceleration.",1983,2,2280,193,0,3,4,4,1,4,1,3,2,5
cffc507312c01839ef2dc32158f2ad3a57efa5ce,"Dispersions of solid spherical grains of diameter D = 0.13cm were sheared in Newtonian fluids of varying viscosity (water and a glycerine-water-alcohol mixture) in the annular space between two concentric drums. The density σ of the grains was balanced against the density ρ of the fluid, giving a condition of no differential forces due to radial acceleration. The volume concentration C of the grains was varied between 62 and 13 %. A substantial radial dispersive pressure was found to be exerted between the grains. This was measured as an increase of static pressure in the inner stationary drum which had a deformable periphery. The torque on the inner drum was also measured. The dispersive pressure P was found to be proportional to a shear stress λ attributable to the presence of the grains. The linear grain concentration λ is defined as the ratio grain diameter/mean free dispersion distance and is related to C by λ=1(C0/C)12−1 where C0 is the maximum possible static volume concentration. Both the stressesT and P, as dimensionless groups TσD2/λη2, and PσD2/λη 2, were found to bear single-valued empirical relations to a dimensionless shear strain group λ½σD2(dU/dy)lη for all the values of λ< 12(C= 57% approx.) where dU/dy is the rate of shearing of the grains over one another, and η the fluid viscosity. This relation gives Tασ(λD)2(dU/dy)2 and T∝λ12ηdU/dy according as dU/dy is large or small, i.e. according to whether grain inertia or fluid viscosity dominate. An alternative semi-empirical relation F = (1+λ)(1+½λ)ηdU/dy was found for the viscous case, when T is the whole shear stress. The ratio T/P was constant at 0·3 approx, in the inertia region, and at 0.75 approx, in the viscous region. The results are applied to a few hitherto unexplained natural phenomena.",1954,1,2296,201,0,0,0,0,0,0,1,0,0,1
d0a90be9e7d2ebb969e7c540b4d1cd4fcb93218e,,1959,0,2432,122,0,0,1,2,3,1,0,0,0,0
a88cc4d8e59e5105683ea299e4c9739e52ce2393,"This paper presents a systematic treatment of the linear theory of scalar gravitational perturbations in the synchronous gauge and the conformal Newtonian (or longitudinal) gauge. It differs from others in the literature in that we give, in both gauges, a complete discussion of all particle species that are relevant to any flat cold dark matter (CDM), hot dark matter (HDM), or CDM+HDM models (including a possible cosmological constant). The particles considered include CDM, baryons, photons, massless neutrinos, and massive neutrinos (an HDM candidate), where the CDM and baryons are treated as fluids while a detailed phase-space description is given to the photons and neutrinos. Particular care is applied to the massive neutrino component, which has been either ignored or approximated crudely in previous works. Isentropic initial conditions on superhorizon scales are derived. The coupled, linearized Boltzmann, Einstein, and fluid equations that govern the evolution of the metric and density perturbations are then solved numerically in both gauges for the standard CDM model and two CDM+HDM models with neutrino mass densities {Omega}{sub {nu}}=0.2 and 0.3, assuming a scale-invariant, adiabatic spectrum of primordial fluctuations. We also give the full details of the cosmic microwave background anisotropy, and present the first accurate calculationsmore » of the angular power spectra in the two CDM+HDM models including photon polarization, higher neutrino multipole moments, and helium recombination. The numerical programs for both gauges are available at http://arcturus.mit.edu/cosmics. {copyright} {ital 1995 The American Astronomical Society.}« less",1994,18,1277,86,0,2,4,11,15,16,15,12,16,24
d3482d91af463d97e64e4a6137f019066526a09a,"On the assumption that pseudoplastic flow is associated with the formation and rupture of structural linkages a new flow equation is derived. The equation takes the form 
ƞ = ƞ∞ + ƞ0 − ƞ∞1 + αD23, 
where D = rate of shear, η0 = limiting viscosity at zero rate of shear, η∞ = limiting viscosity at infinite rate of shear, and α is a constant associated with the rupture of linkages. 
 
Graphical methods for evaluating the three constants η0 , η∞ , and α are presented. 
 
Experimental data are presented on a wide range of pseudoplastic systems, ranging from suspensions to optically clear solutions, in both aqueous and nonaqueous media. In all cases the results conform to the equation with a high degree of accuracy over a wide range of shear rates.",1965,3,1451,68,0,8,2,6,3,4,3,1,2,0
d1e492b3369299373277b27f5c90089af8ba1561,"This book bridges the gap between the theoretical work of the rheologist, and the practical needs of those who have to design and operate the systems in which these materials are handled or processed. It is an established and important reference for senior level mechanical engineers, chemical and process engineers, as well as any engineer or scientist who needs to study or work with these fluids, including pharmaceutical engineers, mineral processing engineers, medical researchers, water and civil engineers. This new edition covers a considerably broader range of topics than its predecessor, including computational fluid dynamics modeling techniques, liquid/solid flows and applications to areas such as food processing, among others.Written by two of the world's leading experts, this is the only dedicated non-Newtonian flow reference in print. Since first publication significant advances have been made in almost all areas covered in this book, which are incorporated in the new edition, including developments in CFD and computational techniques, velocity profiles in pipes, liquid/solid flows and applications to food processing, and new heat/mass transfer methods and models. This book covers both basic rheology and the fluid mechanics of NN fluids. It is a truly self-contained reference for anyone studying or working with the processing and handling of fluids.",2008,0,513,39,0,5,18,22,34,40,47,60,56,55
4b41c58e95168c81e46e5c705da472f901dfc6d5,"We examine the hypothesis that every particle of mass $m$ is subject to a Brownian motion with diffusion coefficient $\frac{\ensuremath{\hbar}}{2m}$ and no friction. The influence of an external field is expressed by means of Newton's law $\mathbf{F}=m\mathbf{a}$, as in the Ornstein-Uhlenbeck theory of macroscopic Brownian motion with friction. The hypothesis leads in a natural way to the Schr\""odinger equation, but the physical interpretation is entirely classical. Particles have continuous trajectories and the wave function is not a complete description of the state. Despite this opposition to quantum mechanics, an examination of the measurement process suggests that, within a limited framework, the two theories are equivalent.",1966,0,1281,56,0,2,3,5,3,3,5,3,6,6
c8831d7d318b8d59f9b958d250a58f253f08bd8a,"This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.",2009,90,5538,1042,9,66,155,236,340,507,531,647,665,702
e5136e9306bf1b0e3d4be0cea384ee9a969a44fa,"Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.",2006,110,75455,5312,0,0,0,0,0,0,1,0,0,0
67556c4f0cfdd1f09fff373768b03638f949be0d,"Chapter 3 deals with probability distributions, discrete and continuous densities, distribution functions, bivariate distributions, means, variances, covariance, correlation, and some random process material. Chapter 4 is a detailed study of the concept of utility including the psychological aspects, risk, attributes, rules for utilities, multidimensional utility, and normal form of analysis. Chapter 5 treats games and optimization, linear optimization, and mixed strategies. Entropy is the topic of Chapter 6 with sections devoted to entropy, disorder, information, Shannon’s theorem, demon’s roulette, Maxwell– Boltzmann distribution, Schrodinger’s nutshell, maximum entropy probability distributions, blackbodies, and Bose–Einstein distribution. Chapter 7 is standard statistical fare including transformations of random variables, characteristic functions, generating functions, and the classic limit theorems such as the central limit theorem and the laws of large numbers. Chapter 8 is about exchangeability and inference with sections on Bayesian techniques and classical inference. Partial exchangeability is also treated. Chapter 9 considers such things as order statistics, extreme value, intensity, hazard functions, and Poisson processes. Chapter 10 covers basic elements of risk and reliability, while Chapter 11 is devoted to curve fitting, regression, and Monte Carlo simulation. There is an ample number of exercises at the ends of the chapters with answers or comments on many of them in an appendix in the back of the book. Other appendices are on the common discrete and continuous distributions and mathematical aspects of integration.",2007,0,18858,4790,0,0,0,0,0,0,0,1,7,941
4d8a5338042da99819746ff835b6f299135e2023,Contents: Prefaces. The Concepts of Power Analysis. The t-Test for Means. The Significance of a Product Moment rs (subscript s). Differences Between Correlation Coefficients. The Test That a Proportion is .50 and the Sign Test. Differences Between Proportions. Chi-Square Tests for Goodness of Fit and Contingency Tables. The Analysis of Variance and Covariance. Multiple Regression and Correlation Analysis. Set Correlation and Multivariate Methods. Some Issues in Power Analysis. Computational Procedures.,1969,0,106838,18082,0,0,0,0,0,0,0,0,0,0
75c2f465d59739dbc06b70fd34dc3c1b2336103e,"Abstract The need for a simply applied quantitative assessment of handedness is discussed and some previous forms reviewed. An inventory of 20 items with a set of instructions and response- and computational-conventions is proposed and the results obtained from a young adult population numbering some 1100 individuals are reported. The separate items are examined from the point of view of sex, cultural and socio-economic factors which might appertain to them and also of their inter-relationship to each other and to the measure computed from them all. Criteria derived from these considerations are then applied to eliminate 10 of the original 20 items and the results recomputed to provide frequency-distribution and cumulative frequency functions and a revised item-analysis. The difference of incidence of handedness between the sexes is discussed.",1971,12,30932,2544,0,0,0,0,0,0,0,0,0,0
ec3d71a2fdd01968a6dc638ee261715a0f118c1e,"Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An overdispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of overdispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au",2009,10,21510,1160,0,0,0,0,0,0,1,1,7,65
a2893118e14c29a23472b02249b4641b9971786b,"Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.",2005,85,26845,2904,0,0,0,0,0,0,0,0,0,0
d17669e4f3f4dd3a180bde84dd54a508d0dc22f4,"A comprehensive, but simple-to-use software package for executing a range of standard numerical analysis and operations used in quantitative paleontology has been developed. The program, called PAST (PAleontological STatistics), runs on standard Windows computers and is available free of charge. PAST integrates spreadsheet-type data entry with univariate and multivariate statistics, curve fitting, timeseries analysis, data plotting, and simple phylogenetic analysis. Many of the functions are specific to paleontology and ecology, and these functions are not found in standard, more extensive, statistical packages. PAST also includes fourteen case studies (data files and exercises) illustrating use of the program for paleontological problems, making it a complete educational package for courses in quantitative methods.",2001,17,17962,3587,0,0,0,0,0,0,0,0,0,0
9529c25408dc86194d417aed73d49ae0e418f1be,"32.03 MB Free download Econometric Analysis of Cross Section and Panel Data book PDF, FB2, EPUB and MOBI. Read online Econometric Analysis of Cross Section and Panel Data which classified as Other that has 776 pages that contain constructive material with lovely reading experience. Reading online Econometric Analysis of Cross Section and Panel Data book will be provide using wonderful book reader and it's might gives you some access to identifying the book content before you download the book.",2002,0,20999,2368,0,0,0,0,0,0,0,3,0,4
1d6a71481bcb38a593480a9f8827079ce99835b1,"Matthew B. Miles, Qualitative Data Analysis A Methods Sourcebook, Third Edition. The Third Edition of Miles & Huberman's classic research methods text is updated and streamlined by Johnny Saldana, author of The Coding Manual for Qualitative Researchers. Several of the data display strategies from previous editions are now presented in re-envisioned and reorganized formats to enhance reader accessibility and comprehension. The Third Edition's presentation of the fundamentals of research design and data management is followed by five distinct methods of analysis: exploring, describing, ordering, explaining, and predicting. Miles and Huberman's original research studies are profiled and accompanied with new examples from Saldana's recent qualitative work. The book's most celebrated chapter, ""Drawing and Verifying Conclusions,"" is retained and revised, and the chapter on report writing has been greatly expanded, and is now called ""Writing About Qualitative Research."" Comprehensive and authoritative, Qualitative Data Analysis has been elegantly revised for a new generation of qualitative researchers. Johnny Saldana, The Coding Manual for Qualitative Researchers, Second Edition. The Second Edition of Johnny Saldana's international bestseller provides an in-depth guide to the multiple approaches available for coding qualitative data. Fully up-to-date, it includes new chapters, more coding techniques and an additional glossary. Clear, practical and authoritative, the book: describes how coding initiates qualitative data analysis; demonstrates the writing of analytic memos; discusses available analytic software; suggests how best to use the book for particular studies. In total, 32 coding methods are profiled that can be applied to a range of research genres from grounded theory to phenomenology to narrative inquiry. For each approach, Saldana discusses the method's origins, a description of the method, practical applications, and a clearly illustrated example with analytic follow-up. A unique and invaluable reference for students, teachers, and practitioners of qualitative inquiry, this book is essential reading across the social sciences. Stephanie D. H. Evergreen, Presenting Data Effectively Communicating Your Findings for Maximum Impact. This is a step-by-step guide to making the research results presented in reports, slideshows, posters, and data visualizations more interesting. Written in an easy, accessible manner, Presenting Data Effectively provides guiding principles for designing data presentations so that they are more likely to be heard, remembered, and used. The guidance in the book stems from the author's extensive study of research reporting, a solid review of the literature in graphic design and related fields, and the input of a panel of graphic design experts. Those concepts are then translated into language relevant to students, researchers, evaluators, and non-profit workers - anyone in a position to have to report on data to an outside audience. The book guides the reader through design choices related to four primary areas: graphics, type, color, and arrangement. As a result, readers can present data more effectively, with the clarity and professionalism that best represents their work.",1994,0,39158,2254,0,0,0,0,0,0,0,0,0,0
0ad5733eafb41274895bf1dce6b92ae8f3d68c60,,2004,0,18827,2817,1,3,6,2,10,716,1441,1290,1233,1117
7bd23e6ec32cb1507a385c21a21150e9c332682f,"genalex is a user-friendly cross-platform package that runs within Microsoft Excel, enabling population genetic analyses of codominant, haploid and binary data. Allele frequency-based analyses include heterozygosity, F statistics, Nei's genetic distance, population assignment, probabilities of identity and pairwise relatedness. Distance-based calculations include amova, principal coordinates analysis (PCA), Mantel tests, multivariate and 2D spatial autocorrelation and twogener. More than 20 different graphs summarize data and aid exploration. Sequence and genotype data can be imported from automated sequencers, and exported to other software. Initially designed as tool for teaching, genalex 6 now offers features for researchers as well. Documentation and the program are available at http://www.anu.edu.au/BoZo/GenAlEx/",2006,12,14540,3329,0,0,0,0,0,0,3,990,1332,1368
0d6cf3cd3794bc31a4e5a8ded4d4ba83bb20f9b0,"BOOK REVIEW: Constructing grounded theory. A practical guide through qualitative analysis Kathy Charmaz, 2006, 208 pp. London: Sage. ISBN 2005928035",2006,0,10705,2251,0,0,0,1,375,666,939,1134,1329,847
91dd073b9bfaf29b6c3d3a58418e2bdc765541ea,,1989,0,45842,9926,0,0,0,0,0,0,0,0,0,0
895860c6083736508d2541900cdf0960eb11592f,"The design, implementation, and capabilities of an extensible visualization system, UCSF Chimera, are discussed. Chimera is segmented into a core that provides basic services and visualization, and extensions that provide most higher level functionality. This architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features. Two unusual extensions are presented: Multiscale, which adds the ability to visualize large‐scale molecular assemblies such as viral coats, and Collaboratory, which allows researchers to share a Chimera session interactively despite being at separate locales. Other extensions include Multalign Viewer, for showing multiple sequence alignments and associated structures; ViewDock, for screening docked ligand orientations; Movie, for replaying molecular dynamics trajectories; and Volume Viewer, for display and analysis of volumetric data. A discussion of the usage of Chimera in real‐world situations is given, along with anticipated future directions. Chimera includes full user documentation, is free to academic and nonprofit users, and is available for Microsoft Windows, Linux, Apple Mac OS X, SGI IRIX, and HP Tru64 Unix from http://www.cgl.ucsf.edu/chimera/. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1605–1612, 2004",2004,70,28253,1711,0,0,0,0,1,0,0,0,0,0
c92d6fa1e30e12946c874e5a8b9aeee3c0155e29,Introduction The Logic of Hierarchical Linear Models Principles of Estimation and Hypothesis Testing for Hierarchical Linear Models An Illustration Applications in Organizational Research Applications in the Study of Individual Change Applications in Meta-Analysis and Other Cases Where Level-1 Variances are Known Three-Level Models Assessing the Adequacy of Hierarchical Models Technical Appendix,1992,2,24107,1999,0,0,0,0,0,0,0,0,0,0
e7c8aa2cb2223f17615c1b1ae3b33095466e95cc,"The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.",2001,21,115756,1655,0,0,0,0,0,0,0,0,1,0
05abdc87bcaf2963fd511672e64ab39d02239aaf,,2007,30,24477,2512,0,1,0,0,1,0,5,6,546,2032
2d6f573c36c5e2153b65859fb080523fc4d842d0,"We announce the release of the fourth version of MEGA software, which expands on the existing facilities for editing DNA sequence data from autosequencers, mining Web-databases, performing automatic and manual sequence alignment, analyzing sequence alignments to estimate evolutionary distances, inferring phylogenetic trees, and testing evolutionary hypotheses. Version 4 includes a unique facility to generate captions, written in figure legend format, in order to provide natural language descriptions of the models and methods used in the analyses. This facility aims to promote a better understanding of the underlying assumptions used in analyses, and of the results generated. Another new feature is the Maximum Composite Likelihood (MCL) method for estimating evolutionary distances between all pairs of sequences simultaneously, with and without incorporating rate variation among sites and substitution pattern heterogeneities among lineages. This MCL method also can be used to estimate transition/transversion bias and nucleotide substitution pattern without knowledge of the phylogenetic tree. This new version is a native 32-bit Windows application with multi-threading and multi-user supports, and it is also available to run in a Linux desktop environment (via the Wine compatibility layer) and on Intel-based Macintosh computers under the Parallels program. The current version of MEGA is available free of charge at (http://www.megasoftware.net).",2007,10,28416,6041,0,0,0,0,1,18,2040,2152,1667,1255
57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,"DAVID bioinformatics resources consists of an integrated biological knowledgebase and analytic tools aimed at systematically extracting biological meaning from large gene/protein lists. This protocol explains how to use DAVID, a high-throughput and integrated data-mining environment, to analyze gene lists derived from high-throughput genomic experiments. The procedure first requires uploading a gene list containing any number of common gene identifiers followed by analysis using one or more text and pathway-mining tools such as gene functional classification, functional annotation chart or clustering and functional annotation table. By following this protocol, investigators are able to gain an in-depth understanding of the biological themes in lists of genes that are enriched in genome-scale studies.",2008,16,27477,2131,0,0,0,0,0,0,0,0,7,39
cad327e1e3a0799202cb40da5da51d7b0616b64e,"Arlequin ver 3.0 is a software package integrating several basic and advanced methods for population genetics data analysis, like the computation of standard genetic diversity indices, the estimation of allele and haplotype frequencies, tests of departure from linkage equilibrium, departure from selective neutrality and demographic equilibrium, estimation or parameters from past population expansions, and thorough analyses of population subdivision under the AMOVA framework. Arlequin 3 introduces a completely new graphical interface written in C++, a more robust semantic analysis of input files, and two new methods: a Bayesian estimation of gametic phase from multi-locus genotypes, and an estimation of the parameters of an instantaneous spatial expansion from DNA sequence polymorphism. Arlequin can handle several data types like DNA sequences, microsatellite data, or standard multi-locus genotypes. A Windows version of the software is freely available on http://cmpg.unibe.ch/software/arlequin3.",2007,148,12555,4436,1,1,21,1208,1437,1392,1211,1052,891,751
e09bb0025f4939a4bd233a70937584b4d7bdd0a7,"BackgroundThe evolutionary analysis of molecular sequence variation is a statistical enterprise. This is reflected in the increased use of probabilistic models for phylogenetic inference, multiple sequence alignment, and molecular population genetics. Here we present BEAST: a fast, flexible software architecture for Bayesian analysis of molecular sequences related by an evolutionary tree. A large number of popular stochastic models of sequence evolution are provided and tree-based models suitable for both within- and between-species sequence data are implemented.ResultsBEAST version 1.4.6 consists of 81000 lines of Java source code, 779 classes and 81 packages. It provides models for DNA and protein sequence evolution, highly parametric coalescent analysis, relaxed clock phylogenetics, non-contemporaneous sequence data, statistical alignment and a wide range of options for prior distributions. BEAST source code is object-oriented, modular in design and freely available at http://beast-mcmc.googlecode.com/ under the GNU LGPL license.ConclusionBEAST is a powerful and flexible evolutionary analysis package for molecular sequence variation. It also provides a resource for the further development of new models and statistical methods of evolutionary analysis.",2007,45,11132,3502,0,0,1,255,973,1270,1219,1114,1018,940
80935b370bac09ce615a002caabc30fbb26f029b,"With its theoretical basis firmly established in molecular evolutionary and population genetics, the comparative DNA and protein sequence analysis plays a central role in reconstructing the evolutionary histories of species and multigene families, estimating rates of molecular evolution, and inferring the nature and extent of selective forces shaping the evolution of genes and genomes. The scope of these investigations has now expanded greatly owing to the development of high-throughput sequencing techniques and novel statistical and computational methods. These methods require easy-to-use computer programs. One such effort has been to produce Molecular Evolutionary Genetics Analysis (MEGA) software, with its focus on facilitating the exploration and analysis of the DNA and protein sequence variation from an evolutionary perspective. Currently in its third major release, MEGA3 contains facilities for automatic and manual sequence alignment, web-based mining of databases, inference of the phylogenetic trees, estimation of evolutionary distances and testing evolutionary hypotheses. This paper provides an overview of the statistical methods, computational tools, and visual exploration modules for data input and the results obtainable in MEGA.",2004,73,12037,3302,1,0,26,1588,2150,1625,1243,879,678,475
7c3b564bbdc8e7e3242257189ab7702d3e095115,"Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.",1985,0,29478,1819,0,0,0,0,0,0,0,0,0,0
e2f4f64a17a05379e45f713d10d7c546bda7734a,"Contents: Preface. Introduction. Bivariate Correlation and Regression. Multiple Regression/Correlation With Two or More Independent Variables. Data Visualization, Exploration, and Assumption Checking: Diagnosing and Solving Regression Problems I. Data-Analytic Strategies Using Multiple Regression/Correlation. Quantitative Scales, Curvilinear Relationships, and Transformations. Interactions Among Continuous Variables. Categorical or Nominal Independent Variables. Interactions With Categorical Variables. Outliers and Multicollinearity: Diagnosing and Solving Regression Problems II. Missing Data. Multiple Regression/Correlation and Causal Models. Alternative Regression Models: Logistic, Poisson Regression, and the Generalized Linear Model. Random Coefficient Regression and Multilevel Models. Longitudinal Regression Methods. Multiple Dependent Variables: Set Correlation. Appendices: The Mathematical Basis for Multiple Regression/Correlation and Identification of the Inverse Matrix Elements. Determination of the Inverse Matrix and Applications Thereof.",1979,0,29548,1563,0,0,0,0,0,0,0,0,0,0
4c97d9b9eb7c158e682844b394b42924af3c5b3f,"Interpretative phenomenological analysis (IPA) is an increasingly popular approach to qualitative inquiry. This handy text covers its theoretical foundations and provides a detailed guide to conducting IPA research. 
 
Extended worked examples from the authors' own studies in health, sexuality, psychological distress and identity illustrate the breadth and depth of IPA research. 
 
Each of the chapters also offers a guide to other good exemplars of IPA research in the designated area. The final section of the book considers how IPA connects with other contemporary qualitative approaches like discourse and narrative analysis and how it addresses issues to do with validity. The book is written in an accessible style and will be extremely useful to students and researchers in psychology and related disciplines in the health and social sciences.",2009,7,6187,1588,6,76,172,280,438,624,709,699,792,811
6126379e9ffbf088fc5cf838ba53a9b95526d0d5,"This journal frequently contains papers that report values of F-statistics estimated from genetic data collected from several populations. These parameters, FST, FIT, and FIS, were introduced by Wright (1951), and offer a convenient means of summarizing population structure. While there is some disagreement about the interpretation of the quantities, there is considerably more disagreement on the method of evaluating them. Different authors make different assumptions about sample sizes or numbers of populations and handle the difficulties of multiple alleles and unequal sample sizes in different ways. Wright himself, for example, did not consider the effects of finite sample size. The purpose of this discussion is to offer some unity to various estimation formulae and to point out that correlations of genes in structured populations, with which F-statistics are concerned, are expressed very conveniently with a set of parameters treated by Cockerham (1 969, 1973). We start with the parameters and construct appropriate estimators for them, rather than beginning the discussion with various data functions. The extension of Cockerham's work to multiple alleles and loci will be made explicit, and the use of jackknife procedures for estimating variances will be advocated. All of this may be regarded as an extension of a recent treatment of estimating the coancestry coefficient to serve as a mea-",1984,56,16881,2899,0,0,0,0,0,0,0,0,0,0
7ea9b1915072f2adff3762b59b7c7d79805fee8e,"If radiocarbon measurements are to be used at all for chronological purposes, we have to use statistical methods for calibration. The most widely used method of calibration can be seen as a simple application of Bayesian statistics, which uses both the information from the new measurement and information from the 14C calibration curve. In most dating applications, however, we have larger numbers of 14C measurements and we wish to relate those to events in the past. Bayesian statistics provides a coherent framework in which such analysis can be performed and is becoming a core element in many 14C dating projects. This article gives an overview of the main model components used in chronological analysis, their mathematical formulation, and examples of how such analyses can be performed using the latest version of the OxCal software (v4). Many such models can be put together, in a modular fashion, from simple elements, with defined constraints and groupings. In other cases, the commonly used ""uniform phase"" models might not be appropriate, and ramped, exponential, or normal distributions of events might be more useful. When considering analyses of these kinds, it is useful to be able run simulations on synthetic data. Methods for performing such tests are discussed here along with other methods of diagnosing possible problems with statistical models of this kind.",2009,69,4532,1444,11,103,201,275,402,450,446,453,577,449
25075e27b0df6f2be5a8c519171bdabd1c3ed817,History Conceptual Foundations Uses and Kinds of Inference The Logic of Content Analysis Designs Unitizing Sampling Recording Data Languages Constructs for Inference Analytical Techniques The Use of Computers Reliability Validity A Practical Guide,1980,0,23667,1925,0,0,0,0,0,0,0,0,0,0
2098391def9f8db6d400497b9ba397d526d227c3,This paper examines eight published reviews each reporting results from several related trials. Each review pools the results from the relevant trials in order to evaluate the efficacy of a certain treatment for a specified medical condition. These reviews lack consistent assessment of homogeneity of treatment effect before pooling. We discuss a random effects approach to combining evidence from a series of experiments comparing two treatments. This approach incorporates the heterogeneity of effects in the analysis of the overall treatment efficacy. The model can be extended to include relevant covariates which would reduce the heterogeneity and allow for more specific therapeutic recommendations. We suggest a simple noniterative procedure for characterizing the distribution of treatment effects in a series of studies.,1986,29,30512,935,0,0,0,0,0,0,0,0,0,0
b9544a1bf4b02c6648dbd12702bc10c00e20e197,"Content analysis is a widely used qualitative research technique. Rather than being a single method, current applications of content analysis show three distinct approaches: conventional, directed, or summative. All three approaches are used to interpret meaning from the content of text data and, hence, adhere to the naturalistic paradigm. The major differences among the approaches are coding schemes, origins of codes, and threats to trustworthiness. In conventional content analysis, coding categories are derived directly from the text data. With a directed approach, analysis starts with a theory or relevant research findings as guidance for initial codes. A summative content analysis involves counting and comparisons, usually of keywords or content, followed by the interpretation of the underlying context. The authors delineate analytic procedures specific to each approach and techniques addressing trustworthiness with hypothetical examples drawn from the area of end-of-life care.",2005,57,24855,943,0,0,0,0,0,0,0,0,0,0
2fcf90089d9f95025e8953812a43f0db2de3af7c,,2009,0,10325,2655,0,0,281,430,655,922,1128,1232,1332,1349
d76bde423b71f1cb900b988311bd2d71b700d506,"The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.",2002,35,21349,493,0,0,0,0,1,0,1,0,1,1
0cd64c55c98cdc4a7ef041a843ff796a995952a4,"Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials' effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure",1997,87,34016,852,0,0,0,0,0,0,0,0,0,0
fc448a7db5a2fac242705bd8e37ae1fc4a858643,"The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence.",2001,431,17448,430,5,1,4,2,1,2,8,515,764,774
9301eab07d6c64ee86651bc15ffab9663a6995b6,"Social Network Analysis Methods And Social network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them. Examples of social structures commonly visualized through social network ...",1996,0,13802,1523,0,0,0,0,0,2,1,0,0,1
1a77e19441f3a0e030998fb2d11d9dd774582403,"The techniques available for the interrogation and analysis of neuroimaging data have a large influence in determining the flexibility, sensitivity, and scope of neuroimaging experiments. The development of such methodologies has allowed investigators to address scientific questions that could not previously be answered and, as such, has become an important research area in its own right. In this paper, we present a review of the research carried out by the Analysis Group at the Oxford Centre for Functional MRI of the Brain (FMRIB). This research has focussed on the development of new methodologies for the analysis of both structural and functional magnetic resonance imaging data. The majority of the research laid out in this paper has been implemented as freely available software tools within FMRIB's Software Library (FSL).",2004,52,10626,1539,0,0,0,0,0,97,419,577,673,764
76ad159a2887b008e5a7335d124c148c13e65465,"We have developed a toolbox and graphic user interface, EEGLAB, running under the crossplatform MATLAB environment (The Mathworks, Inc.) for processing collections of single-trial and/or averaged EEG data of any number of channels. Available functions include EEG data, channel and event information importing, data visualization (scrolling, scalp map and dipole model plotting, plus multi-trial ERP-image plots), preprocessing (including artifact rejection, filtering, epoch selection, and averaging), independent component analysis (ICA) and time/frequency decompositions including channel and component cross-coherence supported by bootstrap statistical methods based on data resampling. EEGLAB functions are organized into three layers. Top-layer functions allow users to interact with the data through the graphic interface without needing to use MATLAB syntax. Menu options allow users to tune the behavior of EEGLAB to available memory. Middle-layer functions allow users to customize data processing using command history and interactive 'pop' functions. Experienced MATLAB users can use EEGLAB data structures and stand-alone signal processing functions to write custom and/or batch analysis scripts. Extensive function help and tutorial information are included. A 'plug-in' facility allows easy incorporation of new EEG modules into the main menu. EEGLAB is freely available (http://www.sccn.ucsd.edu/eeglab/) under the GNU public license for noncommercial use and open source development, together with sample data, user tutorial and extensive documentation.",2004,85,14420,1913,0,0,0,0,0,0,0,0,0,0
e250c4b0cc0180af9f47b97f3b9ff1727a2767aa,"New software, OLEX2, has been developed for the determination, visualization and analysis of molecular crystal structures. The software has a portable mouse-driven workflow-oriented and fully comprehensive graphical user interface for structure solution, refinement and report generation, as well as novel tools for structure analysis. OLEX2 seamlessly links all aspects of the structure solution, refinement and publication process and presents them in a single workflow-driven package, with the ultimate goal of producing an application which will be useful to both chemists and crystallographers.",2009,10,13126,493,0,0,0,0,0,0,3,688,1472,1660
42abddd227d653a0375d7d037ddb885f6c07f66f,"We present Model-based Analysis of ChIP-Seq data, MACS, which analyzes data generated by short read sequencers such as Solexa's Genome Analyzer. MACS empirically models the shift size of ChIP-Seq tags, and uses it to improve the spatial resolution of predicted binding sites. MACS also uses a dynamic Poisson distribution to effectively capture local biases in the genome, allowing for more robust predictions. MACS compares favorably to existing ChIP-Seq peak-finding algorithms, and is freely available.",2008,17,9852,1177,6,43,122,233,339,454,628,662,729,980
20aeb2357e9e215787c7e0d0acfe7a6b598c9103,"This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkisons Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, its easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot. This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and youll learn everything you need in the book. After reading this book youll be able to produce graphics customized precisely for your problems,and youll find it easy to get graphics out of your head and on to the screen or page.",2009,0,20826,1798,0,0,0,0,0,1,2,0,3,9
7a6142cfa79cc01ceced5e144bd0e01a0f241a74,,2009,43,7886,2012,377,456,500,626,706,655,723,661,657,644
94559c249d204110296c39ed4af2042cc4468e68,"The Karhunen-Lo eve basis functions, more frequently referred to as principal components or empirical orthogonal functions (EOFs), of the noise response of the climate system are an important tool for geophysical studies. Many researchers have used this tool to examine the geophysical and climatological phenomena. Perhaps more frequent use of EOFs in recent studies is in conjunction with the development of the signal detection and estimation methods of the background uctuations of a detection variable serve as an orthogonal basis set and are used to design optimal techniques for detecting and estimating signals. A detection and prediction approach is to design a lter or optimal weights for the signal to be detected. It has been reported that weighted averaging of data over the surface of the Earth improved the detectability of climatic changes (Hasselmann 1979; Stefanick 1981; Bell 1982). Since the signal-to-noise ratio (SNR) varies geographically, there exists an optimal geographical weighting of the signal which maximizes the SNR. The design of an optimal weighting function may require detailed knowledge on the natural uctuation of the climate system. A conceptually similar approach is to employ a particular pattern (or patterns) of climatic change for detection and prediction (e.g., Barnett and Hasselmann 1979; Hasselmann 1979). The patterns of interest (also called the predictors) may include the principal components (empirical orthogonal functions) (e. von Storch 1990) among others. This approach also requires complete knowledge of the natural variability of the climate system. To test and improve the detection and prediction techniques addressed above, a complete cross-spectral covariance matrix, or similarly, a complete set of the principal components of natural uctuations of the climate system for each frequency band of the spectrum is necessary. In reality, a reliable spectrum of observational covariance matrix is not available because observations are not suuciently long and sampling errors contaminate the observational records (Preisendorfer and Barnett 1977; North et al. 1982). Further, inadequate spatial coverage of observations may introduce bias. Therefore, the covariance matrix of the noise response is often estimated from a simple stochastic model. Kim and North (1991, 1992) examined the covariance matrix in terms of various second-moment statistics earlier. Examined here are the principal components of the covariance matrix of the surface temperature uctuations in a simple coupled climate model in comparison with observations. The principal components not only are an",2009,52,14081,1663,1,0,3,13,209,905,1082,1144,1134,1079
da7ab2f1b6278472f3671a7430c9a72bad07781f,"PAML, currently in version 4, is a package of programs for phylogenetic analyses of DNA and protein sequences using maximum likelihood (ML). The programs may be used to compare and test phylogenetic trees, but their main strengths lie in the rich repertoire of evolutionary models implemented, which can be used to estimate parameters in models of sequence evolution and to test interesting biological hypotheses. Uses of the programs include estimation of synonymous and nonsynonymous rates (d(N) and d(S)) between two protein-coding DNA sequences, inference of positive Darwinian selection through phylogenetic comparison of protein-coding genes, reconstruction of ancestral genes and proteins for molecular restoration studies of extinct life forms, combined analysis of heterogeneous data sets from multiple gene loci, and estimation of species divergence times incorporating uncertainties in fossil calibrations. This note discusses some of the major applications of the package, which includes example data sets to demonstrate their use. The package is written in ANSI C, and runs under Windows, Mac OSX, and UNIX systems. It is available at -- (http://abacus.gene.ucl.ac.uk/software/paml.html).",2007,203,9005,1776,10,131,298,412,471,583,614,666,638,749
ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed,"The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. That is, if p is the probability mass function for the population, S = {S1, S2, * *, Sk} is a partition of EN, and ui, i = 1, 2, * , k, is the conditional mean of p over the set Si, then W2(S) = ff=ISi f z u42 dp(z) tends to be low for the partitions S generated by the method. We say 'tends to be low,' primarily because of intuitive considerations, corroborated to some extent by mathematical analysis and practical computational experience. Also, the k-means procedure is easily programmed and is computationally economical, so that it is feasible to process very large samples on a digital computer. Possible applications include methods for similarity grouping, nonlinear prediction, approximating multivariate distributions, and nonparametric tests for independence among several variables. In addition to suggesting practical classification methods, the study of k-means has proved to be theoretically interesting. The k-means concept represents a generalization of the ordinary sample mean, and one is naturally led to study the pertinent asymptotic behavior, the object being to establish some sort of law of large numbers for the k-means. This problem is sufficiently interesting, in fact, for us to devote a good portion of this paper to it. The k-means are defined in section 2.1, and the main results which have been obtained on the asymptotic behavior are given there. The rest of section 2 is devoted to the proofs of these results. Section 3 describes several specific possible applications, and reports some preliminary results from computer experiments conducted to explore the possibilities inherent in the k-means idea. The extension to general metric spaces is indicated briefly in section 4. The original point of departure for the work described here was a series of problems in optimal classification (MacQueen [9]) which represented special",1967,17,23393,1186,0,0,0,0,0,0,0,0,0,0
d40ee5dd758c525dfb9932d726bb4e844b7b8478,"Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.",2006,45,14283,1233,0,0,0,0,0,0,1,5,104,1123
a193120975be25b4ba2764e6d7bf9dc01588aafb,"ACADEMICIANS SEEM to be moving toward the elimination of ratio analysis as an analytical technique in assessing the performance of the business enterprise. Theorists downgrade arbitrary rules of thumb, such as company ratio comparisons, widely used by practitioners. Since attacks on the relevance of ratio analysis emanate from many esteemed members of the scholarly world, does this mean that ratio analysis is limited to the world of ""nuts and bolts""? Or, has the significance of such an approach been unattractively garbed and therefore unfairly handicapped? Can we bridge the gap, rather than sever the link, between traditional ratio ""analysis"" and the more rigorous statistical techniques which have become popular among academicians in recent years? The purpose of this paper is to attempt an assessment of this issue-the quality of ratio analysis as an analytical technique. The prediction of corporate bankruptcy is used as an illustrative case.' Specifically, a set of financial and economic ratios will be investigated in a bankruptcy prediction context wherein a multiple discriminant statistical methodology is employed. The data used in the study are limited to manufacturing corporations. A brief review of the development of traditional ratio analysis as a technique for investigating corporate performance is presented in section I. In section II the shortcomings of this approach are discussed and multiple discriminant analysis is introduced with the emphasis centering on its compatibility with ratio analysis in a bankruptcy prediction context. The discriminant model is developed in section III, where an initial sample of sixty-six firms is utilized to establish a function which best discriminates between companies in two mutually exclusive groups: bankrupt and non-bankrupt firms. Section IV reviews empirical results obtained from the initial sample and several secondary samples, the latter being selected to examine the reliability of the discriminant",1968,0,11355,2161,0,0,0,0,0,0,0,0,0,0
39dae53515afb42664369c291ec6d1ce34d778bd,"BackgroundCorrelation networks are increasingly being used in bioinformatics applications. For example, weighted gene co-expression network analysis is a systems biology method for describing the correlation patterns among genes across microarray samples. Weighted correlation network analysis (WGCNA) can be used for finding clusters (modules) of highly correlated genes, for summarizing such clusters using the module eigengene or an intramodular hub gene, for relating modules to one another and to external sample traits (using eigengene network methodology), and for calculating module membership measures. Correlation networks facilitate network based gene screening methods that can be used to identify candidate biomarkers or therapeutic targets. These methods have been successfully applied in various biological contexts, e.g. cancer, mouse genetics, yeast genetics, and analysis of brain imaging data. While parts of the correlation network methodology have been described in separate publications, there is a need to provide a user-friendly, comprehensive, and consistent software implementation and an accompanying tutorial.ResultsThe WGCNA R software package is a comprehensive collection of R functions for performing various aspects of weighted correlation network analysis. The package includes functions for network construction, module detection, gene selection, calculations of topological properties, data simulation, visualization, and interfacing with external software. Along with the R package we also present R software tutorials. While the methods development was motivated by gene expression data, the underlying data mining approach can be applied to a variety of different settings.ConclusionThe WGCNA package provides R functions for weighted correlation network analysis, e.g. co-expression network analysis of gene expression data. The R package along with its source code and additional material are freely available at http://www.genetics.ucla.edu/labs/horvath/CoexpressionNetwork/Rpackages/WGCNA.",2008,51,8999,1052,2,9,29,60,112,163,263,361,592,783
b6fbb3a44a8e946b4a231118a737a396517c83de,"AIM
This paper is a description of inductive and deductive content analysis.


BACKGROUND
Content analysis is a method that may be used with either qualitative or quantitative data and in an inductive or deductive way. Qualitative content analysis is commonly used in nursing studies but little has been published on the analysis process and many research books generally only provide a short description of this method.


DISCUSSION
When using content analysis, the aim was to build a model to describe the phenomenon in a conceptual form. Both inductive and deductive analysis processes are represented as three main phases: preparation, organizing and reporting. The preparation phase is similar in both approaches. The concepts are derived from the data in inductive content analysis. Deductive content analysis is used when the structure of analysis is operationalized on the basis of previous knowledge.


CONCLUSION
Inductive content analysis is used in cases where there are no previous studies dealing with the phenomenon or when it is fragmented. A deductive approach is useful if the general aim was to test a previous theory in a different situation or to compare categories at different time periods.",2008,65,11804,608,0,0,0,0,0,2,414,1016,1090,1289
c98386ddf2fe4973da42187a9e3c9167095acf4e,"During the past 30 years, meta-analysis has been an indispensable tool for revealing the hidden meaning of our research literatures. The four articles in this special section on meta-analysis illustrate some of the complexities entailed in meta-analysis methods. Although meta-analysis is a powerful tool for advancing cumulative knowledge, researchers can be confused by the complicated issues involved in the methodology. Each of these four articles contributes both to advancing this methodology and to the increasing complexities that can befuddle researchers. In these comments, the author attempts to clarify both of these aspects and provide a perspective on the methodological issues examined in these articles.",2008,114,15135,702,0,0,0,0,0,0,0,0,0,4
0e2532c31c992ac0930998561932f198b467572d,"This article examines the function of documents as a data source in qualitative research and discusses document analysis procedure in the context of actual research experiences. Targeted to research novices, the article takes a nuts‐and‐bolts approach to document analysis. It describes the nature and forms of documents, outlines the advantages and limitations of document analysis, and offers specific examples of the use of documents in the research process. The application of document analysis to a grounded theory study is illustrated.",2009,34,4561,534,0,12,16,59,127,198,275,417,534,725
dd1b3a3793619cec8994cc7cca10e6dee656fb7b,"UNLABELLED
Research over the last few years has revealed significant haplotype structure in the human genome. The characterization of these patterns, particularly in the context of medical genetic association studies, is becoming a routine research activity. Haploview is a software package that provides computation of linkage disequilibrium statistics and population haplotype patterns from primary genotype data in a visually appealing and interactive interface.


AVAILABILITY
http://www.broad.mit.edu/mpg/haploview/


CONTACT
jcbarret@broad.mit.edu",2005,12,13384,1397,0,0,0,1,79,1112,1159,1137,1018,973
6fb968167f3c9c76d00d085a57b265cb912ad012,"Data Mining Methods and Models is the second volume of a three-book series on data mining authored by Larose. The following review was performed independently of LaRose’s other two books. Paraphrasing from the Preface, the goal of this book is to “explore the process of data mining from the point of view of model building.” Nevertheless, the reader will soon be aware that this book is not intended to provide a systematic or comprehensive coverage of various data mining algorithms. Instead, it considers supervised learning or predictive modeling only, and it walks the reader through the data mining process merely with a few selected modeling methods such as (generalized) linear modeling and the Bayesian approach. The book has seven chapters. Chapter 1 introduces dimension reduction, with a focus on principal components analysis (PCA) types of techniques. Chapters 2, 3, and 4 provide a detailed coverage of simple linear regression, multiple linear regression, and logistic regression, respectively. Chapter 5 introduces naive Bayes estimation and Bayesian networks. In Chapter 6, the basic idea of genetic algorithms is discussed. Finally, Chapter 7 presents a case study example of modeling response to direct mail marketing within the CRISP (crossindustry standard process) framework. This book is very easy to read, and this is absolutely the strength which many readers, especially those nonstatistically oriented ones, will greatly appreciate. Predictive modeling is perhaps the most technical part in a data mining process. The author has done an excellent job in making this difficult topic accessible to a broad audience. For example, I like the way in which Bayesian networks are introduced in Chapter 5. After the reader goes through a churn example on naive Bayes estimation in a step-by-step manner, Bayesian belief networks become easily understood as natural extensions. The overall style of the book is clear and patient. The main limitation of the book is its limited coverage. An inspired reader would expect to see a much more extended list of topics. Hastie, Tibishirani, and Friedman (2001) gave a full and more technical account of various data mining algorithms. The inclusion of genetic algorithms in Chapter 6 seems novel when compared to Hastie, Tibishirani, and Friedman (2001), but at the same time, a little unexpected as a separate chapter, since a genetic algorithm involves a stochastics search scheme, which is somewhat involved given the elementary nature of this text. Another noteworthy issue is that the author does not make an attempt to distinguish between conventional statistical analysis and data mining. I found a few errors. On Page 25, for example, it should be ai = 1, instead of ai = 1/4. Also, in the frame on the top of Page 211, it might have been “Posterior Odds,” instead of “Posterior Odds Ratio.” The book uses three different software packages to implement the ideas including SPSS with Clementine, Minitab, and WEKA, which might not be appealing. On the other hand, it is justifiable as it allows one to perform data mining with affordable costs. In summary, I recommend this fairly readable book for adoption in a graduate-level introductory course on data mining, especially when the students come from varied backgrounds.",2008,5,6073,1828,57,100,190,265,350,401,473,572,669,712
85dfac3a261fbdea3b4e1a6f3264c6384bbc4485,"Qualitative content analysis as described in published literature shows conflicting opinions and unsolved issues regarding meaning and use of concepts, procedures and interpretation. This paper provides an overview of important concepts (manifest and latent content, unit of analysis, meaning unit, condensation, abstraction, content area, code, category and theme) related to qualitative content analysis; illustrates the use of concepts related to the research procedure; and proposes measures to achieve trustworthiness (credibility, dependability and transferability) throughout the steps of the research procedure. Interpretation in qualitative content analysis is discussed in light of Watzlawick et al.'s [Pragmatics of Human Communication. A Study of Interactional Patterns, Pathologies and Paradoxes. W.W. Norton & Company, New York, London] theory of communication.",2004,58,14395,1105,0,0,0,0,0,0,0,0,0,5
38825d4f600ceb71825eca070543f7ebb3cfc7eb,"Analysis of decision making under risk has been dominated by expected utility theory, which generally accounts for people's actions. Presents a critique of expected utility theory as a descriptive model of decision making under risk, and argues that common forms of utility theory are not adequate, and proposes an alternative theory of choice under risk called prospect theory. In expected utility theory, utilities of outcomes are weighted by their probabilities. Considers results of responses to various hypothetical decision situations under risk and shows results that violate the tenets of expected utility theory. People overweight outcomes considered certain, relative to outcomes that are merely probable, a situation called the ""certainty effect."" This effect contributes to risk aversion in choices involving sure gains, and to risk seeking in choices involving sure losses. In choices where gains are replaced by losses, the pattern is called the ""reflection effect."" People discard components shared by all prospects under consideration, a tendency called the ""isolation effect."" Also shows that in choice situations, preferences may be altered by different representations of probabilities. Develops an alternative theory of individual decision making under risk, called prospect theory, developed for simple prospects with monetary outcomes and stated probabilities, in which value is given to gains and losses (i.e., changes in wealth or welfare) rather than to final assets, and probabilities are replaced by decision weights. The theory has two phases. The editing phase organizes and reformulates the options to simplify later evaluation and choice. The edited prospects are evaluated and the highest value prospect chosen. Discusses and models this theory, and offers directions for extending prospect theory are offered. (TNM)",1979,0,20071,607,0,0,0,0,0,0,0,0,0,0
b07ce649d6f6eb636872527104b0209d3edc8188,"Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.",1974,0,16986,737,0,0,1,3,0,1,1,3,3,1
d96faee1898de7a052115ecfe533a5b2fd1151c0,"This paper develops a new approach to the problem of testing the existence of a level relationship between a dependent variable and a set of regressors, when it is not known with certainty whether the underlying regressors are trend- or first-difference stationary. The proposed tests are based on standard F- and t-statistics used to test the significance of the lagged levels of the variables in a univariate equilibrium correction mechanism. The asymptotic distributions of these statistics are non-standard under the null hypothesis that there exists no level relationship, irrespective of whether the regressors are I(0) or I(1). Two sets of asymptotic critical values are provided: one when all regressors are purely I(1) and the other if they are all purely I(0). These two sets of critical values provide a band covering all possible classifications of the regressors into purely I(0), purely I(1) or mutually cointegrated. Accordingly, various bounds testing procedures are proposed. It is shown that the proposed tests are consistent, and their asymptotic distribution under the null and suitably defined local alternatives are derived. The empirical relevance of the bounds procedures is demonstrated by a re-examination of the earnings equation included in the UK Treasury macroeconometric model. Copyright © 2001 John Wiley & Sons, Ltd.",2001,46,11358,2341,0,0,0,0,0,0,0,0,0,11
a82d7e0dc7b2d3a1b159e0902bb9f3017788a786,"A comprehensive, but simple-to-use software package for executing a range of standard numerical analysis and operations used in quantitative paleontology has been developed. The program, called PAST (PAleontological STatistics), runs on standard Windows computers and is available free of charge. PAST integrates spreadsheet-type data entry with univariate and multivariate statistics, curve fitting, timeseries analysis, data plotting, and simple phylogenetic analysis. Many of the functions are specific to paleontology and ecology, and these functions are not found in standard, more extensive, statistical packages. PAST also includes fourteen case studies (data files and exercises) illustrating use of the program for paleontological problems, making it a complete educational package for courses in quantitative methods.",2001,31,10362,1793,0,0,0,0,0,0,0,0,0,17
39dfeda235027e1bed00ab33bcb2ad16831677f8,"Hypothesis-testing methods for multivariate data are needed to make rigorous probability statements about the effects of factors and their interactions in experiments. Analysis of variance is particularly powerful for the analysis of univariate data. The traditional multivariate analogues, however, are too stringent in their assumptions for most ecological multivariate data sets. Non-parametric methods, based on permutation tests, are preferable. This paper describes a new non-parametric method for multivariate analysis of variance, after McArdle and Anderson (in press). It is given here, with several applications in ecology, to provide an alternative and perhaps more intuitive formulation for ANOVA (based on sums of squared distances) to complement the description pro- vided by McArdle and Anderson (in press) for the analysis of any linear model. It is an improvement on previous non-parametric methods because it allows a direct additive partitioning of variation for complex models. It does this while maintaining the flexibility and lack of formal assumptions of other non-parametric methods. The test- statistic is a multivariate analogue to Fisher's F-ratio and is calculated directly from any symmetric distance or dissimilarity matrix. P-values are then obtained using permutations. Some examples of the method are given for tests involving several factors, including factorial and hierarchical (nested) designs and tests of interactions.",2001,127,10950,1442,0,0,0,0,0,0,0,0,0,173
21c41fcec6ac8a7f55c539ac199247f200633753,"Microarrays can measure the expression of thousands of genes to identify changes in expression between different biological states. Methods are needed to determine the significance of these changes while accounting for the enormous number of genes. We describe a method, Significance Analysis of Microarrays (SAM), that assigns a score to each gene on the basis of change in gene expression relative to the standard deviation of repeated measurements. For genes with scores greater than an adjustable threshold, SAM uses permutations of the repeated measurements to estimate the percentage of genes identified by chance, the false discovery rate (FDR). When the transcriptional response of human cells to ionizing radiation was measured by microarrays, SAM identified 34 genes that changed at least 1.5-fold with an estimated FDR of 12%, compared with FDRs of 60 and 84% by using conventional methods of analysis. Of the 34 genes, 19 were involved in cell cycle regulation and 3 in apoptosis. Surprisingly, four nucleotide excision repair genes were induced, suggesting that this repair pathway for UV-damaged DNA might play a previously unrecognized role in repairing DNA damaged by ionizing radiation.",2001,54,11565,1626,0,0,1,3,250,864,864,894,921,881
31e243a7fb9a4dcd411896c262c94ca48a7c6ece,"We provide a concise overview of time series analysis in the time and frequency domains, with lots of references for further reading.",1986,132,11225,1310,0,1,1,0,0,0,1,0,0,2
7494a3c88adeae87f48d20927aac72c67bcf9eb1,"Factor analysis, path analysis, structural equation modeling, and related multivariate statistical methods are based on maximum likelihood or generalized least squares estimation developed for covariance structure models. Large-sample theory provides a chi-square goodness-of-fit test for comparing a model against a general alternative model based on correlated variables. This model comparison is insufficient for model evaluation: In large samples virtually any model tends to be rejected as inadequate, and in small samples various competing models, if evaluated, might be equally acceptable. A general null model based on modified independence among variables is proposed to provide an additional reference point for the statistical and scientific evaluation of covariance structure models. Use of the null model in the context of a procedure that sequentially evaluates the statistical necessity of various sets of parameters places statistical methods in covariance structure analysis into a more complete framework. The concepts of ideal models and pseudo chi-square tests are introduced, and their roles in hypothesis testing are developed. The importance of supplementing statistical evaluation with incremental fit indices associated with the comparison of hierarchical models is also emphasized. Normed and nonnormed fit indices are developed and illustrated.",1980,39,15198,1518,0,0,0,0,0,0,0,0,0,1
0b6e580b00b144fcafcf857fd96cefb6046f0e1c,"THE DESIGN AND ANALYSIS OF EXPERIMENTS. By Oscar Kempthorne. New York, John Wiley and Sons, Inc., 1952. 631 pp. $8.50. This book by a teacher of statistics (as well as a consultant for ""experimenters"") is a comprehensive study of the philosophical background for the statistical design of experiment. It is necessary to have some facility with algebraic notation and manipulation to be able to use the volume intelligently. The problems are presented from the theoretical point of view, without such practical examples as would be helpful for those not acquainted with mathematics. The mathematical justification for the techniques is given. As a somewhat advanced treatment of the design and analysis of experiments, this volume will be interesting and helpful for many who approach statistics theoretically as well as practically. With emphasis on the ""why,"" and with description given broadly, the author relates the subject matter to the general theory of statistics and to the general problem of experimental inference. MARGARET J. ROBERTSON",1953,0,13728,1741,0,0,0,0,0,0,0,0,0,0
dbc447956c16e27cfb030e40552359d3c79bc690,"Whole-genome association studies (WGAS) bring new computational, as well as analytic, challenges to researchers. Many existing genetic-analysis tools are not designed to handle such large data sets in a convenient manner and do not necessarily exploit the new opportunities that whole-genome data bring. To address these issues, we developed PLINK, an open-source C/C++ WGAS tool set. With PLINK, large data sets comprising hundreds of thousands of markers genotyped for thousands of individuals can be rapidly manipulated and analyzed in their entirety. As well as providing tools to make the basic analytic steps computationally efficient, PLINK also supports some novel approaches to whole-genome data that take advantage of whole-genome coverage. We introduce PLINK and describe the five main domains of function: data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation. In particular, we focus on the estimation and use of identity-by-state and identity-by-descent information in the context of population-based whole-genome studies. This information can be used to detect and correct for population stratification and to identify extended chromosomal segments that are shared identical by descent between very distantly related individuals. Analysis of the patterns of segmental sharing has the potential to map disease loci that contain multiple rare variants in a population-based linkage analysis.",2007,47,22265,2672,0,0,0,0,0,1,0,2,4,57
a2893118e14c29a23472b02249b4641b9971786b,"Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.",2005,85,26845,2904,0,0,0,0,0,0,0,0,0,0
ed73eb5b0fcb517d1e13feaa1e09be163e7f7cde,"We present an efficient scheme for calculating the Kohn-Sham ground state of metallic systems using pseudopotentials and a plane-wave basis set. In the first part the application of Pulay's DIIS method (direct inversion in the iterative subspace) to the iterative diagonalization of large matrices will be discussed. Our approach is stable, reliable, and minimizes the number of order ${\mathit{N}}_{\mathrm{atoms}}^{3}$ operations. In the second part, we will discuss an efficient mixing scheme also based on Pulay's scheme. A special ``metric'' and a special ``preconditioning'' optimized for a plane-wave basis set will be introduced. Scaling of the method will be discussed in detail for non-self-consistent and self-consistent calculations. It will be shown that the number of iterations required to obtain a specific precision is almost independent of the system size. Altogether an order ${\mathit{N}}_{\mathrm{atoms}}^{2}$ scaling is found for systems containing up to 1000 electrons. If we take into account that the number of k points can be decreased linearly with the system size, the overall scaling can approach ${\mathit{N}}_{\mathrm{atoms}}$. We have implemented these algorithms within a powerful package called VASP (Vienna ab initio simulation package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semiconducting surfaces, phonons in simple metals, transition metals, and semiconductors) and turned out to be very reliable. \textcopyright{} 1996 The American Physical Society.",1996,1,56162,431,0,0,0,0,0,0,0,0,0,0
bad1be638b556812f6a08b277c0fa2d2e92d5e96,"We present a detailed description and comparison of algorithms for performing ab-initio quantum-mechanical calculations using pseudopotentials and a plane-wave basis set. We will discuss: (a) partial occupancies within the framework of the linear tetrahedron method and the finite temperature density-functional theory, (b) iterative methods for the diagonalization of the Kohn-Sham Hamiltonian and a discussion of an efficient iterative method based on the ideas of Pulay's residual minimization, which is close to an order Natoms2 scaling even for relatively large systems, (c) efficient Broyden-like and Pulay-like mixing methods for the charge density including a new special ‘preconditioning’ optimized for a plane-wave basis set, (d) conjugate gradient methods for minimizing the electronic free energy with respect to all degrees of freedom simultaneously. We have implemented these algorithms within a powerful package called VAMP (Vienna ab-initio molecular-dynamics package). The program and the techniques have been used successfully for a large number of different systems (liquid and amorphous semiconductors, liquid simple and transition metals, metallic and semi-conducting surfaces, phonons in simple metals, transition metals and semiconductors) and turned out to be very reliable.",1996,56,36799,242,0,0,0,0,0,0,0,0,0,0
32d662d196223bb46f8a970f6df68ce69a9a6c2f,The University of Wisconsin Genetics Computer Group (UWGCG) has been organized to develop computational tools for the analysis and publication of biological sequence data. A group of programs that will interact with each other has been developed for the Digital Equipment Corporation VAX computer using the VMS operating system. The programs available and the conditions for transfer are described.,1984,9,13500,577,1,0,0,7,18,44,47,86,100,904
5d54560c6c88eecccf18cdce4255ce63cc91cb36,"The thermodynamic properties of 154 mineral end-members, 13 silicate liquid end-members and 22 aqueous fluid species are presented in a revised and updated data set. The use of a temperature-dependent thermal expansion and bulk modulus, and the use of high-pressure equations of state for solids and fluids, allows calculation of mineral-fluid equilibria to 100 kbar pressure or higher. A pressure-dependent Landau model for order-disorder permits extension of disordering transitions to high pressures, and, in particular, allows the alpha-beta quartz transition to be handled more satisfactorily. Several melt end- members have been included to enable calculation of simple phase equilibria and as a first stage in developing melt mixing models in NCKFMASH. The simple aqueous species density model has been extended to enable speciation calculations and mineral solubility determination involving minerals and aqueous species at high temperatures and pressures. The data set has also been improved by incorporation of many new phase equilibrium constraints, calorimetric studies and new measurements of molar volume, thermal expansion and compressibility. This has led to a significant improvement in the level of agreement with the available experimental phase equilibria, and to greater flexibility in calculation of complex mineral equilibria. It is also shown that there is very good agreement between the data set and the most recent available calorimetric data. kinetics which apply to determining directly the greatest majority of such equilibria in the laboratory, for forming solid solutions, and inclusion of aqueous and silicate melt species), and provides uncertainties especially at lower temperatures, as well as the diYculty of establishing reversals of reactions involving solid allowing the likely uncertainties on the results of thermodynamic calculations to be estimated. This is a solutions. The levels of precision and accuracy required of thermodynamic data in order to be able to forward- critical issue in that calculations using data sets should always involve uncertainty propagation to help evalu- model synthetic and natural mineral assemblages mean that the continuing upgrading and expansion of the ate the results. Because the experimental phase equilib- ria involve overlapping subsets of compositional space, data set by incorporation of new phase equilibrium constraints, calorimetry and new measurements of the derived thermodynamic data are highly correlated, and it is only the inclusion of the correlations which molar volume, thermal expansion and compressibility are more than justified. enables the reliable calculation of uncertainties on mineral reactions to be performed. Earlier work on mineral thermodynamic data sets for rock-forming minerals includes compilations of The thermodynamic data extraction involves using weighted least squares on the diVerent types of data",2004,350,4002,391,139,122,146,177,188,211,195,227,238,226
fcd71a2ca1a15d3b6171670ce82177c81cd9745e,"This article discusses the conduct and evaluatoin of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflect and debate on the important subject of grounding interpretive research methodology.",1999,97,5444,451,23,44,70,98,137,151,167,230,276,291
c896b6c0e8e6be8ba28142e096d705241ce94b9a,"This book is an introduction to level set methods and dynamic implicit surfaces. These are powerful techniques for analyzing and computing moving fronts in a variety of different settings. While it gives many examples of the utility of the methods to a diverse set of applications, it also gives complete numerical analysis and recipes, which will enable users to quickly apply the techniques to real problems. The book begins with a description of implicit surfaces and their basic properties, then devises the level set geometry and calculus toolbox, including the construction of signed distance functions. Part II adds dynamics to this static calculus. Topics include the level set equation itself, Hamilton-Jacobi equations, motion of a surface normal to itself, re-initialization to a signed distance function, extrapolation in the normal direction, the particle level set method and the motion of co-dimension two (and higher) objects. Part III is concerned with topics taken from the fields of Image Processing and Computer Vision. These include the restoration of images degraded by noise and blur, image segmentation with active contours (snakes), and reconstruction of surfaces from unorganized data points. Part IV is dedicated to Computational Physics. It begins with one phase compressible fluid dynamics, then two-phase compressible flow involving possibly different equations of state, detonation and deflagration waves, and solid/fluid structure interaction. Next it discusses incompressible fluid dynamics, including a computer graphics simulation of smoke, free surface flows, including a computer graphics simulation of water, and fully two-phase incompressible flow. Additional related topics include incompressible flames with applications to computer graphics and coupling a compressible and incompressible fluid. Finally, heat flow and Stefan problems are discussed. A student or researcher working in mathematics, computer graphics, science, or engineering interested in any dynamic moving front, which might change its topology or develop singularities, will find this book interesting and useful.",2002,0,5398,423,13,69,140,220,244,318,322,338,362,367
974334c336e87267704cb41e748fc24e5bc0e8e6,"Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.",1996,30,5971,638,27,130,218,233,292,285,332,341,304,351
144adacded5ed56c35a5f157fe231a0459620ec8,"In this article we present a standardized set of 260 pictures for use in experiments investigating differences and similarities in the processing of pictures and words. The pictures are black-and-white line drawings executed according to a set of rules that provide consistency of pictorial representation. The pictures have been standardized on four variables of central relevance to memory and cognitive processing: name agreement, image agreement, familiarity, and visual complexity. The intercorrelations among the four measures were low, suggesting that they are indices of different attributes of the pictures. The concepts were selected to provide exemplars from several widely studied semantic categories. Sources of naming variance, and mean familiarity and complexity of the exemplars, differed significantly across the set of categories investigated. The potential significance of each of the normative variables to a number of semantic and episodic memory tasks is discussed.",1980,39,4690,351,0,0,1,5,8,7,11,14,16,17
b9e43395663f74c581982e9ca97a0d7057a0008c,"LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)≥z(S⋃T)+z(S⋂T) for allS, T inN. Such a function is called submodular. We consider the problem maxS⊂N{a(S):|S|≤K,z(S) submodular}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a “greedy” heuristic always produces a solution whose value is at least 1 −[(K − 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e − 1)/e, where e is the base of the natural logarithm.",1978,13,3951,470,2,3,7,6,6,2,5,6,2,3
981e4615bba324b40b5ef5cac710eab594d5009a,"Abstract The soft set theory offers a general mathematical tool for dealing with uncertain, fuzzy, not clearly defined objects. The main purpose of this paper is to introduce the basic notions of the theory of soft sets, to present the first results of the theory, and to discuss some problems of the future.",1999,11,3324,652,0,0,0,1,3,0,3,1,3,14
b9fdcf9cafc0ef7d66475029e37dbed64ab2ab45,"This document describes release 2.0 of the SimpleScalar tool set, a suite of free, publicly available simulation tools that offer both detailed and high-performance simulation of modern microprocessors. The new release offers more tools and capabilities, precompiled binaries, cleaner interfaces, better documentation, easier installation, improved portability, and higher performance. This paper contains a complete description of the tool set, including retrieval and installation instructions, a description of how to use the tools, a description of the target SimpleScalar architecture, and many details about the internals of the tools and how to customize them. With this guide, the tool set can be brought up and generating results in under an hour (on supported platforms).",1997,15,3460,359,6,31,69,166,212,218,316,290,308,308
eb46bda7b1f60fff84265fec045b473df170caea,"Fuzzy Set Theory - And Its Applications, Third Edition is a textbook for courses in fuzzy set theory. It can also be used as an introduction to the subject. The character of a textbook is balanced with the dynamic nature of the research in the field by including many useful references to develop a deeper understanding among interested readers. The book updates the research agenda (which has witnessed profound and startling advances since its inception some 30 years ago) with chapters on possibility theory, fuzzy logic and approximate reasoning, expert systems, fuzzy control, fuzzy data analysis, decision making and fuzzy set models in operations research. All chapters have been updated. Exercises are included.",1985,194,6446,277,2,5,9,19,27,35,45,55,74,107
dfdb7324a90b5bc11b5c8b39bff6cfa498587c86,,1999,0,4150,347,6,28,82,103,161,196,280,320,331,299
ef4481cbc18c91e7bf0e53693bb77f3608743626,"A family of new measures of point and graph centrality based on early intuitions of Bavelas (1948) is introduced. These measures define centrality in terms of the degree to which a point falls on the shortest path between others and there fore has a potential for control of communication. They may be used to index centrality in any large or small network of symmetrical relations, whether connected or unconnected.",1977,4,7418,301,0,6,2,6,6,2,4,3,1,3
f97ba43adfd5f6a6641c879b854149c4a7df7ca4,"The Penn World Table displays a set of national accounts economic time series covering many countries. Its expenditure entries are denominated in a common set of prices in a common currency so that real quantity comparisons can be made, both between countries and over time. It also provides information about relative prices within and between countries, as well as demographic data and capital stock estimates. This updated, revised, and expanded Mark 5 version of the table includes more countries, years, and variables of interest to economic researchers. The Table is available on personal computer diskettes and through BITNET.",1991,9,3478,236,7,51,64,95,150,164,176,183,224,236
c62015ab9a87bdf67425e916cc14cfac2fe7455c,"A level set method for capturing the interface between two fluids is combined with a variable density projection method to allow for computation of two-phase flow where the interface can merge/break and the flow can have a high Reynolds number. A distance function formulation of the level set method enables one to compute flows with large density ratios (1000/1) and flows that are surface tension driven; with no emotional involvement. Recent work has improved the accuracy of the distance function formulation and the accuracy of the advection scheme. We compute flows involving air bubbles and water drops, to name a few. We validate our code against experiments and theory.",1994,0,3945,186,0,12,18,21,29,54,38,68,75,116
126900d94473a2f06a4b2a265c77fa44df1fd8d1,"Given a collection<inline-equation><f> <sc>F</sc></f></inline-equation> of subsets of <?Pub Fmt italic>S<?Pub Fmt /italic> ={1,…,<?Pub Fmt italic>n<?Pub Fmt /italic>}, <?Pub Fmt italic>setcover<?Pub Fmt /italic> is the problem of selecting as few as possiblesubsets from <inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union covers<?Pub Fmt italic>S,<?Pub Fmt /italic>, and <?Pub Fmt italic>maxk-cover<?Pub Fmt /italic> is the problem of selecting<?Pub Fmt italic>k<?Pub Fmt /italic> subsets from<inline-equation> <f> <sc>F</sc></f></inline-equation> such that their union has maximum cardinality. Both these problems areNP-hard.   We prove that (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) ln<?Pub Fmt italic>n<?Pub Fmt /italic> is a threshold below   which setcover cannot be approximated efficiently, unless NP has slightlysuperpolynomial time algorithms. This closes the gap (up to low-orderterms) between the ratio of approximation achievable by the greedyalogorithm (which is (1 - <?Pub Fmt italic>o<?Pub Fmt /italic>(1)) lnn), and provious results of Lund and Yanakakis, that showed hardness ofapproximation within a ratio of <inline-equation><f><fen lp=""par""><lim align=""r""><op><rf>log</rf></op><ll>2</ll></lim>n<rp post=""par""></fen>/2≃0.72</f></inline-equation> ln <?Pub Fmt italic>n<?Pub Fmt /italic>. For max<?Pub Fmt italic>k<?Pub Fmt /italic>-cover, we show an approximationthreshold of (1 - 1/<?Pub   Fmt italic>e<?Pub Fmt /italic>)(up tolow-order terms), under assumption that <inline-equation><f>P≠NP</f><?Pub   Caret></inline-equation>.",1998,35,3199,250,34,31,35,36,48,67,77,121,110,123
fc3eb090e39d71295c362458b8a0c48d2c5d8377,"During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.",2009,29,2470,275,2,15,50,84,127,154,182,189,257,313
0ca21629c5f2da8a8a704335ab09e9b33f73f133,"This article presents a new data set on inequality in the distribution of income. The authors explain the criteria they applied in selecting data on Gini coefficients and on individual quintile groups' income shares. Comparison of the new data set with existing compilations reveals that the data assembled here represent an improvement in quality and a significant expansion in coverage, although differences in the definition of the underlying data might still affect inter temporal and international comparability. Based on this new data set, the authors do not find a systematic link between growth and changes in aggregate inequality. They do find a strong positive relationship between growth and reduction of poverty.",1996,70,3149,299,6,31,67,128,133,140,203,163,183,168
2c276714c18231f1ff2c7f7727cc479506a5b27a,"We examine explanations for corporate financing-, dividend-, and compensation-policy issues. We document robust empirical relations among corporate policy decisions and various firm characteristics. Our evidence suggests contracting theories are more important in explaining cross-sectional variation in observed financial, dividend, and compensation policies than either tax-based or signaling theories.",1992,33,3812,188,2,12,23,29,31,40,61,80,86,112
2805537bec87a6177037b18f9a3a9d3f1038867b,"The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.",1997,71,3542,215,4,18,24,38,66,50,73,72,72,70
c5561bda7ff1438096f784e553cf60385bddd09a,"Communities have the potential to function effectively and adapt successfully in the aftermath of disasters. Drawing upon literatures in several disciplines, we present a theory of resilience that encompasses contemporary understandings of stress, adaptation, wellness, and resource dynamics. Community resilience is a process linking a network of adaptive capacities (resources with dynamic attributes) to adaptation after a disturbance or adversity. Community adaptation is manifest in population wellness, defined as high and non-disparate levels of mental and behavioral health, functioning, and quality of life. Community resilience emerges from four primary sets of adaptive capacities—Economic Development, Social Capital, Information and Communication, and Community Competence—that together provide a strategy for disaster readiness. To build collective resilience, communities must reduce risk and resource inequities, engage local people in mitigation, create organizational linkages, boost and protect social supports, and plan for not having a plan, which requires flexibility, decision-making skills, and trusted sources of information that function in the face of unknowns.",2008,191,3057,240,10,29,59,99,121,196,251,304,315,326
e48f4d8dedc8ef449edf3d5917bc285901b42bf7,"A set of face stimuli called the NimStim Set of Facial Expressions is described. The goal in creating this set was to provide facial expressions that untrained individuals, characteristic of research participants, would recognize. This set is large in number, multiracial, and available to the scientific community online. The results of psychometric evaluations of these stimuli are presented. The results lend empirical support for the validity and reliability of this set of facial expressions as determined by accurate identification of expressions and high intra-participant agreement across two testing sessions, respectively.",2009,63,2698,195,43,82,127,205,220,219,244,276,273,284
e5a9fb3d49ef54f049509231e5883a14bef070f0,"A fast marching level set method is presented for monotonically advancing fronts, which leads to an extremely fast scheme for solving the Eikonal equation. Level set methods are numerical techniques for computing the position of propagating fronts. They rely on an initial value partial differential equation for a propagating level set function and use techniques borrowed from hyperbolic conservation laws. Topological changes, corner and cusp development, and accurate determination of geometric properties such as curvature and normal direction are naturally obtained in this setting. This paper describes a particular case of such methods for interfaces whose speed depends only on local position. The technique works by coupling work on entropy conditions for interface motion, the theory of viscosity solutions for Hamilton-Jacobi equations, and fast adaptive narrow band level set methods. The technique is applicable to a variety of problems, including shape-from-shading problems, lithographic development calculations in microchip manufacturing, and arrival time problems in control theory.",1996,27,3010,266,9,12,13,31,34,40,44,89,84,106
5f2f9cd5f3d0a2693a86b74cacb70e4a1c71ebc1,A contracted Gaussian basis set (6‐311G**) is developed by optimizing exponents and coefficients at the Mo/ller–Plesset (MP) second‐order level for the ground states of first‐row atoms. This has a triple split in the valence s and p shells together with a single set of uncontracted polarization functions on each atom. The basis is tested by computing structures and energies for some simple molecules at various levels of MP theory and comparing with experiment.,1980,7,10917,8,0,0,0,0,0,0,0,0,0,0
348cd9726be5e5740ab751c15fbad4b60d98246d,"Whereas much of organic chemistry has classically dealt with the preparation and study of the properties of individual molecules, an increasingly significant portion of the activity in chemical research involves understanding and utilizing the nature of the interactions between molecules. Two representative areas of this evolution are supramolecular chemistry and molecular recognition. The interactions between molecules are governed by intermolecular forces whose energetic and geometric properties are much less well understood than those of classical chemical bonds between atoms. Among the strongest of these interactions, however, are hydrogen bonds, whose directional properties are better understood on the local level (that is, for a single hydrogen bond) than many other types of non-bonded interactions. Nevertheless, the means by which to characterize, understand, and predict the consequences of many hydrogen bonds among molecules, and the resulting formation of molecular aggregates (on the microscopic scale) or crystals (on the macroscopic scale) has remained largely enigmatic. One of the most promising systematic approaches to resolving this enigma was initially developed by the late M. C. Etter, who applied graph theory to recognize, and then utilize, patterns of hydrogen bonding for the understanding and design of molecular crystals. In working with Etter's original ideas the power and potential utility of this approach on one hand, and on the other, the need to develop and extend the initial Etter formalism was generally recognized. It with that latter purpose that we originally undertook the present review.",1995,0,7110,156,1,14,17,24,36,66,68,132,147,171
a31dfa5eb97fced9494dfa1d88578da6827bf78d,"Spend your time even for only few minutes to read a book. Reading a book will never reduce and waste your time to be useless. Reading, for some people become a need that is to do every day such as spending time for eating. Now, what about you? Do you like to read a book? Now, we will show you a new book enPDFd fuzzy set theory and its applications that can be a new way to explore the knowledge. When reading this book, you can get one thing to always remember in every reading time, even step by step.",1993,0,3808,110,32,80,70,92,98,103,84,107,97,104
35f87841b45e820bc1e1bfc66ac85b6d313d6f05,"We propose a new multiphase level set framework for image segmentation using the Mumford and Shah model, for piecewise constant and piecewise smooth optimal approximations. The proposed method is also a generalization of an active contour model without edges based 2-phase segmentation, developed by the authors earlier in T. Chan and L. Vese (1999. In Scale-Space'99, M. Nilsen et al. (Eds.), LNCS, vol. 1682, pp. 141–151) and T. Chan and L. Vese (2001. IEEE-IP, 10(2):266–277). The multiphase level set formulation is new and of interest on its own: by construction, it automatically avoids the problems of vacuum and overlap; it needs only log n level set functions for n phases in the piecewise constant case; it can represent boundaries with complex topologies, including triple junctions; in the piecewise smooth case, only two level set functions formally suffice to represent any partition, based on The Four-Color Theorem. Finally, we validate the proposed models by numerical results for signal and image denoising and segmentation, implemented using the Osher and Sethian level set method.",2002,72,2581,279,2,16,56,77,110,121,131,159,157,177
e4c2d802cf9fe8de8f213727512e18bbfe3dc631,"Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which retains some of the attractive features of existing methods and overcomes some of their limitations. The authors' techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori assumption about the object's topology is made. A single instance of the authors' model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian (1988) to model propagating solid/liquid interfaces with curvature-dependent speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient field with constant speed or a speed that depends on the curvature. It is moved by solving a ""Hamilton-Jacobi"" type equation written for a function in which the interface is a particular level set. A speed term synthesized from the image is used to stop the interface in the vicinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. The authors present a variety of ways of computing the evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images. >",1995,94,3515,159,25,42,51,52,76,99,106,119,133,171
f991a2a0e09f442a2ad528bbcab8fc0992f7cf3b,"[1] The historical surface temperature data set HadCRUT provides a record of surface temperature trends and variability since 1850. A new version of this data set, HadCRUT3, has been produced, benefiting from recent improvements to the sea surface temperature data set which forms its marine component, and from improvements to the station records which provide the land data. A comprehensive set of uncertainty estimates has been derived to accompany the data: Estimates of measurement and sampling error, temperature bias effects, and the effect of limited observational coverage on large-scale averages have all been made. Since the mid twentieth century the uncertainties in global and hemispheric mean temperatures are small, and the temperature increase greatly exceeds its uncertainty. In earlier periods the uncertainties are larger, but the temperature increase over the twentieth century is still significantly larger than its uncertainty.",2006,55,2008,315,16,77,141,153,214,233,263,246,151,111
65c260fce4fb6fe44ac44562b75d38b0ac5c38f5,"Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods.",2009,60,1883,341,0,21,42,84,103,138,192,204,218,195
cf9f2a48528e50a5625eb8b8200d86a93e098b7a,"We describe the construction of a 10' latitude/longitude data set of mean monthly sur- face climate over global land areas, excluding Antarctica. The climatology includes 8 climate ele- ments —precipitation, wet-day frequency, temperature, diurnal temperature range, relative humid- ity, sunshine duration, ground frost frequency and windspeed—and was interpolated from a data set of station means for the period centred on 1961 to 1990. Precipitation was first defined in terms of the parameters of the Gamma distribution, enabling the calculation of monthly precipitation at any given return period. The data are compared to an earlier data set at 0.5o latitude/longitude resolution and show added value over most regions. The data will have many applications in applied climatology, biogeochemical modelling, hydrology and agricultural meteorology and are available through the International Water Management Institute World Water and Climate Atlas (http://www.iwmi.org) and the Climatic Research Unit (http://www.cru.uea.ac.uk).",2002,34,2127,359,1,8,21,46,69,89,109,153,139,176
0ac187c25dc630aca84751947f01879e479196e0,"The 6‐31G* and 6‐31G** basis sets previously introduced for first‐row atoms have been extended through the second‐row of the periodic table. Equilibrium geometries for one‐heavy‐atom hydrides calculated for the two‐basis sets and using Hartree–Fock wave functions are in good agreement both with each other and with the experimental data. HF/6‐31G* structures, obtained for two‐heavy‐atom hydrides and for a variety of hypervalent second‐row molecules, are also in excellent accord with experimental equilibrium geometries. No large deviations between calculated and experimental single bond lengths have been noted, in contrast to previous work on analogous first‐row compounds, where limiting Hartree–Fock distances were in error by up to a tenth of an angstrom. Equilibrium geometries calculated at the HF/6‐31G level are consistently in better agreement with the experimental data than are those previously obtained using the simple split‐valance 3‐21G basis set for both normal‐ and hypervalent compounds. Normal‐mode vibrational frequencies derived from 6‐31G* level calculations are consistently larger than the corresponding experimental values, typically by 10%–15%; they are of much more uniform quality than those obtained from the 3‐21G basis set. Hydrogenation energies calculated for normal‐ and hypervalent compounds are in moderate accord with experimental data, although in some instances large errors appear. Calculated energies relating to the stabilities of single and multiple bonds are in much better accord with the experimental energy differences.",1982,13,5402,6,0,8,14,26,42,30,47,41,33,46
9fb7b636edeaf344394fdf37481d7b83eec75358,Recently Viola et al. [2001] have introduced a rapid object detection. scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce a novel set of rotated Haar-like features. These novel features significantly enrich the simple features of Viola et al. and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%.,2002,9,3141,167,2,16,55,79,103,155,197,201,224,236
9c6f95c53af4a60acef42aa38529ab91dcce7351,"The relatively small diffuse function‐augmented basis set, 3‐21+G, is shown to describe anion geometries and proton affinities adequately. The diffuse sp orbital exponents are recommended for general use to augment larger basis sets.",1983,24,4512,1,0,3,12,20,20,16,31,31,39,44
63c659fc8a2a8238bb8952b00d1128450a7cce4b,"Light synchronizes mammalian circadian rhythms with environmental time by modulating retinal input to the circadian pacemaker—the suprachiasmatic nucleus (SCN) of the hypothalamus. Such photic entrainment requires neither rods nor cones, the only known retinal photoreceptors. Here, we show that retinal ganglion cells innervating the SCN are intrinsically photosensitive. Unlike other ganglion cells, they depolarized in response to light even when all synaptic input from rods and cones was blocked. The sensitivity, spectral tuning, and slow kinetics of this light response matched those of the photic entrainment mechanism, suggesting that these ganglion cells may be the primary photoreceptors for this system.",2002,52,2793,152,48,77,76,95,82,111,89,112,111,129
60c6bc0aafe922a8a1a1fb16e76ce0bab6cfdc98,"Descriptive set theory has been one of the main areas of research in set theory for almost a century. This text attempts to present a largely balanced approach, which combines many elements of the different traditions of the subject. It includes a wide variety of examples, exercises (over 400), and applications, in order to illustrate the general concepts and results of the theory. This text provides a first basic course in classical descriptive set theory and covers material with which mathematicians interested in the subject for its own sake or those that wish to use it in their field should be familiar. Over the years, researchers in diverse areas of mathematics, such as logic and set theory, analysis, topology, probability theory, etc., have brought to the subject of descriptive set theory their own intuitions, concepts, terminology and notation.",1987,0,2231,247,1,0,0,0,0,0,0,2,7,14
5ae073986408c9931bf6887fafb85e253866f7cc,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",2001,0,2010,237,8,28,38,55,62,62,86,86,116,97
b4e0ca86d00efc6c548939e3ed614bf64dd9d0ab,"Let A be a binary matrix of size m × n, let cT be a positive row vector of length n and let e be the column vector, all of whose m components are ones. The set-covering problem is to minimize cTx subject to Ax ≥ e and x binary. We compare the value of the objective function at a feasible solution found by a simple greedy heuristic to the true optimum. It turns out that the ratio between the two grows at most logarithmically in the largest column sum of A. When all the components of cT are the same, our result reduces to a theorem established previously by Johnson and Lovasz.",1979,4,2540,188,0,3,3,7,6,9,2,6,10,7
d243b5eb81a8501cc0477c47a4ce7d4feb524aee,"In this paper, we present a new variational formulation for geometric active contours that forces the level set function to be close to a signed distance function, and therefore completely eliminates the need of the costly re-initialization procedure. Our variational formulation consists of an internal energy term that penalizes the deviation of the level set function from a signed distance function, and an external energy term that drives the motion of the zero level set toward the desired image features, such as object boundaries. The resulting evolution of the level set function is the gradient flow that minimizes the overall energy functional. The proposed variational level set formulation has three main advantages over the traditional level set formulations. First, a significantly larger time step can be used for numerically solving the evolution partial differential equation, and therefore speeds up the curve evolution. Second, the level set function can be initialized with general functions that are more efficient to construct and easier to use in practice than the widely used signed distance function. Third, the level set evolution in our formulation can be easily implemented by simple finite difference scheme and is computationally more efficient. The proposed algorithm has been applied to both simulated and real images with promising results.",2005,28,2041,191,2,12,51,92,141,172,176,210,221,181
9ceedeb5e2ebc39a08c70e2d42d69bca0e064bc1,"A Monte Carlo evaluation of 30 procedures for determining the number of clusters was conducted on artificial data sets which contained either 2, 3, 4, or 5 distinct nonoverlapping clusters. To provide a variety of clustering solutions, the data sets were analyzed by four hierarchical clustering methods. External criterion measures indicated excellent recovery of the true cluster structure by the methods at the correct hierarchy level. Thus, the clustering present in the data was quite strong. The simulation results for the stopping rules revealed a wide range in their ability to determine the correct number of clusters in the data. Several procedures worked fairly well, whereas others performed rather poorly. Thus, the latter group of rules would appear to have little validity, particularly for data sets containing distinct clusters. Applied researchers are urged to select one or more of the better criteria. However, users are cautioned that the performance of some of the criteria may be data dependent.",1985,56,2759,108,4,2,14,10,10,16,10,18,27,37
afdf0c581ad8e194eb6a58a0f7582e49e77bf85d,"We have expanded the reference set of proteins used in SELCON3 by including 11 additional proteins (selected from the reference sets of Yang and co-workers and Keiderling and co-workers). Depending on the wavelength range and whether or not denatured proteins are included in the reference set, five reference sets were constructed with the number of reference proteins varying from 29 to 48. The performance of three popular methods for estimating protein secondary structure fractions from CD spectra (implemented in software packages CONTIN, SELCON3, and CDSSTR) and a variant of CONTIN, CONTIN/LL, that incorporates the variable selection method in the locally linearized model in CONTIN, were examined using the five reference sets described here, and a 22-protein reference set. Secondary structure assignments from DSSP were used in the analysis. The performances of all three methods were comparable, in spite of the differences in the algorithms used in the three software packages. While CDSSTR performed the best with a smaller reference set and larger wavelength range, and CONTIN/LL performed the best with a larger reference set and smaller wavelength range, the performances for individual secondary structures were mixed. Analyzing protein CD spectra using all three methods should improve the reliability of predicted secondary structural fractions. The three programs are provided in CDPro software package and have been modified for easier use with the different reference sets described in this paper. CDPro software is available at the website: http://lamar.colostate.edu/ approximately sreeram/CDPro.",2000,48,2541,68,0,8,34,52,66,85,102,106,118,141
5cee6b7fb97778fb9343a308f5b6982ef9c97135,"Abstract In this paper, the authors study the theory of soft sets initiated by Molodtsov. The authors define equality of two soft sets, subset and super set of a soft set, complement of a soft set, null soft set, and absolute soft set with examples. Soft binary operations like AND, OR and also the operations of union, intersection are defined. De Morgan's laws and a number of results are verified in soft set theory.",2003,10,1944,156,0,1,2,0,3,12,20,43,90,123
166641a1c482e2aadbafed7d5f7e637924442237,This paper describes an extension to the set of Basic Linear Algebra Subprograms. The extensions are targeted at matrix-vector operations that should provide for efficient and portable implementations of algorithms for high-performance computers,1990,53,1933,177,24,27,49,53,58,56,80,86,59,48
b72b370e9494b7ad715969292ff1476df1b1e2fb,"The study of sets is important and thus popular in the business and economic world for three major reasons: Basic understanding of concepts in sets and set algebra provides a form of logical language through which business specialists can communicate important concepts and ideas. Set algebra is used in solving counting problems of a logical nature. The study of set algebra provides a solid background to understanding of probability and statistics, which are important business decision-making tools.",2007,86,1685,131,99,90,88,74,89,100,110,105,91,108
475bbf493d8246031a5152c8005a5c567231307c,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",2007,54,2188,9,3,27,75,83,133,145,189,190,197,190
6e50b1cb4131104e40dc18b4de5398e7bb892e03,"In the context of structural optimization we propose a new numerical method based on a combination of the classical shape derivative and of the level-set method for front propagation. We implement this method in two and three space dimensions for a model of linear or nonlinear elasticity. We consider various objective functions with weight and perimeter constraints. The shape derivative is computed by an adjoint method. The cost of our numerical algorithm is moderate since the shape is captured on a fixed Eulerian mesh. Although this method is not specifically designed for topology optimization, it can easily handle topology changes. However, the resulting optimal shape is strongly dependent on the initial guess.",2004,40,1825,141,11,26,45,48,54,58,75,76,85,94
7d23f522cf17ac3de97d6f16fa8f616d716fb383,"This paper presents a new approach to structural topology optimization. We represent the structural boundary by a level set model that is embedded in a scalar function of a higher dimension. Such level set models are flexible in handling complex topological changes and are concise in describing the boundary shape of the structure. Furthermore, a well-founded mathematical procedure leads to a numerical algorithm that describes a structural optimization as a sequence of motions of the implicit boundaries converging to an optimum solution and satisfying specified constraints. The result is a 3D topology optimization technique that demonstrates outstanding flexibility of handling topological changes, fidelity of boundary representation and degree of automation. We have implemented the algorithm with the use of several robust and efficient numerical techniques of level set methods. The benefit and the advantages of the proposed method are illustrated with several 2D examples that are widely used in the recent literature of topology optimization, especially in the homogenization based methods.",2003,28,1957,64,2,18,29,33,36,60,54,72,85,75
0eb6c9584cb52d91ef202e9bfa0e92676eb0ecf4,"The air–sea fluxes of momentum, heat, freshwater and their components have been computed globally from 1948 at frequencies ranging from 6-hourly to monthly. All fluxes are computed over the 23 years from 1984 to 2006, but radiation prior to 1984 and precipitation before 1979 are given only as climatological mean annual cycles. The input data are based on NCEP reanalysis only for the near surface vector wind, temperature, specific humidity and density, and on a variety of satellite based radiation, sea surface temperature, sea-ice concentration and precipitation products. Some of these data are adjusted to agree in the mean with a variety of more reliable satellite and in situ measurements, that themselves are either too short a duration, or too regional in coverage. The major adjustments are a general increase in wind speed, decrease in humidity and reduction in tropical solar radiation. The climatological global mean air–sea heat and freshwater fluxes (1984–2006) then become 2 W/m2 and −0.1 mg/m2 per second, respectively, down from 30 W/m2 and 3.4 mg/m2 per second for the unaltered data. However, decadal means vary from 7.3 W/m2 (1977–1986) to −0.3 W/m2 (1997–2006). The spatial distributions of climatological fluxes display all the expected features. A comparison of zonally averaged wind stress components across ocean sub-basins reveals large differences between available products due both to winds and to the stress calculation. Regional comparisons of the heat and freshwater fluxes reveal an alarming range among alternatives; typically 40 W/m2 and 10 mg/m2 per second, respectively. The implied ocean heat transports are within the uncertainty of estimates from ocean observations in both the Atlantic and Indo-Pacific basins. They show about 2.4 PW of tropical heating, of which 80% is transported to the north, mostly in the Atlantic. There is similar good agreement in freshwater transport at many latitudes in both basins, but neither in the South Atlantic, nor at 35°N.",2009,134,1286,94,14,38,47,73,121,123,124,124,121,113
901b357c7d4ab59298bd0872554f3f34091c40ff,"I argue that research on organizational configurations has been limited by a mismatch between theory and methods and introduce set-theoretic methods as a viable alternative for overcoming this mismatch. I demonstrate the value of such methods for studying organizational configurations and discuss their applicability for examining equifinality and limited diversity among configurations, as well as their relevance to other research fields such as complementarities theory, complexity theory, and the resource-based view",2007,142,1344,140,13,22,29,40,49,60,85,72,78,155
f0a8e0bf935ae8cb55c09841483dd671d46add28,"Abstract A generalized model of rough sets called variable precision model (VP-model), aimed at modelling classification problems involving uncertain or imprecise information, is presented. The generalized model inherits all basic mathematical properties of the original model introduced by Pawlak. The main concepts are introduced formally and illustrated with simple examples. The application of the model to analysis of knowledge representation systems is also discussed.",1993,53,1892,154,7,10,12,23,22,30,23,41,37,44
71cd76a801da510b3714043629e3bc5e13a3e1ce,"Summary. To systematically identify and analyze the 15 HA and 9 NA subtypes of influenza A virus, we need reliable, simple methods that not only characterize partial sequences but analyze the entire influenza A genome. We designed primers based on the fact that the 15 and 21 terminal segment specific nucleotides of the genomic viral RNA are conserved between all influenza A viruses and unique for each segment. The primers designed for each segment contain influenza virus specific nucleotides at their 3′-end and non-influenza virus nucleotides at the 5′-end. With this set of primers, we were able to amplify all eight segments of N1, N2, N4, N5, and N8 subtypes. For N3, N6, N7, and N9 subtypes, the segment specific sequences of the neuraminidase genes are different. Therefore, we optimized the primer design to allow the amplification of those neuraminidase genes as well. The resultant primer set is suitable for all influenza A viruses to generate full-length cDNAs, to subtype viruses, to sequence their DNA, and to construct expression plasmids for reverse genetics systems.",2001,38,1758,77,2,3,7,14,24,32,52,68,104,145
4c1214f1356353b32291d1b25c33f4669a6a0a7b,"The properties of positively invariant sets are involved in many different problems in control theory, such as constrained control, robustness analysis, synthesis and optimization. In this paper we provide an overview of the literature concerning positively invariant sets and their application to the analysis and synthesis of control systems.",1999,251,1836,112,2,17,29,47,53,63,84,85,90,92
58ea5f275157eb4fe2e67e1e062b9ec5848f14ed,"Though it incorporates much new material, this new edition preserves the general character of the book in providing a collection of solutions of the equations of diffusion and describing how these solutions may be obtained.",1956,22,18631,775,0,0,0,0,0,0,0,1,0,0
ab7b5917515c460b90451e67852171a531671ab8,"We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.",1993,30,4702,620,6,16,18,32,36,33,42,65,74,105
9d0c4389436d3ab8ed329dba8c58e8ac6737fd3b,"Many models for the spread of infectious diseases in populations have been analyzed mathematically and applied to specific diseases. Threshold theorems involving the basic reproduction number $R_{0}$, the contact number $\sigma$, and the replacement number $R$ are reviewed for the classic SIR epidemic and endemic models. Similar results with new expressions for $R_{0}$ are obtained for MSEIR and SEIR endemic models with either continuous age or age groups. Values of $R_{0}$ and $\sigma$ are estimated for various diseases including measles in Niger and pertussis in the United States. Previous models with age structure, heterogeneity, and spatial structure are surveyed.",2000,226,4873,259,0,10,20,43,55,80,121,124,129,154
9036f1e03c4da901805c56e169efa57869194885,,1988,1,4642,288,6,24,42,60,88,108,117,163,163,187
6ea24fcbbe780ce565251757b0a8cec5c753b90d,List of figures List of tables Preface 1. Introduction: psychology and anthropology I Part I. Theory in Practice: 2. Missionaries and cannibals (indoors) 3. Life after school 4. Psychology and anthropology II Part II. Practice in Theory: 5. Inside the supermarket (outdoors) and from the veranda 6. Out of trees of knowledge into fields for activity 7. Through the supermarket 8. Outdoors: a social anthropology of cognition in practice Notes References.,1988,0,3900,296,2,7,27,21,16,49,53,64,72,60
d162573ec41926a8bad701fab1eec6036a599030,"The Mathematics of Computerized Tomography covers the relevant mathematical theory of the Radon transform and related transforms and also studies more practical questions such as stability, sampling, resolution, and accuracy. Quite a bit of attention is given to the derivation, analysis, and practical examination of reconstruction algorithm, for both standard problems and problems with incomplete data.",1986,0,3326,278,3,1,14,17,24,29,38,38,24,39
7ba5078a4f81c74deb4c5e10fcb4aceb6eb635ef,"Graduate Texts in Mathematics bridge the gap between passive study and creative understanding, offering graduate-level introductions to advanced topics in mathematics. The volumes are carefully written as teaching aids and highlight characteristic features of the theory. Although these books are frequently used as textbooks in graduate courses, they are also suitable for individual study.    Series Editors:  Sheldon Axler, San Francisco State University  Kenneth Ribet, University of California, Berkeley  Advisory Board:  Alejandro Adem, University of British Columbia  David Eisenbud, University of California, Berkeley & MSRI  Irene M. Gamba, The University of Texas at Austin  J.F.Jardine, University of Western Ontario  Jeffrey C. Lagarias, University of Michigan  Ken Ono, Emory University  Jeremy Quastel, University of Toronto  Fadil Santosa, University of Minnesota  Barry Simon, California Institute of Technology",1982,0,8759,50,8,8,6,9,16,14,21,14,20,18
2331485d13ac3c7ad8b65c3d847ec4eefa293b59,,1957,0,3558,315,1,2,1,5,3,3,3,5,6,5
d3d720baa7080594c2fad06bb2ac90cc46666735,"The goals of this chapter are (1) to outline and substantiate a broad conceptualization of what it means to think mathematically, (2) to summarize the literature relevant to understanding mathematical thinking and problem solving, and (3) to point to new directions in research, development, and assessment consonant with an emerging understanding of mathematical thinking and the goals for instruction outlined here. The use of the phrase “learning to think mathematically” in this chapter’s title is deliberately broad. Although the original charter for this chapter was to review the literature on problem solving and metacognition, the literature itself is somewhat ill defined and poorly grounded. As the literature summary will make clear, problem solving has been used with multiple meanings that range from “working rote exercises” to “doing mathematics as a professional”; metacognition has multiple and almost disjoint meanings (from knowledge about one’s thought processes to self-regulation during problem solving) that make it difficult to use as a concept. This chapter outlines the various meanings that have been ascribed to these terms and discusses their role in mathematical thinking. The discussion will not have the character of a classic literature review, which is typically encyclopedic in its references and telegraphic in its discussions of individual papers or results. It will, instead, be selective and illustrative, with main points illustrated by extended discussions of pertinent examples. Problem solving has, as predicted in the 1980 Yearbook of the National Council of Teachers of Mathematics (Krulik, 1980, p. xiv), been the theme of the 1980s. The decade began with NCTM’s widely heralded statement, in its Agenda for Action, that “problem solving must be the focus of school mathematics” (NCTM, 1980, p. 1). It concluded with the publication of Everybody Counts (National Research Council, 1989) and the Curriculum and Evaluation Standards for School Mathematics (NCTM, 1989), both of which emphasize problem solving. One might infer, then, that there is general acceptance of the idea that the primary goal of mathematics instruction should be to have students become competent problem solvers. Yet, given the multiple interpretations of the term, the goal is hardly clear. Equally unclear is the role that problem solving, once adequately characterized, should play in the larger context of school mathematics. What are the goals for mathematics instruction, and how does problem solving fit within those goals? Such questions are complex. Goals for mathematics instruction depend on one’s conceptualization of what mathematics is, and what it means to understand mathematics. Such conceptualizations vary widely. At one end of the spectrum, mathematical knowledge is seen as a body of facts and procedures dealing with quantities, magnitudes, and forms, and the relationships among them; knowing mathematics is seen as having mastered these facts and procedures. At the other end of the spectrum, mathematics is conceptualized as the “science of patterns,” an (almost) empirical discipline closely akin to the sciences in its emphasis on pattern-seeking on the basis of empirical evidence. The author’s view is that the former perspective trivializes mathematics; that a curriculum based on mastering a corpus of mathematical facts and procedures is severely impoverished—in much the same way that an English curriculum would be considered impoverished if it focused largely, if not exclusively, on issues of grammar. The author characterizes the mathematical enterprise as follows:",2009,239,2890,266,123,144,163,155,189,196,226,163,163,178
6b7f08a8a3950a7df36c3a7cfe56a9a35d6daa4f,"Abstract : Even today, many complex and important skills, such as those required for language use and social interaction, are learned informally through apprenticeshiplike methods -- i.e., methods involving not didactic teaching, but observation, coaching, and successive approximation while carrying out a variety of tasks and activities. The differences between formal schooling and apprenticeship methods are many, but for our purposes, one is most important. Perhaps as a by-product of the specialization of learning in schools, skills and knowledge taught in schools have become abstracted from their uses in the world. In apprenticeship learning, on the other hand, target skills are not only continually in use by skilled practitioners, but are instrumental to the accomplishment of meaningful tasks. Said differently, apprenticeship embeds the learning of skills and knowledge in the social and functional context of their use. This difference is not academic, but has serious implications for the nature of the knowledge that students acquire. This paper attempts to elucidate some of those implications through a proposal for the retooling of apprenticeship methods for the teaching and learning of cognitive skills. Specifically, we propose the development of a new cognitive apprenticeship to teach students the thinking and problem-solving skills involved in school subjects such as reading, writing and mathematics.",1988,24,4582,154,10,23,28,29,66,81,73,87,103,95
f2e713dd0a1ee11892a90e0fb448dd5981e63550,,2000,7,7982,20,159,178,250,232,270,267,306,357,411,430
ebc5da28ca50a224960fb97a5e617c5fdcb4e92a,"This edited collection describes how the Autonomous Learning Behaviours (ALB) model, formulated by Fennema and Peterson, specifically relates to gender differences in mathematics education, learning and performance. The book provides a background to the debate on gender differences; considers the interactions between internal beliefs and external influences, as well as their effects on learning math; and provides a summary of the latest research relevant to the ALB model. Gender differences in learning mathematics is examined from a variety of perspectives, strengthened by longitudinal studies and a cross-cultural American and Australian perspective..",1990,0,211,3,0,0,4,4,7,14,29,8,8,4
7f3b042c8337fe9195ed6ca0cb017b76bbf1ff7c,"868 NOTICES OF THE AMS VOLUME 47, NUMBER 8 In April 2000 the National Council of Teachers of Mathematics (NCTM) released Principles and Standards for School Mathematics—the culmination of a multifaceted, three-year effort to update NCTM’s earlier standards documents and to set forth goals and recommendations for mathematics education in the prekindergarten-through-grade-twelve years. As the chair of the Writing Group, I had the privilege to interact with all aspects of the development and review of this document and with the committed groups of people, including the members of the Writing Group, who contributed immeasurably to this process. This article provides some background about NCTM and the standards, the process of development, efforts to gather input and feedback, and ways in which feedback from the mathematics community influenced the document. The article concludes with a section that provides some suggestions for mathematicians who are interested in using Principles and Standards.",2000,17,2165,296,10,25,47,49,66,78,79,118,115,118
a3b7e80260891dcd3844b1835df8dee3a1cd67c7,"L'A. propose quelques observations concernant l'ouvrage de Stanislas Dehaene The number sense. How the mind creates mathematics (1997) qui explore tous les aspects de la relation entre les hommes et les nombres : la numerosite chez les autres animaux, la numerosite et le calcul simple chez les bebes, l'histoire de l'expression du nombre dans le langage, l'histoire de la notation du nombre, le circuit neuronal necessaire pour faire de l'arithmetique et du calcul, la localisation dans le cerveau, l'ordre mathematique de l'univers, etc ... L'A. examine ici en particulier les questions portant sur la relation entre les nombres et le langage dans une perspective cognitive, puis explique ce que Dehaene entend par le sens du nombre en caracterisant les mathematiques comme une formalisation progressive de nos intuitions sur les ensembles, le nombre, l'espace, le temps et la logique",1998,0,2213,227,4,14,21,28,32,48,41,66,55,68
c9c852bc0e8774336734ff339c1592b2cf2d849f,,1982,0,3802,11,14,14,16,26,18,19,20,30,26,20
843632163541323571f14e92064bfb19a05f8f8d,,1992,0,3467,20,8,14,22,34,57,62,68,73,69,80
86b05bc7e953e683fa839ad01d6100a8f99558df,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",1991,0,2501,113,5,15,20,26,27,27,24,43,37,42
932ebbd6ae4b1c90dd85d3b38a59bfa610ea465b,"Foreword by Dennis Sparks Acknowledgments About the Authors Introduction What Has Happened Since the First and Second Editions The Enduring Challenges of Professional Development Carrying on Susan Loucks-Horsley's Work Purpose of the Book Changes in the Third Edition The Audience for This Book Organization of the Book How to Use This Book Values Shared by the Authors 1. A Framework for Designing Professional Development Inputs Into the Design Process The Design and Implementation Process 2. Knowledge and Beliefs Supporting Effective Professional Development Learners and Learning Teachers and Teaching The Nature of Science and Mathematics Adult Learning and Professional Development The Change Process 3. Context Factors Influencing Professional Development Students and Their Learning Needs Teachers and Their Learning Needs Curriculum, Instruction, Assessment Practices, and the Learning Environment Organizational Culture and Professional Learning Communities Leadership National, State, and Local Policies Available Resources Families and Communities Resources for Investigating Context 4. Critical Issues to Consider in Designing Professional Development Building Capacity for Sustainability Making Time for Professional Development Developing Leadership Ensuring Equity Building a Professional Learning Culture Garnering Public Support Scaling Up 5. Strategies for Professional Learning Selecting Strategies for a Professional Development Structures A Repertoire of Stratgies for Professional Learning 6. The Design Framework in Action Tapping the Knowledge Bases, Framing Beliefs: ""We Stood on the Shoulders of Giants"" Knowledge and Beliefs About the Nature of Learning and Teaching Mathematics and Science Equity Matters: ""All Humans Are Educable"" Knowledge and Beliefs About Teachers Knowledge of Effective Professional Development Knowledge of the Change Process Reflect and Revise: Experience as a Source of Knowledge Making Compromises Context The Professional Development Design Process Design Framework in Action: Cases References Index",1997,0,2108,142,0,8,14,32,41,48,61,64,91,107
73d73133142b945c362ff3d209bba7058af54622,Part 1 Hypergeometric Functions and Elliptic Integrals: Some Basic Topics In Analytical Dynamics The Problem of Two Bodies Two-Body Orbits and the Initial-Value Problem Solving Kepler's Equation Two-Body Orbital Boundary Value Problem Solving Lambert's Problem Appendices. Part 2 Non-Keplerian Motion: Patched-Conic Orbits and Perturbation Methods Variation of Parameters Two-Body Orbital Transfer Numerical Integration of Differential Equations The Celestial Position Fix Space Navigation Appendices.,1987,0,2018,203,3,4,11,12,17,13,13,18,18,13
6555e4f1ca5bcb835cc77b45b7cdb930fb4f5bf0,This book will be released simultaneously with Release 2.0 of Mathematica and will cover all the new features of Release 2.0. This new edition maintains the format of the original book and is the single most important user guide and reference for Mathematica--all users of Mathematica will need this edition. Includes 16 pages of full-color graphics.,1991,0,2566,65,53,108,158,177,205,226,232,186,145,114
a9386eb6808b41238381c708f2642bcb7dc34b29,"This book is about mathematical ideas, about what mathematics means-and why. Abstract ideas, for the most part, arise via conceptual metaphor-metaphorical ideas projecting from the way we function in the everyday physical world. Where Mathematics Comes From argues that conceptual metaphor plays a central role in mathematical ideas within the cognitive unconscious-from arithmetic and algebra to sets and logic to infinity in all of its forms.",2002,40,1919,107,19,40,48,55,72,74,102,91,120,102
fcd0ed6947c8aafe2ec5d971323176b2001a8af7,,1981,0,2409,83,1,5,14,42,71,84,104,127,127,116
fce6d3a85f28288f5d55760ef7575ffda7732e78,,1995,0,2069,85,2,4,4,5,8,6,7,10,18,21
a6891af0e04724965532095dec1b8198b943e31e,"PART A: ORDINARY DIFFERENTIAL EQUATIONS (ODE'S). Chapter 1. First-Order ODE's. Chapter 2. Second Order Linear ODE's. Chapter 3. Higher Order Linear ODE's. Chapter 4. Systems of ODE's Phase Plane, Qualitative Methods. Chapter 5. Series Solutions of ODE's Special Functions. Chapter 6. Laplace Transforms. PART B: LINEAR ALGEBRA, VECTOR CALCULUS. Chapter 7. Linear Algebra: Matrices, Vectors, Determinants: Linear Systems. Chapter 8. Linear Algebra: Matrix Eigenvalue Problems. Chapter 9. Vector Differential Calculus: Grad, Div, Curl. Chapter 10. Vector Integral Calculus: Integral Theorems. PART C: FOURIER ANALYSIS, PARTIAL DIFFERENTIAL EQUATIONS. Chapter 11. Fourier Series, Integrals, and Transforms. Chapter 12. Partial Differential Equations (PDE's). Chapter 13. Complex Numbers and Functions. Chapter 14. Complex Integration. Chapter 15. Power Series, Taylor Series. Chapter 16. Laurent Series: Residue Integration. Chapter 17. Conformal Mapping. Chapter 18. Complex Analysis and Potential Theory. PART E: NUMERICAL ANALYSIS SOFTWARE. Chapter 19. Numerics in General. Chapter 20. Numerical Linear Algebra. Chapter 21. Numerics for ODE's and PDE's. PART F: OPTIMIZATION, GRAPHS. Chapter 22. Unconstrained Optimization: Linear Programming. Chapter 23. Graphs, Combinatorial Optimization. PART G: PROBABILITY STATISTICS. Chapter 24. Data Analysis: Probability Theory. Chapter 25. Mathematical Statistics. Appendix 1: References. Appendix 2: Answers to Odd-Numbered Problems. Appendix 3: Auxiliary Material. Appendix 4: Additional Proofs. Appendix 5: Tables. Index.",1974,0,2416,145,5,1,3,3,2,6,2,3,4,4
eb6cfa0c51dce24ca6eb40d5072f4488f0931f2b,"This paper sets forth a way of interpreting mathematics classrooms that aims to account for how students develop mathematical beliefs and values and, consequently, how they become intellectually autonomous in mathematics. To do so, we advance the notion of sociomathematical norms, that is, normative aspects of mathematical discussions that are specific to students' mathematical activity. The explication of sociomathematical norms extends our previous work on general classroom social norms that sustain inquiry-based discussion and argumentation. Episodes from a second-grade classroom where mathematics instruction generally followed an inquiry tradition are used to clarify the processes by which sociomathematical norms are interactively constituted and to illustrate how these norms regulate mathematical argumentation and influence learning opportunities for both the students and the teacher. In doing so, we both clarify how students develop a mathematical disposition and account for students' development of increasing intellectual autonomy in mathematics. In the process, the teacher's role as a representative of the mathematical community is elaborated. For the past several years, we have been engaged in a research and development project at the elementary school level that has both pragmatic and theoretical goals. On one hand, we wish to support teachers as they establish classroom environments that facilitate students' mathematical conceptual development. On the other hand, we wish to investigate children's mathematical learning in the classroom. The latter involves developing perspectives that are useful for interpreting and attempting to make sense of the complexity of classroom life. The purpose of this paper is to set forth a way of interpreting classroom life that aims to account for how students develop specific mathematical beliefs and values and, consequently, how they become intellectually autonomous in mathematics, that is, how they come to develop a mathematical disposition (National Council of Teachers of Mathematics, 1991). To that end, we focus on classroom norms that we call sociomathematical norms. These norms are distinct from general classroom social norms in that they are specific to the mathematical aspects of students' activity. As a means of introducing and elaborating the theoretical discussion in this paper, we present episodes from a classroom that we have studied extensively. The episodes have been",1996,21,1617,112,1,8,7,12,22,28,46,33,50,44
2ed2cd231aad7ea8a2b69a60d6df7539d6ee107f,"Vol. 72: The Syntax and Semantics of lnfimtary Languages. Edited by J. Barwtse. IV, 268 pages. 1968. DM 18,I $ 5.00 Vol. 73: P. E. Conner, Lectures on the Action of a Finite Group. IV, 123 pages. 1968. DM 10,1 $ 2.80 Vol. 74:A Frohlich, Formal Groups. IV, 140pages. 1968. DM12, -I $3.30 Vol. 75: G. Lumer, Algebras de fonctions et espaces de Hardy. VI, 80 pages. 1968. DM 8,I $ 2. 20 Vol. 76: R. G. Swan, Algebraic K-Theory. IV, 262 pages. 1968. DM18,I$ 5.00",2001,497,1653,59,73,89,93,103,106,99,152,152,126,57
e0fa9cec8a16f31f481c9c02d869b2311e079fc0,"Results of 151 studies were integrated by meta-analysis to scrutinize the construct mathematics anxiety. Mathematics anxiety is related to poor performance on mathematics achievement tests. It relates inversely to positive attitudes toward mathematics and is bound directly to avoidance of the subject. Variables that exhibit differential mathematics anxiety levels include ability, school grade level, and undergraduate fields of study, with preservice arithmetic teachers especially prone to mathematics anxiety. Females display higher levels than males. However, mathematics anxiety appears more strongly linked with poor performance and avoidance of mathematics in precollege males than females. A variety of treatments are effective in reducing mathematics anxiety. Improved mathematics performance consistently accompanies valid treatment.",1990,23,1454,277,1,3,4,7,9,3,10,7,10,6
f5cf6daa6eeb53008e3004a7ce70e0f015017a63,1 Introduction.- 2 Limit Process Expansions Applied to Ordinary Differential Equations.- 3 Multiple-Variable Expansion Procedures.- 4 Applications to Partial Differential Equations.- 5 Examples from Fluid Mechanics.- Author Index.,1969,0,2302,84,9,25,40,38,45,28,56,38,42,30
bbbff45c27dccd114818ef334075db0a34b0e4fa,"Reviewers have consistently concluded that males perform better on mathematics tests than females do. To make a refined assessment of the magnitude of gender differences in mathematics performance, we performed a meta-analysis of 100 studies. They yielded 254 independent effect sizes, representing the testing of 3,175,188 Ss. Averaged over all effect sizes based on samples of the general population, d was -0.05, indicating that females outperformed males by only a negligible amount. For computation, d was -0.14 (the negative value indicating superior performance by females). For understanding of mathematical concepts, d was -0.03; for complex problem solving, d was 0.08. An examination of age trends indicated that girls showed a slight superiority in computation in elementary school and middle school. There were no gender differences in problem solving in elementary or middle school; differences favoring men emerged in high school (d = 0.29) and in college (d = 0.32). Gender differences were smallest and actually favored females in samples of the general population, grew larger with increasingly selective samples, and were largest for highly selected samples and samples of highly precocious persons. The magnitude of the gender difference has declined over the years; for studies published in 1973 or earlier d was 0.31, whereas it was 0.14 for studies published in 1974 or later. We conclude that gender differences in mathematics performance are small. Nonetheless, the lower performance of women in problem solving that is evident in high school requires attention.",1990,104,1662,79,5,10,17,27,32,49,75,42,37,22
f008393f240508c1686a468b2c67371a7cdcb354,,2009,0,948,91,8,35,43,56,81,101,105,96,74,112
da1b7c3ece9e017b83cb7ab213673b2b89fd1134,,1989,0,1478,223,3,15,21,54,68,85,103,107,127,87
2c888d9d033439a2d0a414efe4228e9177618930,,1969,0,2361,5,3,3,4,7,8,6,9,12,11,7
813c3c045849f82cac2db2f26cee0ca306349f28,"Children's mathematical skills were considered in relation to executive functions. Using multiple measures-including the Wisconsin Card Sorting Task (WCST), dual-task performance, Stroop task, and counting span-it was found that mathematical ability was significantly correlated with all measures of executive functioning, with the exception of dual-task performance. Furthermore, regression analyses revealed that each executive function measure predicted unique variance in mathematics ability. These results are discussed in terms of a central executive with diverse functions (Shallice & Burgess, 1996) and with recent evidence from Miyake, et al. (2000) showing the unity and diversity among executive functions. It is proposed that the particular difficulties for children of lower mathematical ability are lack of inhibition and poor working memory, which result in problems with switching and evaluation of new strategies for dealing with a particular task. The practical and theoretical implications of these results are discussed, along with suggestions for task changes and longitudinal studies that would clarify theoretical and developmental issues related to executive functioning.",2001,79,1318,73,0,2,5,18,38,25,30,34,54,60
ac84c7e453c6848f5b873492e56f74d11bde71aa,"Introduction to applied mathematics , Introduction to applied mathematics , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1988,0,1739,76,18,29,33,39,51,55,59,50,53,72
deac6a43706ea11bdd9fb07a73b54a08f0c114fb,"Teachers and teacher educators interested in synthesizing their current practice with new mathematics standards will welcome this highly useful volume. Presented are cases of mathematics instruction drawn from research of nearly 500 classroom lessons. Readers will gain insight about how to foster a challenging, cognitively rich, and exciting classroom climate that propels students toward a richer understanding of mathematics.",2009,0,713,82,39,42,46,57,62,50,74,59,57,52
f65040c3aa910788931c27c73006c5f3bb1e7e85,,2008,10,1100,87,44,48,62,72,80,103,88,77,98,55
34fb621cf73b7bad25d77ff1229467a34f736474,"Children's number competencies over 6 time points, from the beginning of kindergarten to the middle of 1st grade, were examined in relation to their mathematics achievement over 5 later time points, from the end of 1st grade to the end of 3rd grade. The relation between early number competence and mathematics achievement was strong and significant throughout the study period. A sequential process growth curve model showed that kindergarten number competence predicted rate of growth in mathematics achievement between 1st and 3rd grades as well as achievement level through 3rd grade. Further, rate of growth in early number competence predicted mathematics performance level in 3rd grade. Although low-income children performed more poorly than their middle-income counterparts in mathematics achievement and progressed at a slower rate, their performance and growth were mediated through relatively weak kindergarten number competence. Similarly, the better performance and faster growth of children who entered kindergarten at an older age were explained by kindergarten number competence. The findings show the importance of early number competence for setting children's learning trajectories in elementary school mathematics.",2009,70,764,38,3,16,21,32,54,72,66,77,73,94
69727a18c999db42c1e0062dd75870b4751ecc90,,2002,0,1397,4,56,59,58,63,81,92,111,128,120,121
b9b73e52ce88f609ffe0a66760c92b8cea21715b,"Constructivist theory has been prominent in recent research on mathematics learning and has provided a basis for recent mathematics education reform efforts. Although constructivism has the potential to inform changes in mathematics teaching, it offers no particular vision of how mathematics should be taught; models of teaching based on constructivism are needed. Data are presented from a whole-class, constructivist teaching experiment in which problems of teaching practice required the teacher/researcher to explore the pedagogical implications of his theoretical (constructivist) perspectives. The analysis of the data led to the development of a model of teacher decision making with respect to mathematical tasks. Central to this model is the creative tension between the teacher's goals with regard to student learning and his responsibility to be sensitive and responsive to the mathematical thinking of the students.",1995,74,1276,163,8,14,8,13,18,18,22,41,25,39
c0f83e632def1b85df9e4f316b673d9eac05b3f1,"ion is not inevitable. In fact, shopkeepers have never abstracted the structures of modules which regulate their financial exchanges because they haven’t the motivation to do so. Schematization and formulation The schematization and the formulation of the structure follows the process of identification and of updating. Diénès does not plan any general situations specific to this stage (would something which is well conceived of spell itself out clearly?) but the representation by a graph is often envisaged as a simplified but natural and direct expression of the thought of a child. Representing objects by points and operators by arrows is learned by the use of imitation, like a language. Symbolization Symbolization is the transcription in a new language of properties represented in the preceding stage.",1997,50,1182,186,1,2,5,9,11,15,20,30,53,48
c62e9c1114644b601296d860dd643ff86473071b,"Studies of teachers’ use of mathematics curriculum materials are particularly timely given the current availability of reform-inspired curriculum materials and the increasingly widespread practice of mandating the use of a single curriculum to regulate mathematics teaching. A review of the research on mathematics curriculum use over the last 25 years reveals significant variation in findings and in theoretical foundations. The aim of this review is to examine the ways that central constructs of this body of research—such as curriculum use, teaching, and curriculum materials—are conceptualized and to consider the impact of various conceptualizations on knowledge in the field. Drawing on the literature, the author offers a framework for characterizing and studying teachers’ interactions with curriculum materials.",2005,119,998,143,2,9,21,37,52,49,56,50,64,78
06bdbd4f011e9497fddfc47654116feab28b63bd,"Between 5% and 8% of school-age children have some form of memory or cognitive deficit that interferes with their ability to learn concepts or procedures in one or more mathematical domains. A review of the arithmetical competencies of these children is provided, along with discussion of underlying memory and cognitive deficits and potential neural correlates. The deficits are discussed in terms of three subtypes of mathematics learning disability and in terms of a more general framework for linking research in mathematical cognition to research in learning disabilities.",2004,97,1067,158,5,27,28,49,42,51,68,57,83,95
feeef8d91cec3a9ffe5cf135a36a7f8596426ae1,"Amid ongoing public speculation about the reasons for sex differences in careers in science and mathematics, we present a consensus statement that is based on the best available scientific evidence. Sex differences in science and math achievement and ability are smaller for the mid-range of the abilities distribution than they are for those with the highest levels of achievement and ability. Males are more variable on most measures of quantitative and visuospatial ability, which necessarily results in more males at both high- and low-ability extremes; the reasons why males are often more variable remain elusive. Successful careers in math and science require many types of cognitive abilities. Females tend to excel in verbal abilities, with large differences between females and males found when assessments include writing samples. High-level achievement in science and math requires the ability to communicate effectively and comprehend abstract ideas, so the female advantage in writing should be helpful in all academic domains. Males outperform females on most measures of visuospatial abilities, which have been implicated as contributing to sex differences on standardized exams in mathematics and science. An evolutionary account of sex differences in mathematics and science supports the conclusion that, although sex differences in math and science performance have not directly evolved, they could be indirectly related to differences in interests and specific brain and cognitive systems. We review the brain basis for sex differences in science and mathematics, describe consistent effects, and identify numerous possible correlates. Experience alters brain structures and functioning, so causal statements about brain differences and success in math and science are circular. A wide range of sociocultural forces contribute to sex differences in mathematics and science achievement and ability—including the effects of family, neighborhood, peer, and school influences; training and experience; and cultural practices. We conclude that early experience, biological factors, educational policy, and cultural context affect the number of women and men who pursue advanced study in science and math and that these effects add and interact in complex ways. There are no single or simple answers to the complex questions about sex differences in science and mathematics.",2007,486,904,89,4,19,34,48,46,75,75,61,68,69
d4d7370670ffa790900ff05f96c208c9eda7d58b,"Graph theory models the Internet mathematically, and a number of plausible mathematically intersecting network models for the Internet have been developed and studied. Simultaneously, Internet researchers have developed methodology to use real data to validate, or invalidate, proposed Internet models. The authors look at these parallel developments, particularly as they apply to scale-free network models of the preferential attachment type.",2009,73,228,11,8,19,28,28,24,18,26,16,16,9
e38ac5b98197284537fca03d8592adaae48841d4,"To understand the difficulties that many students have with comprehension of mathematics, we must determine the cognitive functioning underlying the diversity of mathematical processes. What are the cognitive systems that are required to give access to mathematical objects? Are these systems common to all processes of knowledge or, on the contrary, some of them are specific to mathematical activity? Starting from the paramount importance of semiotic representation for any mathematical activity, we put forward a classification of the various registers of semiotic representations that are mobilized in mathematical processes. Thus, we can reveal two types of transformation of semiotic representations: treatment and conversion. These two types correspond to quite different cognitive processes. They are two separate sources of incomprehension in the learning of mathematics. If treatment is the more important from a mathematical point of view, conversion is basically the deciding factor for learning. Supporting empirical data, at any level of curriculum and for any area of mathematics, can be widely and methodologically gathered: some empirical evidence is presented in this paper.",2006,28,900,111,10,12,20,25,59,41,61,55,62,91
952a90c12294b254aac85b35b0b15e6e34736aaa,"If looking for the ebook Implementing Mathematics with The Nuprl Proof Development System by R L Constable in pdf form, in that case you come on to the right site. We present full variant of this ebook in DjVu, PDF, ePub, doc, txt forms. You can reading Implementing Mathematics with The Nuprl Proof Development System online by R L Constable either downloading. Additionally to this ebook, on our website you may read guides and different art eBooks online, either download their. We wish to invite your note what our site does not store the book itself, but we grant url to website wherever you may load either reading online. So that if want to load pdf Implementing Mathematics with The Nuprl Proof Development System by R L Constable, then you have come on to correct website. We have Implementing Mathematics with The Nuprl Proof Development System txt, ePub, PDF, doc, DjVu forms. We will be pleased if you go back afresh.",1986,180,1451,79,10,13,34,33,42,47,56,66,69,62
786364fdd5a792fec5c5aaf23b87acbc4b57818b,"Abstract This study examines changes in teachers’ thinking as they participated in a video club designed to help them learn to notice and interpret students’ mathematical thinking. First, we investigate changes in teachers’ talk about classroom video segments before and after participation in the video club. Second, we identify three paths along which teachers learned to notice students’ mathematical thinking in this context: Direct, Cyclical, and Incremental. Finally, we explore ways the video club context influenced teacher learning. Understanding different forms of teacher learning provides insight for research on teacher cognition and may inform the design of video-based professional development.",2008,96,735,68,7,12,19,36,34,38,64,82,69,79
ab91f8b2372ec432d8f93c86b545cd9729446291,,1978,0,1478,112,0,0,1,3,3,4,5,8,6,6
7d25dbb45d8e5a12c2a12bab6ce7561752a045ca,"This survey provides a brief and selective overview of research in the philosophy of mathematics education. It asks what makes up the philosophy of mathematics education, what it means, what questions it asks and answers, and what is its overall importance and use? It provides overviews of critical mathematics education, and the most relevant modern movements in the philosophy of mathematics. A case study is provided of an emerging research tradition in one country. This is the Hermeneutic strand of research in the philosophy of mathematics education in Brazil. This illustrates one orientation towards research inquiry in the philosophy of mathematics education. It is part of a broader practice of ‘philosophical archaeology’: the uncovering of hidden assumptions and buried ideologies within the concepts and methods of research and practice in mathematics education. An extensive bibliography is also included.",1991,213,1180,142,3,10,14,18,26,27,41,34,22,28
4e034faaae5aa8821082af33da6da46925b12fb3,"Abstract The decomposition method can be an effective procedure for analytical solution of a wide class of dynamical systems without linearization or weak nonlinearity assumptions, closure approximations, perturbation theory, or restrictive assumptions on stochasticitiy.",1988,84,1326,97,0,1,0,1,0,1,1,2,1,2
4eee6b36dbe0e204d0fdbdae7cb36dcaf569d1d9,,1992,0,1194,140,1,5,7,7,5,16,11,24,25,19
e7a5ff509962afa8850e1e682ebc224018366054,"Although it is often assumed that abilities that reflect basic numerical understanding, such as numerical comparison, are related to children's mathematical abilities, this relationship has not been tested rigorously. In addition, the extent to which symbolic and nonsymbolic number processing play differential roles in this relationship is not yet understood. To address these questions, we collected mathematics achievement measures from 6- to 8-year-olds as well as reaction times from a numerical comparison task. Using the reaction times, we calculated the size of the numerical distance effect exhibited by each child. In a correlational analysis, we found that the individual differences in the distance effect were related to mathematics achievement but not to reading achievement. This relationship was found to be specific to symbolic numerical comparison. Implications for the role of basic numerical competency and the role of accessing numerical magnitude information from Arabic numerals for the development of mathematical skills and their impairment are discussed.",2009,42,587,57,20,21,40,52,53,63,70,75,63,43
cd3a45bdd2c0ebdcfe4f2353da16c8c11ae5cb7f,"Boston College is an equal opportunity, affi rmative action employer.",2004,0,1142,63,2,15,33,42,62,78,86,90,152,109
142dad2d2e8e5060eb9561b7744cbc5b4bb90990,"From the Publisher: 
This text is designed for the sophomore/junior level introduction to discrete mathematics taken by students preparing for future coursework in areas such as math,computer science and engineering. Rosen has become a bestseller largely due to how effectively it addresses the main portion of the discrete market,which is typically characterized as the mid to upper level in rigor. The strength of Rosen's approach has been the effective balance of theory with relevant applications,as well as the overall comprehensive nature of the topic coverage.",1984,2,1330,50,0,0,0,0,0,0,1,2,3,6
069ca4d3ce5fdcdad511ec6cb1a6bb5fccf1f298,"The aim of this paper is to obtain an effective rule (or algorithm) for distinguishing sentences from nonsentences, which works not only for the formal languages of interest to the mathematical logician, but also for natural languages such as English, or at least for fragments of such languages. An attempt to formulate such an algorithm is implicit in the work of Ajdukiewicz (1935). His method, later elaborated by Bar-Hillel (1953), depends on a kind of arithmetization of the so-called parts of speech, here called syntactic types.",1958,23,1499,136,0,1,2,0,2,1,1,1,2,2
1f80d103e387aeaa3513078745c41027b0f0f2a2,"Reprinted with permission from the Fall 2005 issue of American Educator, the quarterly journal of the American Federation of Teachers, AFL-CIO.",2005,17,898,78,3,9,28,37,43,65,50,68,88,91
fd3163724c815360c80e71b8e839b8cbfe567e2b,"We used structural modeling procedures to assess the influence of past math grades, math ability perceptions, performance expectancies, and value perceptions on the level of math anxiety reported in a sample of 7th- through 9th-grade students (N = 250). A second set of analyses examined the relative influence of these performance, self-perception, and affect variables on students' subsequent grades and course enrollment intentions in mathematics. The findings indicated that math anxiety was most directly related to students' math ability perceptions, performance expectancies, and value perceptions. Students' performance expectancies predicted subsequent math grades, whereas their value perceptions predicted course enrollment intentions. Math anxiety did not have significant direct effects on either grades or intentions. The findings also suggested that the pattern of relations are similar for boys and girls. The results are discussed in relation to expectancy-value and self-efficacy theories of academic achievement. A strong background in mathematics is critical for many career and job opportunities in today's increasingly technological society. However, many academically capable students prematurely restrict their educational and career options by discontinuing their mathematical training early in high school. Several recent surveys (National Assessment of Educational Progress [NAEP], 1988; National Center for Educational Statistics [NCES], 1984) indicate that only half of all high school graduates enroll in mathematics courses beyond the 10th grade. These reports also indicate that fewer women than men enroll in the more advanced courses in high school mathematics (NAEP, 1988; NCES, 1984), although the ""gender gap"" is beginning to narrow (Chipman & Thomas, 1985; Eccles, 1987). Furthermore, students of both sexes, but particularly women, do not attain a high level of mathematical competency, even if they have completed 4 years of high school math (NAEP, 1988).",1990,72,1231,41,1,1,2,5,13,12,41,16,8,14
0c24dd1033bc83cdb51c18345bc2ad8866cbca03,,1964,0,1304,195,0,0,1,1,0,0,2,0,1,0
fa96245255f18739dab9e70cc4ae5dda0765b509,"There is a story about two friends, who were classmates in high school, talking about their jobs. One of them became a statistician and was working on population trends. He showed a reprint to his former classmate. The reprint started, as usual, with the Gaussian distribution and the statistician explained to his former classmate the meaning of the symbols for the actual population, for the average population, and so on. His classmate was a bit incredulous and was not quite sure whether the statistician was pulling his leg. “How can you know that?” was his query. “And what is this symbol here?” “Oh,” said the statistician, “this is π.” “What is that?” “The ratio of the circumference of the circle to its diameter.” “Well, now you are pushing your joke too far,” said the classmate, “surely the population has nothing to do with the circumference of the circle.”",1960,11,1647,38,0,1,1,0,0,1,1,1,0,0
a16f7f1fe98951487b7b83097b47f43f9e83ac1c,"Early childhood mathematics is vitally important for young children's present and future educational success. Research demonstrates that virtually all young children have the capability to learn and become competent in mathematics. Furthermore, young children enjoy their early informal experiences with mathematics. Unfortunately, many children's potential in mathematics is not fully realized, especially those children who are economically disadvantaged. This is due, in part, to a lack of opportunities to learn mathematics in early childhood settings or through everyday experiences in the home and in their communities. Improvements in early childhood mathematics education can provide young children with the foundation for school success. Relying on a comprehensive review of the research, Mathematics Learning in Early Childhood lays out the critical areas that should be the focus of young children's early mathematics education, explores the extent to which they are currently being incorporated in early childhood settings, and identifies the changes needed to improve the quality of mathematics experiences for young children. This book serves as a call to action to improve the state of early childhood mathematics. It will be especially useful for policy makers and practitioners-those who work directly with children and their families in shaping the policies that affect the education of young children.",2009,0,439,33,4,10,17,26,46,38,56,52,34,53
e2ce8e9f362fb1caf22cf5b7ad038dc9753c1190,"In this article we discuss efforts to design and empirically test measures of teachers’ content knowledge for teaching elementary mathematics. We begin by reviewing the literature on teacher knowledge, noting how scholars have organized such knowledge. Next we describe survey items we wrote to represent knowledge for teaching mathematics and results from factor analysis and scaling work with these items. We found that teachers’ knowledge for teaching elementary mathematics was multidimensional and included knowledge of various mathematical topics (e.g., number and operations, algebra) and domains (e.g., knowledge of content, knowledge of students and content). The constructs indicated by factor analysis formed psychometrically acceptable scales.",2004,37,896,68,2,5,12,30,56,37,52,48,71,93
99eaa4a03d94547bfb294460ce78ff9c753e0c2b,"Using contemporary data from the U.S. and other nations, we address 3 questions: Do gender differences in mathematics performance exist in the general population? Do gender differences exist among the mathematically talented? Do females exist who possess profound mathematical talent? In regard to the first question, contemporary data indicate that girls in the U.S. have reached parity with boys in mathematics performance, a pattern that is found in some other nations as well. Focusing on the second question, studies find more males than females scoring above the 95th or 99th percentile, but this gender gap has significantly narrowed over time in the U.S. and is not found among some ethnic groups and in some nations. Furthermore, data from several studies indicate that greater male variability with respect to mathematics is not ubiquitous. Rather, its presence correlates with several measures of gender inequality. Thus, it is largely an artifact of changeable sociocultural factors, not immutable, innate biological differences between the sexes. Responding to the third question, we document the existence of females who possess profound mathematical talent. Finally, we review mounting evidence that both the magnitude of mean math gender differences and the frequency of identification of gifted and profoundly gifted females significantly correlate with sociocultural factors, including measures of gender equality across nations.",2009,56,434,21,8,16,22,30,40,36,43,24,52,60
deed2d9573e8fae764ff690af527724be18b062e,"The History of the Soliton Derivation of the Korteweg-de Vries, Nonlinear Schrodinger and Other Important and Canonical Equations of Mathematical Physics Soliton Equation Families and Solution Methods The -Function, the Hirota Method, the Painleve Property and Backlund Transformations for the Korteweg-de Vries Family of Soliton Equations Connecting Links among the Miracles of Soliton Mathematics.",1987,0,1155,68,10,14,18,26,27,27,35,29,34,32
2cfd620128b62301624e36e7b4626072b151910d,"Williams textbook of endocrinology / , Williams textbook of endocrinology / , کتابخانه دیجیتال جندی شاپور اهواز",1985,0,3465,33,0,4,20,14,12,17,16,11,22,37
af06e7856d5b62fe8242d9e25b1cbbe63c77e449,"Clinical gynecologic endocrinology and infertility , Clinical gynecologic endocrinology and infertility , کتابخانه دیجیتال جندی شاپور اهواز",1983,0,2717,157,6,11,16,10,23,16,3,13,21,16
8c3a30dd7261f876c8dd211fde049a1e8dfbe8eb,"The stress response is subserved by the stress system, which is located both in the central nervous system and the periphery. The principal effectors of the stress system include corticotropin-releasing hormone (CRH); arginine vasopressin; the proopiomelanocortin-derived peptides alpha-melanocyte-stimulating hormone and beta-endorphin, the glucocorticoids; and the catecholamines norepinephrine and epinephrine. Appropriate responsiveness of the stress system to stressors is a crucial prerequisite for a sense of well-being, adequate performance of tasks, and positive social interactions. By contrast, inappropriate responsiveness of the stress system may impair growth and development and may account for a number of endocrine, metabolic, autoimmune, and psychiatric disorders. The development and severity of these conditions primarily depend on the genetic vulnerability of the individual, the exposure to adverse environmental factors, and the timing of the stressful events, given that prenatal life, infancy, childhood, and adolescence are critical periods characterized by increased vulnerability to stressors.",2005,111,1545,104,14,47,57,74,79,81,83,90,105,107
a1c6862fd3f82ffb6e4245eae37b8e139b87675d,"This report presents an algorithm to assist primary care physicians, endocrinologists, and others in the management of adult, nonpregnant patients with type 2 diabetes mellitus. In order to minimize the risk of diabetes-related complications, the goal of therapy is to achieve a hemoglobin A1c (A1C) of 6.5% or less, with recognition of the need for individualization to minimize the risks of hypoglycemia. We provide therapeutic pathways stratified on the basis of current levels of A1C, whether the patient is receiving treatment or is drug naïve. We consider monotherapy, dual therapy, and triple therapy, including 8 major classes of medications (biguanides, dipeptidyl-peptidase-4 inhibitors, incretin mimetics, thiazolidinediones, alpha-glucosidase inhibitors, sulfonylureas, meglitinides, and bile acid sequestrants) and insulin therapy (basal, premixed, and multiple daily injections), with or without orally administered medications. We prioritize choices of medications according to safety, risk of hypoglycemia, efficacy, simplicity, anticipated degree of patient adherence, and cost of medications. We recommend only combinations of medications approved by the US Food and Drug Administration that provide complementary mechanisms of action. It is essential to monitor therapy with A1C and self-monitoring of blood glucose and to adjust or advance therapy frequently (every 2 to 3 months) if the appropriate goal for each patient has not been achieved. We provide a flow-chart and table summarizing the major considerations. This algorithm represents a consensus of 14 highly experienced clinicians, clinical researchers, practitioners, and academicians and is based on the American Association of Clinical Endocrinologists/American College of Endocrinology Diabetes Guidelines and the recent medical literature.",2009,62,949,76,10,121,187,186,163,106,48,46,27,20
52b7bf3ba59b31f362aa07f957f1543a29a4279e,The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.,1995,26,32225,2920,0,0,0,0,0,0,0,0,0,0
0b7c3ad049492e9337f376ec46aea040598afad6,"An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. We present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each mobile host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic self starting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.",1999,38,12415,1293,0,0,2,1,3,4,3,729,822,786
6716697767fc601efc7690f40820d9ea7a7bf57c,"The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w...",1998,44,13261,1218,0,0,1,0,0,0,1,2,3,210
5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53,"From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.",2000,1,13173,1036,0,1,0,0,0,1,0,6,466,804
6f3a06207ac5a86841e3044e89e2464ed442d74a,"A logging instrument contains a pulsed neutron source and a pair of radiation detectors spaced along the length of the instrument. The radiation detectors are gated differently from each other to provide an indication of formation porosity which is substantially independent of the formation salinity. In the preferred embodiment, the electrical signals indicative of radiation detected by the long-spaced detector are gated for almost the entire interval between neutron pulses and the short-spaced signals are gated for a significantly smaller time interval which commences soon after the termination of a given neutron burst. The signals from the two detectors are combined in a ratio circuit for determination of porosity.",2001,12,12447,1429,2,0,1,2,1,4,694,771,798,873
012f7d74a24ba58e0d965295e4a2eea4dc33531f,"This paper contains the likelihood analysis of vector autoregressive models allowing for cointegration. The author derives the likelihood ratio test for cointegrating rank and finds it asymptotic distribution. He shows that the maximum likelihood estimator of the cointegrating relations can be found by reduced rank regression and derives the likelihood ratio test of structural hypotheses about these relations. The author shows that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian, allowing inference for hypotheses on the cointegrating relation to be conducted using the Chi("" squared"") distribution. Copyright 1991 by The Econometric Society.",1991,50,10588,796,0,0,0,0,0,0,66,189,211,221
06bb5771e6b8a9356c5f4ae28c98b4397c043349,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.",2004,285,8996,719,128,142,184,265,276,392,390,427,421,513
7abeda3a20c13bfee416d94efa313ff870656fec,"Support vector machine (SVM) is a popular technique for classication. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signicant steps. In this guide, we propose a simple procedure, which usually gives reasonable results.",2008,16,6929,438,235,315,354,377,480,536,632,696,614,617
04b23f577c20d1a0e2a67aadda555f58e6d23d6e,"This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.",2008,57,4227,606,42,80,110,160,183,219,261,288,338,414
70e3da6c426ca384f78f77474cbbf00a436038a2,"Contents: Introduction: Differential Equations and Dynamical Systems.- An Introduction to Chaos: Four Examples.- Local Bifurcations.- Averaging and Perturbation from a Geometric Viewpoint.- Hyperbolic Sets, Sympolic Dynamics, and Strange Attractors.- Global Bifurcations.- Local Codimension Two Bifurcations of Flows.- Appendix: Suggestions for Further Reading. Postscript Added at Second Printing. Glossary. References. Index.",1983,1,13388,693,0,0,0,0,0,0,0,1,2,1
40212e9474c3ddf3d8c6ffd13dd3211ec9406c49,"This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.",1998,48,8549,660,26,43,75,120,208,272,332,363,449,391
ccce1cf96f641b3581fba6f4ce2545f4135a15e3,"In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM's. The approach is illustrated on a two-spiral benchmark classification problem.",1999,15,7732,704,0,0,0,16,51,72,104,145,178,238
7c46799502bebfe6a9ae0f457b7b8b92248ec260,"An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.",1980,47,7945,298,3,14,27,9,28,44,61,54,102,110
fb6b4b57f431a0cfbb83bb2af8beab4ee694e94c,"DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer.In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98% accurate, while the baseline method is only 86% accurate.",2002,98,5320,484,16,65,82,125,177,224,200,253,247,288
4de39c94e340a108fff01a90a67b0c17c86fb981,"This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm.",1999,0,5914,580,16,31,81,127,154,176,228,332,310,335
8ff61b8e097ccdb784a35b466ba9e130c2502513,"Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.",2003,10,4726,553,36,71,102,134,152,208,190,246,242,241
ecc8fc05c5cc22b86cac987245b840a6c2021f1e,"This book gives a detailed mathematical and statistical analysis of the cointegrated vector autoregresive model. This model had gained popularity because it can at the same time capture the short-run dynamic properties as well as the long-run equilibrium behaviour of many non-stationary time series. It also allows relevant economic questions to be formulated in a consistent statistical framework. Part I of the book is planned so that it can be used by those who want to apply the methods without going into too much detail about the probability theory. The main emphasis is on the derivation of estimators and test statistics through a consistent use of the Guassian likelihood function. It is shown that many different models can be formulated within the framework of the autoregressive model and the interpretation of these models is discussed in detail. In particular, models involving restrictions on the cointegration vectors and the adjustment coefficients are discussed, as well as the role of the constant and linear drift. In Part II, the asymptotic theory is given the slightly more general framework of stationary linear processes with i.i.d. innovations. Some useful mathematical tools are collected in Appendix A, and a brief summary of weak convergence in given in Appendix B. The book is intended to give a relatively self-contained presentation for graduate students and researchers with a good knowledge of multivariate regression analysis and likelihood methods. The asymptotic theory requires some familiarity with the theory of weak convergence of stochastic processes. The theory is treated in detail with the purpose of giving the reader a working knowledge of the techniques involved. Many exercises are provided. The theoretical analysis is illustrated with the empirical analysis of two sets of economic data. The theory has been developed in close contract with the application and the methods have been implemented in the computer package CATS in RATS as a result of a rcollaboation with Katarina Juselius and Henrik Hansen.",1996,0,4586,442,10,26,60,77,80,111,157,158,172,191
7f755d620b57acf27a16ff95923c5677ff8198bb,"Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such ""all-together"" methods. We then compare their performance with three methods based on binary classifications: ""one-against-all,"" ""one-against-one,"" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the ""one-against-one"" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.",2002,32,6276,326,14,47,89,144,215,231,266,332,342,378
d5f169880e30e1f76827d72f862555d00b01bed9,"In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.",1975,7,7589,563,1,3,1,1,3,1,2,2,0,1
c564aa7639a08c280423489e52b6e32055c9aa7f,"1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems.",1991,0,7080,472,6,31,90,141,187,255,235,268,276,315
9c4da62e9e89e65ac78ee271e424e8b498053e8c,"Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.",1999,259,5700,256,38,61,98,150,218,310,375,417,482,488
d611f303ebab48a9cb9e265ba42c538be9f198ba,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.",1994,28,6633,708,3,2,10,21,29,53,76,114,190,261
d68725804eadecf83d707d89e12c5132bf376187,,2001,22,4213,909,1,15,39,78,82,114,132,145,178,200
384bb3944abe9441dcd2cede5e7cd7353e9ee5f7,,1999,0,4915,367,2,12,15,40,55,91,110,158,175,190
bd3a6b06537db6e66aaa2e345ab3af7f307640ad,"This paper shows how we can estimate VAR's formulated in levels and test general restrictions on the parameter matrices even if the processes may be integrated or cointegrated of an arbitrary order. We can apply a usual lag selection procedure to a possibly integrated or cointegrated VAR since the standard asymptotic theory is valid (as far as the order of integration of the process does not exceed the true lag length of the model). Having determined a lag length k, we then estimate a (k + dmax)th-order VAR where dmax is the maximal order of integration that we suspect might occur in the process. The coefficient matrices of the last dmax lagged vectors in the model are ignored (since these are regarded as zeros), and we can test linear or nonlinear restrictions on the first k coefficient matrices using the standard asymptotic theory.",1995,21,4350,546,2,5,8,16,20,30,29,37,35,50
2d86e239a9e9741f22be1d8c1feed7a44da1bdc1,"This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software.",2000,0,3311,280,11,50,97,124,187,245,250,279,280,276
e52fb14e4beccc5e88a33c1fe5c7d6e780831ae1,"A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.",1996,12,3463,285,0,2,6,9,10,12,13,13,30,36
e2bbea031af4e0aab292323c6dcd128050b26540,"From the Publisher: 
Engineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book.",1968,0,5473,352,0,1,8,17,23,39,36,49,46,28
fe84db9e87a513b285ab32147cd901782e66616d,"The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.",1998,77,5673,131,0,2,0,2,93,125,168,193,237,284
00f66c12904a003f46077f444e6e0c64b8115bb0,"Several polycations possessing substantial buffering capacity below physiological pH, such as lipopolyamines and polyamidoamine polymers, are efficient transfection agents per se--i.e., without the addition of cell targeting or membrane-disruption agents. This observation led us to test the cationic polymer polyethylenimine (PEI) for its gene-delivery potential. Indeed, every third atom of PEI is a protonable amino nitrogen atom, which makes the polymeric network an effective ""proton sponge"" at virtually any pH. Luciferase reporter gene transfer with this polycation into a variety of cell lines and primary cells gave results comparable to, or even better than, lipopolyamines. Cytotoxicity was low and seen only at concentrations well above those required for optimal transfection. Delivery of oligonucleotides into embryonic neurons was followed by using a fluorescent probe. Virtually all neurons showed nuclear labeling, with no toxic effects. The optimal PEI cation/anion balance for in vitro transfection is only slightly on the cationic side, which is advantageous for in vivo delivery. Indeed, intracerebral luciferase gene transfer into newborn mice gave results comparable (for a given amount of DNA) to the in vitro transfection of primary rat brain endothelial cells or chicken embryonic neurons. Together, these properties make PEI a promising vector for gene therapy and an outstanding core for the design of more sophisticated devices. Our hypothesis is that its efficiency relies on extensive lysosome buffering that protects DNA from nuclease degradation, and consequent lysosomal swelling and rupture that provide an escape mechanism for the PEI/DNA particles.",1995,4,5631,186,2,16,37,73,124,130,124,152,151,159
74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,"This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.",1999,28,3028,403,4,18,39,49,75,93,109,141,148,163
0e48afa17a9508792c6dabf95c2b3a7171366256,"A retroviral vector system based on the human immunodeficiency virus (HIV) was developed that, in contrast to a murine leukemia virus-based counterpart, transduced heterologous sequences into HeLa cells and rat fibroblasts blocked in the cell cycle, as well as into human primary macrophages. Additionally, the HIV vector could mediate stable in vivo gene transfer into terminally differentiated neurons. The ability of HIV-based viral vectors to deliver genes in vivo into nondividing cells could increase the applicability of retroviral vectors in human gene therapy.",1996,83,4772,131,29,92,122,165,225,191,218,217,218,190
fc4c9a3177f53de98f0ebb2d070629d10f646917,,2005,0,2960,453,55,93,113,104,153,161,189,215,253,214
6434a32dfa090fd5e5ae3809958dd68bacb5839c,"This paper addresses the problem of the classification of hyperspectral remote sensing images by support vector machines (SVMs). First, we propose a theoretical discussion and experimental analysis aimed at understanding and assessing the potentialities of SVM classifiers in hyperdimensional feature spaces. Then, we assess the effectiveness of SVMs with respect to conventional feature-reduction-based approaches and their performances in hypersubspaces of various dimensionalities. To sustain such an analysis, the performances of SVMs are compared with those of two other nonparametric classifiers (i.e., radial basis function neural networks and the K-nearest neighbor classifier). Finally, we study the potentially critical issue of applying binary SVMs to multiclass problems in hyperspectral data. In particular, four different multiclass strategies are analyzed and compared: the one-against-all, the one-against-one, and two hierarchical tree-based strategies. Different performance indicators have been used to support our experimental studies in a detailed and accurate way, i.e., the classification accuracy, the computational time, the stability to parameter setting, and the complexity of the multiclass architecture. The results obtained on a real Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral dataset allow to conclude that, whatever the multiclass strategy adopted, SVMs are a valid and effective alternative to conventional pattern recognition approaches (feature-reduction procedures combined with a classification method) for the classification of hyperspectral remote sensing data.",2004,54,2902,284,4,16,36,61,70,92,102,115,134,135
a8797f1d253c75669d96e6fcceda2be3f8534e1d,"Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.",2002,57,3072,239,13,26,62,79,106,97,125,141,152,161
830107cb2b579b300cb2c2dddae94191bd6d508a,"This paper considers estimation and testing of vector autoregressio n coefficients in panel data, and applies the techniques to analyze the dynamic relationships between wages an d hours worked in two samples of American males. The model allows for nonstationary individual effects and is estimated by applying instrumental variables to the quasi-differenced autoregressive equations. The empirical results suggest the absence of lagged hours in the wage forecasting equation. The results also show that lagged hours is important in the hours equation. Copyright 1988 by The Econometric Society.",1988,0,3434,114,2,3,6,6,7,12,11,14,18,27
53fcc056f79e04daf11eb798a7238e93699665aa,"This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On realworld sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.",1998,23,2785,273,5,13,12,18,43,47,69,70,96,92
58931b142b6c01869cf22f4e74ce967230fe6069,"We have developed a new expression vector which allows efficient selection for transfectants that express foreign genes at high levels. The vector is composed of a ubiquitously strong promoter based on the beta-actin promoter, a 69% subregion of the bovine papilloma virus genome, and a mutant neomycin phosphotransferase II-encoding gene driven by a weak promoter, which confers only marginal resistance to G418. Thus, high concentrations of G418 (approx. 800 micrograms/ml) effectively select for transfectants containing a high vector copy number (greater than 300). We tested this system by producing human interleukin-2 (IL-2) in L cells and Chinese hamster ovary (CHO) cells, and the results showed that high concentrations of G418 efficiently yielded L cell and CHO cell transfectants stably producing IL-2 at levels comparable with those previously attained using gene amplification. The vector sequences were found to have integrated into the host chromosome, and were stably maintained in the transfectants for several months.",1991,0,3872,96,0,2,3,6,10,24,44,67,135,150
4ead4d868721e8b38c266415a1f6201cd147a78a,"SUMMARY This paper is concerned with the representation of a multivariate sample of size n as points P1, P2, ..., PI in a Euclidean space. The interpretation of the distance A(Pi, Pj) between the ith andjth members of the sample is discussed for some commonly used types of analysis, including both Q and R techniques. When all the distances between n points are known a method is derived which finds their co-ordinates referred to principal axes. A set of necessary and sufficient conditions for a solution to exist in real Euclidean space is found. Q and R techniques are defined as being dual to one another when they both lead to a set of n points with the same inter-point distances. Pairs of dual techniques are derived. In factor analysis the distances between points whose co-ordinates are the estimated factor scores can be interpreted as D2 with a singular dispersion matrix.",1966,25,3613,214,3,8,4,12,13,15,22,24,19,24
3367b98c29988f21a7ac06b76297bc925e13c1d9,"The paper describes a general methodology for the fitting of measured or calculated frequency domain responses with rational function approximations. This is achieved by replacing a set of starting poles with an improved set of poles via a scaling procedure. A previous paper (Gustavsen et al., 1997) described the application of the method to smooth functions using real starting poles. This paper extends the method to functions with a high number of resonance peaks by allowing complex starting poles. Fundamental properties of the method are discussed and details of its practical implementation are described. The method is demonstrated to be very suitable for fitting network equivalents and transformer responses. The computer code is in the public domain, available from the first author.",1999,8,2614,290,2,6,13,20,28,56,77,81,127,139
33e938103ecc37e4e9e286d63ac6ef6fd14e0796,"We have developed a new expression vector which allows efficient selection for transfectants that express foreign genes at high levels. The vector is composed of a ubiquitously strong promoter based on the beta-actin promoter, a 69% subregion of the bovine papilloma virus genome, and a mutant neomycin phosphotransferase II-encoding gene driven by a weak promoter, which confers only marginal resistance to G418. Thus, high concentrations of G418 (approx. 800 micrograms/ml) effectively select for transfectants containing a high vector copy number (greater than 300). We tested this system by producing human interleukin-2 (IL-2) in L cells and Chinese hamster ovary (CHO) cells, and the results showed that high concentrations of G418 efficiently yielded L cell and CHO cell transfectants stably producing IL-2 at levels comparable with those previously attained using gene amplification. The vector sequences were found to have integrated into the host chromosome, and were stably maintained in the transfectants for several months.",1991,39,4312,25,0,3,4,4,24,43,85,126,162,206
3ee73f2bf6cd154efd65783be0761cb6b7478196,"Support vector machines (SVMs) are becoming popular in a wide variety of biological applications. But, what exactly are SVMs and how do they work? And what are their most promising applications in the life sciences?",2006,31,2329,231,6,17,37,62,52,72,83,113,118,141
24e6cf0796237f21c780a3f0c996817f57b3a1bd,Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.,2004,24,3496,5,0,0,0,0,0,0,0,0,1,0
69d6c6305c2cc2283f5a8a92316803800ecb6b7f,,1985,0,3075,193,1,0,0,2,3,2,1,4,12,10
c7822d12beec5cfa81c48ced04d34d4ecf2654b5,"1. Introduction 2. The space-phasor model of A.C. machines 3. Vector and direct torque control of synchronous machines 4. Vector and direct torque control of induction machines 5. Torque control of switched reluctance motors 6. Effects of magnetic saturation 7. Artificial intelligence-based steady-state and transient analysis of electrical machines, estimators 8. Self-commissioning Index",1998,0,2402,241,2,7,30,31,57,59,86,84,145,126
8d73c0d0c92446102fdb6cc728b5d69674a1a387,"We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results.",2000,75,2572,207,12,36,28,45,62,96,97,110,102,143
282ec09590a275606dee13a76b269ce3b3c5a88c,"Four new antibiotic-resistant derivatives of the broad-host-range (bhr) cloning vector pBBR1MCS have been constructed. These new plasmids have several advantages over many of the currently available bhr vectors in that: (i) they are relatively small (< 5.3 kb), (ii) they possess an extended multiple cloning site (MCS), (iii) they allow direct selection of recombinant plasmid molecules in Escherichia coli via disruption of the LacZ alpha peptide, (iv) they are mobilizable when the RK2 transfer functions are provided in trans and (v) they are compatible with IncP, IncQ and IncW group plasmids, as well as with ColE1- and P15a-based replicons.",1995,10,3005,143,0,0,4,13,30,43,60,59,68,87
cfc6d0c8260594ebc5dd20ee558d29b1014ed41a,"In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.",2002,30,2169,327,6,16,24,54,69,81,95,126,126,163
28ec1fe81dfc6eebe359898ff79960e24876032e,"Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries. Problems associated with initialization and poor convergence to boundary concavities, however, have limited their utility. This paper presents a new external force for active contours, largely solving both problems. This external force, which we call gradient vector flow (GVF), is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image. It differs fundamentally from traditional snake external forces in that it cannot be written as the negative gradient of a potential function, and the corresponding snake is formulated directly from a force balance condition rather than a variational formulation. Using several two-dimensional (2-D) examples and one three-dimensional (3-D) example, we show that GVF has a large capture range and is able to move snakes into boundary concavities.",1998,41,2804,167,4,12,28,43,55,73,107,117,163,190
88ebd3cd81e9745433fc25d312322cc6b609e4d9,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.",1994,8,3503,64,0,2,2,8,15,29,49,69,97,158
da9df5d83da5f948e6bbc4e8ae3fe144044bf0e7,"Monetary policy and the private sector behaviour of the U.S. economy are modelled as a time varying structural vector autoregression, where the sources of time variation are both the coefficients and the variance covariance matrix of the innovations. The paper develops a new, simple modelling strategy for the law of motion of the variance covariance matrix and proposes an efficient Markov chain Monte Carlo algorithm for the model likelihood/posterior numerical evaluation. The main empirical conclusions are: (1) both systematic and non-systematic monetary policy have changed during the last 40 years—in particular, systematic responses of the interest rate to inflation and unemployment exhibit a trend toward a more aggressive behaviour, despite remarkable oscillations; (2) this has had a negligible effect on the rest of the economy. The role played by exogenous non-policy shocks seems more important than interest rate policy in explaining the high inflation and unemployment episodes in recent U.S. economic history.",2003,62,1733,415,8,7,9,11,31,37,55,62,69,84
9008cdacbdcff8a218a6928e94fe7c6dfc237b24,"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.",1997,20,2850,92,4,41,52,55,77,135,125,184,184,169
d8b68dadb16dc9b95b91d3776f3231c9754d919f,"ABSTRACT Vectors derived from human immunodeficiency virus (HIV) are highly efficient vehicles for in vivo gene delivery. However, their biosafety is of major concern. Here we exploit the complexity of the HIV genome to provide lentivirus vectors with novel biosafety features. In addition to the structural genes, HIV contains two regulatory genes, tat and rev, that are essential for HIV replication, and four accessory genes that encode critical virulence factors. We previously reported that the HIV type 1 accessory open reading frames are dispensable for efficient gene transduction by a lentivirus vector. We now demonstrate that the requirement for the tat gene can be offset by placing constitutive promoters upstream of the vector transcript. Vectors generated from constructs containing such a chimeric long terminal repeat (LTR) transduced neurons in vivo at very high efficiency, whether or not they were produced in the presence of Tat. When the rev gene was also deleted from the packaging construct, expression of gag and pol was strictly dependent on Rev complementation in trans. By the combined use of a separate nonoverlapping Rev expression plasmid and a 5′ LTR chimeric transfer construct, we achieved optimal yields of vector of high transducing efficiency (up to 107transducing units [TU]/ml and 104 TU/ng of p24). This third-generation lentivirus vector uses only a fractional set of HIV genes: gag, pol, and rev. Moreover, the HIV-derived constructs, and any recombinant between them, are contingent on upstream elements and trans complementation for expression and thus are nonfunctional outside of the vector producer cells. This split-genome, conditional packaging system is based on existing viral sequences and acts as a built-in device against the generation of productive recombinants. While the actual biosafety of the vector will ultimately be proven in vivo, the improved design presented here should facilitate testing of lentivirus vectors.",1998,63,2807,91,3,17,65,48,76,85,67,95,111,123
f368f95a533c910cb3ef71c8f4aa45df22a31168,,1964,0,3589,82,0,1,1,3,0,1,3,0,6,5
43ffa2c1a06a76e58a333f2e7d0bd498b24365ca,"The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems.",1996,4,2501,120,3,8,25,19,19,23,33,40,75,75
626a5da1bfc0f4b38be27f867f95daa061655f94,"The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs using a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based on exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental results assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate an improvement of generalization performance.",2002,33,2237,130,26,37,81,112,119,125,146,130,155,105
76f96dadd80b19bde49e0e1f07bfa9fe8485eeec,"From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",2001,0,2663,73,2,10,26,60,70,66,77,81,119,133
f640afd3488eb756451592b93dc5b060852511fd,"The current challenge, now that two plant genomes have been sequenced, is to assign a function to the increasing number of predicted genes. In Arabidopsis, approximately 55% of genes can be assigned a putative function, however, less than 8% of these have been assigned a function by direct experimental evidence. To identify these functions, many genes will have to undergo comprehensive analyses, which will include the production of chimeric transgenes for constitutive or inducible ectopic expression, for antisense or dominant negative expression, for subcellular localization studies, for promoter analysis, and for gene complementation studies. The production of such transgenes is often hampered by laborious conventional cloning technology that relies on restriction digestion and ligation. With the aim of providing tools for high throughput gene analysis, we have produced a Gateway-compatible Agrobacterium sp. binary vector system that facilitates fast and reliable DNA cloning. This collection of vectors is freely available, for noncommercial purposes, and can be used for the ectopic expression of genes either constitutively or inducibly. The vectors can be used for the expression of protein fusions to the Aequorea victoria green fluorescent protein and to the β-glucuronidase protein so that the subcellular localization of a protein can be identified. They can also be used to generate promoter-reporter constructs and to facilitate efficient cloning of genomic DNA fragments for complementation experiments. All vectors were derived from pCambia T-DNA cloning vectors, with the exception of a chemically inducible vector, for Agrobacterium sp.-mediated transformation of a wide range of plant species.",2003,31,2347,144,1,5,17,45,88,79,92,117,128,179
8da26b27d092807f704a91446494cc1a53ad2d15,"This paper describes the joint dynamics of bond yields and macroeconomic variables in a Vector Autoregression, where identifying restrictions are based on the absence of arbitrage. Using a term structure model with inflation and economic growth factors, we investigate how macro variables affect bond prices and the dynamics of the yield curve. The setup accommodates higher order autoregressive lags for the macro factors. The macro variables are augmented by traditional unobserved term structure factors. We find that the forecasting performance of a VAR improves when no-arbitrage restrictions are imposed. Models that incorporate macro factors forecast better than traditional term structure models with only unobservable factors. Variance decompositions show that macro factors explain up to 85% of the variation in bond yields. Macro factors primarily explain movements at the short end and middle of the yield curve while unobservable factors still account for most of the movement at the long end of the yield curve.",2001,109,1808,369,11,12,24,39,52,94,90,124,110,111
81a5952532cdd48eec5e3dc326907c36a70e0a24,"A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.",1984,88,2920,82,6,21,35,33,75,77,80,124,151,124
90902e16f4e8ff5e3c4bf0f971380af5753aacdd,Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.,2004,40,1711,226,5,17,19,34,49,81,69,95,89,117
33dedd088742944d3f7bc1159f296123a66a9f15,"Designer Stem Cells Despite their promise for use as disease models and in regenerative medicine, the generation of human-induced pluripotent stem (iPS) cells has been hindered by the integration of vector and transgenes in the host cell genome. Recent studies using the Cre/LoxP recombination strategy and the piggyBac transposon approach have approached this objective. However, Yu et al. (p. 797, published online 26 March) now show the derivation of human iPS cells from postnatal foreskin fibroblasts using the nonintegrating oriP/EBNA1-based episomal vectors. The resultant iPS cells show characteristics of human embryonic stem cells and are free of vector and transgenes. Human induced pluripotent stem cells can be generated without integration of exogenous DNA into their genomes. Reprogramming differentiated human cells to induced pluripotent stem (iPS) cells has applications in basic biology, drug development, and transplantation. Human iPS cell derivation previously required vectors that integrate into the genome, which can create mutations and limit the utility of the cells in both research and clinical applications. We describe the derivation of human iPS cells with the use of nonintegrating episomal vectors. After removal of the episome, iPS cells completely free of vector and transgene sequences are derived that are similar to human embryonic stem (ES) cells in proliferative and developmental potential. These results demonstrate that reprogramming human somatic cells does not require genomic integration or the continued presence of exogenous reprogramming factors and removes one obstacle to the clinical application of human iPS cells.",2009,43,2178,33,118,222,259,251,239,216,174,175,131,120
22ec7d2f1d4f802a135e0ec6611e0c508f1fca47,"This chapter presents the most basic results on topological vector spaces. With the exception of the last section, the scalar field over which vector spaces are defined can be an arbitrary, non-discrete valuated field K; K is endowed with the uniformity derived from its absolute value. The purpose of this generality is to clearly identify those properties of the commonly used real and complex number field that are essential for these basic results. Section 1 discusses the description of vector space topologies in terms of neighborhood bases of 0, and the uniformity associated with such a topology. Section 2 gives some means for constructing new topological vector spaces from given ones. The standard tools used in working with spaces of finite dimension are collected in Section 3, which is followed by a brief discussion of affine subspaces and hyperplanes (Section 4). Section 5 studies the extremely important notion of boundedness. Metrizability is treated in Section 6. This notion, although not overly important for the general theory, deserves special attention for several reasons; among them are its connection with category, its role in applications in analysis, and its role in the history of the subject (cf. Banach [1]). Restricting K to subfields of the complex numbers, Section 7 discusses the transition from real to complex fields and vice versa.",1967,0,3172,35,8,7,37,36,42,60,57,77,68,56
529cf7a716e6c9da99c6a468730f22398f75c1a4,"A real-time obstacle avoidance method for mobile robots which has been developed and implemented is described. This method, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously with range data sampled by onboard range sensors. The VFH method subsequently uses a two-stage data-reduction process to compute the desired control commands for the vehicle. Experimental results from a mobile robot traversing densely cluttered obstacle courses in smooth and continuous motion and at an average speed of 0.6-0.7 m/s are shown. A comparison of the VFN method to earlier methods is given. >",1991,35,2370,130,8,14,19,30,27,36,40,36,48,37
93aa298b40bb3ec23c25239089284fdf61ded917,"Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.",2004,16,1456,304,3,60,68,65,74,94,107,108,104,131
ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36,"Support Vector Machines (SVM’s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM’s introduce the notion of a “kernel induced feature space” which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM’s is that the higher-dimensional space doesn’t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system’s likelihood to perform well on unseen data) of SVM’s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM’s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM’s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than “yes/no” classification).",2002,20,1959,162,57,84,145,152,160,160,155,129,107,112
42371fda17462ef1ce69ddabe42f923195d5762f,"Structural vector autoregressions (VARs) are widely used to trace out the effect of monetary policy innovations on the economy. However, the sparse information sets typically used in these empirical models lead to at least two potential problems with the results. First, to the extent that central banks and the private sector have information not reflected in the VAR, the measurement of policy innovations is likely to be contaminated. A second problem is that impulse responses can be observed only for the included variables, which generally constitute only a small subset of the variables that the researcher and policymaker care about. In this paper we investigate one potential solution to this limited information problem, which combines the standard structural VAR analysis with recent developments in factor analysis for large data sets. We find that the information that our factor-augmented VAR (FAVAR) methodology exploits is indeed important to properly identify the monetary transmission mechanism. Overall, our results provide a comprehensive and coherent picture of the effect of monetary policy on the economy.",2004,77,1594,229,7,19,30,32,48,70,91,81,91,103
d8fcfd198038d418c30e007fb858ed19e45f4681,"We develop an on-demand multipath distance vector protocol for mobile ad hoc networks. Specifically, we propose multipath extensions to a well-studied single path routing protocol known as ad hoc on-demand distance vector (AODV). The resulting protocol is referred to as ad hoc on-demand multipath distance vector (AOMDV). The protocol computes multiple loop-free and link-disjoint paths. Loop-freedom is guaranteed by using a notion of ""advertised hopcount"". Link-disjointness of multiple paths is achieved by using a particular property of flooding. Performance comparison of AOMDV with AODV using ns-2 simulations shows that AOMDV is able to achieve a remarkable improvement in the end-to-end delay-often more than a factor of two, and is also able to reduce routing overheads by about 20%.",2001,51,1701,179,1,8,34,61,61,84,98,104,123,107
455d9a4ff96561d543acbcb2aa81d6cd8fcd20df,"My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently.",1998,84,2436,38,3,7,12,20,23,40,45,60,55,59
e90ea03e84087480384e8e7fa3a65182cfd6e9c0,"A leading physicist delves into relativity and experimental applications Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity offers a Nobel laureate's perspectives on the wealth of data technological developments have brought to expand upon Einstein's theory. Unique in basing relativity on the Principle of Equivalence of Gravitation and Inertia over Riemannian geometry, this book explores relativity experiments and observational cosmology to provide a sound foundation upon which analyses can be made. Covering special and general relativity, tensor analysis, gravitation, curvature, and more, this book provides an engaging, insightful introduction to the forces that shape the universe.",1973,81,4113,456,1,6,3,6,6,10,11,10,10,10
0665b56ad2271286f2a71cfef5c6ab310ea4de50,,1980,0,6692,615,0,1,2,14,10,14,8,16,17,29
f51351e4b07ff1cd5ee4238663a06bb255802c8f,Manifold Theory. Tensors. Semi-Riemannian Manifolds. Semi-Riemannian Submanifolds. Riemannian and Lorenz Geometry. Special Relativity. Constructions. Symmetry and Constant Curvature. Isometries. Calculus of Variations. Homogeneous and Symmetric Spaces. General Relativity. Cosmology. Schwarzschild Geometry. Causality in Lorentz Manifolds. Fundamental Groups and Covering Manifolds. Lie Groups. Newtonian Gravitation.,1983,0,3284,256,0,0,1,2,6,4,7,12,11,11
daf937b2c82ad95e991596036b8d023d43eb4460,"Preface Notation Important formulae and physical constants 1. Introduction 2. Special relativity, non-inertial effects and electromagnetism 3. Differential geometry I: vectors, forms and absolute differentiation 4. Differential geometry II: geodesics and curvature 5. Einstein field equations, the Schwarzschild solution and experimental test of general relativity 6. Gravitomagnetic effects: gyroscopes and clocks 7. Gravitational collapse and black holes 8. Action principles, conservation laws and the Cauchy problem 9. Gravitational radiation 10. Cosmology 11. Gravitation and field theory References Index.",2009,115,77,12,3,3,7,5,3,7,3,5,7,12
673def1212cf6dc2d3026951e46a6904fb293117,"The status of experimental tests of general relativity and of theoretical frameworks for analyzing them is reviewed and updated. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of local Lorentz invariance and clock experiments. Ongoing tests of EEP and of the inverse square law are searching for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light deflection, the Shapiro time delay, the perihelion advance of Mercury, the Nordtvedt effect in lunar motion, and frame-dragging. Gravitational wave damping has been detected in an amount that agrees with general relativity to better than half a percent using the Hulse-Taylor binary pulsar, and a growing family of other binary pulsar systems is yielding new tests, especially of strong-field effects. Current and future tests of relativity will center on strong gravity and gravitational waves.",2001,671,1296,26,0,1,0,0,0,0,0,0,2,2
30f921ce45573aa7e7d0864f1cfe2abf36230cce,"This paper summarizes the author's recently published findings about differences in people's work-related values among 50 countries. In view of these differences, ethnocentric management theories (those based on the value system of one particular country) have become untenable. This concept is illustrated for the fields of leadership, organization, and motivation.",1983,5,3007,269,0,0,3,6,5,6,6,10,8,15
e4636b2555401d5079417aa80ffaf4490cb91ba3,"This article—summarizing the authors’ then novel formulation of General Relativity—appeared as Chap. 7, pp. 227–264, in Gravitation: an introduction to current research, L. Witten, ed. (Wiley, New York, 1962), now long out of print. Intentionally unretouched, this republication as Golden Oldie is intended to provide contemporary accessibility to the flavor of the original ideas. Some typographical corrections have been made: footnote and page numbering have changed–but not section nor equation numbering, etc. Current institutional affiliations are encoded in: arnowitt@physics.tamu.edu, deser@brandeis.edu, misner@umd.edu.",2004,14,1710,90,39,44,63,51,61,64,91,111,106,112
db55ed6b64d35509854623f6d57ca3529bc57c85,1. Special Relativity and Flat Spacetime. 2. Manifolds. 3. Curvature. 4. Gravitation. 5. The Schwarzchild Solution. 6. More General Black Holes. 7. Perturbation Theory and Gravitational Radiation. 8. Cosmology. 9. Quantum Field Theory in Curved Spacetime. 10. Appendicies.,2003,0,1508,173,2,8,15,25,34,60,64,73,83,91
145f08ebcfbb1a5417d77fa7a5c7494bdcf1a95f,"List of contributors Preface 1. An introductory survey S. W. Hawking and W. Israel 2. The confrontation between gravitation theory and experiment C. M. Will 3. Gravitational-radiation experiments D. H. Douglass and V. B. Braginsky 4. The initial value problem and the dynamical formulation of general relativity A. E. Fischer and J. E. Marsden 5. Global structure of spacetimes R. Geroch and G. T. Horowitz 6. The general theory of the mechanical, electromagnetic and thermodynamic properties of black holes B. Carter 7. An introduction to the theory of the Kerr metric and its peturbations S. Chandrasekhar 8. Black hole astrophysics R. D. Blandford and K. S. Thorne 9. The big bang cosmology - enigmas and nostrums R. H. Dicke and P. J. E. Peebles 10. Cosmology and the early universe Ya B. Zel'dovitch 11. Anisotropic and inhomogeneous relativistic cosmologies M. A. H. MacCallum 12. Singularities and time-asymmetry R. Penrose 13. Quantum field theory in curved spacetime G. W. Gibbons 14. Quantum gravity: the new synthesis B. S. DeWitt 15. The path-integral approach to quantum gravity S. W. Hawking 16. Ultraviolet divergences in quantum theories of gravitation S. Weinberg References Index.",1979,0,2119,84,4,9,23,22,26,33,32,26,19,19
78037360d244887bce8e9dd16d451744d91dd03e,,1960,0,2053,171,0,3,4,11,8,13,11,8,25,16
4b58b298114ee0a77b471ee2268072ee63e87c3b,,1969,0,2218,59,1,3,2,3,5,5,9,4,4,9
ef8395964a6b17068d49b6c71344e30ae0c019c1,"According to general relativity, photons are deflected and delayed by the curvature of space-time produced by any mass. The bending and delay are proportional to γ + 1, where the parameter γ is unity in general relativity but zero in the newtonian model of gravity. The quantity γ - 1 measures the degree to which gravity is not a purely geometric effect and is affected by other fields; such fields may have strongly influenced the early Universe, but would have now weakened so as to produce tiny—but still detectable—effects. Several experiments have confirmed to an accuracy of ∼0.1% the predictions for the deflection and delay of photons produced by the Sun. Here we report a measurement of the frequency shift of radio photons to and from the Cassini spacecraft as they passed near the Sun. Our result, γ = 1 + (2.1 ± 2.3) × 10-5, agrees with the predictions of standard general relativity with a sensitivity that approaches the level at which, theoretically, deviations are expected in some cosmological models.",2003,22,1361,89,6,35,59,69,57,56,89,73,78,74
f883cb14548fd2a87b9853628c0a8d08baae982f,"This is an introduction to the by now fifteen years old research field of canonical quantum general relativity, sometimes called ""loop quantum gravity"". The term ""modern"" in the title refers to the fact that the quantum theory is based on formulating classical general relativity as a theory of connections rather than metrics as compared to in original version due to Arnowitt, Deser and Misner. Canonical quantum general relativity is an attempt to define a mathematically rigorous, non-perturbative, background independent theory of Lorentzian quantum gravity in four spacetime dimensions in the continuum. The approach is minimal in that one simply analyzes the logical consequences of combining the principles of general relativity with the principles of quantum mechanics. The requirement to preserve background independence has lead to new, fascinating mathematical structures which one does not see in perturbative approaches, e.g. a fundamental discreteness of spacetime seems to be a prediction of the theory providing a first substantial evidence for a theory in which the gravitational field acts as a natural UV cut-off. An effort has been made to provide a self-contained exposition of a restricted amount of material at the appropriate level of rigour which at the same time is accessible to graduate students with only basic knowledge of general relativity and quantum field theory on Minkowski space.",2007,3,1161,158,19,44,64,89,115,100,98,93,83,89
287dd15fd2862b28db56f81b91a94b82c09cdaad,"AbstractThe status of experimental tests of general relativity and of theoretical frameworks for analysing them are reviewed. Einstein’s equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of special relativity, and the gravitational redshift experiment. Future tests of EEP and of the inverse square law will search for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light defl
ection the Shapiro time delay, the perihelion advance of Mercury, and the Nordtvedt effect in lunar motion. Gravitational wave damping has been detected in an amount that agrees with general relativity to half a percent using the Hulse-Taylor binary pulsar, and new binary pulsar systems may yield further improvements. When direct observation of gravitational radiation from astrophysical sources begins, new tests of general relativity will be possible.",2001,346,1195,128,9,7,13,9,15,22,29,41,46,46
61225179ce90adce138d842ada42c2c18ce621f4,"Rapid interstellar travel by means of spacetime wormholes is described in a way that is useful for teaching elementary general relativity. The description touches base with Carl Sagan’s novel Contact, which, unlike most science fiction novels, treats such travel in a manner that accords with the best 1986 knowledge of the laws of physics. Many objections are given against the use of black holes or Schwarzschild wormholes for rapid interstellar travel. A new class of solutions of the Einstein field equations is presented, which describe wormholes that, in principle, could be traversed by human beings. It is essential in these solutions that the wormhole possess a throat at which there is no horizon; and this property, together with the Einstein field equations, places an extreme constraint on the material that generates the wormhole’s spacetime curvature: In the wormhole’s throat that material must possess a radial tension τ0 with the enormous magnitude τ0∼ (pressure at the center of the most massive of ne...",1988,0,1474,100,1,2,6,7,6,6,8,6,6,17
383eedafaec868d6168b13a956c7fc41d8bf2087,"A generalization of Einstein's gravitational theory is discussed in which the spin of matter as well as its mass plays a dynamical role. The spin of matter couples to a non-Riemannian structure in space-time, Cartan's torsion tensor. The theory which emerges from taking this coupling into account, the ${U}_{4}$ theory of gravitation, predicts, in addition to the usual infinite-range gravitational interaction medicated by the metric field, a new, very weak, spin contact interaction of gravitational origin. We summarize here all the available theoretical evidence that argues for admitting spin and torsion into a relativistic gravitational theory. Not least among this evidence is the demonstration that the ${U}_{4}$ theory arises as a local gauge theory for the Poincar\'e group in space-time. The deviations of the ${U}_{4}$ theory from standard general relativity are estimated, and the prospects for further theoretical development are assessed.",1976,146,1860,34,3,12,27,20,46,28,21,35,28,35
bf387e0f57f6f3b878ae09fed558a21b57ff35cb,"SummaryAn approach to shock waves, boundary surfaces and thin shells in general relativity is developed in which their histories are characterized in a purely geometrical way by the extrinsic curvatures of their imbeddings in space-time. There is some gain in simplicity and ease of application over previous treatments in that no mention of « admissible » or, indeed, any space-time co-ordinates is needed. The formalism is applied to a study of the dynamics of thin shells of dust.RiassuntoSi sviluppa un’approssimazione alle onde d’urto, superfici di contorno e strati sottili in relatività generale in cui le loro storie sono caratterizzate in modo puramente geometrico dalle curvature estrinseche delle loro giaciture nello spazio-tempo. Si ha un qualche guadagno in semplicità e facilità di applicazione rispetto alle trattazioni precedenti in quanto non è necessaria alcuna menzione di « ammissibilità » o, nella fattispecie di alcuna coordinata spazio-temporale. Si applica questo formalismo allo studio della dinamica degli strati sottili di polveri.",1966,3,1640,42,0,1,4,2,1,0,2,0,4,3
9284029d7c39a83d285d8a7cb859c5715228e1f3,,1984,0,1418,77,1,4,5,5,7,4,5,5,13,8
3561dcbcfcd9fe7b58a926d1da8515b1c88bb72c,Introduction 1. Elementary principles 2. The tensor calculus 3. The law of gravitation 4. Relativity mechanics 5. Curvature of space and time 6. Electricity 7. World geometry Supplementary notes Bibliography Index.,1924,0,1206,71,4,1,1,2,1,3,0,0,1,3
927285079badad9009fb579f1539cca0db4edba4,"In 1921, five years after the appearance of his comprehensive paper on general relativity and twelve years before he left Europe permanently to join the Institute for Advanced Study, Albert Einstein visited Princeton University, where he delivered the Stafford Little Lectures for that year. These four lectures constituted an overview of his then-controversial theory of relativity. Princeton University Press made the lectures available under the title ""The Meaning of Relativity,"" the first book by Einstein to be produced by an American publisher. As subsequent editions were brought out by the Press, Einstein included new material amplifying the theory. A revised version of the appendix ""Relativistic Theory of the Non-Symmetric Field,"" added to the posthumous edition of 1956, was Einstein's last scientific paper.",1946,0,1650,102,0,0,0,1,1,1,0,1,3,2
07ecafbabe5cb83c52725d554f8c42583f2bebfb,Research data on dominant work-related values patterns in 53 countries and regions are used to suggest how definitions of the quality of life are affected by national culture patterns.,1984,9,1241,102,0,1,5,2,2,6,4,4,6,8
5d9aacb0eea230539e9ea476e87c42e2fcf031e6,"FOREWORD ACKNOWLEDGEMENTS 1. Lorentzian Geometry 2. Special Relativity 3. General Relativity and the Einstein Equations 4. Schwarzschild Space-time and Black Holes 5. Cosmology 6. Local Cauchy Problem 7. Constraints 8. Other Hyperbolic-Elliptic systems 9. Relativistic Fluids 10. Kinetic Theory 11. Progressive Waves 12. Global Hyperbolicity and Causality 13. Singularities 14. Stationary Space-times and Black Holes 15. Global Existence Theorems, Asymptotically Euclidean Data 16. Global existence theorems, cosmological case APPENDICES I. Sobolev Spaces II. Elliptic Systems III. Second Order Quasidiagonal Systems IV. General Hyperbolic Systems V. Cauchy Kovalevski and Fuchs theorems VI. Conformal Methods VII. Kaluza Klein Formulas",2009,0,413,64,9,25,24,24,34,37,48,41,48,27
eef500f59b5d007955e2cf24ad797c8f8116845c,"This paper is divided into four parts. In part A, some general considerations about gravitational radiation are followed by a treatment of the scalar wave equation in the manner later to be applied to Einstein’s field equations. In part B, a co-ordinate system is specified which is suitable for investigation of outgoing gravitational waves from an isolated axi-symmetric reflexion-symmetric system . The metric is expanded in negative powers of a suitably defined radial co-ordinate r, and the vacuum field equations are investigated in detail. It is shown that the flow of information to infinity is controlled by a single function of two variables called the news function. Together with initial conditions specified on a light cone, this function fully defines the behaviour of the system . No constraints of any kind are encountered. In part C, the transformations leaving the metric in the chosen form are determined. An investigation of the corresponding transformations in Minkowski space suggests that no generality is lost by assuming that the transformations, like the metric, may be expanded in negative powers of r. In part D, the mass of the system is defined in a way which in static metrics agrees with the usual definition. The principal result of the paper is then deduced, namely, that the mass of a system is constant if and only if there is no news; if there is news, the mass decreases monotonically so long as it continues. The linear approximation is next discussed, chiefly for its heuristic value, and employed in the analysis of a receiver for gravitational waves. Sandwich waves are constructed, and certain non-radiative but non-static solutions are discussed. This part concludes with a tentative classification of time-dependent solutions of the types considered.",1962,6,1459,50,0,4,1,6,6,3,12,3,8,6
3c5c9c3cf20e888d30673b8b73cbedf9dc0906db,"I show that it is possible to formulate the Relativity postulates in a way that does not lead to inconsistencies in the case of spacetimes whose short-distance structure is governed by an observer-independent length scale. The consistency of these postulates proves incorrect the expectation that modifications of the rules of kinematics involving the Planck length would necessarily require the introduction of a preferred class of inertial observers. In particular, it is possible for every inertial observer to agree on physical laws supporting deformed dispersion relations of the type E2-c2 p2-c4m2 + f(E, p, m; Lp) =0, at least for certain types of f.",2000,51,910,23,0,5,32,39,39,46,38,39,43,49
e1b96d80e424f4a1976f4abd07eabef6fa52afc7,"The first use of microalgae by humans dates back 2000 years to the Chinese, who used Nostoc to survive during famine. However, microalgal biotechnology only really began to develop in the middle of the last century. Nowadays, there are numerous commercial applications of microalgae. For example, (i) microalgae can be used to enhance the nutritional value of food and animal feed owing to their chemical composition, (ii) they play a crucial role in aquaculture and (iii) they can be incorporated into cosmetics. Moreover, they are cultivated as a source of highly valuable molecules. For example, polyunsaturated fatty acid oils are added to infant formulas and nutritional supplements and pigments are important as natural dyes. Stable isotope biochemicals help in structural determination and metabolic studies. Future research should focus on the improvement of production systems and the genetic modification of strains. Microalgal products would in that way become even more diversified and economically competitive.",2006,105,3421,171,1,15,45,71,134,161,217,290,284,361
8a7f8f9227f8b843fbbe6c0c991670bbc8f9832f,"Using bank-level data for 80 countries in the year's 1988-95, this article shows that differences in interest margins and bank profitability reflect a variety of determinants: bank characteristics, macroeconomic conditions, explicit and implicit bank taxation, deposit insurance regulation, overall financial structure, and underlying legal and institutional indicators. A larger ratio of bank assets to gross domestic product and a lower market concentration ratio lead to lower margins and profits, controlling for differences in bank activity, leverage, and the macroeconomic environment. Foreign banks have higher margins and profits than domestic banks in developing countries, while the opposite holds in industrial countries. Also, there is evidence that the corporate tax burden is fully passed onto bank customers, while higher reserve requirements are not, especially in developing countries.",1999,53,2226,211,10,6,26,42,32,35,51,51,49,54
1ffbf5ada9068d39951c01689978ca5adae9eed2,Entrepreneurship has been the engine propelling much of the growth of the business sector as well as a driving force behind the rapid expansion of the social sector. This article offers a comparative analysis of commercial and social entrepreneurship using a prevailing analytical model from commercial entrepreneurship. The analysis highlights key similarities and differences between these two forms of entrepreneurship and presents a framework on how to approach the social entrepreneurial process more systematically and effectively. We explore the implications of this analysis of social entrepreneurship for both practitioners and researchers.,2006,30,2040,160,8,13,31,48,81,104,117,133,160,163
3e2ab599c6a7fb19db5e3ccd8c2aaf5ea61678ba,"Many microorganisms, especially bacteria, produce biosurfactants when grown on water-immiscible substrates. Biosurfactants are more effective, selective, environmentally friendly, and stable than many synthetic surfactants. Most common biosurfactants are glycolipids in which carbohydrates are attached to a long-chain aliphatic acid, while others, like lipopeptides, lipoproteins, and heteropolysaccharides, are more complex. Rapid and reliable methods for screening and selection of biosurfactant-producing microorganisms and evaluation of their activity have been developed. Genes involved in rhamnolipid synthesis (rhlAB) and regulation (rhlI and rhlR) in Pseudomonas aeruginosa are characterized, and expression of rhlAB in heterologous hosts is discussed. Genes for surfactin production (sfp, srfA, and comA) in Bacillus spp. are also characterized. Fermentative production of biosurfactants depends primarily on the microbial strain, source of carbon and nitrogen, pH, temperature, and concentration of oxygen and metal ions. Addition of water-immiscible substrates to media and nitrogen and iron limitations in the media result in an overproduction of some biosurfactants. Other important advances are the use of water-soluble substrates and agroindustrial wastes for production, development of continuous recovery processes, and production through biotransformation. Commercialization of biosurfactants in the cosmetic, food, health care, pulp- and paper-processing, coal, ceramic, and metal industries has been proposed. However, the most promising applications are cleaning of oil-contaminated tankers, oil spill management, transportation of heavy crude oil, enhanced oil recovery, recovery of crude oil from sludge, and bioremediation of sites contaminated with hydrocarbons, heavy metals, and other pollutants. Perspectives for future research and applications are also discussed.",1997,229,1403,95,1,8,12,17,20,26,24,36,34,36
f42239347b34560fdedad44f302e52cb7144b1e4,"Abstract Surfactants are surface-active compounds capable of reducing surface and interfacial tension at the interfaces between liquids, solids and gases, thereby allowing them to mix or disperse readily as emulsions in water or other liquids. The enormous market demand for surfactants is currently met by numerous synthetic, mainly petroleum-based, chemical surfactants. These compounds are usually toxic to the environment and non-biodegradable. They may bio-accumulate and their production, processes and by-products can be environmentally hazardous. Tightening environmental regulations and increasing awareness for the need to protect the ecosystem have effectively resulted in an increasing interest in biosurfactants as possible alternatives to chemical surfactants. Biosurfactants are amphiphilic compounds of microbial origin with considerable potential in commercial applications within various industries. They have advantages over their chemical counterparts in biodegradability and effectiveness at extreme temperature or pH and in having lower toxicity. Biosurfactants are beginning to acquire a status as potential performance-effective molecules in various fields. At present biosurfactants are mainly used in studies on enhanced oil recovery and hydrocarbon bioremediation. The solubilization and emulsification of toxic chemicals by biosurfactants have also been reported. Biosurfactants also have potential applications in agriculture, cosmetics, pharmaceuticals, detergents, personal care products, food processing, textile manufacturing, laundry supplies, metal treatment and processing, pulp and paper processing and paint industries. Their uses and potential commercial applications in these fields are reviewed.",2000,187,1459,31,0,5,21,22,28,32,38,56,79,67
62cf465bbf50dae7906de5492d721452086c9744,"Abstract. In response to the rapidly growing field of proteomics, the use of recombinant proteins has increased greatly in recent years. Recombinant hybrids containing a polypeptide fusion partner, termed affinity tag, to facilitate the purification of the target polypeptides are widely used. Many different proteins, domains, or peptides can be fused with the target protein. The advantages of using fusion proteins to facilitate purification and detection of recombinant proteins are well-recognized. Nevertheless, it is difficult to choose the right purification system for a specific protein of interest. This review gives an overview of the most frequently used and interesting systems: Arg-tag, calmodulin-binding peptide, cellulose-binding domain, DsbA, c-myc-tag, glutathione S-transferase, FLAG-tag, HAT-tag, His-tag, maltose-binding protein, NusA, S-tag, SBP-tag, Strep-tag, and thioredoxin.",2002,174,1307,53,2,7,38,50,59,58,75,77,97,81
1204f9d5f1a1ad3f78d44cd06712e033305f9a96,"The commercial culture of microalgae is now over 30 years old with the main microalgal species grown being Chlorella and Spirulina for health food, Dunaliella salina for β-carotene, Haematococcus pluvialis for astaxanthin and several species for aquaculture. The culture systems currently used to grow these algae are generally fairly unsophisticated. For example, Dunaliella salina is cultured in large (up to approx. 250 ha) shallow open-air ponds with no artificial mixing. Similarly, Chlorella and Spirulina also are grown outdoors in either paddle-wheel mixed ponds or circular ponds with a rotating mixing arm of up to about 1 ha in area per pond. The production of microalgae for aquaculture is generally on a much smaller scale, and in many cases is carried out indoors in 20-40 1 carboys or in large plastic bags of up to approximately 1000 1 in volume. More recently, a helical tubular photobioreactor system, the BIOCOIL™, has been developed which allows these algae to be grown reliably outdoors at high cell densities in semi-continuous culture. Other closed photobioreactors such as fiat panels are also being developed. The main problem facing the commercialisation of new microalgae and microalgal products is the need for closed culture systems and the fact that these are very capital intensive. The high cost of microalgal culture systems relates to the need for light and the relatively slow growth rate of the algae. Although this problem has been avoided in some instances by growing the algae heterotrophically, not all algae or algal products can be produced this way.",1999,32,1184,73,0,0,13,9,16,8,14,22,16,39
b60577c146c6d3492d8200dafe462f89864f060d,"Summary The foregoing examples illustrate how the theory developed here may be employed to make estimates concerning the condition of a commercial marine fishery. The examples employed, although having perhaps as complete information as any available for this purpose, leave something to be desired. In particular, in both of these examples, very little or no data are available concerning intensity of fishing and abundance for the early period of development of the fishery, well before the maximum catches are reached. A great deal of precision would be added to the estimate if such information were available. We may emphasize, therefore, the desirability of obtaining detailed information on the total catch and catch-per-unit-of-effort from as early in the development of a commercial fishery as may be possible. Measurements of fishing mortality rates at more than one level of population would also be desirable, since they would make possible verification of the adequacy of the form of equation (13a) for describing the changes in population under the joint influences of growth and fishing. In order to apply the theory developed here to the tropical tuna fishery, it will be necessary to compile statistics of catch, abundance and intensity of fishing over a considerable series of years, beginning as early in the history of the fishery as possible. This task is well under way. It will also be necessary to obtain some estimate of the rate of fishing mortality, or to devise some other means of estimating the constant k 2 . Estimation of fishing mortality from tagging promises to be a difficult problem for the tunas. Exploration of other means of obtaining the relationship between U and P appears, therefore, to constitute an important line of investigation.",1991,8,1381,84,16,16,8,12,19,18,15,25,24,27
7c854f6ad964decd450acefbc52ea96b7570dce7,"A method has been described for the isolation of DNA from micro-organisms which yields stable, biologically active, highly polymerized preparations relatively free from protein and RNA. Alternative methods of cell disruption and DNA isolation have been described and compared. DNA capable of transforming homologous strains has been used to test various steps in the procedure and preparations have been obtained possessing high specific activities. Representative samples have been characterized for their thermal stability and sedimentation behaviour.",1961,22,9927,226,5,37,58,86,111,150,154,191,175,211
37126de1bbf70ba0349d8b72af8bb9357c3b12cf,"Today's surface ocean is saturated with respect to calcium carbonate, but increasing atmospheric carbon dioxide concentrations are reducing ocean pH and carbonate ion concentrations, and thus the level of calcium carbonate saturation. Experimental evidence suggests that if these trends continue, key marine organisms—such as corals and some plankton—will have difficulty maintaining their external calcium carbonate skeletons. Here we use 13 models of the ocean–carbon cycle to assess calcium carbonate saturation under the IS92a ‘business-as-usual’ scenario for future emissions of anthropogenic carbon dioxide. In our projections, Southern Ocean surface waters will begin to become undersaturated with respect to aragonite, a metastable form of calcium carbonate, by the year 2050. By 2100, this undersaturation could extend throughout the entire Southern Ocean and into the subarctic Pacific Ocean. When live pteropods were exposed to our predicted level of undersaturation during a two-day shipboard experiment, their aragonite shells showed notable dissolution. Our findings indicate that conditions detrimental to high-latitude ecosystems could develop within decades, not centuries as suggested previously.",2005,63,4073,358,11,62,108,153,218,253,259,337,360,335
f32b0b1ea1e0282ab5c0f8ca23c295f3c3b69198,"Molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes (particularly so among microorganisms). Consequently, the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level. Molecular comparisons show that life on this planet divides into three primary groupings, commonly known as the eubacteria, the archaebacteria, and the eukaryotes. The three are very dissimilar, the differences that separate them being of a more profound nature than the differences that separate typical kingdoms, such as animals and plants. Unfortunately, neither of the conventionally accepted views of the natural relationships among living systems--i.e., the five-kingdom taxonomy or the eukaryote-prokaryote dichotomy--reflects this primary tripartite division of the living world. To remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a ""domain."" Life on this planet would then be seen as comprising three domains, the Bacteria, the Archaea, and the Eucarya, each containing two or more kingdoms. (The Eucarya, for example, contain Animalia, Plantae, Fungi, and a number of others yet to be defined). Although taxonomic structure within the Bacteria and Eucarya is not treated herein, Archaea is formally subdivided into the two kingdoms Euryarchaeota (encompassing the methanogens and their phenotypically diverse relatives) and Crenarchaeota (comprising the relatively tight clustering of extremely thermophilic archaebacteria, whose general phenotype appears to resemble most the ancestral phenotype of the Archaea.",1990,50,5773,374,4,57,89,118,106,112,143,127,151,146
05509d207766c7bac327e55ddca03212c48faca0,"Multicellular organisms live, by and large, harmoniously with microbes. The cornea of the eye of an animal is almost always free of signs of infection. The insect flourishes without lymphocytes or antibodies. A plant seed germinates successfully in the midst of soil microbes. How is this accomplished? Both animals and plants possess potent, broad-spectrum antimicrobial peptides, which they use to fend off a wide range of microbes, including bacteria, fungi, viruses and protozoa. What sorts of molecules are they? How are they employed by animals in their defence? As our need for new antibiotics becomes more pressing, could we design anti-infective drugs based on the design principles these molecules teach us?",2002,101,7070,271,57,133,191,218,263,241,295,347,382,416
6dd84bd0edbafc92d03c57d6bea2d4e07093ea80,,1963,0,4356,459,0,4,4,5,3,8,5,12,9,11
816d9b199d07340238fd263a6369271c556b6ca8,"Interactions between organisms are a major determinant of the distribution and abundance of species. Ecology textbooks (e.g., Ricklefs 1984, Krebs 1985, Begon et al. 1990) summarise these important interactions as intra- and interspecific competition for abiotic and biotic resources, predation, parasitism and mutualism. Conspicuously lacking from the list of key processes in most text books is the role that many organisms play in the creation, modification and maintenance of habitats. These activities do not involve direct trophic interactions between species, but they are nevertheless important and common. The ecological literature is rich in examples of habitat modification by organisms, some of which have been extensively studied (e.g. Thayer 1979, Naiman et al. 1988).",1994,89,5139,165,2,2,14,27,39,61,56,63,64,92
af47ec845f1b60e2117bb1a093de224429786e8c,"Functional partnerships between proteins are at the core of complex cellular phenotypes, and the networks formed by interacting proteins provide researchers with crucial scaffolds for modeling, data reduction and annotation. STRING is a database and web resource dedicated to protein–protein interactions, including both physical and functional interactions. It weights and integrates information from numerous sources, including experimental repositories, computational prediction methods and public text collections, thus acting as a meta-database that maps all interaction evidence onto a common set of genomes and proteins. The most important new developments in STRING 8 over previous releases include a URL-based programming interface, which can be used to query STRING from other resources, improved interaction prediction via genomic neighborhood in prokaryotes, and the inclusion of protein structures. Version 8.0 of STRING covers about 2.5 million proteins from 630 organisms, providing the most comprehensive view on protein–protein interactions currently available. STRING can be reached at http://string-db.org/.",2008,39,2214,201,0,96,239,282,225,195,179,201,221,124
36e2c49a23cb62bc5f47c82603373970e975b87d,"The rhizosphere encompasses the millimeters of soil surrounding a plant root where complex biological and ecological processes occur. This review describes recent advances in elucidating the role of root exudates in interactions between plant roots and other plants, microbes, and nematodes present in the rhizosphere. Evidence indicating that root exudates may take part in the signaling events that initiate the execution of these interactions is also presented. Various positive and negative plant-plant and plant-microbe interactions are highlighted and described from the molecular to the ecosystem scale. Furthermore, methodologies to address these interactions under laboratory conditions are presented.",2006,209,3163,137,9,38,63,86,108,122,169,242,244,261
dddb532a1fa61531bf24b3a4bd0f9a81b747aa43,"Physical ecosystem engineers are organisms that directly or indirectly control the availability of resources to other organisms by causing physical state changes in biotic or abiotic materials. Physical ecosystem engineering by organisms is the physical modification, maintenance, or creation of habitats. Ecological effects of engineers on many other species occur in virtually all ecosystems because the physical state changes directly create nonfood resources such as living space, directly control abiotic resources, and indirectly modulate abiotic forces that, in turn, affect resource use by other organisms. Trophic interactions and resource competition do not constitute engineering. Engineering can have significant or trivial effects on other species, may involve the physical structure of an organism (like a tree) or structures made by an organism (like a beaver dam), and can, but does not invariably, have feedback effects on the engineer. We argue that engineering has both negative and positive effects on species richness and abundances at small scales, but the net effects are probably positive at larger scales encompassing engineered and nonengineered environments in ecological and evolutionary space and time. Models of the population dynamics of engineers suggest that the engineer/habitat equilibrium is often, but not always, locally stable and may show long-term cycles, with potential ramifications for community and ecosystem stability. As yet, data adequate to parameterize such a model do not exist for any engineer species. Because engineers control flows of energy and materials but do not have to participate in these flows, energy, mass, and stoichiometry do not appear to be useful in predicting which engineers will have big effects. Empirical observations suggest some potential generalizations about which species will be important engineers in which ecosystems. We point out some of the obvious, and not so obvious, ways in which engineering and trophic relations interact, and we call for greater research on physical ecosystem engineers, their impacts, and their interface with trophic relations.",1997,69,2035,73,2,6,26,28,34,41,55,59,45,101
2105cf67e643f73b0812ddd3252d23b812330fb3,"Investigating diversity in asexual organisms using molecular markers involves the assignment of individuals to clonal lineages and the subsequent analysis of clonal diversity. Assignment is possible using a distance matrix in combination with a user-specified threshold, defined as the maximum distance between two individuals that are considered to belong to the same clonal lineage. Analysis of clonal diversity requires tests for differences in diversity and clonal composition between populations. We developed two programs, GENOTYPE and GENODIVE for such analyses of clonal diversity in asexually reproducing organisms. Additionally, genotype can be used for detecting genotyping errors in studies of sexual organisms.",2004,9,1704,73,0,3,10,11,16,30,33,60,101,124
279f02e2e821df40bb7dea6f9afc1bd0e2d530e4,"The helix-loop-helix (HLH) family of transcriptional regulatory proteins are key players in a wide array of developmental processes. Over 240 HLH proteins have been identified to date in organisms ranging from the yeast Saccharomyces cerevisiae to humans (6). Studies in Xenopus laevis, Drosophila melanogaster, and mice have convincingly demonstrated that HLH proteins are intimately involved in developmental events such as cellular differentiation, lineage commitment, and sex determination. In yeast, HLH proteins regulate several important metabolic pathways, including phosphate uptake and phospholipid biosynthesis (19, 67, 112). In multicellular organisms, HLH factors are required for a multitude of important developmental processes, including neurogenesis, myogenesis, hematopoiesis, and pancreatic development (12, 86, 127, 179). The purpose of this review is to examine the structure and functional properties of HLH proteins. 
 
 
 
E-box sites: elements mediating cell-type-specific gene transcription. 
Gene transcription of the immunoglobulin heavy-chain (IgH) gene has long been known to be regulated, in part, by a cis-acting DNA element known as the IgH intronic enhancer (109, 156). By in vivo methylation protection assays, a number of sites were identified in both the IgH and the kappa light-chain gene enhancers which were specifically protected in B cells but not in nonlymphoid cells (41). These elements shared a signature motif which consisted of the core hexanucleotide sequence, CANNTG, and were subsequently dubbed E boxes (41). A total of five E-box elements are present in the IgH gene enhancer: μE1, μE2, μE3, μE4, and μE5. The Ig kappa enhancer also contains three cannonical E boxes, designated κE1, κE2, and κE3. E-box sites have been subsequently found in B-cell-specific promoter and enhancer elements, including a subset of Ig light-chain gene promoters, the IgH and Ig light-chain 3′ enhancers, and, more recently, the λ5 promoter (110, 118, 156). 
 
E-box elements have also been identified in promoter and enhancer elements that regulate muscle-, neuron-, and pancreas-specific gene expression. For example, in muscle, the muscle creatine kinase gene, acetylcholine receptor genes α and δ, and the myosin light-chain gene all require E-box elements for full activity (27, 51, 85). A number of genes whose expression is limited to the pancreas also require E-box sites for proper expression. The insulin and somatostatin genes, for example, contain E-box sites that, when multimerized, are sufficient to regulate pancreatic β-cell-specific gene expression (168). More recently, E-box regulatory sites have been identified in a number of neuron-specific genes, including the opsin, hippocalcin, beta 2 subunit of the neuronal nicotinic acetylcholine receptor, and muscarinic acetylcholine receptor genes (1, 21, 52, 125). 
 
 
 
 
E-box sites: cognate recognition sequence for HLH proteins. 
Two proteins, termed E12 and E47, were originally identified as binding to the κE2/μE5 site (65, 102). They have a region of homology with the Drosophila Daughterless protein, the myogenic differentiation factor MyoD, members of the achaete-scute gene complex, and the Myc family of transcription factors (102). This stretch of conserved residues, known as the Myc homology region, appeared to be critical for the DNA binding properties of E12 and E47 (102). The E12 and E47 proteins, which differ only within this Myc homology region, arise by alternative splicing of the E2A gene (157). This conserved sequence, which was modeled as two amphipathic alpha helices separated by a flexible loop structure, was named the HLH motif and shown to function as a dimerization domain. 
 
 
 
 
The HLH structure. 
The solution structure of the basic HLH (bHLH)-leucine zipper (LZ) factor Max first confirmed the existence of the HLH motif (44). Subsequently, the three-dimensional structure of the E47 bHLH polypeptide bound to its E-box recognition site, CACCTG, has been solved at 2.8-Å resolution (38). A number of interesting features were revealed from analysis of the E47 crystal structure. The E47 dimer forms a parallel, four-helix bundle which allows the basic region to contact the major groove (38). In addition to the basic region, residues in the loop and helix 2 also make contact with DNA (38). Stable interaction of the HLH domain is favored by van der Waals interactions between conserved hydrophobic residues (38). The E47 dimer is centered over the E box, with each monomer interacting with either a CAC or CAG half-site. A glutamate present in the basic region of each subunit makes contact with the cytosine and adenine bases in the E-box half-site. An adjacent arginine residue stabilizes the position of the glutamate by direct interaction with these nucleotides and additionally the phosphodiester backbone. Both the glutamate and the arginine residues are conserved in most bHLH proteins, consistent with a role in specific DNA binding (6, 38, 102). 
 
 
 
 
Classification of the HLH proteins. 
Owing to the large number of HLH proteins that have been described, a classification scheme that was based upon tissue distribution, dimerization capabilities, and DNA-binding specificities was devised (Fig. ​(Fig.1)1) (101). Class I HLH proteins, also known as the E proteins, include E12, E47, HEB, E2-2, and Daughterless. These proteins are expressed in many tissues and capable of forming either homo- or heterodimers (103). The DNA-binding specificity of class I proteins is limited to the E-box site. Class II HLH proteins, which include members such as MyoD, myogenin, Atonal, NeuroD/BETA2, and the achaete-scute complex, show a tissue-restricted pattern of expression. With few exceptions, they are incapable of forming homodimers and preferentially heterodimerize with the E proteins. Class I-class II heterodimers can bind both canonical and noncanonical E-box sites (103). Class III HLH proteins include the Myc family of transcription factors, TFE3, SREBP-1, and the microphthalmia-associated transcription factor, Mi. Proteins of this class contain an LZ adjacent to the HLH motif (66, 177). Class IV HLH proteins define a family of molecules, including Mad, Max, and Mxi, that are capable of dimerizing with the Myc proteins or with one another (7, 22, 174). A group of HLH proteins that lack a basic region, including Id and emc, define the class V HLH proteins (18, 39, 47). Class V members are negative regulators of class I and class II HLH proteins (18, 39, 47). Class VI HLH proteins have as their defining feature a proline in their basic region. This group includes the Drosophila proteins Hairy and Enhancer of split (76, 141). Finally, the class VII HLH proteins are categorized by the presence of the bHLH-PAS domain and include members such as the aromatic hydrocarbon receptor (AHR), the AHR nuclear-translocator (Arnt), hypoxia-inducible factor 1α, and the Drosophila Single-minded and Period proteins (34). 
 
 
 
FIG. 1 
 
Multiple sequence alignment and classification of some representative members of the HLH family of transcription factors. Shown is a dendrogram created by aligning the sequences of the indicated HLH proteins by the Clustal W algorithm (160). 
 
 
 
Recently, another classification method of HLH proteins has been described (6). Based on the amino acid sequences of 242 HLH proteins, a phylogenetic tree was created to group family members according to evolutionary relationships (6). Four major groups, A through D, which comprise more than 24 protein families were identified (6). The groupings were based upon DNA-binding specificity as well as conservation of amino acids at certain positions (6). As the number of HLH proteins continues to grow, this evolutionary or “natural” classification may provide a more accurate and convenient means of categorization.",2000,193,1642,121,24,58,74,77,104,71,64,70,83,88
b39c51b5e5f106430dfa6a234fcc671b7782e45e,"In the last 10 years, a large family of secreted signaling molecules has been discovered that appear to mediate many key events in normal growth and development. The family is known as the TGF-p superfamily (Massague 1990), a name taken from the first member of the family to be isolated (transforming growth factor-^l). This name is somewhat misleading, because TGF-p 1 has a large number of effects in different systems (Spom and Roberts 1992). It actually inhibits the proliferation of many different cell lines, and its original ""transforming"" activity may be due to secondary effects on matrix pro­ duction and synthesis of other growth factors (Moses et al. 1990). The two dozen other members of the TGF-p superfamily have a remarkable range of activities. In Diosophila, a TGF-p-related gene is required for dorsoventral axis formation in early embryos, communication between tissue layers in gut development, and correct proximal distal patterning of adult appendages. In Xenopus, a TGF-p-related gene is expressed specifically at one end of fertilized eggs and may function in early signaling events that lay out the basic body plan. In mammals, TGF-p-related molecules have been found that control sexual development, pituitary hormone production, and the creation of bones and cartilage. The recognition of TGF-p superfamily members in many different organ­ isms and contexts provides one of the major unifying themes in recent molecular studies of animal growth and development. The rough outlines of the TGF-p family were first rec­ ognized in the 1980s. Since that time, a number of ex­ cellent reviews have appeared that summarize the prop­ erties of different family members (Ying 1989; Massague 1990; Lyons et al. 1991; Spom and Roberts 1992). Here, I will focus on four areas that have seen major progress in the last 3 years: structural characterization of the signal­ ing molecule, isolation of new family members, cloning of receptor molecules, and new genetic tests of the func­ tions of these factors in different organisms.",1994,106,1982,45,31,91,159,151,161,143,121,109,86,71
3fa772c422d9bd85e451822e4fcc58c98d5c481d,"Reactive oxygen species (ROS) have multifaceted roles in the orchestration of plant gene expression and gene-product regulation. Cellular redox homeostasis is considered to be an ""integrator"" of information from metabolism and the environment controlling plant growth and acclimation responses, as well as cell suicide events. The different ROS forms influence gene expression in specific and sometimes antagonistic ways. Low molecular antioxidants (e.g., ascorbate, glutathione) serve not only to limit the lifetime of the ROS signals but also to participate in an extensive range of other redox signaling and regulatory functions. In contrast to the low molecular weight antioxidants, the ""redox"" states of components involved in photosynthesis such as plastoquinone show rapid and often transient shifts in response to changes in light and other environmental signals. Whereas both types of ""redox regulation"" are intimately linked through the thioredoxin, peroxiredoxin, and pyridine nucleotide pools, they also act independently of each other to achieve overall energy balance between energy-producing and energy-utilizing pathways. This review focuses on current knowledge of the pathways of redox regulation, with discussion of the somewhat juxtaposed hypotheses of ""oxidative damage"" versus ""oxidative signaling,"" within the wider context of physiological function, from plant cell biology to potential applications.",2009,440,1176,70,16,73,98,122,126,107,109,109,81,90
5f74244d62bdc42a8ef6606495508e40d647366f,"The potential of oxygen free radicals and other reactive oxygen species (ROS) to damage tissues and cellular components, called oxidative stress, in biological systems has become a topic of significant interest for environmental toxicology studies. The balance between prooxidant endogenous and exogenous factors (i.e., environmental pollutants) and antioxidant defenses (enzymatic and nonenzymatic) in biological systems can be used to assess toxic effects under stressful environmental conditions, especially oxidative damage induced by different classes of chemical pollutants. The role of these antioxidant systems and their sensitivity can be of great importance in environmental toxicology studies. In the past decade, numerous studies on the effects of oxidative stress caused by some environmental pollutants in terrestrial and aquatic species were published. Increased numbers of agricultural and industrial chemicals are entering the aquatic environment and being taken up into tissues of aquatic organisms. Transition metals, polycyclic aromatic hydrocarbons, organochlorine and organophosphate pesticides, polychlorinated biphenyls, dioxins, and other xenobiotics play important roles in the mechanistic aspects of oxidative damage. Such a diverse array of pollutants stimulate a variety of toxicity mechanisms, such as oxidative damage to membrane lipids, DNA, and proteins and changes to antioxidant enzymes. Although there are considerable gaps in our knowledge of cellular damage, response mechanisms, repair processes, and disease etiology in biological systems, free radical reactions and the production of toxic ROS are known to be responsible for a variety of oxidative damages leading to adverse health effects and diseases. In the past decade, mammalian species were used as models for the study of molecular biomarkers of oxidative stress caused by environmental pollutants to elucidate the mechanisms underlying cellular oxidative damage and to study the adverse effects of some environmental pollutants with oxidative potential in chronic exposure and/or sublethal concentrations. This review summarizes current knowledge and advances in the understanding of such oxidative processes in biological systems. This knowledge is extended to specific applications in aquatic organisms because of their sensitivity to oxidative pollutants, their filtration capacity, and their potential for environmental toxicology studies.",2006,164,1364,71,7,10,33,40,63,71,72,96,104,128
175152bdf0eeeab0cc4fa457784dd8ebdda132a6,"The paper that follows is based on notes taken by Dr. R. S. Pierce on five lectures given by the author at the California Institute of Technology in January 1952. They have been revised by the author but they reflect, apart from minor changes, the lectures as they were delivered. The subject-matter, as the title suggests, is the role of error in logics, or in the physical implementation of logics—–in automatasynthesis. Error is viewed, therefore, not as an extraneous and misdirected or misdirecting accident, but as an essential part of the process under consideration—–its importance in the synthesis of automata being fully comparable to that of the factor which is normally considered, the intended and correct logical structure. Our present treatment of error is unsatisfactory and ad hoc. It is the author’s conviction, voiced over many years, that error should be treated by thermodynamical methods, and be the subject of a thermodynamical theory, as information has been, by the work of L. Szilard and C. E. Shannon (cf. 5.2). The present treatment falls far short of achieving this, but it assembles, it is hoped, some of the building materials, which will have to enter into the final structure. The author wants to express his thanks to K. A. Brueckner and M. Gell-Mann, then at the University of Illinois, to whose discussions in 1951 he owes some important stimuli on this subject; to Dr. R. S. Pierce at the California Institute of Technology, on whose excellent notes this exposition is based; and to the California Institute of Technology, whose invitation to deliver these lectures combined with the very warm reception by the audience, caused him to write this paper in its present form, and whose cooperation in connection with the present publication is much appreciated.",1956,7,2192,147,0,2,8,4,2,9,7,7,2,4
9932ff03877ee134b4fa753c7555ae6281b6ad3e,"Ocean-going ships carry, as ballast, seawater that is taken on in port and released at subsequent ports of call. Plankton samples from Japanese ballast water released in Oregon contained 367 taxa. Most taxa with a planktonic phase in their life cycle were found in ballast water, as were all major marine habitat and trophic groups. Transport of entire coastal planktonic assemblages across oceanic barriers to similar habitats renders bays, estuaries, and inland waters among the most threatened ecosystems in the world. Presence of taxonomically difficult or inconspicuous taxa in these samples suggests that ballast water invasions are already pervasive.",1993,29,1626,42,4,11,22,31,25,47,37,63,52,60
1666fad6fbd651cada1d4ad8eb5831d7f5fbafb0,"Fractal-like networks effectively endow life with an additional fourth spatial dimension. This is the origin of quarter-power scaling that is so pervasive in biology. Organisms have evolved hierarchical branching networks that terminate in size-invariant units, such as capillaries, leaves, mitochondria, and oxidase molecules. Natural selection has tended to maximize both metabolic capacity, by maximizing the scaling of exchange surface areas, and internal efficiency, by minimizing the scaling of transport distances and times. These design principles are independent of detailed dynamics and explicit models and should apply to virtually all organisms.",1999,6,1428,86,3,28,33,50,43,71,66,89,60,61
c18600920e1b9bfd04a6c7baa82c0ad239aea803,,2001,106,1477,80,0,14,12,20,31,42,43,63,72,86
9233c0090a4a0f804f4a294775d395bb55400e2d,"Many ecosystem services are delivered by organisms that depend on habitats that are segregated spatially or temporally from the location where services are provided. Management of mobile organisms contributing to ecosystem services requires consideration not only of the local scale where services are delivered, but also the distribution of resources at the landscape scale, and the foraging ranges and dispersal movements of the mobile agents. We develop a conceptual model for exploring how one such mobile-agent-based ecosystem service (MABES), pollination, is affected by land-use change, and then generalize the model to other MABES. The model includes interactions and feedbacks among policies affecting land use, market forces and the biology of the organisms involved. Animal-mediated pollination contributes to the production of goods of value to humans such as crops; it also bolsters reproduction of wild plants on which other services or service-providing organisms depend. About one-third of crop production depends on animal pollinators, while 60-90% of plant species require an animal pollinator. The sensitivity of mobile organisms to ecological factors that operate across spatial scales makes the services provided by a given community of mobile agents highly contextual. Services vary, depending on the spatial and temporal distribution of resources surrounding the site, and on biotic interactions occurring locally, such as competition among pollinators for resources, and among plants for pollinators. The value of the resulting goods or services may feed back via market-based forces to influence land-use policies, which in turn influence land management practices that alter local habitat conditions and landscape structure. Developing conceptual models for MABES aids in identifying knowledge gaps, determining research priorities, and targeting interventions that can be applied in an adaptive management context.",2007,199,1172,51,7,36,59,52,62,80,98,107,85,95
04b68c1b6ee148d7b1332198d03b571728792bc8,"We have carried out detailed statistical analyses of integral membrane proteins of the helix‐bundle class from eubacterial, archaean, and eukaryotic organisms for which genome‐wide sequence data are available. Twenty to 30% of all ORFs are predicted to encode membrane proteins, with the larger genomes containing a higher fraction than the smaller ones. Although there is a general tendency that proteins with a smaller number of transmembrane segments are more prevalent than those with many, uni‐cellular organisms appear to prefer proteins with 6 and 12 transmembrane segments, whereas Caenorhabditis elegansandHomo sapienshave a slight preference for proteins with seven transmembrane segments. In all organisms, there is a tendency that membrane proteins either have manytransmembrane segments with short connecting loops or few transmembrane segments with large extra‐membraneous domains. Membrane proteins from all organisms studied, except possibly the archaeon Methanococcus jannaschii, follow the so‐called “positive‐inside” rule; i.e., they tend to have a higher frequency of positively charged residues in cytoplasmic than in extra‐cytoplasmic segments.",1998,45,1444,26,7,20,35,32,34,31,42,48,72,79
42eb47797fe541dd1e9ebe46f31df3cbda9e66ec,"Searches for genes involved in the ageing process have been made in genetically tractable model organisms such as yeast, the nematode Caenorhabditis elegans , Drosophila melanogaster fruitflies and mice. These genetic studies have established that ageing is indeed regulated by specific genes, and have allowed an analysis of the pathways involved, linking physiology, signal transduction and gene regulation. Intriguing similarities in the phenotypes of many of these mutants indicate that the mutations may also perturb regulatory systems that control ageing in higher organisms.",2000,104,1290,31,0,42,108,86,100,93,65,65,62,71
830fdb7b17cfa7b96693c41f04416c05ccbf34e6,"A full description of a protein's function requires knowledge of all partner proteins with which it specifically associates. From a functional perspective, ‘association’ can mean direct physical binding, but can also mean indirect interaction such as participation in the same metabolic pathway or cellular process. Currently, information about protein association is scattered over a wide variety of resources and model organisms. STRING aims to simplify access to this information by providing a comprehensive, yet quality-controlled collection of protein–protein associations for a large number of organisms. The associations are derived from high-throughput experimental data, from the mining of databases and literature, and from predictions based on genomic context analysis. STRING integrates and ranks these associations by benchmarking them against a common reference set, and presents evidence in a consistent and intuitive web interface. Importantly, the associations are extended beyond the organism in which they were originally described, by automatic transfer to orthologous protein pairs in other organisms, where applicable. STRING currently holds 730 000 proteins in 180 fully sequenced organisms, and is available at http://string.embl.de/.",2004,22,1220,64,3,37,63,80,47,43,55,57,60,64
9c82f82047b5dd52fda35fbca3622cbe11ce3b8e,"Large proteins are usually expressed in a eukaryotic system while smaller ones are expressed in prokaryotic systems. For proteins that require glycosylation, mammalian cells, fungi or the baculovirus system is chosen. The least expensive, easiest and quickest expression of proteins can be carried out in Escherichia coli. However, this bacterium cannot express very large proteins. Also, for S-S rich proteins, and proteins that require post-translational modifications, E. coli is not the system of choice. The two most utilized yeasts are Saccharomyces cerevisiae and Pichia pastoris. Yeasts can produce high yields of proteins at low cost, proteins larger than 50 kD can be produced, signal sequences can be removed, and glycosylation can be carried out. The baculoviral system can carry out more complex post-translational modifications of proteins. The most popular system for producing recombinant mammalian glycosylated proteins is that of mammalian cells. Genetically modified animals secrete recombinant proteins in their milk, blood or urine. Similarly, transgenic plants such as Arabidopsis thaliana and others can generate many recombinant proteins.",2009,141,818,52,6,31,49,55,67,83,74,89,99,84
24971a32453170ec29c7335e2afb72f426bfce24,"The organization of biological activities into daily cycles is universal in organisms as diverse as cyanobacteria, fungi, algae, plants, flies, birds and man. Comparisons of circadian clocks in unicellular and multicellular organisms using molecular genetics and genomics have provided new insights into the mechanisms and complexity of clock systems. Whereas unicellular organisms require stand-alone clocks that can generate 24-hour rhythms for diverse processes, organisms with differentiated tissues can partition clock function to generate and coordinate different rhythms. In both cases, the temporal coordination of a multi-oscillator system is essential for producing robust circadian rhythms of gene expression and biological activity.",2005,166,1216,55,7,52,63,71,63,65,69,83,69,79
71c5df87d5e6e7e5a00dc7c3668ac7248814a33f,"Models that describe the spread of invading organisms often assume that the dispersal distances of propagules are normally distributed. In contrast, measured dispersal curves are typically leptokurtic, not normal. In this paper, we consider a class of models, integrodifference equations, that directly incorporate detailed dispersal data as well as population growth dynamics. We provide explicit formulas for the speed of invasion for compensatory growth and for different choices of the propagule redistribution kernel and apply these formulas to the spread of D. pseudoobscura. We observe that: (1) the speed of invasion of a spreading population is extremely sensitive to the precise shape of the redistribution kernel and, in particular, to the tail of the distribution; (2) fat-tailed kernels can generate accelerating invasions rather than constant-speed travelling waves; (3) normal redistribution kernels (and by inference, many reaction-diffusion models) may grossly underestimate rates of spread of invading populations in comparison with models that incorporate more realistic leptokurtic distributions; and (4) the relative superiority of different redistribution kernels depends, in general, on the precise magnitude of the net reproductive rate. The addition of an Allee effect to an integrodifference equation may decrease the overall rate of spread. An Allee effect may also introduce a critical range; the population must surpass this spatial thresh-old in order to invade successfully. Fat-tailed kernels and Allee effects provide alternative explanations for the accelerating rates of spread observed for many invasions.",1996,0,1298,97,0,8,11,23,25,33,38,54,50,47
f9df3e2cc08af8119d6234f81147628080bc343f,"Choices of synonymous codons in unicellular organisms are here reviewed, and differences in synonymous codon usages between Escherichia coli and the yeast Saccharomyces cerevisiae are attributed to differences in the actual populations of isoaccepting tRNAs. There exists a strong positive correlation between codon usage and tRNA content in both organisms, and the extent of this correlation relates to the protein production levels of individual genes. Codon-choice patterns are believed to have been well conserved during the course of evolution. Examination of silent substitutions and tRNA populations in Enterobacteriaceae revealed that the evolutionary constraint imposed by tRNA content on codon usage decelerated rather than accelerated the silent-substitution rate, at least insofar as pairs of taxonomically related organisms were examined. Codon-choice patterns of multicellular organisms are briefly reviewed, and diversity in G+C percentage at the third position of codons in vertebrate genes--as well as a possible causative factor in the production of this diversity--is discussed.",1985,38,1604,70,0,8,16,20,29,24,25,13,29,30
216fce085f7112eeec2a9b9252b7f8735d2b846f,,1991,25,2444,28,28,46,62,56,49,28,47,33,29,31
d2167c06eaa23893331ab0ae062e57ce22b89e29,"UNLABELLED
Biologists and other scientists routinely need to know times of divergence between species and to construct phylogenies calibrated to time (timetrees). Published studies reporting time estimates from molecular data have been increasing rapidly, but the data have been largely inaccessible to the greater community of scientists because of their complexity. TimeTree brings these data together in a consistent format and uses a hierarchical structure, corresponding to the tree of life, to maximize their utility. Results are presented and summarized, allowing users to quickly determine the range and robustness of time estimates and the degree of consensus from the published literature.


AVAILABILITY
TimeTree is available at http://www.timetree.net",2006,12,1039,115,2,2,9,11,38,55,79,82,130,123
b6297363c4b33f0b882170add35c8e5cfa25a79d,"Unprecedented development along tropical shorelines is causing severe degradation of coral reefs primarily from increases in sedimentation. Sediment particles smother reef organisms and reduce light available for photosynthesis. Excessive sedmentation can adversely affect the structure and function of the coral reef ecosystem by altering both physical and biological processes. Mean sediment rates and suspended sediment concentrations for reefs not subject to stresses from human activities are < 1 to ca 10 mg cm-* d-' and < 10 mg I-', respectively. Chronic rates and concentrations above these values are 'hlgh'. Heavy sedmentation is associated with fewer coral species, less live coral, lower coral growth rates, greater abundance of branching forms, reduced coral recruitment, decreased calcification, decreased net productivity of corals, and slower rates of reef accretion. Coral species have different capabilities of clearing themselves of sediment particles or surviving lower light levels. Sedlment rejection is a function of morphology, orientation, growth habit, and behavior; and of the amount and type of se lment . Coral growth rates are not simple indicators of sediment levels. Decline of tropical fisheries is partially attributable to deterioration of coral reefs, seagrass beds, and mangroves from sedimentation. Sedimentation can alter the complex interactions between fish and their reef habitat. For example, sedimentation can lull major reef-building corals, leading to eventual collapse of the reef framework. A decline in the amount of shelter the reef provides leads to reductions in both number of individuals and number of species of fish. Currently, we are unable to rigorously predict the responses of coral reefs and reef organisms to excessive sedimentation from coastal development and other sources. Given information on the amount of sediment which will be introduced into the reef environment, the coral community composition, the depth of the reef, the percent coral cover, and the current patterns, we should be able to predict the consequences of a particular activity. Models of physical processes (e.g. sediment transport) must be complemented with better understanding of organism and ecosystem responses to sediment stress. Specifically, we need data on the threshold levels for reef orgarusms and for the reef ecosystem as a whole the levels above which sedimentation has lethal effects for particular species and above which normal functioning of the reef ceases. Additional field studies on the responses of reef organisms to both temgenous and calcium carbonate sediments are necessary. To effectively assess trends on coral reefs, e.g. changes in abundance and spatial arrangement of dominant benthic organisms, scientists must start using standardized monitoring methods. Long-term data sets are critical for tracking these complex ecosystems.",1990,109,1322,116,1,3,9,6,9,8,9,18,17,19
d6dc753be567413c42f02f02f35da476221575ca,"(2002). The Effects of Harmful Algal Blooms on Aquatic Organisms. Reviews in Fisheries Science: Vol. 10, No. 2, pp. 113-390.",2002,1964,1156,85,0,10,24,37,49,54,64,59,64,66
f90500d0f00a6c20303c28076eb0818f90e70ec2,"Although the nonlinear optical effect known as second-harmonic generation (SHG) has been recognized since the earliest days of laser physics and was demonstrated through a microscope over 25 years ago, only in the past few years has it begun to emerge as a viable microscope imaging contrast mechanism for visualization of cell and tissue structure and function. Only small modifications are required to equip a standard laser-scanning two-photon microscope for second-harmonic imaging microscopy (SHIM). Recent studies of the three-dimensional in vivo structures of well-ordered protein assemblies, such as collagen, microtubules and muscle myosin, are beginning to establish SHIM as a nondestructive imaging modality that holds promise for both basic research and clinical pathology. Thus far the best signals have been obtained in a transmitted light geometry that precludes in vivo measurements on large living animals. This drawback may be addressed through improvements in the collection of SHG signals via an epi-illumination microscope configuration. In addition, SHG signals from certain membrane-bound dyes have been shown to be highly sensitive to membrane potential. Although this indicates that SHIM may become a valuable tool for probing cell physiology, the small signal size would limit the number of photons that could be collected during the course of a fast action potential. Better dyes and optimized microscope optics could ultimately lead to the imaging of neuronal electrical activity with SHIM.",2003,31,1194,23,1,17,26,47,71,66,66,84,62,97
ddf280021cb2ebce1d42dc28d6b6897fd3163eaa,"Metallic nanoparticles are among the most widely used types of engineered nanomaterials; however, little is known about their environmental fate and effects. To assess potential environmental effects of engineered nanometals, it is important to determine which species are sensitive to adverse effects of various nanomaterials. In the present study, zebrafish, daphnids, and an algal species were used as models of various trophic levels and feeding strategies. To understand whether observed effects are caused by dissolution, particles were characterized before testing, and particle concentration and dissolution were determined during exposures. Organisms were exposed to silver, copper, aluminum, nickel, and cobalt as both nanoparticles and soluble salts as well as to titanium dioxide nanoparticles. Our results indicate that nanosilver and nanocopper cause toxicity in all organisms tested, with 48-h median lethal concentrations as low as 40 and 60 microg/L, respectively, in Daphnia pulex adults, whereas titanium dioxide did not cause toxicity in any of the tests. Susceptibility to nanometal toxicity differed among species, with filter-feeding invertebrates being markedly more susceptible to nanometal exposure compared with larger organisms (i.e., zebrafish). The role of dissolution in observed toxicity also varied, being minor for silver and copper but, apparently, accounting for most of the toxicity with nickel. Nanoparticulate forms of metals were less toxic than soluble forms based on mass added, but other dose metrics should be developed to accurately assess concentration-response relationships for nanoparticle exposures.",2008,21,792,52,1,21,39,72,69,87,84,81,66,76
a411f6a0e6473137ac1a538f7cee65722fa3584f,"Genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. Knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. The goal of the Gene Ontology Consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. To this end, three independent ontologies accessible on the World-Wide Web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component.",2000,35,31100,1372,0,0,0,0,1,0,0,1,0,1
82e320e06b1c717b0d924d257aa7b6710f53a38e,"1. Oxygen is a toxic gas - an introductionto oxygen toxicity and reactive species 2. The chemistry of free radicals and related 'reactive species' 3. Antioxidant defences Endogenous and Diet Derived 4. Cellular responses to oxidative stress: adaptation, damage, repair, senescence and death 5. Measurement of reactive species 6. Reactive species can pose special problems needing special solutions. Some examples. 7. Reactive species can be useful some more examples 8. Reactive species can be poisonous: their role in toxicology 9. Reactive species and disease: fact, fiction or filibuster? 10. Ageing, nutrition, disease, and therapy: A role for antioxidants?",1985,0,20976,1139,0,0,0,0,0,0,1,0,0,0
fd495d6cf7c3169bc58550fdf32be6e16e2800f8,"The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. The goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. We describe details of our aims and methods, identify current challenges, compare Bioconductor to other open bioinformatics projects, and provide working examples.",2004,69,11702,986,0,0,1,0,7,451,826,920,966,1006
30eecc8a7b7346a5e0c3a6648b0e156faad3a786,"cellular viewpoint. That is all very well. In any case academics will swallow almost anything if it's their job. Even if the botanists of the 18th century, the well-intentioned of the 19th century and the latterday enthusiasts of this century believed in the cell, is it an approach that is truly relevant? Such must surely be the attitude of today's sceptical medical students, and of many of their now middle-aged predecessors. Well, let a battle-scarred predecessor speak. Even a conventional diet of comparative and classical morphology, combined with exposure to prefashionable Eltonian ecology, and a native distrust of big-business 'nouvelle vague' biology has not blinkered your reviewer to the virtues of Alberts' Molecular Biology of the Cell. This is undoubtedly a landmark for the student and his teacher in the field of preclinical medicine. Many American undergraduate texts in biochemistry and molecular genetics clearly outsell competing English publications. Only English textbooks in immunology remain popular, and effective sellers in this field. The successful texts in these areas eschew traditional academic contrapuntal argument. The new bestselling formula involves clarity of presentation, the effective use of multi-coloured diagrams and the detailed exposition of basic processes. Such didactic approaches suit undergraduates today, and certainly ease the hardship for their seniors needing a refresher",1983,0,8546,650,0,6,11,7,18,12,11,16,26,19
ddf06cf0d375fb9404fe30c5f1d7858d74080e9c,The European Molecular Biology Open Software Suite (EMBOSS) is a mature package of software tools developed for the molecular biology community. It includes a comprehensive set of applications for molecular sequence analysis and other tasks and integrates popular third-party software packages under a consistent interface. EMBOSS includes extensive C programming libraries and is a platform to develop and release software in the true open source spirit.,2000,4,7763,588,3,10,22,85,127,216,255,308,348,346
667b43c8adad628c60c20810197cdeec6679714e,"Countless millions of people have died from tuberculosis, a chronic infectious disease caused by the tubercle bacillus. The complete genome sequence of the best-characterized strain of Mycobacterium tuberculosis, H37Rv, has been determined and analysed in order to improve our understanding of the biology of this slow-growing pathogen and to help the conception of new prophylactic and therapeutic interventions. The genome comprises 4,411,529 base pairs, contains around 4,000 genes, and has a very high guanine + cytosine content that is reflected in the biased amino-acid content of the proteins. M. tuberculosis differs radically from other bacteria in that a very large portion of its coding capacity is devoted to the production of enzymes involved in lipogenesis and lipolysis, and to two new families of glycine-rich proteins with a repetitive structure that may represent a source of antigenic variation.",1998,48,7413,404,61,288,335,362,366,301,329,350,328,297
b65ec8eec4933b3a9425d7dc981fdc27e4260077,"Vascular endothelial growth factor (VEGF) is a key regulator of physiological angiogenesis during embryogenesis, skeletal growth and reproductive functions. VEGF has also been implicated in pathological angiogenesis associated with tumors, intraocular neovascular disorders and other conditions. The biological effects of VEGF are mediated by two receptor tyrosine kinases (RTKs), VEGFR-1 and VEGFR-2, which differ considerably in signaling properties. Non-signaling co-receptors also modulate VEGF RTK signaling. Currently, several VEGF inhibitors are undergoing clinical testing in several malignancies. VEGF inhibition is also being tested as a strategy for the prevention of angiogenesis, vascular leakage and visual loss in age-related macular degeneration.",2003,151,8467,465,35,253,343,392,448,520,507,562,542,626
a462ee3d64edf189e075451ca7cb728bcc9e5fb3,"A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century.",2004,130,7015,296,71,215,292,332,349,427,434,444,531,480
73d7d1adf83ee16e12e6b442ec15357da218c06d,"Diabetes-specific microvascular disease is a leading cause of blindness, renal failure and nerve damage, and diabetes-accelerated atherosclerosis leads to increased risk of myocardial infarction, stroke and limb amputation. Four main molecular mechanisms have been implicated in glucose-mediated vascular damage. All seem to reflect a single hyperglycaemia-induced process of overproduction of superoxide by the mitochondrial electron-transport chain. This integrating paradigm provides a new conceptual framework for future research and drug discovery.",2001,183,7809,418,3,39,172,208,285,315,374,423,421,445
ede7e9d3e65a6c3a834b12b6456e55ff88d926cd,"The prime objective for every life form is to deliver its genetic material, intact and unchanged, to the next generation. This must be achieved despite constant assaults by endogenous and environmental agents on the DNA. To counter this threat, life has evolved several systems to detect DNA damage, signal its presence and mediate its repair. Such responses, which have an impact on a wide range of cellular events, are biologically significant because they prevent diverse human diseases. Our improving understanding of DNA-damage responses is providing new avenues for disease management.",2009,134,4184,215,4,139,217,257,304,351,376,411,452,369
1debd200d72b1fde3b71e3e87753b10e51fbbae0,"Introduction to amphibia - the world of amphibians, historical resume, prospects for the future. Part 1 Life History: reproductive strategies - reproductive cycles, reproductive mode, quantitative aspects, parental care, evolution of reproductive strategies courtship and mating - location of breeding site, secondary sexual characters, courtship behaviour, fertilization and oviposition, sexual selection, evolution of mating systems vocalization - anuran communication system, mechanisms of sound production and reception, kinds of vocalizations and their functions, abiotic factors affecting vocalization, interspecific significance of vocalization, phylogenetic implications of vocalization eggs and development - spermatozoa and fertilization, egg structure, egg development, hatching and birth, development and amphibian diversity larvae - morphology of larvae, adaptive types of larvae, physiology and ecology, social behaviour, evolutionary significance of larvae metamorphosis - endocrine control, other biochemical changes, morphological changes, neoteny, ecological and evolutionary significance of metamorphosis. Part 2 Ecology: relationships with the environment - water economy, temperature, gas exchange, energy metabolism and energy budgets, ecological synthesis food and feeding - prey selection, location of prey, capture of prey, evolution of prey-capturing mechanisms and strategies enemies and defence - diseases, parasites, predators, anti-predator mechanisms, evolution of defence mechanisms population biology - characteristics of individuals, movements and territoriality, demography, factors regulating populations community ecology and species diversity - community structure, species diversity, evolution of amphibian communities. Part 3 Morphology: musculoskeletal system - skull and hyobranchium, axial system, appendicular system, integration of functional units integumentary, sensory and visceral systems - integument, sensory receptor systems, nervous system, circulatory and respiratory systems, urogenital system, digestive system, endocrine glands, evolutionary considerations. Part 4 Evolution: origin and early evolution - nature of a tetrapod, primitive tetrapods, tetrapod affinities (lungfishes or lobe-fins?), diversity and evolution of early tetrapods, status of the lissamphibia cytogenetic, molecular and genomic evolution - cytogenetics, molecular evolution, genomic evolution phylogeny - caudata, gymnophiona, anura biogeography - biogeographic principles, historical setting, lissamphibia, caudata, gymnophiona, anura classification.",1986,0,4551,355,4,21,37,34,52,51,57,51,48,68
3ed436b2b92d50a4af31015774472101652eb983,"Living in an oxygenated environment has required the evolution of effective cellular strategies to detect and detoxify metabolites of molecular oxygen known as reactive oxygen species. Here we review evidence that the appropriate and inappropriate production of oxidants, together with the ability of organisms to respond to oxidative stress, is intricately connected to ageing and life span.",2000,116,8085,282,4,65,154,169,225,258,304,305,340,331
3e8d033a962a04c5b25d58d81135cf469186ed49,"D ifferent areas in biology are often criticized for being either too myopic and not asking questions about how their results fit into a larger framework or for being too broad and ignoring important details. A few research programs are now attempting to make general claims derived from simple, micro-level phenomena. This is great progress for biology, resolving some of the tensions between different approaches. A merging of these fields may lead to even greater advances. One of the most exciting of these new programs is ecological stoichiometry (ES), which has been spearheaded by Sterner and Elser. Their recent book provides an overview of this developing field. With clear exposition they explain how analysis at the level of molecules can have broad implications for many macro-level ecological phenomena. Moreover, they keep their theory closely tied to available data and to the design of new experiments, thereby bridging the gap between two dividing perspectives in biology. As such, Ecological Stoichiometry contains a much needed overview of this research program as well as a good example of future directions in biology. Sterner and Elser use the first part of their book to develop the essential ideas behind ES. The foremost assumption is that an entire organism can be treated as a single, extremely large molecule. Humans, for example, are reducible to the formula:",2002,17,3513,566,7,18,61,100,120,131,128,140,198,181
55c30317e9e3027d77025add26341bf8618c080a,,1981,0,3838,485,0,6,17,25,37,36,46,55,54,70
c1b0e411a0ab95363236fedaedc4103739d44efb,"Samenvatting:The book for introductory microbiology, Brock's Biology of Microorganismscontinues its long tradition of impeccable scholarship, outstanding art,and accuracy. It balances the most current coverage with the majorclassical concepts essential for understanding the science. A six-partpresentation covers principles of microbiology; evolutionary microbiologyand microbial diversity; metabolic diversity and microbial ecology;immunology, pathogenicity, and host responses; microbial diseases; andmicroorganisms as tools for industry and research. For researchers, groupleaders, senior scientists in pharmaceuticals, chemicals and biochemicalbiotechnology companies, and public health laboratories.",1996,0,4448,233,0,2,11,14,37,50,78,97,127,144
2943c165f95fecb7a4f6ae37703a11697962f4be,"Allometric scaling relations, including the 3/4 power law for metabolic rates, are characteristic of all organisms and are here derived from a general model that describes how essential materials are transported through space-filling fractal networks of branching tubes. The model assumes that the energy dissipated is minimized and that the terminal tubes do not vary with body size. It provides a complete analysis of scaling relations for mammalian circulatory systems that are in agreement with data. More generally, the model predicts structural and functional properties of vertebrate cardiovascular and respiratory systems, plant vascular systems, insect tracheal tubes, and other distribution networks.",1997,41,3981,292,5,31,49,56,76,85,116,150,156,166
4b80558804d91c9e421ec3d1e24d0c87567cffb9,"The study of eye movements and oculomotor disorders has, for four decades, greatly benefitted from the application of control theoretic concepts. This paper is an example of a complementary approach based on the theory of nonlinear dynamical systems. Recently, a nonlinear dynamics model of the saccadic system was developed, comprising a symmetric piecewise-smooth system of six first-order autonomous ordinary differential equations. A preliminary numerical investigation of the model revealed that in addition to generating normal saccades, it could also simulate inaccurate saccades, and the oscillatory instability known as congenital nystagmus (CN). By varying the parameters of the model, several types of CN oscillations were produced, including jerk, bidirectional jerk and pendular nystagmus. The aim of this study was to investigate the bifurcations and attractors of the model, in order to obtain a classification of the simulated oculomotor behaviours. The application of standard stability analysis techniques, together with numerical work, revealed that the equations have a rich bifurcation structure. In addition to Hopf, homoclinic and saddlenode bifurcations organised by a Takens-Bogdanov point, the equations can undergo nonsmooth pitchfork bifurcations and nonsmooth gluing bifurcations. Evidence was also found for the existence of Hopf-initiated canards. The simulated jerk CN waveforms were found to correspond to a pair of post-canard symmetry-related limit cycles, which exist in regions of parameter space where the equations are a slow-fast system. The slow and fast phases of the simulated oscillations were attributed to the geometry of the corresponding slow manifold. The simulated bidirectional jerk and Corresponding author O.E. Akman: The School of Mathematics, The University of Manchester, P.O. Box 88, Manchester M60 1QD, UK. Present address: Mathematics Institute, University of Warwick, Coventry CV4 7AL, UK. e-mail: oakman@maths.warwick.ac.uk D.S. Broomhead: The School of Mathematics, The University of Manchester, P.O. Box 88, Manchester M60 1QD, UK. e-mail: david.broomhead@manchester.ac.uk R.V. Abadi: Faculty of Life Sciences, Moffat Building, The University of Manchester, Sackville St, Manchester M60 1QD, UK. e-mail: richard.abadi@manchester.ac.uk R.A. Clement: Visual Sciences Unit, Institute of Child Health, U.C.L., London WC1N 1EH, UK. e-mail: R.clement@ich.ucl.ac.uk Mathematics Subject Classification (2000): 37N25",1961,15,4737,433,1,0,0,0,0,0,0,0,1,0
4e98194e61ce6de2f033947ec65370ce0cf1d97d,,1979,0,7583,269,42,77,106,130,149,145,188,146,186,190
1a1761eb00bdce0c1c7887a29168852d0cada096,"Astrocytes are specialized glial cells that outnumber neurons by over fivefold. They contiguously tile the entire central nervous system (CNS) and exert many essential complex functions in the healthy CNS. Astrocytes respond to all forms of CNS insults through a process referred to as reactive astrogliosis, which has become a pathological hallmark of CNS structural lesions. Substantial progress has been made recently in determining functions and mechanisms of reactive astrogliosis and in identifying roles of astrocytes in CNS disorders and pathologies. A vast molecular arsenal at the disposal of reactive astrocytes is being defined. Transgenic mouse models are dissecting specific aspects of reactive astrocytosis and glial scar formation in vivo. Astrocyte involvement in specific clinicopathological entities is being defined. It is now clear that reactive astrogliosis is not a simple all-or-none phenomenon but is a finely gradated continuum of changes that occur in context-dependent manners regulated by specific signaling events. These changes range from reversible alterations in gene expression and cell hypertrophy with preservation of cellular domains and tissue structure, to long-lasting scar formation with rearrangement of tissue structure. Increasing evidence points towards the potential of reactive astrogliosis to play either primary or contributing roles in CNS disorders via loss of normal astrocyte functions or gain of abnormal effects. This article reviews (1) astrocyte functions in healthy CNS, (2) mechanisms and functions of reactive astrogliosis and glial scar formation, and (3) ways in which reactive astrocytes may cause or contribute to specific CNS disorders and lesions.",2009,289,3439,219,1,31,129,184,256,285,312,328,374,399
87de747cb11577979a9398cb5f42d3927a38fabb,■ Abstract Contributions from the field of population biology hold promise for understanding and managing invasiveness; invasive species also offer excellent opportunities to study basic processes in population biology. Life history studies and demographic models may be valuable for examining the introduction of invasive species and identifying life history stages where management will be most effective. Evolutionary processes may be key features in determining whether invasive species establish and spread. Studies of genetic diversity and evolutionary changes should be useful for,2001,142,3393,241,0,15,45,89,123,122,146,167,192,222
e625c8787d34a5113ee817f52d0debfb95fcb1ca,"Rho GTPases are molecular switches that control a wide variety of signal transduction pathways in all eukaryotic cells. They are known principally for their pivotal role in regulating the actin cytoskeleton, but their ability to influence cell polarity, microtubule dynamics, membrane transport pathways and transcription factor activity is probably just as significant. Underlying this biological complexity is a simple biochemical idea, namely that by switching on a single GTPase, several distinct signalling pathways can be coordinately activated. With spatial and temporal activation of multiple switches factored in, it is not surprising to find Rho GTPases having such a prominent role in eukaryotic cell biology.",2002,92,4454,173,1,152,315,316,322,274,271,258,278,250
cd709dbb5085f28971ba3ba6ecc65800cf9ecf5b,"This review focuses on the mechanisms regulating the synthesis, secretion, biological actions, and therapeutic relevance of the incretin peptides glucose-dependent insulinotropic polypeptide (GIP) and glucagon-like peptide-1 (GLP-1). The published literature was reviewed, with emphasis on recent advances in our understanding of the biology of GIP and GLP-1. GIP and GLP-1 are both secreted within minutes of nutrient ingestion and facilitate the rapid disposal of ingested nutrients. Both peptides share common actions on islet beta-cells acting through structurally distinct yet related receptors. Incretin-receptor activation leads to glucose-dependent insulin secretion, induction of beta-cell proliferation, and enhanced resistance to apoptosis. GIP also promotes energy storage via direct actions on adipose tissue, and enhances bone formation via stimulation of osteoblast proliferation and inhibition of apoptosis. In contrast, GLP-1 exerts glucoregulatory actions via slowing of gastric emptying and glucose-dependent inhibition of glucagon secretion. GLP-1 also promotes satiety and sustained GLP-1-receptor activation is associated with weight loss in both preclinical and clinical studies. The rapid degradation of both GIP and GLP-1 by the enzyme dipeptidyl peptidase-4 has led to the development of degradation-resistant GLP-1-receptor agonists and dipeptidyl peptidase-4 inhibitors for the treatment of type 2 diabetes. These agents decrease hemoglobin A1c (HbA1c) safely without weight gain in subjects with type 2 diabetes. GLP-1 and GIP integrate nutrient-derived signals to control food intake, energy absorption, and assimilation. Recently approved therapeutic agents based on potentiation of incretin action provide new physiologically based approaches for the treatment of type 2 diabetes.",2007,249,2814,233,8,80,120,177,191,248,256,229,249,265
e84cc71e71ee1d52df52b45ff6c95167fd447308,"Molecular cell biology , Molecular cell biology , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1986,0,5755,136,0,10,24,32,19,40,34,47,45,62
bb2d0d378f82472e4286b451f9b9fb584922321d,"To understand biology at the system level, we must examine the structure and dynamics of cellular and organismal function, rather than the characteristics of isolated parts of a cell or organism. Properties of systems, such as robustness, emerge as central issues, and understanding these properties may have an impact on the future of medicine. However, many breakthroughs in experimental devices, advanced software, and analytical methods are required before the achievements of systems biology can live up to their much-touted potential.",2002,12,4018,139,29,114,141,165,180,240,232,251,281,285
f25de4182a75525841ad40b0b8bfe7337a9cf792,"MOTIVATION
Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.


RESULTS
We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.


AVAILABILITY
The specification of SBML Level 1 is freely available from http://www.sbml.org/",2003,86,3167,276,33,77,119,177,167,168,215,199,200,218
3d6e5f2caa50a415cee56acf5860905022250e6a,"As a discipline, phylogenetics is becoming transformed by a flood of molecular data. These data allow broad questions to be asked about the history of life, but also present difficult statistical and computational problems. Bayesian inference of phylogeny brings a new perspective to a number of outstanding issues in evolutionary biology, including the analysis of large phylogenetic trees and complex evolutionary models and the detection of the footprint of natural selection in DNA sequences.",2001,34,2558,477,0,40,96,119,127,146,112,128,172,149
acb5571a8ab6ae2be5fbabd04451162006fcab49,"Denitrification is a distinct means of energy conservation, making use of N oxides as terminal electron acceptors for cellular bioenergetics under anaerobic, microaerophilic, and occasionally aerobic conditions. The process is an essential branch of the global N cycle, reversing dinitrogen fixation, and is associated with chemolithotrophic, phototrophic, diazotrophic, or organotrophic metabolism but generally not with obligately anaerobic life. Discovered more than a century ago and believed to be exclusively a bacterial trait, denitrification has now been found in halophilic and hyperthermophilic archaea and in the mitochondria of fungi, raising evolutionarily intriguing vistas. Important advances in the biochemical characterization of denitrification and the underlying genetics have been achieved with Pseudomonas stutzeri, Pseudomonas aeruginosa, Paracoccus denitrificans, Ralstonia eutropha, and Rhodobacter sphaeroides. Pseudomonads represent one of the largest assemblies of the denitrifying bacteria within a single genus, favoring their use as model organisms. Around 50 genes are required within a single bacterium to encode the core structures of the denitrification apparatus. Much of the denitrification process of gram-negative bacteria has been found confined to the periplasm, whereas the topology and enzymology of the gram-positive bacteria are less well established. The activation and enzymatic transformation of N oxides is based on the redox chemistry of Fe, Cu, and Mo. Biochemical breakthroughs have included the X-ray structures of the two types of respiratory nitrite reductases and the isolation of the novel enzymes nitric oxide reductase and nitrous oxide reductase, as well as their structural characterization by indirect spectroscopic means. This revealed unexpected relationships among denitrification enzymes and respiratory oxygen reductases. Denitrification is intimately related to fundamental cellular processes that include primary and secondary transport, protein translocation, cytochrome c biogenesis, anaerobic gene regulation, metalloprotein assembly, and the biosynthesis of the cofactors molybdopterin and heme D1. An important class of regulators for the anaerobic expression of the denitrification apparatus are transcription factors of the greater FNR family. Nitrate and nitric oxide, in addition to being respiratory substrates, have been identified as signaling molecules for the induction of distinct N oxide-metabolizing enzymes.",1997,1002,3066,275,0,21,39,36,57,78,78,75,97,117
fd08d0e4326a8686e784f011a6181b873c7b85ad,"Based on the work of the Tropical Soil Biology and Fertility (TSBF) Programme, this is a handbook of recommended and validated methods for the characterization and analysis of tropical soils, with the aim of achieving sustainable use of soil resources. The objectives of the programme revolve around five main themes: synchrony of nutrient release and plant growth demands; management of soil organic matter; soil water balance; effects and management of soil fauna; and integration of biological processes into the maintenance of soil fertility. The methods given are endorsed by the International Soil Science Society and are part of the International Union of Biological Sciences and the UNESCO Man and the Biosphere Programme.",1990,0,2864,275,0,6,5,12,16,25,35,31,54,52
066e24bd08719e6163903f7e630f49296c5e009d,"The establishment of a vascular supply is required for organ development and differentiation as well as for tissue repair and reproductive functions in the adult1. Neovascularization (angiogenesis) is also implicated in the pathogenesis of a number of disorders. These include: proliferative retinopathies, age-related macular degeneration, tumors, rheumatoid arthritis, and psoriasis1,2. A strong correlation has been noted between density of microvessels in primary breast cancers and their nodal metastases and patient survival3. Similarly, a correlation has been reported between vascularity and invasive behavior in several other tumors4–6.",1997,245,4445,171,15,88,196,219,203,215,236,277,223,228
f91c8be04929b0443b026f582e14450a5804d7df,,1990,0,2818,232,0,20,17,26,24,32,29,34,28,50
f54a5c14e14feda0bf4b8fb30a8a3958adce16bf,"Prostaglandins and leukotrienes are potent eicosanoid lipid mediators derived from phospholipase-released arachidonic acid that are involved in numerous homeostatic biological functions and inflammation. They are generated by cyclooxygenase isozymes and 5-lipoxygenase, respectively, and their biosynthesis and actions are blocked by clinically relevant nonsteroidal anti-inflammatory drugs, the newer generation coxibs (selective inhibitors of cyclooxygenase-2), and leukotriene modifiers. The prime mode of prostaglandin and leukotriene action is through specific G protein-coupled receptors, many of which have been cloned recently, thus enabling specific receptor agonist and antagonist development. Important insights into the mechanisms of inflammatory responses, pain, and fever have been gleaned from our current understanding of eicosanoid biology.",2001,67,3341,147,2,64,119,137,160,142,188,186,161,188
336dc5ace3125bd95cccbf7a9ff53beced98584b,"DAMBE (data analysis in molecular biology and evolution) is an integrated software package for converting, manipulating, statistically and graphically describing, and analyzing molecular sequence data with a user-friendly Windows 95/98/2000/NT interface. DAMBE is free and can be downloaded from http://web.hku.hk/~xxia/software/software.htm. The current version is 4.0.36.",2001,13,2184,472,5,22,36,74,69,69,121,159,144,164
95ff3f28e7fc3fdfe025e6d6cd49b9d98e85b3db,1. Introduction 2. Archetypes of the weak hydrogen bond 3. Other weak and non-conventional hydrogen bonds 4. The weak hydrogen bond in supramolecular chemistry 5. The weak hydrogen bond in biological structures 6. Conclusions Appendix,1999,0,3703,137,8,25,76,114,127,170,192,198,216,229
98f9550b50d0aede9d8b10651a5757b45a722ac6,"Approximately one percent of the human genome encodes proteins that either regulate or are regulated by direct interaction with members of the Rho family of small GTPases. Through a series of complex biochemical networks, these highly conserved molecular switches control some of the most fundamental processes of cell biology common to all eukaryotes, including morphogenesis, polarity, movement, and cell division. In the first part of this review, we present the best characterized of these biochemical pathways; in the second part, we attempt to integrate these molecular details into a biological context.",2005,194,2779,185,4,103,165,209,203,226,199,227,241,207
0a7c0cbdeec3ef2157c5c4f606f079542b2c56fb,"SUMMARY Tetracyclines were discovered in the 1940s and exhibited activity against a wide range of microorganisms including gram-positive and gram-negative bacteria, chlamydiae, mycoplasmas, rickettsiae, and protozoan parasites. They are inexpensive antibiotics, which have been used extensively in the prophlylaxis and therapy of human and animal infections and also at subtherapeutic levels in animal feed as growth promoters. The first tetracycline-resistant bacterium, Shigella dysenteriae, was isolated in 1953. Tetracycline resistance now occurs in an increasing number of pathogenic, opportunistic, and commensal bacteria. The presence of tetracycline-resistant pathogens limits the use of these agents in treatment of disease. Tetracycline resistance is often due to the acquisition of new genes, which code for energy-dependent efflux of tetracyclines or for a protein that protects bacterial ribosomes from the action of tetracyclines. Many of these genes are associated with mobile plasmids or transposons and can be distinguished from each other using molecular methods including DNA-DNA hybridization with oligonucleotide probes and DNA sequencing. A limited number of bacteria acquire resistance by mutations, which alter the permeability of the outer membrane porins and/or lipopolysaccharides in the outer membrane, change the regulation of innate efflux systems, or alter the 16S rRNA. New tetracycline derivatives are being examined, although their role in treatment is not clear. Changing the use of tetracyclines in human and animal health as well as in food production is needed if we are to continue to use this class of broad-spectrum antimicrobials through the present century.",2001,394,3127,197,3,34,52,59,72,87,116,106,125,120
7c72b917a38b09e6d3ab19d28a4344ba54edb6ae,"Part I. Exact String Matching: The Fundamental String Problem: 1. Exact matching: fundamental preprocessing and first algorithms 2. Exact matching: classical comparison-based methods 3. Exact matching: a deeper look at classical methods 4. Semi-numerical string matching Part II. Suffix Trees and their Uses: 5. Introduction to suffix trees 6. Linear time construction of suffix trees 7. First applications of suffix trees 8. Constant time lowest common ancestor retrieval 9. More applications of suffix trees Part III. Inexact Matching, Sequence Alignment and Dynamic Programming: 10. The importance of (sub)sequence comparison in molecular biology 11. Core string edits, alignments and dynamic programming 12. Refining core string edits and alignments 13. Extending the core problems 14. Multiple string comparison: the Holy Grail 15. Sequence database and their uses: the motherlode Part IV. Currents, Cousins and Cameos: 16. Maps, mapping, sequencing and superstrings 17. Strings and evolutionary trees 18. Three short topics 19. Models of genome-level mutations.",1997,0,3106,176,4,21,32,49,77,110,130,160,175,204
b7c89b0247fc7009ab51dd0873b5f6c3b6ec144e,"Electron-transfer reactions between ions and 
molecules in solution have been the subject of 
considerable experimental study during the past 
three decades. Experimental results have also been 
obtained on related phenomena, such as reactions 
between ions or molecules and electrodes, charge-transfer 
spectra, photoelectric emission spectra of 
ionic solutions, chemiluminescent electron transfers, 
electron transfer through frozen media, and 
electron transfer through thin hydrocarbon-like 
films on electrodes.",1985,276,5704,66,3,23,48,73,55,75,85,95,119,155
11fba3d8fd169d54526c4c360d88fad4a6e0d1a9,,1987,0,4764,80,21,92,114,193,210,192,201,170,259,310
ec94b60c8e678e4fd7473d743babc23cba45a280,"Cellular functions, such as signal transmission, are carried out by 'modules' made up of many species of interacting molecules. Understanding how modules work has depended on combining phenomenological analysis with molecular studies. General principles that govern the structure and behaviour of modules may be discovered with help from synthetic sciences such as engineering and computer science, from stronger interactions between experiment and theory in cell biology, and from an appreciation of evolutionary constraints.",1999,30,3464,79,2,34,56,81,116,165,176,198,215,212
5345c358466cb5e11801687820b247afb999b01a,"The authors regret the inability to cite all of the primary literature contributing to this review due to length considerations. The authors thank F. Chan, T. Migone, M. Peter, J. Puck, R. Siegel, H. Walczak, and J. Wang for insightful comments on the manuscript. N. K. is a Scholar of the Leukemia Society. Supported in part by grants from the National Institutes of Health (R. M. L., N. K.).",2001,241,3570,111,56,141,210,172,192,182,180,184,204,193
b2528cd8034ea7cb49313ebd872804b93dfabed3,,2007,0,2679,162,84,94,146,130,159,185,194,216,195,192
6072973ad37f4e5763e8495d562eebe24af0696b,"Retinoids play an important role in development, differentiation, and homeostasis. The discovery of retinoid receptors belonging to the superfamily of nuclear ligand‐activated transcriptional regulators has revolutionized our molecular understanding as to how these structurally simple molecules exert their pleiotropic effects. Diversity in the control of gene expression by retinoid signals is generated through complexity at different levels of the signaling pathway. A major source of diversity originates from the existence of two families of retinoid acid (RA) receptors (R), the RAR isotypes (α, β, and γ) and the three RXR isotypes (α, β, and γ), and their numerous isoforms, which bind as RXR/RAR heterodimers to the polymorphic cis.acting response elements of RA target genes. The possibility of cross‐modulation (cross‐talk) with cell‐surface receptors signaling pathways, as well as the finding that RARs and RXRs interact with multiple putative coactivators and/or corepressors, generates additional levels of complexity for the array of combinatorial effects that underlie the pleiotropic effects of retinoids. This review focuses on recent developments, particularly in the area of structure‐ function relationships.—Chambon, P. A decade of molecular biology of retinoic acid receptors. FASEB J. 10, 940‐954 (1996)",1996,119,2867,153,5,76,112,139,185,177,143,167,154,181
33b776abcc465b79c54623e213a4dfe079da0ca4,"General methods bacterial strains and cloning vectors enzymes that modify DNA and RNA in vitro amplification of DNA using - the polymerase chain reaction (PCR) and the thermostable Taq DNA polymerase, introduction DNA restriction fragment analysis and preparation, introduction in vitro labeling of probes and filter hybridization plasmid DNA preparation for E. Coli hosts preparation of DNA from Lambda bacteriophage clones subcloning fragments into plasmid vectors and plasmid construction preparation of genomic DNA preparation and analysis of RNA from eukaryotic cells - overview geonomic cloning generatino of cDNA libaries preparation of subtractive cDNA, introduction chain termination sequencing transfection of mammalian cells, introduction basic methods of protein analysis in situ hybridization transgenic mouse preparation detection and in vitro generation of specific mutations in genes and cDNAs, introduction appendices.",1986,0,3456,60,3,26,112,175,232,254,212,191,120,161
3537ee388a43c5df1f985ddcc97382e61dbe33d6,"Human natural killer (NK) cells comprise approximately 15% of all circulating lymphocytes. Owing to their early production of cytokines and chemokines, and ability to lyse target cells without prior sensitization, NK cells are crucial components of the innate immune system. Human NK cells can be divided into two subsets based on their cell-surface density of CD56--CD56(bright) and CD56(dim)--each with distinct phenotypic properties. Now, there is ample evidence to suggest that these NK-cell subsets have unique functional attributes and, therefore, distinct roles in the human immune response. The CD56(dim) NK-cell subset is more naturally cytotoxic and expresses higher levels of Ig-like NK receptors and FCgamma receptor III (CD16) than the CD56(bright) NK-cell subset. By contrast, the CD56(bright) subset has the capacity to produce abundant cytokines following activation of monocytes, but has low natural cytotoxicity and is CD16(dim) or CD16(-). In addition, we will discuss other cell-surface receptors expressed differentially by human NK-cell subsets and the distinct functional properties of these subsets.",2001,64,2541,204,0,34,54,71,103,100,97,119,114,144
bbe46d4075ba4fd4344d614753a6a6b65e3c15e5,"Professional phagocytes generate high levels of reactive oxygen species (ROS) using a superoxide-generating NADPH oxidase as part of their armoury of microbicidal mechanisms. The multicomponent phagocyte oxidase (Phox), which has been well characterized over the past three decades, includes the catalytic subunit gp91phox. Lower levels of ROS are seen in non-phagocytic cells, but are usually thought to be 'accidental' byproducts of aerobic metabolism. The discovery of a family of superoxide-generating homologues of gp91phox has led to the concept that ROS are 'intentionally' generated in these cells with distinctive cellular functions related to innate immunity, signal transduction and modification of the extracellular matrix.",2004,83,2712,160,16,96,141,145,162,176,185,194,196,187
752c9501216b9514bf24e7cf09e9657fdb550b94,"The prostaglandin endoperoxide H synthases-1 and 2 (PGHS-1 and PGHS-2; also cyclooxygenases-1 and 2, COX-1 and COX-2) catalyze the committed step in prostaglandin synthesis. PGHS-1 and 2 are of particular interest because they are the major targets of nonsteroidal anti-inflammatory drugs (NSAIDs) including aspirin, ibuprofen, and the new COX-2 inhibitors. Inhibition of the PGHSs with NSAIDs acutely reduces inflammation, pain, and fever, and long-term use of these drugs reduces fatal thrombotic events, as well as the development of colon cancer and Alzheimer's disease. In this review, we examine how the structures of these enzymes relate mechanistically to cyclooxygenase and peroxidase catalysis, and how differences in the structure of PGHS-2 confer on this isozyme differential sensitivity to COX-2 inhibitors. We further examine the evidence for independent signaling by PGHS-1 and PGHS-2, and the complex mechanisms for regulation of PGHS-2 gene expression.",2000,255,2674,146,3,45,99,127,141,171,189,172,166,148
ffac9d302a4f5a18bc69320e97ed7a4264469bd3,"Preface. Reference System Usage. Frequently Used Abbreviations. Introduction and Historical Perspectives. Ethylene Analysis and Properties of the Gas. The Biosynthesis of Ethylene. Regulation of Ethylene Production by Internal, Environmental, and Stress Factors. Roles and Physiological Effects of Ethylene in Plant Physiology: Dormancy, Growth and Development. Fruit Ripening, Abscission and Postharvest Disorders. The Mechanisms of Ethylene Action. Ethylene in the Environment. The Role of Ethylene in Agriculture. References. Index.",1973,0,3074,169,2,10,15,31,12,19,26,30,19,25
a72c494aba62e75c696566d924f7b70da9e4c3fe,"Since its publication in 2000, Biochemistry & Molecular Biology of Plants, has been hailed as a major contribution to the plant sciences literature and critical acclaim has been matched by global sales success. Maintaining the scope and focus of the first edition, the second will provide a major update, include much new material and reorganise some chapters to further improve the presentation. This book is meticulously organised and richly illustrated, having over 1,000 full-colour illustrations and 500 photographs. It is divided into five parts covering: Compartments: Cell Reproduction: Energy Flow; Metabolic and Developmental Integration; and Plant Environment and Agriculture. Specific changes to this edition include: Completely revised with over half of the chapters having a major rewrite. Includes two new chapters on signal transduction and responses to pathogens. Restructuring of section on cell reproduction for improved presentation. Dedicated website to include all illustrative material. Biochemistry & Molecular Biology of Plants holds a unique place in the plant sciences literature as it provides the only comprehensive, authoritative, integrated single volume book in this essential field of study.",2002,0,3146,64,96,112,127,150,196,227,225,200,232,261
140f7c7d5fafba3fc9911476c6cbc911b35e93a0,"Hydrophilic polymers are the center of research emphasis in nanotechnology because of their perceived “intelligence”. They can be used as thin films, scaffolds, or nanoparticles in a wide range of biomedical and biological applications. Here we highlight recent developments in engineering uncrosslinked and crosslinked hydrophilic polymers for these applications. Natural, biohybrid, and synthetic hydrophilic polymers and hydrogels are analyzed and their thermodynamic responses are discussed. In addition, examples of the use of hydrogels for various therapeutic applications are given. We show how such systems’ intelligent behavior can be used in sensors, microarrays, and imaging. Finally, we outline challenges for the future in integrating hydrogels into biomedical applications.",2006,274,3088,52,5,29,67,120,156,202,231,245,276,263
90c3c31cee756baa411d2f2be30fd03f7e7e3c08,"Predicting which species are probable invaders has been a long-standing goal of ecologists, but only recently have quantitative methods been used to achieve such a goal. Although restricted to few taxa, these studies reveal clear relationships between the characteristics of releases and the species involved, and the successful establishment and spread of invaders. For example, the probability of bird establishment increases with the number of individuals released and the number of release events. Also, the probability of plant invasiveness increases if the species has a history of invasion and reproduces vegetatively. These promising quantitative approaches should be more widely applied to allow us to predict patterns of invading species more successfully.",2001,34,2607,157,9,25,59,106,98,125,162,149,159,153
0f22ce024aa171f62f5677b68afda409ee29fcb0,The comparative method for studying adaptation why worry about phylogeny? reconstructing phylogenetic trees and ancestral character states comparative analysis of discrete data comparative analysis of continuous variables determining the form of comparative relationships.,1991,0,2849,95,10,65,128,81,122,123,122,139,122,124
f0e58ab853ba27ae924cf11813a924bd69c72f65,"Living organisms have regular patterns and routines that involve obtaining food and carrying out life history stages such as breeding, migrating, molting, and hibernating. The acquisition, utilization, and storage of energy reserves (and other resources) are critical to lifetime reproductive success. There are also responses to predictable changes, e.g., seasonal, and unpredictable challenges, i.e., storms and natural disasters. Social organization in many populations provides advantages through cooperation in providing basic necessities and beneficial social support. But there are disadvantages owing to conflict in social hierarchies and competition for resources. Here we discuss the concept of allostasis, maintaining stability through change, as a fundamental process through which organisms actively adjust to both predictable and unpredictable events. Allostatic load refers to the cumulative cost to the body of allostasis, with allostatic overload being a state in which serious pathophysiology can occur. Using the balance between energy input and expenditure as the basis for applying the concept of allostasis, we propose two types of allostatic overload. Type 1 allostatic overload occurs when energy demand exceeds supply, resulting in activation of the emergency life history stage. This serves to direct the animal away from normal life history stages into a survival mode that decreases allostatic load and regains positive energy balance. The normal life cycle can be resumed when the perturbation passes. Type 2 allostatic overload begins when there is sufficient or even excess energy consumption accompanied by social conflict and other types of social dysfunction. The latter is the case in human society and certain situations affecting animals in captivity. In all cases, secretion of glucocorticosteroids and activity of other mediators of allostasis such as the autonomic nervous system, CNS neurotransmitters, and inflammatory cytokines wax and wane with allostatic load. If allostatic load is chronically high, then pathologies develop. Type 2 allostatic overload does not trigger an escape response, and can only be counteracted through learning and changes in the social structure.",2003,97,2548,133,8,36,51,59,71,107,113,132,138,152
666db4c90f03cb71cd87a220a0ae4ddd910610d3,"
 Publisher Summary
 
 Studies of cytotoxicity by human lymphocytes revealed not only that both allogeneic and syngeneic tumor cells were lysed in a non-MHC-restricted fashion, but also that lymphocytes from normal donors were often cytotoxic. Lymphocytes from any healthy donor, as well as peripheral blood and spleen lymphocytes from several experimental animals, in the absence of known or deliberate sensitization, were found to be spontaneously cytotoxic in vitro for some normal fresh cells, most cultured cell lines, immature hematopoietic cells, and tumor cells. This type of nonadaptive, non-MHC-restricted cellmediated cytotoxicity was defined as “natural” cytotoxicity, and the effector cells mediating natural cytotoxicity were functionally defined as natural killer (NK) cells. The existence of NK cells has prompted a reinterpretation of both the studies of specific cytotoxicity against spontaneous human tumors and the theory of immune surveillance, at least in its most restrictive interpretation. Unlike cytotoxic T cells, NK cells cannot be demonstrated to have clonally distributed specificity, restriction for MHC products at the target cell surface, or immunological memory. NK cells cannot yet be formally assigned to a single lineage based on the definitive identification of a stem cell, a distinct anatomical location of maturation, or unique genotypic rearrangements.
 
",1989,1219,3024,78,2,22,76,89,128,101,95,132,126,139
83cb6105617265494e4184527a91a49165ef19e3,"Wolbachia are common intracellular bacteria that are found in arthropods and nematodes. These alphaproteobacteria endosymbionts are transmitted vertically through host eggs and alter host biology in diverse ways, including the induction of reproductive manipulations, such as feminization, parthenogenesis, male killing and sperm–egg incompatibility. They can also move horizontally across species boundaries, resulting in a widespread and global distribution in diverse invertebrate hosts. Here, we review the basic biology of Wolbachia, with emphasis on recent advances in our understanding of these fascinating endosymbionts.",2008,110,2065,123,3,36,57,111,132,154,153,154,215,199
84f5633b1e4cd3b1ec034b8e2610e4e959950884,"Recognized more than a decade ago, NKT cells differentiate from mainstream thymic precursors through instructive signals emanating during TCR engagement by CD1d-expressing cortical thymocytes. Their semi-invariant alphabeta TCRs recognize isoglobotrihexosylceramide, a mammalian glycosphingolipid, as well as microbial alpha-glycuronylceramides found in the cell wall of Gram-negative, lipopolysaccharide-negative bacteria. This dual recognition of self and microbial ligands underlies innate-like antimicrobial functions mediated by CD40L induction and massive Th1 and Th2 cytokine and chemokine release. Through reciprocal activation of NKT cells and dendritic cells, synthetic NKT ligands constitute promising new vaccine adjuvants. NKT cells also regulate a range of immunopathological conditions, but the mechanisms and the ligands involved remain unknown. NKT cell biology has emerged as a new field of research at the frontier between innate and adaptive immunity, providing a powerful model to study fundamental aspects of the cell and structural biology of glycolipid trafficking, processing, and recognition.",2007,254,1999,145,43,135,161,163,200,183,176,160,153,114
a75b4efbf7503f370fe751002ad3a34a00ed7ec3,"Malignant astrocytic gliomas such as glioblastoma are the most common and lethal intracranial tumors. These cancers exhibit a relentless malignant progression characterized by widespread invasion throughout the brain, resistance to traditional and newer targeted therapeutic approaches, destruction of normal brain tissue, and certain death. The recent confluence of advances in stem cell biology, cell signaling, genome and computational science and genetic model systems have revolutionized our understanding of the mechanisms underlying the genetics, biology and clinical behavior of glioblastoma. This progress is fueling new opportunities for understanding the fundamental basis for development of this devastating disease and also novel therapies that, for the first time, portend meaningful clinical responses.",2007,350,2168,124,0,46,112,153,199,186,201,183,199,173
3558b67481229e3fc23ebb9246d5423961288d61,"A free radical is any molecule that has an odd number of electrons. Free radicals, which can occur in both organic (i.e., quinones) and inorganic molecules (i.e., O(2)), are highly reactive and, therefore, transient. Free radicals are generated in vivo as by products of normal metabolism. They are also produced when an organism is exposed to ionizing radiation, to drugs capable of redox cycling, or to xenobiotics that can form free radical metabolites in situ. Cellular targets at risk from free radical damage depend on the nature of the radical and its site of generation. In this review we survey cellular sources of free radicals and the reactions they can undergo and discuss cellular defenses and adaptive mechanisms.",1982,0,3171,65,0,2,31,57,59,74,102,116,98,133
49cdf16489ee008649a00037cb22041a7bcd96ec,"CRC handbook of chemistry and physics , CRC handbook of chemistry and physics , کتابخانه مرکزی دانشگاه علوم پزشکی تهران",1990,0,17364,970,0,0,0,0,0,0,0,0,0,0
47552b2aa5f1137bc46b01daa91b9175837cc380,"Mathematical Introduction Acoustic Phonons Plasmons, Optical Phonons, and Polarization Waves Magnons Fermion Fields and the Hartree-Fock Approximation Many-body Techniques and the Electron Gas Polarons and the Electron-phonon Interaction Superconductivity Bloch Functions - General Properties Brillouin Zones and Crystal Symmetry Dynamics of Electrons in a Magnetic Field: de Haas-van Alphen Effect and Cyclotron Resonance Magnetoresistance Calculation of Energy Bands and Fermi Surfaces Semiconductor Crystals I: Energy Bands, Cyclotron Resonance, and Impurity States Semiconductor Crystals II: Optical Absorption and Excitons Electrodynamics of Metals Acoustic Attenuation in Metals Theory of Alloys Correlation Functions and Neutron Diffraction by Crystals Recoilless Emission Green's Functions - Application to Solid State Physics Appendix: Perturbation Theory and the Electron Gas Index.",1954,0,24309,953,0,0,0,0,0,0,0,0,0,0
958b8af460128a4c79e14f39329ca89e2e1294ad,1 The Atmosphere. 2 Atmospheric Trace Constituents. 3 Chemical Kinetics. 4 Atmospheric Radiation and Photochemistry. 5 Chemistry of the Stratosphere. 6 Chemistry of the Troposphere. 7 Chemistry of the Atmospheric Aqueous Phase. 8 Properties of the Atmospheric Aerosol. 9 Dynamics of Single Aerosol Particles. 10 Thermodynamics of Aerosols. 11 Nucleation. 12 Mass Transfer Aspects of Atmospheric Chemistry. 13 Dynamics of Aerosol Populations. 14 Organic Atmospheric Aerosols. 15 Interaction of Aerosols with Radiation. 16 Meteorology of the Local Scale. 17 Cloud Physics. 18 Atmospheric Diffusion. 19 Dry Deposition. 20 Wet Deposition. 21 General Circulation of the Atmosphere. 22 Global Cycles: Sulfur and Carbon. 23 Climate and Chemical Composition of the Atmosphere. 24 Aerosols and Climate. 25 Atmospheric Chemical Transport Models. 26 Statistical Models.,1997,0,12093,1517,0,0,0,0,0,0,0,0,1,1
a657fb77b5515d1968c331ebbda8919d5786d8be,"This biennial Review summarizes much of particle physics. Using data from previous editions., plus 2778 new measurements from 645 papers, we list, evaluate, and average measured properties of gauge bosons, leptons, quarks, mesons, and baryons. We also summarize searches for hypothetical particles such as Higgs bosons, heavy neutrinos, and supersymmetric particles. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as the Standard Model, particle detectors., probability, and statistics. Among the 108 reviews are many that are new or heavily revised including those on CKM quark-mixing matrix, V-ud & V-us, V-cb & V-ub, top quark, muon anomalous magnetic moment, extra dimensions, particle detectors, cosmic background radiation, dark matter, cosmological parameters, and big bang cosmology.",1996,761,13037,1066,2,0,1,0,0,1,1,3,3,20
e1d1e7dc2606b1d60cb85057ab7c5bbd52067661,,1979,0,14304,364,1,0,0,0,0,0,0,0,0,0
bf97653c065591cc23324b1ff5ddac020be67b0b,,1979,0,10408,438,0,0,0,0,0,0,42,123,123,177
34d19c80d05ef3ecb2c15d11663adb50f378710f,"The Monte Carlo method is a computer simulation method which uses random numbers to simulate statistical fluctuations. The method is used to model complex systems with many degrees of freedom. Probability distributions for these systems are generated numerically and the method then yields numerically exact information on the models. Such simulations may be used to see how well a model system approximates a real one or to see how valid the assumptions are in an analytical theory. A short and systematic theoretical introduction to the method forms the first part of this book. The second part is a practical guide with plenty of examples and exercises for the student. Problems treated by simple sampling (random and self-avoiding walks, percolation clusters, etc.) and by importance sampling (Ising models etc.) are included, along with such topics as finite-size effects and guidelines for the analysis of Monte Carlo simulations. The two parts together provide an excellent introduction to the theory and practice of Monte Carlo simulations.",1992,0,897,36,10,16,13,14,16,11,16,16,23,20
8d8e334076d16ba6cb157ae72f751276c1c1c78c,,1992,0,9911,258,76,100,123,380,418,368,299,257,320,280
af15b85cebb0a803f78f191a0fff1ff0ef3266c4,"Meeting the Universe Halfway is an ambitious book with far-reaching implications for numerous fields in the natural sciences, social sciences, and humanities. In this volume, Karen Barad, theoretical physicist and feminist theorist, elaborates her theory of agential realism. Offering an account of the world as a whole rather than as composed of separate natural and social realms, agential realism is at once a new epistemology, ontology, and ethics. The starting point for Barad’s analysis is the philosophical framework of quantum physicist Niels Bohr. Barad extends and partially revises Bohr’s philosophical views in light of current scholarship in physics, science studies, and the philosophy of science as well as feminist, poststructuralist, and other critical social theories. In the process, she significantly reworks understandings of space, time, matter, causality, agency, subjectivity, and objectivity.

In an agential realist account, the world is made of entanglements of “social” and “natural” agencies, where the distinction between the two emerges out of specific intra-actions. Intra-activity is an inexhaustible dynamism that configures and reconfigures relations of space-time-matter. In explaining intra-activity, Barad reveals questions about how nature and culture interact and change over time to be fundamentally misguided. And she reframes understanding of the nature of scientific and political practices and their “interrelationship.” Thus she pays particular attention to the responsible practice of science, and she emphasizes changes in the understanding of political practices, critically reworking Judith Butler’s influential theory of performativity. Finally, Barad uses agential realism to produce a new interpretation of quantum physics, demonstrating that agential realism is more than a means of reflecting on science; it can be used to actually do science.",2007,75,4103,345,10,12,38,62,87,102,170,217,250,367
60da513d2995b2710ad9fde8d263d96a128ffb99,"The PYTHIA program can be used to generate high-energy-physics `events', i.e. sets of outgoing particles produced in the interactions between two incoming particles. The objective is to provide as accurate as possible a representation of event properties in a wide range of reactions, with emphasis on those where strong interactions play a role, directly or indirectly, and therefore multihadronic final states are produced. The physics is then not understood well enough to give an exact description; instead the program has to be based on a combination of analytical results and various QCD-based models. This physics input is summarized here, for areas such as hard subprocesses, initial- and final-state parton showers, beam remnants and underlying events, fragmentation and decays, and much more. Furthermore, extensive information is provided on all program elements: subroutines and functions, switches and parameters, and particle and process data. This should allow the user to tailor the generation task to the topics of interest. 
The information in this edition of the manual refers to PYTHIA version 6.200, of 31 August 2001. 
The official reference to the latest published version is T. Sj\""ostrand, P. Ed\'en, C. Friberg, L. L\""onnblad, G. Miu, S. Mrenna and E. Norrbin, Computer Physics Commun. 135 (2001) 238.",2001,433,7441,288,6,14,20,38,53,67,137,199,256,324
a844f4ce5d00e8f62ba3a756c7112ed207e3b605,"N G van Kampen 1981 Amsterdam: North-Holland xiv + 419 pp price Dfl 180 This is a book which, at a lower price, could be expected to become an essential part of the library of every physical scientist concerned with problems involving fluctuations and stochastic processes, as well as those who just enjoy a beautifully written book. It provides an extensive graduate-level introduction which is clear, cautious, interesting and readable.",1983,0,4260,674,2,0,4,3,4,6,4,5,4,6
539f4cdfd83a6e3487c4134509c9eb687c145dc0,,1972,0,7697,428,1,8,7,9,11,40,48,57,86,84
0095c84f0a1a39cbc56930aab7839de0aab37505,The transformation of snow to ice mass balance heat budget and climatology structure and deformation of ice hydraulics and glaciers glacier sliding deformation of subglacial till structures and fabrics in glaciers and ice sheets distribution of temperature in glaciers and ice sheets steady flow of glaciers and ice sheets flow of ice shelves and ice streams non-steady flow of glaciers and ice sheets surging and tidewater glaciers ice core studies.,1981,0,4027,544,7,14,22,20,19,29,28,11,28,21
a442ba386113f089b64c55c423b39d99f4cee80e,"These are a set of notes I have made, based on lectures given by M.Moore at the University of Manchester Jan-June ’08. Please e-mail me with any comments/corrections: jap@watering.co.uk.",1962,0,6320,524,3,2,1,2,2,1,4,3,3,1
5366ae191ffd171dd8a5053b5356abee031ffaa4,Preface. Acknowledgments. Introduction. Hydrolysis and Condensation I: Nonsilicates. Hydrolysis and Condensation II: Silicates. Particulate Sols and Gels. Gelation. Aging of Gels. Theory of Deformation and Flow in Gels. Drying. Structural Evolution during Consolidation. Surface Chemistry and Chemical Modification. Sintering. Comparison of Gel-Derived and Conventional Ceramics. Film Formation. Applications. Index.,1990,0,7796,338,3,16,46,31,85,89,100,145,140,129
25371c278c41a7aa99a6446ab419a6925d278b76,"It has been recognized for some time that the spontaneous emission by atoms is not necessarily a fixed and immutable property of the coupling between matter and space, but that it can be controlled by modification of the properties of the radiation field. This is equally true in the solid state, where spontaneous emission plays a fundamental role in limiting the performance of semiconductor lasers, heterojunction bipolar transistors, and solar cells. If a three-dimensionally periodic dielectric structure has an electromagnetic band gap which overlaps the electronic band edge, then spontaneous emission can be rigorously forbidden.",1987,12,11487,177,0,0,0,0,0,0,0,0,0,0
5b9180506668890dff1491e5f0be295735427631,"3rd edition, complete modern revision C. Kittel London: John Wiley. 1966. Pp. 648. Price £4 14s. Kind's new edition is to be welcomed. There is a revised format and attractive illustrations, and with the inclusion of much new material this book has become one of the best sources for undergraduate teaching. It is above all an interesting book, likely to give the student a wish to dig deeper into the solid state.",1967,37,5539,323,0,5,4,4,2,8,17,8,9,5
f031e06500d6afe6e7c6593530bf84b7bad3c15c,"Part 1 Liquid crystals - main types and properties: introduction - what is a liquid crystal? the building blocks nematics and cholesterics smectics columnar phases more on long-, quasi-long and short-range order remarkable features of liquid crystals. Part 2 Long- and short-range order in nematics: definition of an order parameter statistical theories of the nematic order phenomonological description of the nematic-isotopic mixtures. Part 3 Static distortion in a nematic single crystal: principles of the continuum theory magnetic field effects electric field effects in an insulating nematic fluctuations in the alignment hydrostatics of nematics. Part 4 Defects and textures in nematics: observations disclination lines point disclinations walls under magnetic fields umbilics surface disclinations. Part 5 Dynamical properties of nematics: the equations of ""nematodynamics"" experiments measuring the Leslie co-efficients convective instabilities under electric fields molecular motions. Part 6 Cholesterics: optical properties of an ideal helix agents influencing the pitch dynamical properties textures and defects in cholesterics. Part 7 Smectics: symmetry of the main smectic phases continuum description of smectics A and C remarks on phase and precritical phenomena.",1974,0,7666,258,2,33,53,59,57,74,71,92,76,97
a0c3e1d0353f00fd9df6931af82010f2172168cf,Preface to the first edition. Preface to the second edition. Abbreviated references. I. Stochastic variables. II. Random events. III. Stochastic processes. IV. Markov processes. V. The master equation. VI. One-step processes. VII. Chemical reactions. VIII. The Fokker-Planck equation. IX. The Langevin approach. X. The expansion of the master equation. XI. The diffusion type. XII. First-passage problems. XIII. Unstable systems. XIV. Fluctuations in continuous systems. XV. The statistics of jump events. XVI. Stochastic differential equations. XVII. Stochastic behavior of quantum systems.,1981,0,6448,245,0,3,21,57,55,82,86,82,85,62
b029856e343340fc7c034f514dc15a2ad49f6c1a,"This paper reviews recent experimental and theoretical progress concerning many-body phenomena in dilute, ultracold gases. It focuses on effects beyond standard weak-coupling descriptions, such as the Mott-Hubbard transition in optical lattices, strongly interacting gases in one and two dimensions, or lowest-Landau-level physics in quasi-two-dimensional gases in fast rotation. Strong correlations in fermionic gases are discussed in optical lattices or near-Feshbach resonances in the BCS-BEC crossover.",2007,576,4518,146,33,117,220,244,322,319,323,285,304,356
529595f0bbf7d8d38354436f5ce7a3293e66bd05,"On the program it says this is a keynote speech--and I don't know what a keynote speech is. I do not intend in any way to suggest what should be in this meeting as a keynote of the subjects or anything like that. I have my own things to say and to talk about and there's no implication that anybody needs to talk about the same thing or anything like it. So what I want to talk about is what Mike Dertouzos suggested that nobody would talk about. I want to talk about the problem of simulating physics with computers and I mean that in a specific way which I am going to explain. The reason for doing this is something that I learned about from Ed Fredkin, and my entire interest in the subject has been inspired by him. It has to do with learning something about the possibilities of computers, and also something about possibilities in physics. If we suppose that we know all the physical laws perfectly, of course we don't have to pay any attention to computers. It's interesting anyway to entertain oneself with the idea that we've got something to learn about physical laws; and if I take a relaxed view here (after all I 'm here and not at home) I'll admit that we don't understand everything. The first question is, What kind of computer are we going to use to simulate physics? Computer theory has been developed to a point where it realizes that it doesn't make any difference; when you get to a universal computer, it doesn't matter how it's manufactured, how it's actually made. Therefore my question is, Can physics be simulated by a universal computer? I would like to have the elements of this computer locally interconnected, and therefore sort of think about cellular automata as an example (but I don't want to force it). But I do want something involved with the",1999,59,5495,206,65,65,90,99,87,124,118,149,151,167
c210410d292663287dd188832f17d4917ad6b03f,"Allis and Herlin Thermodynamics and Statistical Mechanics Becker Introduction to Theoretical Mechanics Clark Applied X-rays Collin Field Theory of Guided Waves Evans The Atomic Nucleus Finkelnburg Atomic Physics Ginzton Microwave Measurements Green Nuclear Physics Gurney Introduction to Statistical Mechanics Hall Introduction to Electron Microscopy Hardy and Perrin The Principles of Optics Harnwell Electricity and Electromagnetism Harnwell and Livingood Experimental Atomic Physics Harnwell and Stephens Atomic Physics Henley and Thirring Elementary Quantum Field Theory Houston Principles of Mathematical Physics Hund High-frequency Measurements Kennard Kinetic Theory of Gases Lane Superfluid Physics Leighton Principles of Modern Physics Lindsay Mechanical Radiation Livingston and Blewett Particle Accelerators Middleton An Introduction to Statistical Communication Theory Morse Vibration and Sound Morse and Feshbach Methods of Theoretical Physics Muskat Physical Principles of Oil Production Present Kinetic Theory of Gases Read Dislocations in Crystals Richtmyer, Kennard, and Lauritsen Introduction to Modern Physics Schiff Quantum Mechanics Seitz The Modern Theory of Solids Slater Introduction to Chemical Physics Slater Quantum Theory of Matter Slater Quantum Theory of Atomic Structure, Vol. I Slater Quantum Theory of Atomic Structure, Vol. II Slater Quantum Theory of Molecules and Solids, Vol. 1 Slater and Frank Electromagnetism Slater and Frank Introduction to Theoretical Physics Slater and Frank Mechanics Smythe Static and Dynamic Electricity Stratton Electromagnetic Theory Thorndike Mesons: A Summary of Experimental Facts Townes and Schawlow Microwave Spectroscopy White Introduction to Atomic Spectra",1955,11,9463,155,7,19,22,33,33,48,61,49,55,82
63e9a5df23631a9584ba9a7745f9b12e85cf4f95,"Statistical Physics. By F. Mandl. Pp. xiii + 379. (Wiley: London and New York, July 1971.) £2.75. Statistical Physics. By A. Isihara. Pp. xv + 439. (Academic: New York and London, June 1971.) $18.50; £8.65.",1971,4,3702,288,3,1,1,0,4,1,3,2,4,3
d4686dec40085e3f5415c21b162ec06efdece47c,,1973,0,32448,17,0,0,0,0,0,0,0,0,0,0
06b8f0e6e9918fad9c381ac98a13f134febfdb2e,"Preface. Introduction. PART I: SEMICONDUCTOR PHYSICS. Energy Bands and Carrier Concentration in Thermal Equilibrium. Carrier Transport Phenomena. PART II: SEMICONDUCTOR DEVICES. p-n Junction. Bipolar Transistor and Related Devices. MOSFET and Related Devices. MESFET and Related Devices. Microwave Diodes, Quantum-Effect, and Hot-Electron Devices. Photonic Devices. PART III: SEMICONDUCTOR TECHNOLOGY. Crystal Growth and Epitaxy. Film Formation. Lithography and Etching. Impurity Doping. Integrated Devices. Appendix A: List of Symbols. Appendix B: International Systems of Units (SI Units). Appendix C: Unit Prefixes. Appendix D: Greek Alphabet. Appendix E: Physical Constants. Appendix F: Properties of Important Element and Binary Compound Semiconductors at 300 K. Appendix G: Properties of Si and GaAs at 300 K. Appendix H: Derivation of the Density of States in Semiconductor. Appendix I: Derivation of Recombination Rate for Indirect Recombination. Appendix J: Calculation of the Transmission Coefficient for a Symmetric Resonant-Tunneling Diode. Appendix K: Basic Kinetic Theory of Gases. Appendix L: Answers to Selected Problems. Index.",1985,0,3384,222,0,0,9,5,17,25,27,35,38,37
594ee853ff36c1d26d1d2e067d53a97531dc5dea,,1973,0,6569,202,57,60,63,69,71,75,75,89,78,99
8de008afdc3535d09dbbb33e52976a4277ba51d7,"Contents: General results and concepts on invariant sets and attractors.- Elements of functional analysis.- Attractors of the dissipative evolution equation of the first order in time: reaction-diffusion equations.- Fluid mechanics and pattern formation equations.- Attractors of dissipative wave equations.- Lyapunov exponents and dimensions of attractors.- Explicit bounds on the number of degrees of freedom and the dimension of attractors of some physical systems.- Non-well-posed problems, unstable manifolds. lyapunov functions, and lower bounds on dimensions.- The cone and squeezing properties.- Inertial manifolds.- New chapters: Inertial manifolds and slow manifolds the nonselfadjoint case.",1993,4,3674,229,38,46,69,70,73,96,86,79,109,113
fc00cf7afd7b3fc4a6fad4c4ba8bc20b64ae5c07,"Nanocrystals are fundamental to modern science and technology. Mastery over the shape of a nanocrystal enables control of its properties and enhancement of its usefulness for a given application. Our aim is to present a comprehensive review of current research activities that center on the shape-controlled synthesis of metal nanocrystals. We begin with a brief introduction to nucleation and growth within the context of metal nanocrystal synthesis, followed by a discussion of the possible shapes that a metal nanocrystal might take under different conditions. We then focus on a variety of experimental parameters that have been explored to manipulate the nucleation and growth of metal nanocrystals in solution-phase syntheses in an effort to generate specific shapes. We then elaborate on these approaches by selecting examples in which there is already reasonable understanding for the observed shape control or at least the protocols have proven to be reproducible and controllable. Finally, we highlight a number of applications that have been enabled and/or enhanced by the shape-controlled synthesis of metal nanocrystals. We conclude this article with personal perspectives on the directions toward which future research in this field might take.",2009,564,4355,34,38,139,265,377,437,455,469,444,400,361
16ef4cc3a80ee7ba8f59e0a55b2ef134c31e18b3,"The representation of physics problems in relation to the organization of physics knowledge is investigated in experts and novices. Four experiments examine (a) the existence of problem categories as a basis for representation; (b) differences in the categories used by experts and novices; (c) differences in the knowledge associated with the categories; and (d) features in the problems that contribute to problem categorization and representation. Results from sorting tasks and protocols reveal that experts and novices begin their problem representations with specifiably different problem categories, and completion of the representations depends on the knowledge associated with the categories. For, the experts initially abstract physics principles to approach and solve a problem representation, whereas novices base their representation and approaches on the problem's literal features.",1981,26,5021,129,1,17,24,30,34,70,81,62,83,96
972528bb6930bbb2b9936e79d68383c6886c5aae,"1. Introduction.- 1.1 What Is the Subject of Gas Discharge Physics.- 1.2 Typical Discharges in a Constant Electric Field.- 1.3 Classification of Discharges.- 1.4 Brief History of Electric Discharge Research.- 1.5 Organization of the Book. Bibliography.- 2. Drift, Energy and Diffusion of Charged Particles in Constant Fields.- 2.1 Drift of Electrons in a Weakly Ionized Gas.- 2.2 Conduction of Ionized Gas.- 2.3 Electron Energy.- 2.4 Diffusion of Electrons.- 2.5 Ions.- 2.6 Ambipolar Diffusion.- 2.7 Electric Current in Plasma in the Presence of Longitudinal Gradients of Charge Density.- 2.8 Hydrodynamic Description of Electrons.- 3. Interaction of Electrons in an Ionized Gas with Oscillating Electric Field and Electromagnetic Waves.- 3.1 The Motion of Electrons in Oscillating Fields.- 3.2 Electron Energy.- 3.3 Basic Equations of Electrodynamics of Continuous Media.- 3.4 High-Frequency Conductivity and Dielectric Permittivity of Plasma.- 3.5 Propagation of Electromagnetic, Waves in Plasmas.- 3.6 Total Reflection of Electromagnetic Waves from Plasma and Plasma Oscillations.- 4. Production and Decay of Charged Particles.- 4.1 Electron Impact Ionization in a Constant Field.- 4.2 Other Ionization Mechanisms.- 4.3 Bulk Recombination.- 4.4 Formation and Decay of Negative Ions.- 4.5 Diffusional Loss of Charges.- 4.6 Electron Emission from Solids.- 4.7 Multiplication of Charges in a Gas via Secondary Emission.- 5. Kinetic Equation for Electrons in a Weakly Ionized Gas Placed in an Electric Field.- 5.1 Description of Electron Processes in Terms of the Velocity Distribution Function.- 5.2 Formulation of the Kinetic Equation.- 5.3 Approximation for the Angular Dependence of the Distribution Function.- 5.4 Equation of the Electron Energy Spectrum.- 5.5 Validity Criteria for the Spectrum Equation.- 5.6 Comparison of Some Conclusions Implied by the Kinetic Equation with the Result of Elementary Theory.- 5.7 Stationary Spectrum of Electrons in a Field in the Case of only Elastic Losses.- 5.8 Numerical Results for Nitrogen and Air.- 5.9 Spatially Nonuniform Fields of Arbitrary Strength.- 6. Electric Probes.- 6.1 Introduction. Electric Circuit.- 6.2 Current-Voltage Characteristic of a Single Probe.- 6.3 Theoretical Foundations of Electronic Current Diagnostics of Rarefied Plasmas.- 6.4 Procedure for Measuring the Distribution Function.- 6.5 Ionic Current to a Probe in Rarefied Plasma.- 6.6 Vacuum Diode Current and Space-Charge Layer Close to a Charged Body.- 6.7 Double Probe.- 6.8 Probe in a High-Pressure Plasma.- 7. Breakdown of Gases in Fields of Various Frequency Ranges.- 7.1 Essential Characteristics of the Phenomenon.- 7.2 Breakdown and Triggering of Self-Sustained Discharge in a Constant Homogeneous Field at Moderately Large Product of Pressure and Discharge Gap Width.- 7.3 Breakdown in Microwave Fields and Interpretation of Experimental Data Using the Elementary Theory.- 7.4 Calculation of Ionization Frequencies and Breakdown Thresholds Using the Kinetic Equation.- 7.5 Optical Breakdown.- 7.6 Methods of Exciting an RF Field in a Discharge Volume.- 7.7 Breakdown in RF and Low-Frequency Ranges.- 8. Stable Glow Discharge.- 8.1 General Structure and Observable Features.- 8.2 Current-Voltage Characteristic of Discharge Between Electrodes.- 8.3 Dark Discharge and the Role Played by Space Charge in the Formation of the Cathode Layer.- 8.4 Cathode Layer.- 8.5 Transition Region Between the Cathode Layer and the Homogeneous Positive Column.- 8.6 Positive Column.- 8.7 Heating of the Gas and Its Effect on the Current-Voltage Characteristic.- 8.8 Electronegative Gas Plasma.- 8.9 Discharge in Fast Gas Flow.- 8.10 Anode Layer.- 9. Glow Discharge Instabilities and Their Consequences.- 9.1 Causes and Consequences of Instabilities.- 9.2 Quasisteady Parameters.- 9.3 Field and Electron Temperature Perturbations in the Case of Quasisteady-State Te.- 9.4 Thermal Instability.- 9.5 Attachment Instability.- 9.6 Some Other Frequently Encountered Destabilizing Mechanisms.- 9.7 Striations.- 9.8 Contraction of the Positive Column.- 10. Arc Discharge.- 10.1 Definition and Characteristic Features of Arc Discharge.- 10.2 Arc Types.- 10.3 Arc Initiation.- 10.4 Carbon Arc in Free Air.- 10.5 Hot Cathode Arc: Processes near the Cathode.- 10.6 Cathode Spots and Vacuum Arc.- 10.7 Anode Region.- 10.8 Low-Pressure Arc with Externally Heated Cathode.- 10.9 Positive Column of High-Pressure Arc (Experimental Data).- 10.10 Plasma Temperature and V - i Characteristic of High-Pressure Arc Columns.- 10.11 The Gap Between Electron and Gas Temperatures in ""Equilibrium"" Plasma.- 11. Suslainment and Production of Equilibrium Plasma by Fields in Various Frequency Ranges.- 11.1 Introduction. Energy Balance in Plasma.- 11.2 Arc Column in a Constant Field.- 11.3 Inductively Coupled Radio-Frequency Discharge.- 11.4 Discharge in Microwave Fields.- 11.5 Continuous Optical Discharges.- 11.6 Plasmatrons: Generators of Dense Low-Temperature Plasma.- 12. Spark and Corona Discharges.- 12.1 General Concepts.- 12.2 Individual Electron Avalanche.- 12.3 Concept of Streamers.- 12.4 Breakdown and Streamers in Electronegative Gases (Air) in Moderately Wide Gaps with a Uniform Field.- 12.5 Spark Channel.- 12.6 Corona Discharge.- 12.7 Models of Streamer Propagation.- 12.8 Breakdown in Long Air Gaps with Strongly Nonuniform Fields (Experimental Data).- 12.9 Leader Mechanism of Breakdown of Long Gaps.- 12.10 Return Wave (Return Stroke).- 12.11 Lightning.- 12.12 Negative Stepped Leader.- 13. Capacitively Coupled Radio-Frequency Discharge.- 13.1 Drift Oscillations of Electron Gas.- 13.2 Idealized Model of the Passage of High-Frequency Current Through a Long Plane Gap at Elevated Pressures.- 13.3 V - i Characteristic of Homogeneous Positive Columns.- 13.4 Two Forms of CCRF Discharge Realization and Constant Positive Potential of Space: Experiment.- 13.5 Electrical Processes in a Nonconducting Electrode Layer and the Mechanism of Closing the Circuit Current.- 13.6 Constant Positive Potential of the Weak-Current Discharge Plasma.- 13.7 High-Current Mode.- 13.8 The Structure of a Medium-Pressure Discharge: Results of Numerical Modeling.- 13.9 Normal Current Density in Weak-Current Mode and Limits on the Existence of this Mode.- 14. Discharges in High-Power CW CO2 Lasers.- 14.1 Principles of Operation of Electric-Discharge CO2 Lasers.- 14.2 Two Methods of Heat Removal from Lasers.- 14.3 Methods of Suppressing Instabilities.- 14.4 Organization of Large-Volume Discharges Involving Gas Pumping.- References.",1991,0,3812,117,2,1,8,7,26,35,26,49,59,87
4e72246678077558b4623954f0301bfb82b20807,"1. Introductory Material.- 1.1. Harmonic Oscillators and Phonons.- 1.2. Second Quantization for Particles.- 1.3. Electron - Phonon Interactions.- A. Interaction Hamiltonian.- B. Localized Electron.- C. Deformation Potential.- D. Piezoelectric Interaction.- E. Polar Coupling.- 1.4. Spin Hamiltonians.- A. Homogeneous Spin Systems.- B. Impurity Spin Models.- 1.5. Photons.- A. Gauges.- B. Lagrangian.- C. Hamiltonian.- 1.6. Pair Distribution Function.- Problems.- 2. Green's Functions at Zero Temperature.- 2.1. Interaction Representation.- A. Schrodinger.- B. Heisenberg.- C. Interaction.- 2.2. S Matrix.- 2.3. Green's Functions.- 2.4. Wick's Theorem.- 2.5. Feynman Diagrams.- 2.6. Vacuum Polarization Graphs.- 2.7. Dyson's Equation.- 2.8. Rules for Constructing Diagrams.- 2.9. Time-Loop S Matrix.- A. Six Green's Functions.- B. Dyson's Equation.- 2.10. Photon Green's Functions.- Problems.- 3. Green's Functions at Finite Temperatures.- 3.1. Introduction.- 3.2. Matsubara Green's Functions.- 3.3. Retarded and Advanced Green's Functions.- 3.4. Dyson's Equation.- 3.5. Frequency Summations.- 3.6. Linked Cluster Expansions.- A. Thermodynamic Potential.- B. Green's Functions.- 3.7. Real Time Green's Functions.- Wigner Distribution Function.- 3.8. Kubo Formula for Electrical Conductivity.- A. Transverse Fields, Zero Temperature.- B. Finite Temperatures.- C. Zero Frequency.- D. Photon Self-Energy.- 3.9. Other Kubo Formulas.- A. Pauli Paramagnetic Susceptibility.- B. Thermal Currents and Onsager Relations.- C. Correlation Functions.- Problems.- 4. Exactly Solvable Models.- 4.1. Potential Scattering.- A. Reaction Matrix.- B. T Matrix.- C. Friedel's Theorem.- D. Phase Shifts.- E. Impurity Scattering.- F. Ground State Energy.- 4.2. Localized State in the Continuum.- 4.3. Independent Boson Models.- A. Solution by Canonical Transformation.- B. Feynman Disentangling of Operators.- C. Einstein Model.- D. Optical Absorption and Emission.- E. Sudden Switching.- F. Linked Cluster Expansion.- 4.4. Tomonaga Model.- A. Tomonaga Model.- B. Spin Waves.- C. Luttinger Model.- D. Single-Particle Properties.- E. Interacting System of Spinless Fermions.- F. Electron Exchange.- 4.5. Polaritons.- A. Semiclassical Discussion.- B. Phonon-Photon Coupling.- C. Exciton-Photon Coupling.- Problems.- 5. Electron Gas.- 5.1. Exchange and Correlation.- A. Kinetic Energy.- B. Direct Coulomb.- C. Exchange.- D. Seitz' Theorem.- E. ?(2a).- F. ?(2b).- G. ?(2c).- H. High-Density Limit.- I. Pair Distribution Function.- 5.2. Wigner Lattice and Metallic Hydrogen.- Metallic Hydrogen.- 5.3. Cohesive Energy of Metals.- 5.4. Linear Screening.- 5.5. Model Dielectric Functions.- A. Thomas-Fermi.- B. Lindhard, or RPA.- C. Hubbard.- D. Singwi-Sjolander.- 5.6. Properties of the Electron Gas.- A. Pair Distribution Function.- B. Screening Charge.- C. Correlation Energies.- D. Compressibility.- 5.7. Sum Rules.- 5.8. One-Electron Properties.- A. Renormalization Constant ZF.- B. Effective Mass.- C. Pauli Paramagnetic Susceptibility.- D. Mean Free Path.- Problems.- 6. Electron-Phonon Interaction.- 6.1 Frohlich Hamiltonian.- A. Brillouin-Wigner Perturbation Theory.- B. Rayleigh-Schrodinger Perturbation Theory.- C. Strong Coupling Theory.- D. Linked Cluster Theory.- 6.2 Small Polaron Theory.- A. Large Polarons.- B. Small Polarons.- C. Diagonal Transitions.- D. Nondiagonal Transitions.- E. Dispersive Phonons.- F. Einstein Model.- G. Kubo Formula.- 6.3 Heavily Doped Semiconductors.- A. Screened Interaction.- B. Experimental Verifications.- C. Electron Self-Energies.- 6.4 Metals.- A. Phonons in Metals.- B. Electron Self-Energies.- Problems.- 7. dc Conductivities.- 7.1. Electron Scattering by Impurities.- A. Boltzmann Equation.- B. Kubo Formula: Approximate Solution.- C. Kubo Formula: Rigorous Solution.- D. Ward Identities.- 7.2. Mobility of Frohlich Polarons.- A. Single-Particle Properties.- B. ??1 Term in the Mobility.- 7.3. Electron-Phonon Interactions in Metals.- A. Force-Force Correlation Function.- B. Kubo Formula.- C. Mass Enhancement.- D. Thermoelectric Power.- 7.4. Quantum Boltzmann Equation.- A. Derivation of the Quantum Boltzmann Equation.- B. Gradient Expansion.- C. Electron Scattering by Impurities.- D. T2 Contribution to the Electrical Resistivity.- Problems.- 8. Optical Properties of Solids.- 8.1. Nearly Free-Electron System.- A. General Properties.- B. Force-Force Correlation Functions.- C. Frohlich Polarons.- D. Interband Transitions.- E. Phonons.- 8.2. Wannier Excitons.- A. The Model.- B. Solution by Green's Functions.- C. Core-Level Spectra.- 8.3. X-Ray Spectra in Metals.- A. Physical Model.- B. Edge Singularities.- C. Orthogonality Catastrophe.- D. MND Theory.- E. XPS Spectra.- Problems.- 9. Superconductivity.- 9.1. Cooper Instability.- 9.2. BCS Theory.- 9.3. Electron Tunneling.- A. Tunneling Hamiltonian.- B. Normal Metals.- C. Normal-Superconductor.- D. Two Superconductors.- E. Josephson Tunneling.- 9.4. Infrared Absorption.- 9.5. Acoustic Attenuation.- 9.6. Excitons in Superconductors.- 9.7. Strong Coupling Theory.- Problems.- 10. Liquid Helium.- 10.1. Pairing Theory.- A. Hartree and Exchange.- B. Bogoliubov Theory of 4He.- 10.2. 4He: Ground State Properties.- A. Off-Diagonal Long-Range Order.- B. Correlated Basis Functions.- C. Experiments on nk.- 10.3. 4He: Excitation Spectrum.- A. Bijl-Feynman Theory.- B. Improved Excitation Spectra.- C. Superfluidity.- 10.4. 3He: Normal Liquid.- A. Fermi Liquid Theory.- B. Experiments and Microscopic Theories.- C. Interaction between Quasiparticles: Excitations.- D. Quasiparticle Transport.- 10.5. Superfluid 3He.- A. Triplet Pairing.- B. Equal Spin Pairing.- Problems.- 11. Spin Fluctuations.- 11.1. Kondo Model.- A. High-Temperature Scattering.- B. Low-Temperature State.- C. Kondo Temperature.- 11.2. Anderson Model.- A. Collective States.- B. Green's Functions.- C. Spectroscopies.- Problems.- References.- Author Index.",1981,0,5112,151,0,3,10,18,20,17,20,21,21,24
8a54948904ebbdb40aaf12b8e79aed5a3ef09ef3,,1995,0,3034,270,11,27,28,46,58,49,38,70,74,80
1e491759364db68bf0175c1de47edbd42aca8c61,,1956,0,4482,186,1,2,35,46,50,72,54,63,70,72
524954d758539d0db33cf89a2ebbb8ecf7b73235,,1969,0,15412,25,0,0,0,0,0,0,0,0,0,0
565f416e97eac888d88b367ebdaadbb6c52b405b,,1993,0,2772,254,1,5,19,18,14,16,12,14,23,41
1e44defdfe2e7b795ac2b8d2b5a28c9bfb547248,,1943,0,6431,127,0,0,1,4,6,5,17,6,12,20
5aeff09300fcda097719430e0dab6ab1ab3b0264,"An introduction to fractional calculus, P.L. Butzer & U. Westphal fractional time evolution, R. Hilfer fractional powers of infinitesimal generators of semigroups, U. Westphal fractional differences, derivatives and fractal time series, B.J. West and P. Grigolini fractional kinetics of Hamiltonian chaotic systems, G.M. Zaslavsky polymer science applications of path integration, integral equations, and fractional calculus, J.F. Douglas applications to problems in polymer physics and rheology, H. Schiessel et al applications of fractional calculus and regular variation in thermodynamics, R. Hilfer.",2000,8,4646,111,13,12,51,37,37,55,68,77,112,148
9e0c986e906869febb0f87258edab602bc51fcaf,,1964,0,4121,180,27,26,37,44,46,50,44,44,56,56
4c42f07d606f510b7409227db33bb41627bde1b2,1. Peculiarities of d=1 2. Bosonization 3. Luttinger liquids 4. Refinements 5. Microscopic methods 6. Spin 1/2 chains 7. Interacting fermions on a lattice 8. Coupled fermionic chains 9. Disordered systems 10. Boundaries and isolated impurities 11. Significant others A. Basics of many body B. Not so important fine technical points C. Correlation functions D. Bosonization directory E. Sine-Gordon F. Numerical solution,2004,0,2653,261,27,42,87,137,153,151,155,168,160,172
d1168dc15e456520d5a78d0c04fa2985654b39f4,,1971,0,3746,144,2,2,4,4,4,3,2,4,4,4
e3e2901a80d7f663fc4ff7cf89456cf79d040e6e,"This book is designed for the junior-senior thermodynamics course given in all departments as a standard part of the curriculum. The book is devoted to a discussion of some of the basic physical concepts and methods useful in the description of situations involving systems which consist of very many particulars. It attempts, in particular, to introduce the reader to the disciplines of thermodynamics, statistical mechanics, and kinetic theory from a unified and modern point of view. The presentation emphasizes the essential unity of the subject matter and develops physical insight by stressing the microscopic content of the theory.",1965,0,3323,180,0,1,4,1,5,5,2,5,6,3
fdc354d08984f0b36be818b8a287a767cad727ed,Air Pollutants Effects of Air Pollution Sources of Pollutants in Combustion Processes Gas-Phase Atmospheric Chemistry Aqueous-Phase Atmospheric Chemistry Mass Transfer Aspects of Atmospheric Chemistry Properties of Aerosols Dynamics of Single Aerosol Particles Thermodynamics of Aerosols and Nucleation Theory Dynamics of Aerosol Population Air Pollution Meteorology Micrometeorology Atmospheric Diffusion Theories The Gaussian Plume Equation The Atmospheric Diffusion Equation and Air Quality Models Atmospheric Removal Processes and Residence Times Air Pollution Statistics Acid Rain Index.,1986,0,2652,272,1,15,21,31,51,56,52,74,70,77
37491ed087839d86c9ba7783cb12ccc6c9918bf6,"Microfabricated integrated circuits revolutionized computation by vastly reducing the space, labor, and time required for calculations. Microfluidic systems hold similar promise for the large-scale automation of chemistry and biology, suggesting the possibility of numerous experiments performed rapidly and in parallel, while consuming little reagent. While it is too early to tell whether such a vision will be realized, significant progress has been achieved, and various applications of significant scientific and practical interest have been developed. Here a review of the physics of small volumes (nanoliters) of fluids is presented, as parametrized by a series of dimensionless numbers expressing the relative importance of various physical phenomena. Specifically, this review explores the Reynolds number Re, addressing inertial effects; the Peclet number Pe, which concerns convective and diffusive transport; the capillary number Ca expressing the importance of interfacial tension; the Deborah, Weissenberg, and elasticity numbers De, Wi, and El, describing elastic effects due to deformable microstructural elements like polymers; the Grashof and Rayleigh numbers Gr and Ra, describing density-driven flows; and the Knudsen number, describing the importance of noncontinuum molecular effects. Furthermore, the long-range nature of viscous flows and the small device dimensions inherent in microfluidics mean that the influence of boundaries is typically significant. A variety of strategies have been developed to manipulate fluids by exploiting boundary effects; among these are electrokinetic effects, acoustic streaming, and fluid-structure interactions. The goal is to describe the physics behind the rich variety of fluid phenomena occurring on the nanoliter scale using simple scaling arguments, with the hopes of developing an intuitive sense for this occasionally counterintuitive world.",2005,1080,3427,70,17,77,129,221,219,226,241,278,285,260
a8d6e6553a2724f2aa11d31fd9d3d617d9e86d57,,1972,0,3384,168,0,5,0,5,6,5,15,6,9,16
e419cfbbdd1de7f9a2ed6bb2d5392840dcb2a4fd,"Statistical physics has proven to be a fruitful framework to describe phenomena outside the realm of traditional physics. Recent years have witnessed an attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. A wide list of topics are reviewed ranging from opinion and cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, and social spreading. The connections between these problems and other, more traditional, topics of statistical physics are highlighted. Comparison of model results with empirical data from social systems are also emphasized.",2007,1011,2977,77,5,33,85,140,183,249,230,273,276,252
281f3c1d9cc26c75c09510be27e02f11456e36e5,,2006,0,3568,26,5,20,39,56,84,108,133,211,269,346
82b6305d085d0f77a64c6e2ff0c1d413be23275d,,2009,0,1985,194,69,78,99,127,146,152,158,153,162,179
eabb2f4d4b9f767475cd5dc37d70c32196545a4c,Partial table of contents: THE ALGEBRA OF LINEAR TRANSFORMATIONS AND QUADRATIC FORMS. Transformation to Principal Axes of Quadratic and Hermitian Forms. Minimum-Maximum Property of Eigenvalues. SERIES EXPANSION OF ARBITRARY FUNCTIONS. Orthogonal Systems of Functions. Measure of Independence and Dimension Number. Fourier Series. Legendre Polynomials. LINEAR INTEGRAL EQUATIONS. The Expansion Theorem and Its Applications. Neumann Series and the Reciprocal Kernel. The Fredholm Formulas. THE CALCULUS OF VARIATIONS. Direct Solutions. The Euler Equations. VIBRATION AND EIGENVALUE PROBLEMS. Systems of a Finite Number of Degrees of Freedom. The Vibrating String. The Vibrating Membrane. Green's Function (Influence Function) and Reduction of Differential Equations to Integral Equations. APPLICATION OF THE CALCULUS OF VARIATIONS TO EIGENVALUE PROBLEMS. Completeness and Expansion Theorems. Nodes of Eigenfunctions. SPECIAL FUNCTIONS DEFINED BY EIGENVALUE PROBLEMS. Bessel Functions. Asymptotic Expansions. Additional Bibliography. Index.,1962,0,4077,68,26,18,22,34,30,38,27,35,32,42
04434bdd67911dfce26d57466e9a02d70de0cb61,"Preface 1. Overview 2. Structure and scattering 3. Thermodynamics and statistical mechanics 4. Mean-field theory 5. Field theories, critical phenomena, and the renormalization group 6. Generalized elasticity 7. Dynamics: correlation and response 8. Hydrodynamics 9. Topological defects 10. Walls, kinks and solitons Glossary Index.",2000,0,2557,178,56,97,82,114,111,115,112,110,124,149
3f639b1dc7a1d60129248888bc7f388b148dccf7,Preface 1. Basic tools 2. Elasticity and Hooke's law 3. Seismic wave propagation 4. Effective media 5. Granular media 6. Fluid effects on wave propagation 7. Empirical relations 8. Flow and diffusion 9. Electrical properties Appendices.,1998,0,2195,211,2,1,17,24,26,37,43,64,48,60
52e1eba3b9132599c4788c0ed80751a0975aed80,"BiFeO3 is perhaps the only material that is both magnetic and a strong ferroelectric at room temperature. As a result, it has had an impact on the field of multiferroics that is comparable to that of yttrium barium copper oxide (YBCO) on superconductors, with hundreds of publications devoted to it in the past few years. In this Review, we try to summarize both the basic physics and unresolved aspects of BiFeO3 (which are still being discovered with several new phase transitions reported in the past few months) and device applications, which center on spintronics and memory devices that can be addressed both electrically and magnetically.",2009,202,2896,15,17,99,157,193,223,236,300,289,239,309
b26d845e7dcfee0b6f9f236440477be4166711c8,"FIRST EXPERIMENTS IN JET. Results obtained from JET since June 1983 are described which show that this large tokamak behaves in a similar manner to smaller tokamaks, but with correspondingly improved plasma parameters. Long-duration hydrogen and deuterium plasmas (>10 s) have been obtained with electron temperatures reaching > 4 keV for power dissipations < 3 MW and with * Euratom-IPP Association, Institut fur Plasmaphysik, Garching, Federal Republic of Germany. ** Euratom-ENEA Association, Centro di Frascati, Italy. *** Euratom-UKAEA Association, Culham Laboratory, Abingdon, Oxfordshire, United Kingdom. **** University of Dusseldorf, Dusseldorf, Federal Republic of Germany. + Euratom-Ris0 Association, Ris<f> National Laboratory, Roskilde, Denmark. ++ Euratom-CNR Association, Istituto di Física del Plasma, Milan, Italy. +++ Imperial College of Science and Technology, University of London, London, United Kingdom. ++++ Euratom-FOM Association, FOM Instituut voor Plasmafysica,. Nieuwegein, Netherlands. ® Euratom-Suisse Association, Centre de Recherches en Physique des Plasmas, Lausanne, Switzerland.",1987,65,3499,17,185,176,174,228,213,238,197,196,202,178
e8e4b786031e421912cf904d577994ccfcd0ca48,1. General Physical Properties of Rubber 2. Internal Energy and Entropy Changes on Deformation 3. The Elasticity of Long-Chain Molecules 4. The Elasticity of a Molecular Network 5Ex5 Experimental Examination of the Statistical Theory 6. Non-Gaussian Chain Statistics and Network Theory 7. Swelling Phenomena 8. Cross-linking and Modulus 9. Photoelastic Properties of Rubbers 10. The General Strain: Phenomenological Theory 11. Alternative Forms of Strain-Energy Function 12. Large-Deformation Theory: Shear and Torsion 13. Thermodynamic Analysis of Gaussian Network,1949,0,3703,120,0,0,2,4,6,5,7,5,2,4
f666190e624adbd530e3635624eb193c94639865,"This book concerns the use of concepts from statistical physics in the description of financial systems. The authors illustrate the scaling concepts used in probability theory, critical phenomena, and fully developed turbulent fluids. These concepts are then applied to financial time series. The authors also present a stochastic model that displays several of the statistical properties observed in empirical data. Statistical physics concepts such as stochastic dynamics, short- and long-range correlations, self-similarity and scaling permit an understanding of the global behaviour of economic systems without first having to work out a detailed microscopic description of the system. Physicists will find the application of statistical physics concepts to economic systems interesting. Economists and workers in the financial world will find useful the presentation of empirical analysis methods and well-formulated theoretical tools that might help describe systems composed of a huge number of interacting subsystems.",1999,234,2885,140,18,53,88,82,78,118,115,123,149,139
0cf16c3770d1ce1b9b1b4d875741ba51252882f7,"Abstract Ti–Ni-based alloys are quite attractive functional materials not only as practical shape memory alloys with high strength and ductility but also as those exhibiting unique physical properties such as pre-transformation behaviors, which are enriched by various martensitic transformations. The paper starts from phase diagram, structures of martensites, mechanisms of martensitic transformations, premartensitic behavior, mechanism of shape memory and superelastic effects etc., and covers most of the fundamental issues related with the alloys, which include not only martensitic transformations but also diffusional transformations, since the latter greatly affect the former, and are useful to improve shape memory characteristics. Thus the alloy system will serve as an excellent case study of physical metallurgy, as is the case for steels where all kinds of phase transformations are utilized to improve the physical properties. In short this review is intended to give a self-consistent and logical account of key issues on Ti–Ni based alloys from physical metallurgy viewpoint on an up-to-date basis.",2005,345,2950,99,6,37,60,102,100,110,141,134,195,206
c7ca77863c289ecb4bcfdd7e329d90f792d707be,"Physical Metallurgy Principles is intended for use in an introductory course in physical metallurgy and is designed for all engineering students at the junior or senior level. The approach is largely theoretical, but covers all aspects of physical metallurgy and behavior of metals and alloys. The treatment used in this textbook is in harmony with a more fundamental approach to engineering education.",1972,0,2195,50,5,3,3,1,7,9,7,7,8,13
eb4ec95f5ea8aa0b7e54b3b0975494999b113595,"Preface. 1. Introduction. 1.1 Ni-base Alloy Classification. 1.2 History of Nickel and Ni-base Alloys. 1.3 Corrosion Resistance. 1.4 Nickel Alloy Production. 2. Alloying Additions, Phase Diagrams, and Phase Stability. 2.1 Introduction. 2.2 General Influence of Alloying Additions. 2.3 Phase Diagrams for Solid-Solution Alloys. 2.4 Phase Diagrams for Precipitation Hardened Alloys--gamma' Formers. 2.5 Phase Diagrams for Precipitation-Hardened Alloys--gamma"" Formers. 2.6 Calculated Phase Stability Diagrams. 2.7 PHACOMP Phase Stability Calculations. 3. Solid-Solution Strengthened Ni-base Alloys. 3.1 Standard Alloys and Consumables. 3.2 Physical Metallurgy and Mechanical Properties. 3.3 Welding Metallurgy. 3.4 Mechanical Properties of Weldments. 3.5 Weldability. 3.6 Corrosion Resistance. 3.7 Case Studies. 4. Precipitation Strengthened Ni-base Alloys. 4.1 Standard Alloys and Consumables. 4.2 Physical Metallurgy and Mechanical Properties. 4.3 Welding Metallurgy. 4.4 Mechanical Properties of Weldments. 4.5 Weldability. 5. Oxide Dispersion Strengthened Alloys and Nickel Aluminides. 5.1 Oxide Dispersion Strengthened Alloys. 5.2 Nickel Aluminide Alloys. 6. Repair Welding of Ni-base Alloys. 6.1 Solid-Solution Strengthened Alloys. 6.2 Precipitation Strengthened Alloys. 6.3 Single Crystal Superalloys. 7. Dissimilar Welding. 7.1 Application of Dissimilar Welds. 7.2 Influence of Process Parameters on Fusion Zone Composition. 7.3 Carbon, Low Alloys and Stainless Steels. 7.4 Postweld Heat Treatment Cracking in Stainless Steels Welded with Ni-base Filler Metals. 7.5 Super Austenitic Stainless Steels. 7.6 Dissimilar Welds in Ni-base Alloys - Effect on Corrosion Resistance. 7.7 9%Ni Steels. 7.8 Super Duplex Stainless Steels. 7.9 Case Studies. 8. Weldability Testing. 8.1 Introduction. 8.2 The Varestraint Test. 8.3 Modified Cast Pin Tear Test. 8.4 The Sigmajig Test. 8.5 The Hot Ductility Test. 8.6 The Strain-to-Fracture Test. 8.7 Other Weldability Tests. Appendix A Composition of Wrought and Cast Nickel-Base Alloys. Appendix B Composition of Nickel and Nickel Alloy Consumables. Appendix C Corrosion Acceptance Testing Methods. Appendix D Etching Techniques for Ni-base Alloys and Welds. Author Index. Subject Index.",2009,66,699,60,0,3,18,18,47,54,53,71,68,85
2524a882d157961d70b6106459aed3bb442b74c1,"Comprehensive information for the American aluminium industry Collective effort of 53 recognized experts on aluminium and aluminium alloys Joint venture by world renowned authorities-the Aluminium Association Inc. and American Society for Metals. The completely updated source of information on aluminium industry as a whole rather than its individual contributors. this book is an opportunity to gain from The knowledge of the experts working for prestigious companies such as Alcoa, Reynolds Metals Co., Alcan International Ltd., Kaiser Aluminium & Chemical Corp., Martin Marietta Laboratories and Anaconda Aluminium Co. It took four years of diligent work to complete this comprehensive successor to the classic volume, Aluminium, published by ASM in 1967. Contents: Properties of Pure Aluminum Constitution of Alloys Microstructure of Alloys Work Hardening Recovery, Recrystalization and Growth Metallurgy of Heat Treatment and General Principles of Precipitation Hardening Effects of Alloying Elements and Impurities on Properties Corrosion Behaviour Properties of Commercial Casting Alloys Properties of Commercial Wrought Alloys Aluminum Powder and Powder Metallurgy Products.",1984,0,1561,73,0,0,2,7,6,5,3,10,13,14
34058cc7431331f5af8cd63ac3c9df14765c7eac,Preface. 1. Introduction. 2. Phase Diagrams. 3. Alloying Elements and Constitution Diagrams. 4. Martensitic Stainless Steels. 5. Ferritic Stainless Steels. 6. Austenitic Stainless Steels. 7. Duplex Stainless Steels. 8. Precipitation-Hardening Stainless Steels. 9. Dissimilar Welding of Stainless Steels. 10. Weldability Testing. Appendix 1: Nominal Compositions of Stainless Steels. Appendix 2: Etching Techniques for Stainless Steel Welds. Author Index. Subject Index.,2005,0,1025,77,1,3,13,28,27,34,60,56,74,77
d8133fb8e17ed2a5809e03cc43c7914ea49f2b0d,"This practical reference provides thorough and systematic coverage on both basic metallurgy and the practical engineering aspects of metallic material selection and application. Contents includes: Practical information on the engineering properties and applications of steels, cast irons, nonferrous alloys, and metal matrix composites. Concise overviews and practical implications of metallic structure, imperfections, deformation, and phase transformations Process metallurgy of solidification and casting, recovery, recrystallization and grain growth, precipitation hardening Mechanical deformation during processing and in-service properties of fatigue, fracture, and creep. Physical properties and corrosion.",2008,0,567,50,0,1,4,12,16,42,52,57,74,68
eafe94460769dc1dca747eb9d9c9490a6d8ec12f,"Abstract The generation of zinc and zinc alloy coatings on steel is one of the commercially most important processing techniques used to protect steel components exposed to corrosive environments. From a technological standpoint, the principles of galvanizing have remained unchanged since this coating came into use over 200 years ago. However, because of new applications in the automotive and construction industry, a considerable amount of research has recently occurred on all aspects of the galvanizing process and on new types of Zn coatings. This review will discuss the metallurgy of zinc-coated steel from a scientific standpoint to develop relationships to practical applications. Hot-dip zinc coating methods, i.e. batch and continuous processes, will first be reviewed along with Fe–Zn phase equilibria and kinetics. Commercially, the addition of aluminum to the zinc bath results in three important types of coatings, galvanized, galfan and galvalume, and produces complex reactions at the coating/substrate interface. Fe–Zn–Al equilibrium will be reviewed in the light of recent studies of solubility and inhibition layer formation and breakdown. The effect of steel substrate composition on these reactions will also be critically analyzed. The overlay coating formation, or the coating alloy, is specifically chosen for its desired properties. The morphology of the galvanize, galfan and galvalume coating overlays will be reviewed, as well as the effect of heat treatment to produce a galvanneal coating. Finally, the effect of the microstructures of these coatings on the important properties of corrosion, formability, weldability and paintability will be discussed.",2000,64,1046,39,0,2,2,6,13,25,32,48,46,40
5c1af053ef5d859221eab1d0e56418b04d8f9249,"Skripta Fizikalna metalurgija I je sažeti prikaz znanosti o materijalima u kojoj se na znanstvenim i inženjerskim principima tumaci kristalna građa metala, dizajniranje legura i mikrostrukture, te odnos između strukture metala i njegovih mehanickih i fizickih svojstava.",2009,8,395,29,21,25,30,32,25,28,40,25,24,35
abbe5dee41d472e0468ae30e7bc6dc7d53284b2e,"This volume provides a substantial background to microalloyed steels with a wide selection of applications, some of which are very recent. A well-illustrated practical guide, this book acts as a useful source of data and a concise account of the theoretical aspects of the subject. Both academic institutions and the world-wide steel industry will find it indispensable.",1997,0,958,75,0,6,4,10,18,15,16,22,39,32
3e29d4c182490ea94be52b5593f7d218d4782358,"1. Stress and strain 2. Plasticity 3. Strain hardening 4. Plastic instability 5. Temperature and strain-rate dependence 6. Work balance 7. Slab analysis and friction 8. Friction and lubrication 9. Upper-bound analysis 10. Slip-line field analysis 11. Deformation zone geometry 12. Formability 13. Bending 14. Plastic anisotropy 15. Cupping, redrawing and ironing 16. Forming limit diagrams 17. Stamping 18. Hydroforming 19. Other sheet forming operations 20. Formability tests 21. Sheet metal properties.",1993,13,1086,42,9,12,9,14,15,15,9,16,20,17
6b09d20de5ccfcf15453d0f953147fd1e9079587,,1984,0,1147,69,0,0,3,0,2,5,9,8,7,9
7562d6af07c5449a681939842f99809e22a4b53e,"This is a textbook on the structural analysis and design of highway pavements. It presents the theory of pavement design and reviews the methods developed by several organizations, such as the American Association of State Highway and Transportation Officials (AASHTO), the Asphalt Institute (AI), and the Portland Cement Association (PCA). It can be used for an undergraduate course by skipping the appendices or as an advanced graduate course by including them. The book is organized in 13 chapters. Chapter 1 introduces the historical development of pavement design, the major road tests, the various design factors, and the differences in design concepts among highway pavements, airport pavements, and railroad trackbeds. Chapter 2 discusses stresses and strains in flexible pavements. Chapter 3 presents the KENLAYER computer program, based on Burmister's layered theory, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis on the effect of various factors on pavement responses. Chapter 4 discusses stresses and deflections in rigid pavements due to curling, loading, and friction, as well as the design of dowels and joints. Influence charts for determining stresses and deflections are also presented. Chapter 5 presents the KENSLABS computer program, based on the finite element method, including theoretical developments, program description, comparison with available solutions, and sensitivity analysis. Chapter 6 discusses the concept of equivalent single-wheel and single-axle loads and the prediction of traffic. Chapter 7 describes the material characterization for mechanistic-empirical methods of pavement design including the determination of resilient modulus, fatigue and permanent deformation properties, and the modulus of subgrade reaction. Chapter 8 outlines the subdrainage design including general principles, drainage materials, and design procedures. Chapter 9 discusses pavement performance including distress, serviceability, skid resistance, nondestructive testing, and the evaluation of pavement performance. Chapter 10 illustrates the reliability concept of pavement design in which the variabilities of traffic, material, and geometric parameters are all taken into consideration. A probabilistic procedure, developed by Rosenblueth, is described and two probabilistic computer programs including VESYS for flexible pavements and PMRPD for rigid pavements are discussed. Chapter 11 outlines an idealistic mechanistic method of flexible pavement design and presents in detail the AI method and the AASHTO method, as well as the design of flexible pavement shoulders. Chapter 12 outlines an idealistic mechanistic method of rigid pavement design and presents in detail the PCA method and the AASHTO method. The design of continuous reinforced concrete pavements and rigid pavement shoulders is also included. Chapter 13 outlines the design of overlay on both flexible and rigid pavements including the AASHTO, AI, and PCA procedures. An Author Index and a Subject Index are provided.",1992,0,2550,357,0,0,4,7,11,10,22,17,29,31
24b2ae931645336f34b96d336e2c669dbccc81db,"Clinical Periodontology and Implant Dentistry 6ed - Libros de Medicina - Periodoncia - 252,00",2008,0,1107,64,57,52,57,74,90,102,100,107,69,98
20aeb2357e9e215787c7e0d0acfe7a6b598c9103,"This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkisons Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, its easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot. This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and youll learn everything you need in the book. After reading this book youll be able to produce graphics customized precisely for your problems,and youll find it easy to get graphics out of your head and on to the screen or page.",2009,0,20826,1798,0,0,0,0,0,1,2,0,3,9
6dea759b9e6d08b1e8ae7c3ac135234008e26aec,"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as 'Coot'.",2004,21,24370,1589,0,0,0,0,0,0,1,0,0,0
412a0bb5a3baa91b62053d82c562bc172df0439f,"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems",2007,0,12592,918,0,0,0,0,0,0,0,0,5,46
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17518,724,0,0,0,0,0,0,0,1,1,2
d0a47c8c9c4213edd24a120a6912ebc08e348a19,"Abstract In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.",1996,7,10043,920,0,0,0,1,30,82,204,435,631,683
8e3f2b578ce2d2bb969ab64d4ca43eaa147f0674,"Publisher Summary This chapter discusses Raster3D, which is a suite of programs for molecular graphics. Crystallographers were among the first and most avid consumers of graphics workstations. Rapid advances in computer hardware, and particularly in the power of specialized computer graphics boards, have led to successive generations of personal workstations with ever more impressive capabilities for interactive molecular graphics. For many years, it was standard practice in crystallography laboratories to prepare figures by photographing directly from the workstation screen. No matter how beautiful the image on the screen, however, this approach suffers from several intrinsic limitations. Among these is the inherent limitation imposed by the effective resolution of the screen. Use of the graphics hardware in a workstation to generate images for later presentation can also impose other limitations. Designers of workstation hardware must compromise the quality of rendered images to achieve rendering speeds high enough for useful interactive manipulation of three-dimensional objects.",1997,13,3661,63,1,42,159,253,356,444,502,522,404,248
319bef16b09502a4c6d374a947ad74006cc1cda2,,1997,0,3410,0,195,200,182,208,208,197,199,198,178,186
793b4d2f5c2e3ceb53946e94f2ea2ff1d5dc1522,,1995,6,2472,120,2,22,49,88,86,104,131,133,171,202
3da75ffd1b1b7dacf37e0dd880e214fdb47980d5,"Raster3D Version 2.0 is a program suite for the production of photorealistic molecular graphics images. The code is hardware independent, and is particularly suited for use in producing large raster images of macromolecules for output to a film recorder or high-quality color printer. The Raster3D suite contains programs for composing illustrations of space-filling models, ball-and-stick models and ribbon-and-cylinder representations. It may also be used to render figures composed using other graphics tools, notably the widely used program Molscript [Kraulis (1991). J. Appl. Cryst. 24, 946-950].",1994,0,2417,61,1,39,73,147,196,223,238,221,192,208
459d0724afbcd591edde2dcc83e5e2750d878c0e,"These are the short notes for a two hour tutorial on principles and practice of computer graphics and scientific visualization. They are intended to summarize the contents of the tutorial transparencies and slides but they cannot completely replace them since restrictions in space and print quality do not permit the inclusion of figures and example images. For further reference the following standard text should be consulted: [3, 8, 5, 1, 6, 2, 9]",1997,4,2290,58,111,154,133,109,120,143,126,119,111,104
2bbf413f36f366fa73da4dc028a32131b5d205d6,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this �eld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.",2005,398,1911,121,16,67,147,188,202,206,228,165,158,136
356869aa0ae8d598e956c7f2ae884bbf5009c98c,"To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.",2008,16,1404,142,24,129,164,170,135,146,129,103,88,85
f8e1b3bcee316203b4aa843efd5f581bc16849dc,"From the Publisher: 
Visualization is a part of every day life. From weather map generation of financial modelling to MRI technology in medicine to 3D graphics used in movies like Jurassic Park, examples of visualization abound. The book/CD package offers readers the opportunity to practice visualization using a complete C++ programming environment developed by the authors.",1997,0,1949,122,23,34,57,71,83,65,77,108,103,101
f978d481fae83e57202d26d4fbd38e330889ea75,"Graphics processing units (GPUs), originally developed for rendering real-time effects in computer games, now provide unprecedented computational power for scientific applications. In this paper, we develop a general purpose molecular dynamics code that runs entirely on a single GPU. It is shown that our GPU implementation provides a performance equivalent to that of fast 30 processor core distributed memory cluster. Our results show that GPUs already provide an inexpensive alternative to such clusters and discuss implications for the future.",2008,26,1327,40,14,31,70,81,89,135,91,121,100,111
54dddebd5f6259d9f2f9b859007bb150da453535,"A model is developed of the human lower extremity to study how changes in musculoskeletal geometry and musculotendon parameters affect muscle force and its moment about the joints. The lines of action of 43 musculotendon actuators were defined based on their anatomical relationships to three-dimensional bone surface representations. A model for each actuator was formulated to compute its isometric force-length relation. The kinematics of the lower extremity were defined by modeling the hip, knee, ankle, subtalar, and metatarsophalangeal joints. Thus, the force and joint moment that each musculotendon actuator develops can be computed for any body position. The joint moments calculated with the model compare well with experimentally measured isometric joint moments. A graphical interface to the model has also been developed. It allows the user to visualize the musculoskeletal geometry and to manipulate the model parameters to study the biomechanical consequences of orthopaedic surgical procedures. For example, tendon transfer and lengthening procedures can be simulated by adjusting the model parameters according to various surgical techniques. Results of the simulated surgeries can be analyzed quickly in terms of postsurgery muscle forces and other biomechanical variables.<<ETX>>",1990,50,1679,139,2,3,5,7,22,18,15,25,17,24
6cae8a08979c34110a85428f06c973b38828eac6,,2004,0,1519,13,25,41,39,76,137,161,169,171,131,132
3c47192d40b1138f4e757948c64bb846c38c53ba,This paper presents a new reflectance model for rendering computer synthesized images. The model accounts for the relative brightness of different materials and light sources in the same scene. It describes the directional distribution of the reflected light and a color shift that occurs as the reflectance changes with incidence angle. The paper presents a method for obtaining the spectral energy distribution of the light reflected from an object made of a specific real material and discusses a procedure for accurately reproducing the color associated with the spectral energy distribution. The model is applied to the simulation of a metal and a plastic.,1987,30,1665,115,16,19,16,21,10,22,15,19,13,21
5ba7042c5220548c9d5636df3cc2c84bb8641e02,"Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.
 The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.",1980,8,1841,91,1,1,7,2,0,3,6,3,3,6
e80dc3681b94fbba79523201ab180852f901e618,"Abstract The role of computer graphics in different aspects of simulating matter on the atomic scale is discussed. The computer graphics is useful in specifying and examining chemical structures, since it is nowadays possible to study––with density functional theory––complex systems containing up to a few hundreds in-equivalent atoms. Furthermore, computer graphics is also an indispensable tool in analysing computed data and facilitates interpretation of results. In this context XCrySDen ( http://www.xcrysden.org/ ) is presented, a crystalline- and molecular-structure visualisation program, which aims at display of isosurfaces and contours, which can be superimposed on crystalline structures and interactively rotated and manipulated. Another aspect of computer utilisation in simulations that takes advantage of the computer’s graphics capabilities, is that it provides intuitive graphical user interfaces for the simulation setup. It is demonstrated how such interfaces are easily built using the developed GUIB software ( http://www-k3.ijs.si/kokalj/guib/ ).",2003,5,1305,17,0,1,4,18,31,53,43,44,60,64
4c231a788f20951fabc42abaa440c984a8f67661,"Fundamentals of interactive computer graphics , Fundamentals of interactive computer graphics , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,1773,18,12,44,51,91,117,125,146,126,121,90
6c73a2b81ba564a7f6fd89808f7629538adde6da,"1. Introduction. Image Processing as Picture Analysis. The Advantages of Interactive Graphics. Representative Uses of Computer Graphics. Classification of Applications. Development of Hardware and Software for Computer Graphics. Conceptual Framework for Interactive Graphics. 2. Programming in the Simple Raster Graphics Package (SRGP)/. Drawing with SRGP/. Basic Interaction Handling/. Raster Graphics Features/. Limitations of SRGP/. 3. Basic Raster Graphics Algorithms for Drawing 2d Primitives. Overview. Scan Converting Lines. Scan Converting Circles. Scan Convertiing Ellipses. Filling Rectangles. Fillign Polygons. Filling Ellipse Arcs. Pattern Filling. Thick Primiives. Line Style and Pen Style. Clipping in a Raster World. Clipping Lines. Clipping Circles and Ellipses. Clipping Polygons. Generating Characters. SRGP_copyPixel. Antialiasing. 4. Graphics Hardware. Hardcopy Technologies. Display Technologies. Raster-Scan Display Systems. The Video Controller. Random-Scan Display Processor. Input Devices for Operator Interaction. Image Scanners. 5. Geometrical Transformations. 2D Transformations. Homogeneous Coordinates and Matrix Representation of 2D Transformations. Composition of 2D Transformations. The Window-to-Viewport Transformation. Efficiency. Matrix Representation of 3D Transformations. Composition of 3D Transformations. Transformations as a Change in Coordinate System. 6. Viewing in 3D. Projections. Specifying an Arbitrary 3D View. Examples of 3D Viewing. The Mathematics of Planar Geometric Projections. Implementing Planar Geometric Projections. Coordinate Systems. 7. Object Hierarchy and Simple PHIGS (SPHIGS). Geometric Modeling. Characteristics of Retained-Mode Graphics Packages. Defining and Displaying Structures. Modeling Transformations. Hierarchical Structure Networks. Matrix Composition in Display Traversal. Appearance-Attribute Handling in Hierarchy. Screen Updating and Rendering Modes. Structure Network Editing for Dynamic Effects. Interaction. Additional Output Features. Implementation Issues. Optimizing Display of Hierarchical Models. Limitations of Hierarchical Modeling in PHIGS. Alternative Forms of Hierarchical Modeling. 8. Input Devices, Interaction Techniques, and Interaction Tasks. Interaction Hardware. Basic Interaction Tasks. Composite Interaction Tasks. 9. Dialogue Design. The Form and Content of User-Computer Dialogues. User-Interfaces Styles. Important Design Considerations. Modes and Syntax. Visual Design. The Design Methodology. 10. User Interface Software. Basic Interaction-Handling Models. Windows-Management Systems. Output Handling in Window Systems. Input Handling in Window Systems. Interaction-Technique Toolkits. User-Interface Management Systems. 11. Representing Curves and Surfaces. Polygon Meshes. Parametric Cubic Curves. Parametric Bicubic Surfaces. Quadric Surfaces. 12. Solid Modeling. Representing Solids. Regularized Boolean Set Operations. Primitive Instancing. Sweep Representations. Boundary Representations. Spatial-Partitioning Representations. Constructive Solid Geometry. Comparison of Representations. User Interfaces for Solid Modeling. 13. Achromatic and Colored Light. Achromatic Light. Chromatic Color. Color Models for Raster Graphics. Reproducing Color. Using Color in Computer Graphics. 14. The Quest for Visual Realism. Why Realism? Fundamental Difficulties. Rendering Techniques for Line Drawings. Rendering Techniques for Shaded Images. Improved Object Models. Dynamics. Stereopsis. Improved Displays. Interacting with Our Other Senses. Aliasing and Antialiasing. 15. Visible-Surface Determination. Functions of Two Variables. Techniques for Efficient Visible-Surface Determination. Algorithms for Visible-Line Determination. The z-Buffer Algorithm. List-Priority Algorithms. Scan-Line Algorithms. Area-Subdivision Algorithms. Algorithms for Octrees. Algorithms for Curved Surfaces. Visible-Surface Ray Tracing. 16. Illumination And Shading. Illumination Modeling. Shading Models for Polygons. Surface Detail. Shadows. Transparency. Interobject Reflections. Physically Based Illumination Models. Extended Light Sources. Spectral Sampling. Improving the Camera Model. Global Illumination Algorithms. Recursive Ray Tracing. Radiosity Methods. The Rendering Pipeline. 17. Image Manipulation and Storage. What Is an Image? Filtering. Image Processing. Geometric Transformations of Images. Multipass Transformations. Image Compositing. Mechanisms for Image Storage. Special Effects with Images. Summary. 18. Advanced Raster Graphic Architecture. Simple Raster-Display System. Display-Processor Systems. Standard Graphics Pipeline. Introduction to Multiprocessing. Pipeline Front-End Architecture. Parallel Front-End Architectures. Multiprocessor Rasterization Architectures. Image-Parallel Rasterization. Object-Parallel Rasterization. Hybrid-Parallel Rasterization. Enhanced Display Capabilities. 19. Advanced Geometric and Raster Algorithms. Clipping. Scan-Converting Primitives. Antialiasing. The Special Problems of Text. Filling Algorithms. Making copyPixel Fast. The Shape Data Structure and Shape Algebra. Managing Windows with bitBlt. Page Description Languages. 20. Advanced Modeling Techniques. Extensions of Previous Techniques. Procedural Models. Fractal Models. Grammar-Based Models. Particle Systems. Volume Rendering. Physically Based Modeling. Special Models for Natural and Synthetic Objects. Automating Object Placement. 21. Animation. Conventional and Computer-Assisted Animation. Animation Languages. Methods of Controlling Animation. Basic Rules of Animation. Problems Peculiar to Animation. Appendix: Mathematics for Computer Graphics. Vector Spaces and Affine Spaces. Some Standard Constructions in Vector Spaces. Dot Products and Distances. Matrices. Linear and Affine Transformations. Eigenvalues and Eigenvectors. Newton-Raphson Iteration for Root Finding. Bibliography. Index. 0201848406T04062001",1995,0,1285,93,34,46,48,56,47,74,49,43,81,77
a9918035dea348663c149a854ef92a9d222d3680,"A model building and refinement system is described for use with a Vector General 3400 display. The system allows the user to build models using guide atoms and angles to arrive at the final conformation. It has been used to assist in difference Fourier map interpretation at medium and high resolution, and to build a protein molecule into a multiple isomorphous replacement phased electron density map.",1978,7,1682,22,1,2,5,8,10,25,25,44,31,73
d57d75a95d1df5a421de98c34679ac719f374b76,"We design and implement Mars, a MapReduce framework, on graphics processors (GPUs). MapReduce is a distributed programming framework originally proposed by Google for the ease of development of web search applications on a large number of commodity CPUs. Compared with CPUs, GPUs have an order of magnitude higher computation power and memory bandwidth, but are harder to program since their architectures are designed as a special-purpose co-processor and their programming interfaces are typically for graphics applications. As the first attempt to harness GPU's power for MapReduce, we developed Mars on an NVIDIA G80 GPU, which contains over one hundred processors, and evaluated it in comparison with Phoenix, the state-of-the-art MapReduce framework on multi-core CPUs. Mars hides the programming complexity of the GPU behind the simple and familiar MapReduce interface. It is up to 16 times faster than its CPU-based counterpart for six common web applications on a quad-core machine.",2008,35,804,62,2,34,65,67,81,110,104,88,78,48
5c6b7b39f0ea433116f81cb7a3372684e6697b4a,"A liquid proportioning system includes two groups of positive metering tanks, each group consisting of at least two tanks each containing a supply of liquid to be blended and including outlet control device for selectively regulating the volume of liquid leaving the tank per unit of time. The liquids from each tank are fed together through a single conduit to a mixer which continuously blends the liquid components as they are flowing, and the blended liquids are fed to a reservoir tank, from which they are fed to a point of use in accordance with the variable requirements of the latter. One group of positive metering tanks feeds the liquid components to the mixer in exact proportions until these tanks are depleted, at which time the flow from these tanks is shut off automatically and the second group of tanks begins to feed the portioned liquid components to the mixer, while the first group of tanks are refilled. When the requirements of the point of use are decreased, sensors slow down the feeding of the liquid to the reservoir tank by increasing the time interval between the emptying of one group of metering tanks and the commencement of feeding from the other group of tanks.",1983,0,1456,92,0,1,1,3,1,7,5,16,14,22
fa8eaed21e0ff6cba18c8661d422c3be83880909,"BackgroundAnalyses of biomolecules for biodiversity, phylogeny or structure/function studies often use graphical tree representations. Many powerful tree editors are now available, but existing tree visualization tools make little use of meta-information related to the entities under study such as taxonomic descriptions or gene functions that can hardly be encoded within the tree itself (if using popular tree formats). Consequently, a tedious manual analysis and post-processing of the tree graphics are required if one needs to use external information for displaying or investigating trees.ResultsWe have developed TreeDyn, a tool using annotations and dynamic graphical methods for editing and analyzing multiple trees. The main features of TreeDyn are 1) the management of multiple windows and multiple trees per window, 2) the export of graphics to several standard file formats with or without HTML encapsulation and a new format called TGF, which enables saving and restoring graphical analysis, 3) the projection of texts or symbols facing leaf labels or linked to nodes, through manual pasting or by using annotation files, 4) the highlight of graphical elements after querying leaf labels (or annotations) or by selection of graphical elements and information extraction, 5) the highlight of targeted trees according to a source tree browsed by the user, 6) powerful scripts for automating repetitive graphical tasks, 7) a command line interpreter enabling the use of TreeDyn through CGI scripts for online building of trees, 8) the inclusion of a library of packages dedicated to specific research fields involving trees.ConclusionTreeDyn is a tree visualization and annotation tool which includes tools for tree manipulation and annotation and uses meta-information through dynamic graphical operators or scripting to help analyses and annotations of single trees or tree collections.",2006,45,934,82,2,16,31,32,63,73,70,77,84,94
e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1,"The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.
 In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.",2009,43,618,21,7,19,24,19,29,47,62,60,67,81
f74e04c23db19d746096eec7821f10112690f392,"We present a case study on the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. Graphics cards, containing multiple Graphics Processing Units (GPUs), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers and can be thought of as prototypes of the next generation of many-core processors. For certain classes of population-based Monte Carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multicore processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. On a canonical set of stochastic simulation examples including population-based Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we find speedups from 35- to 500-fold over conventional single-threaded computer code. Our findings suggest that GPUs have the potential to facilitate the growth of statistical modeling into complex data-rich domains through the availability of cheap and accessible many-core computation. We believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design. This article has supplementary material online.",2009,49,307,22,1,17,22,35,49,36,34,33,19,18
72418a969890621cfe99e470889ed0bedd0dba98,"We report a parallel Monte Carlo algorithm accelerated by graphics processing units (GPU) for modeling time-resolved photon migration in arbitrary 3D turbid media. By taking advantage of the massively parallel threads and low-memory latency, this algorithm allows many photons to be simulated simultaneously in a GPU. To further improve the computational efficiency, we explored two parallel random number generators (RNG), including a floating-point-only RNG based on a chaotic lattice. An efficient scheme for boundary reflection was implemented, along with the functions for time-resolved imaging. For a homogeneous semi-infinite medium, good agreement was observed between the simulation output and the analytical solution from the diffusion theory. The code was implemented with CUDA programming language, and benchmarked under various parameters, such as thread number, selection of RNG and memory access pattern. With a low-cost graphics card, this algorithm has demonstrated an acceleration ratio above 300 when using 1792 parallel threads over conventional CPU computation. The acceleration ratio drops to 75 when using atomic operations. These results render the GPU-based Monte Carlo simulation a practical solution for data analysis in a wide range of diffuse optical imaging applications, such as human brain or small-animal imaging.",2009,23,599,18,0,18,37,51,32,60,55,52,58,49
852e427df0329c0b9b033783246c3189b65b0a04,"Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. Lately, another property of DG has been growing in importance: The majority of a DG operator is applied in an element-local way, with weak penalty-based element-to-element coupling. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG's high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Using a single US$400 Nvidia GTX 280 GPU, we accelerate a solver for Maxwell's equations on a general 3D unstructured grid by a factor of around 50 relative to a serial computation on a current-generation CPU. In many cases, our algorithms exhibit full use of the device's available memory bandwidth. Example computations achieve and surpass 200gigaflops/s of net application-level floating point work. In this article, we describe and derive the techniques used to reach this level of performance. In addition, we present comprehensive data on the accuracy and runtime behavior of the method.",2009,31,426,37,8,33,45,40,47,42,50,55,35,33
d3c41281a5adb9626c1e05f5fc59ef2f8242438e,"We describe a complete implementation of all‐atom protein molecular dynamics running entirely on a graphics processing unit (GPU), including all standard force field terms, integration, constraints, and implicit solvent. We discuss the design of our algorithms and important optimizations needed to fully take advantage of a GPU. We evaluate its performance, and show that it can be more than 700 times faster than a conventional implementation running on a single CPU core. © 2009 Wiley Periodicals, Inc. J Comput Chem, 2009",2009,25,508,12,15,58,66,67,60,43,36,35,30,32
03aa649535c7e01ac2b3255f2f44131380dc93c7,"Graphics processors (GPUs) provide a vast number of simple, data-parallel, deeply multithreaded cores and high memory bandwidths. GPU architectures are becoming increasingly programmable, offering the potential for dramatic speedups for a variety of general-purpose applications compared to contemporary general-purpose processors (CPUs). This paper uses NVIDIA's C-like CUDA language and an engineering sample of their recently introduced GTX 260 GPU to explore the effectiveness of GPUs for a variety of application types, and describes some specific coding idioms that improve their performance on the GPU. GPU performance is compared to both single-core and multicore CPU performance, with multicore CPU implementations written using OpenMP. The paper also discusses advantages and inefficiencies of the CUDA programming model and some desirable features that might allow for greater ease of use and also more readily support a larger body of applications.",2008,51,671,28,7,52,73,76,77,82,68,64,53,45
f74c526d01e8d7d910a41a3fdf6587e157868c4f,"The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general‐purpose computation to graphics hardware.",2007,302,753,9,48,60,91,70,93,64,81,51,49,30
c715fd808676921a84f248520925f8f5082cc83e,"1: Introduction.- 1.1 Graphics, Image Processing, and Pattern Recognition.- 1.2 Forms of Pictorial Data.- 1.2.1 Class 1: Full Gray Scale and Color Pictures.- 1.2.2 Class 2: Bilevel or ""Few Color"" pictures.- 1.2.3 Class 3: Continuous Curves and Lines.- 1.2.4 Class 4: Points or Polygons.- 1.3 Pictorial Input.- 1.4 Display Devices.- 1.5 Vector Graphics.- 1.6 Raster Graphics.- 1.7 Common Primitive Graphic Instructions.- 1.8 Comparison of Vector and Raster Graphics.- 1.9 Pictorial Editor.- 1.10 Pictorial Transformations.- 1.11 Algorithm Notation.- 1.12 A Few Words on Complexity.- 1.13 Bibliographical Notes.- 1.14 Relevant Literature.- 1.15 Problems.- 2: Digitization of Gray Scale Images.- 2.1 Introduction.- 2.2 A Review of Fourier and other Transforms.- 2.3 Sampling.- 2.3.1 One-dimensional Sampling.- 2.3.2 Two-dimensional Sampling.- 2.4 Aliasing.- 2.5 Quantization.- 2.6 Bibliographical Notes.- 2.7 Relevant Literature.- 2.8 Problems.- Appendix 2.A: Fast Fourier Transform.- 3: Processing of Gray Scale Images.- 3.1 Introduction.- 3.2 Histogram and Histogram Equalization.- 3.3 Co-occurrence Matrices.- 3.4 Linear Image Filtering.- 3.5 Nonlinear Image Filtering.- 3.5.1 Directional Filters.- 3.5.2 Two-part Filters.- 3.5.3 Functional Approximation Filters.- 3.6 Bibliographical Notes.- 3.7 Relevant Literature.- 3.8 Problems.- 4: Segmentation.- 4.1 Introduction.- 4.2 Thresholding.- 4.3 Edge Detection.- 4.4 Segmentation by Region Growing.- 4.4.1 Segmentation by Average Brightness Level.- 4.4.2 Other Uniformity Criteria.- 4.5 Bibliographical Notes.- 4.6 Relevant Literature.- 4.7 Problems.- 5: Projections.- 5.1 Introduction.- 5.2 Introduction to Reconstruction Techniques.- 5.3 A Class of Reconstruction Algorithms.- 5.4 Projections for Shape Analysis.- 5.5 Bibliographical Notes.- 5.6 Relevant Literature.- 5.7 Problems.- Appendix 5.A: An Elementary Reconstruction Program.- 6: Data Structures.- 6.1 Introduction.- 6.2 Graph Traversal Algorithms.- 6.3 Paging.- 6.4 Pyramids or Quad Trees.- 6.4.1 Creating a Quad Tree.- 6.4.2 Reconstructing an Image from a Quad Tree.- 6.4.3 Image Compaction with a Quad Tree.- 6.5 Binary Image Trees.- 6.6 Split-and-Merge Algorithms.- 6.7 Line Encodings and the Line Adjacency Graph.- 6.8 Region Encodings and the Region Adjacency Graph.- 6.9 Iconic Representations.- 6.10 Data Structures for Displays.- 6.11 Bibliographical Notes.- 6.12 Relevant Literature.- 6.13 Problems.- Appendix 6.A: Introduction to Graphs.- 7: Bilevel Pictures.- 7.1 Introduction.- 7.2 Sampling and Topology.- 7.3 Elements of Discrete Geometry.- 7.4 A Sampling Theorem for Class 2 Pictures.- 7.5 Contour Tracing.- 7.5.1 Tracing of a Single Contour.- 7.5.2 Traversal of All the Contours of a Region.- 7.6 Curves and Lines on a Discrete Grid.- 7.6.1 When a Set of Pixels is not a Curve.- 7.6.2 When a Set of Pixels is a Curve.- 7.7 Multiple Pixels.- 7.8 An Introduction to Shape Analysis.- 7.9 Bibliographical Notes.- 7.10 Relevant Literature.- 7.11 Problems.- 8: Contour Filling.- 8.1 Introduction.- 8.2 Edge Filling.- 8.3 Contour Filling by Parity Check.- 8.3.1 Proof of Correctness of Algorithm 8.3.- 8.3.2 Implementation of a Parity Check Algorithm.- 8.4 Contour Filling by Connectivity.- 8.4.1 Recursive Connectivity Filling.- 8.4.2 Nonrecursive Connectivity Filling.- 8.4.3 Procedures used for Connectivity Filling.- 8.4.4 Description of the Main Algorithm.- 8.5 Comparisons and Combinations.- 8.6 Bibliographical Notes.- 8.7 Relevant Literature.- 8.8 Problems.- 9: Thinning Algorithms.- 9.1 Introduction.- 9.2 Classical Thinning Algorithms.- 9.3 Asynchronous Thinning Algorithms.- 9.4 Implementation of an Asynchronous Thinning Algorithm.- 9.5 A Quick Thinning Algorithm.- 9.6 Structural Shape Analysis.- 9.7 Transformation of Bilevel Images into Line Drawings.- 9.8 Bibliographical Notes.- 9.9 Relevant Literature.- 9.10 Problems.- 10: Curve Fitting and Curve Displaying.- 10.1 Introduction.- 10.2 Polynomial Interpolation.- 10.3 Bezier Polynomials.- 10.4 Computation of Bezier Polynomials.- 10.5 Some Properties of Bezier Polynomials.- 10.6 Circular Arcs.- 10.7 Display of Lines and Curves.- 10.7.1 Display of Curves through Differential Equations.- 10.7.2 Effect of Round-off Errors in Displays.- 10.8 A Point Editor.- 10.8.1 A Data Structure for a Point Editor.- 10.8.2 Input and Output for a Point Editor.- 10.9 Bibliographical Notes.- 10.10 Relevant Literature.- 10.11 Problems.- 11: Curve Fitting with Splines.- 11.1 Introduction.- 11.2 Fundamental Definitions.- 11.3 B-Splines.- 11.4 Computation with B-Splines.- 11.5 Interpolating B-Splines.- 11.6 B-Splines in Graphics.- 11.7 Shape Description and B-splines.- 11.8 Bibliographical Notes.- 11.9 Relevant Literature.- 11.10 Problems.- 12: Approximation of Curves.- 12.1 Introduction.- 12.2 Integral Square Error Approximation.- 12.3 Approximation Using B-Splines.- 12.4 Approximation by Splines with Variable Breakpoints.- 12.5 Polygonal Approximations.- 12.5.1 A Suboptimal Line Fitting Algorithm.- 12.5.2 A Simple Polygon Fitting Algorithm.- 12.5.3 Properties of Algorithm 12.2.- 12.6 Applications of Curve Approximation in Graphics.- 12.6.1 Handling of Groups of Points by a Point Editor.- 12.6.2 Finding Some Simple Approximating Curves.- 12.7 Bibliographical Notes.- 12.8 Relevant Literature.- 12.9 Problems.- 13: Surface Fitting and Surface Displaying.- 13.1 Introduction.- 13.2 Some Simple Properties of Surfaces.- 13.3 Singular Points of a Surface.- 13.4 Linear and Bilinear Interpolating Surface Patches.- 13.5 Lofted Surfaces.- 13.6 Coons Surfaces.- 13.7 Guided Surfaces.- 13.7.1 Bezier Surfaces.- 13.7.2 B-Spline Surfaces.- 13.8 The Choice of a Surface Partition.- 13.9 Display of Surfaces and Shading.- 13.10 Bibliographical Notes.- 13.11 Relevant Literature.- 13.12 Problems.- 14: The Mathematics of Two-Dimensional Graphics.- 14.1 Introduction.- 14.2 Two-Dimensional Transformations.- 14.3 Homogeneous Coordinates.- 14.3.1 Equation of a Line Defined by Two Points.- 14.3.2 Coordinates of a Point Defined as the Intersection of Two Lines.- 14.3.3 Duality.- 14.4 Line Segment Problems.- 14.4.1 Position of a Point with respect to a Line.- 14.4.2 Intersection of Line Segments.- 14.4.3 Position of a Point with respect to a Polygon.- 14.4.4 Segment Shadow.- 14.5 Bibliographical Notes.- 14.6 Relevant Literature.- 14.7 Problems.- 15: Polygon Clipping.- 15.1 Introduction.- 15.2 Clipping a Line Segment by a Convex Polygon.- 15.3 Clipping a Line Segment by a Regular Rectangle.- 15.4 Clipping an Arbitrary Polygon by a Line.- 15.5 Intersection of Two Polygons.- 15.6 Efficient Polygon Intersection.- 15.7 Bibliographical Notes.- 15.8 Relevant Literature.- 15.9 Problems.- 16: The Mathematics of Three-Dimensional Graphics.- 16.1 Introduction.- 16.2 Homogeneous Coordinates.- 16.2.1 Position of a Point with respect to a Plane.- 16.2.2 Intersection of Triangles.- 16.3 Three-Dimensional Transformations.- 16.3.1 Mathematical Preliminaries.- 16.3.2 Rotation around an Axis through the Origin.- 16.4 Orthogonal Projections.- 16.5 Perspective Projections.- 16.6 Bibliographical Notes.- 16.7 Relevant Literature.- 16.8 Problems.- 17: Creating Three-Dimensional Graphic Displays.- 17.1 Introduction.- 17.2 The Hidden Line and Hidden Surface Problems.- 17.2.1 Surface Shadow.- 17.2.2 Approaches to the Visibility Problem.- 17.2.3 Single Convex Object Visibility.- 17.3 A Quad Tree Visibility Algorithm.- 17.4 A Raster Line Scan Visibility Algorithm.- 17.5 Coherence.- 17.6 Nonlinear Object Descriptions.- 17.7 Making a Natural Looking Display.- 17.8 Bibliographical Notes.- 17.9 Relevant Literature.- 17.10 Problems.- Author Index.- Algorithm Index.",1981,0,1207,40,0,2,8,14,20,29,42,52,49,68
25e80b49c3546876eaed9b1437ffe7cac32ca00f,"The purpose of this article is to illustrate the steps involved in testing for multigroup invariance using Amos Graphics. Based on analysis of covariance (ANCOV) structures, 2 applications are demonstrated, each of which represents a different set of circumstances. Application 1 focuses on the equivalence of a measuring instrument and tests for its invariance across 3 teacher panels, given baseline models that are identical across groups. Application 2 centers on the equivalence of a postulated theoretical structure across adolescent boys and girls in light of baseline models that are differentially specified across groups. Taken together, these illustrated examples should be of substantial assistance to researchers interested in testing for multigroup invariance using the Amos program.",2004,22,828,111,1,2,12,19,35,40,51,60,46,41
b999766e1685da444ab997b6a21a741a169ad7b1,"Molecular mechanics simulations offer a computational approach to study the behavior of biomolecules at atomic detail, but such simulations are limited in size and timescale by the available computing resources. State‐of‐the‐art graphics processing units (GPUs) can perform over 500 billion arithmetic operations per second, a tremendous computational resource that can now be utilized for general purpose computing as a result of recent advances in GPU hardware and software architecture. In this article, an overview of recent advances in programmable GPUs is presented, with an emphasis on their application to molecular mechanics simulations and the programming techniques required to obtain optimal performance in these cases. We demonstrate the use of GPUs for the calculation of long‐range electrostatics and nonbonded forces for molecular dynamics simulations, where GPU‐based calculations are typically 10–100 times faster than heavily optimized CPU‐based implementations. The application of GPU acceleration to biomolecular simulation is also demonstrated through the use of GPU‐accelerated Coulomb‐based ion placement and calculation of time‐averaged potentials from molecular dynamics trajectories. A novel approximation to Coulomb potential calculation, the multilevel summation method, is introduced and compared with direct Coulomb summation. In light of the performance obtained for this set of calculations, future applications of graphics processors to molecular dynamics simulations are discussed. © 2007 Wiley Periodicals, Inc. J Comput Chem, 2007",2007,143,690,25,2,39,54,64,88,86,85,52,46,38
f6ac5b2dab21d5d9b395a3c8bb7b92dbf996443d,"In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.",2004,36,830,79,11,38,40,53,82,121,85,93,74,70
d7316857b373a5b8dfc9008408dd76ffd8456542,"A typical PC has at least three cooling fans...and one case heater. That "" heater "" would be the graphics-processing unit (GPU) — usually a separate, highly specialized microprocessor chip dedicated to graphics. It does a brilliant job when the PC is running a graphics-intensive game or playing a video. At other times, it's largely underutilized, radiating unused power as heat. In fact, a discrete GPU is the most underutilized component in a PC. Although it's capable of amazing things, it spends much of its time performing routine chores, like scrolling the screen. Yet it has the potential to be the swiftest processing engine in the system — and it's already there, just waiting for something to do. For four years, NVIDIA has waged a campaign to redefine the role of GPUs. Not that graphics aren't important. Pushing pixels has been good business for NVIDIA, the world's leading graphics-processor company. Since the early 1990s, NVIDIA's GPUs have offloaded most of the graphics processing from CPUs in hundreds of millions of personal computers and videogame machines. NVIDIA continues to design its GPUs for superb graphics performance. But since 2005, NVIDIA has cultivated another fast-growing market — GPUs for computing applications beyond graphics. At first, these applications were called "" general-purpose GPU "" (GPGPU) computing. Today, NVIDIA prefers to call it "" GPU computing. "" In the professional market, it's often called high-performance computing. By harnessing the massively parallel-processing resources originally designed for 3D graphics, clever programmers can apply GPUs to a much broader range of computing applications. Some of those applications, such as video transcoding, still involve graphics to some degree. Whereas the CPU might spend an hour or more converting a recorded video for uploading to YouTube or burning a DVD, a GPU can tear through the job in minutes. The GPU can also enhance the video frame-by-frame, using consumer versions of sophisticated software once available only to NASA, law-enforcement agencies, and surveillance experts. Other GPU-computing applications are data-intensive tasks having little or nothing to do with graphics. Examples are stock-trading calculations and seismic-data analysis for oil exploration. Additional examples, such as high-resolution medical imaging, combine graphics with heavy-duty number crunching. On these kinds of workloads, an ordinary GPU can blow away the latest multicore CPUs. Link several GPUs together in a workstation or a cluster of systems, and you've got a "" desktop",2009,10,333,9,3,24,37,44,56,35,43,29,29,13
0a66086a2f23ef968f65395f88cdb2f4d458923a,"Statistical guidelines and expert statements are now available to assist in the analysis and reporting of studies in some biomedical disciplines. We present here a more progressive resource for sample-based studies, meta-analyses, and case studies in sports medicine and exercise science. We offer forthright advice on the following controversial or novel issues: using precision of estimation for inferences about population effects in preference to null-hypothesis testing, which is inadequate for assessing clinical or practical importance; justifying sample size via acceptable precision or confidence for clinical decisions rather than via adequate power for statistical significance; showing SD rather than SEM, to better communicate the magnitude of differences in means and nonuniformity of error; avoiding purely nonparametric analyses, which cannot provide inferences about magnitude and are unnecessary; using regression statistics in validity studies, in preference to the impractical and biased limits of agreement; making greater use of qualitative methods to enrich sample-based quantitative projects; and seeking ethics approval for public access to the depersonalized raw data of a study, to address the need for more scrutiny of research and better meta-analyses. Advice on less contentious issues includes the following: using covariates in linear models to adjust for confounders, to account for individual differences, and to identify potential mechanisms of an effect; using log transformation to deal with nonuniformity of effects and error; identifying and deleting outliers; presenting descriptive, effect, and inferential statistics in appropriate formats; and contending with bias arising from problems with sampling, assignment, blinding, measurement error, and researchers' prejudices. This article should advance the field by stimulating debate, promoting innovative approaches, and serving as a useful checklist for authors, reviewers, and editors.",2009,32,5143,651,13,61,100,112,185,279,343,381,670,728
82e320e06b1c717b0d924d257aa7b6710f53a38e,"1. Oxygen is a toxic gas - an introductionto oxygen toxicity and reactive species 2. The chemistry of free radicals and related 'reactive species' 3. Antioxidant defences Endogenous and Diet Derived 4. Cellular responses to oxidative stress: adaptation, damage, repair, senescence and death 5. Measurement of reactive species 6. Reactive species can pose special problems needing special solutions. Some examples. 7. Reactive species can be useful some more examples 8. Reactive species can be poisonous: their role in toxicology 9. Reactive species and disease: fact, fiction or filibuster? 10. Ageing, nutrition, disease, and therapy: A role for antioxidants?",1985,0,20976,1139,0,0,0,0,0,0,1,0,0,0
d98c9181996d06fe9e91389c1a0385b7e2575762,"Boken presenterer en helhetlig strategi for hvordan myndigheter, helsepersonell, industri og forbrukere kan redusere medisinske feil.",2000,175,13861,443,1,1,0,3,1,0,0,11,730,837
951865c6d89e9e7822700dad42caf8bd29896d48,"It's about integrating individual clinical expertise and the best external evidence

Evidence based medicine, whose philosophical origins extend back to mid-19th century Paris and earlier, remains a hot topic for clinicians, public health practitioners, purchasers, planners, and the public. There are now frequent workshops in how to practice and teach it (one sponsored by the BMJ will be held in London on 24 April); undergraduate1 and postgraduate2 training programmes are incorporating it3 (or pondering how to do so); British centres for evidence based practice have been established or planned in adult medicine, child health, surgery, pathology, pharmacotherapy, nursing, general practice, and dentistry; the Cochrane Collaboration and Britain's Centre for Review and Dissemination in York are providing systematic reviews of the effects of health care; new evidence based practice journals are being launched; and it has become a common topic in the lay media. But enthusiasm has been mixed with some negative reaction.4 5 6 Criticism has ranged from evidence based medicine being old hat to it being a dangerous innovation, perpetrated by the arrogant to serve cost cutters and suppress clinical freedom. As evidence based medicine continues to evolve and adapt, now is a useful time to refine the discussion of what it is and what it is not.

Evidence based medicine is the conscientious, explicit, and judicious use of current best evidence in making decisions about the care of individual patients. The …",1996,43,11759,333,1,1,1,7,0,0,2,2,68,393
be75109902f5689f7114e9e0fa783a12ceaa9b3a,"SUMMARY
In 1995 the American College of Sports Medicine and the Centers for Disease Control and Prevention published national guidelines on Physical Activity and Public Health. The Committee on Exercise and Cardiac Rehabilitation of the American Heart Association endorsed and supported these recommendations. The purpose of the present report is to update and clarify the 1995 recommendations on the types and amounts of physical activity needed by healthy adults to improve and maintain health. Development of this document was by an expert panel of scientists, including physicians, epidemiologists, exercise scientists, and public health specialists. This panel reviewed advances in pertinent physiologic, epidemiologic, and clinical scientific data, including primary research articles and reviews published since the original recommendation was issued in 1995. Issues considered by the panel included new scientific evidence relating physical activity to health, physical activity recommendations by various organizations in the interim, and communications issues. Key points related to updating the physical activity recommendation were outlined and writing groups were formed. A draft manuscript was prepared and circulated for review to the expert panel as well as to outside experts. Comments were integrated into the final recommendation.


PRIMARY RECOMMENDATION
To promote and maintain health, all healthy adults aged 18 to 65 yr need moderate-intensity aerobic (endurance) physical activity for a minimum of 30 min on five days each week or vigorous-intensity aerobic physical activity for a minimum of 20 min on three days each week. [I (A)] Combinations of moderate- and vigorous-intensity activity can be performed to meet this recommendation. [IIa (B)] For example, a person can meet the recommendation by walking briskly for 30 min twice during the week and then jogging for 20 min on two other days. Moderate-intensity aerobic activity, which is generally equivalent to a brisk walk and noticeably accelerates the heart rate, can be accumulated toward the 30-min minimum by performing bouts each lasting 10 or more minutes. [I (B)] Vigorous-intensity activity is exemplified by jogging, and causes rapid breathing and a substantial increase in heart rate. In addition, every adult should perform activities that maintain or increase muscular strength and endurance a minimum of two days each week. [IIa (A)] Because of the dose-response relation between physical activity and health, persons who wish to further improve their personal fitness, reduce their risk for chronic diseases and disabilities or prevent unhealthy weight gain may benefit by exceeding the minimum recommended amounts of physical activity. [I (A)].",2007,128,7122,516,30,242,461,565,666,695,711,735,743,590
6365c7e578b3ba30b85560515f8b7956a2a915cd,"CONTEXT
A prior national survey documented the high prevalence and costs of alternative medicine use in the United States in 1990.


OBJECTIVE
To document trends in alternative medicine use in the United States between 1990 and 1997.


DESIGN
Nationally representative random household telephone surveys using comparable key questions were conducted in 1991 and 1997 measuring utilization in 1990 and 1997, respectively.


PARTICIPANTS
A total of 1539 adults in 1991 and 2055 in 1997.


MAIN OUTCOMES MEASURES
Prevalence, estimated costs, and disclosure of alternative therapies to physicians.


RESULTS
Use of at least 1 of 16 alternative therapies during the previous year increased from 33.8% in 1990 to 42.1% in 1997 (P < or = .001). The therapies increasing the most included herbal medicine, massage, megavitamins, self-help groups, folk remedies, energy healing, and homeopathy. The probability of users visiting an alternative medicine practitioner increased from 36.3% to 46.3% (P = .002). In both surveys alternative therapies were used most frequently for chronic conditions, including back problems, anxiety, depression, and headaches. There was no significant change in disclosure rates between the 2 survey years; 39.8% of alternative therapies were disclosed to physicians in 1990 vs 38.5% in 1997. The percentage of users paying entirely out-of-pocket for services provided by alternative medicine practitioners did not change significantly between 1990 (64.0%) and 1997 (58.3%) (P=.36). Extrapolations to the US population suggest a 47.3% increase in total visits to alternative medicine practitioners, from 427 million in 1990 to 629 million in 1997, thereby exceeding total visits to all US primary care physicians. An estimated 15 million adults in 1997 took prescription medications concurrently with herbal remedies and/or high-dose vitamins (18.4% of all prescription users). Estimated expenditures for alternative medicine professional services increased 45.2% between 1990 and 1997 and were conservatively estimated at $21.2 billion in 1997, with at least $12.2 billion paid out-of-pocket. This exceeds the 1997 out-of-pocket expenditures for all US hospitalizations. Total 1997 out-of-pocket expenditures relating to alternative therapies were conservatively estimated at $27.0 billion, which is comparable with the projected 1997 out-of-pocket expenditures for all US physician services.


CONCLUSIONS
Alternative medicine use and expenditures increased substantially between 1990 and 1997, attributable primarily to an increase in the proportion of the population seeking alternative therapies, rather than increased visits per patient.",1998,31,6713,384,9,118,246,353,411,432,426,458,449,391
0588b721a655327ec6a70bee072b62db323658c8,"OBJECTIVE
To encourage increased participation in physical activity among Americans of all ages by issuing a public health recommendation on the types and amounts of physical activity needed for health promotion and disease prevention.


PARTICIPANTS
A planning committee of five scientists was established by the Centers for Disease Control and Prevention and the American College of Sports Medicine to organize a workshop. This committee selected 15 other workshop discussants on the basis of their research expertise in issues related to the health implications of physical activity. Several relevant professional or scientific organizations and federal agencies also were represented.


EVIDENCE
The panel of experts reviewed the pertinent physiological, epidemiologic, and clinical evidence, including primary research articles and recent review articles.


CONSENSUS PROCESS
Major issues related to physical activity and health were outlined, and selected members of the expert panel drafted sections of the paper from this outline. A draft manuscript was prepared by the planning committee and circulated to the full panel in advance of the 2-day workshop. During the workshop, each section of the manuscript was reviewed by the expert panel. Primary attention was given to achieving group consensus concerning the recommended types and amounts of physical activity. A concise ""public health message"" was developed to express the recommendations of the panel. During the ensuing months, the consensus statement was further reviewed and revised and was formally endorsed by both the Centers for Disease Control and Prevention and the American College of Sports Medicine.


CONCLUSION
Every US adult should accumulate 30 minutes or more of moderate-intensity physical activity on most, preferably all, days of the week.",1995,64,7299,324,34,71,112,142,164,243,246,230,303,343
d1a70f3162140e69cec8444a7fa10e4ab22b0867,"The 11th edition of Harrison's Principles of Internal Medicine welcomes Anthony Fauci to its editorial staff, in addition to more than 85 new contributors. While the organization of the book is similar to previous editions, major emphasis has been placed on disorders that affect multiple organ systems. Important advances in genetics, immunology, and oncology are emphasized. Many chapters of the book have been rewritten and describe major advances in internal medicine. Subjects that received only a paragraph or two of attention in previous editions are now covered in entire chapters. Among the chapters that have been extensively revised are the chapters on infections in the compromised host, on skin rashes in infections, on many of the viral infections, including cytomegalovirus and Epstein-Barr virus, on sexually transmitted diseases, on diabetes mellitus, on disorders of bone and mineral metabolism, and on lymphadenopathy and splenomegaly. The major revisions in these chapters and many",1988,0,7487,252,4,5,2,3,4,7,6,14,12,44
d66529ac36547455eabe1214e5510aee19d575fa,"AbstractReliability refers to the reproducibility of values of a test, assay or other measurement in repeated trials on the same individuals. Better reliability implies better precision of single measurements and better tracking of changes in measurements in research or practical settings. The main measures of reliability are within-subject random variation, systematic change in the mean, and retest correlation. A simple, adaptable form of within-subject variation is the typical (standard) error of measurement: the standard deviation of an individual’s repeated measurements. For many measurements in sports medicine and science, the typical error is best expressed as a coefficient of variation (percentage of the mean). A biased, more limited form of within-subject variation is the limits of agreement: the 95% likely range of change of an individual’s measurements between 2 trials. Systematic changes in the mean of a measure between consecutive trials represent such effects as learning, motivation or fatigue; these changes need to be eliminated from estimates of within-subject variation. Retest correlation is difficult to interpret, mainly because its value is sensitive to the heterogeneity of the sample of participants. Uses of reliability include decision-making when monitoring individuals, comparison of tests or equipment, estimation of sample size in experiments and estimation of the magnitude of individual differences in the response to a treatment. Reasonable precision for estimates of reliability requires approximately 50 study participants and at least 3 trials. Studies aimed at assessing variation in reliability between tests or equipment require complex designs and analyses that researchers seldom perform correctly. A wider understanding of reliability and adoption of the typical error as the standard measure of reliability would improve the assessment of tests and equipment in our disciplines.
",2000,25,3704,579,3,14,18,28,41,62,62,89,104,117
9bd569e2cd6cce34e11b00cc82c7c11c875cc9db,"The clinical performance of a laboratory test can be described in terms of diagnostic accuracy, or the ability to correctly classify subjects into clinically relevant subgroups. Diagnostic accuracy refers to the quality of the information provided by the classification device and should be distinguished from the usefulness, or actual practical value, of the information. Receiver-operating characteristic (ROC) plots provide a pure index of accuracy by demonstrating the limits of a test's ability to discriminate between alternative states of health over the complete spectrum of operating conditions. Furthermore, ROC plots occupy a central or unifying position in the process of assessing and using diagnostic tools. Once the plot is generated, a user can readily go on to many other activities such as performing quantitative ROC analysis and comparisons of tests, using likelihood ratio to revise the probability of disease in individual subjects, selecting decision thresholds, using logistic-regression analysis, using discriminant-function analysis, or incorporating the tool into a clinical strategy by using decision analysis.",1993,81,6102,281,6,33,43,57,84,76,99,104,122,137
876f3e75121b2de085d850ca8d4816f17e9c9ab7,"Medicinal plants and traditional medicine in Africa , Medicinal plants and traditional medicine in Africa , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",1982,0,5116,202,0,0,0,2,4,3,1,7,7,8
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
bb2832354af5f5690710fa951a1db18b4176f5ca,"Minimal measurement error (reliability) during the collection of interval- and ratio-type data is critically important to sports medicine research. The main components of measurement error are systematic bias (e.g. general learning or fatigue effects on the tests) and random error due to biological or mechanical variation. Both error components should be meaningfully quantified for the sports physician to relate the described error to judgements regarding ‘analytical goals’ (the requirements of the measurement tool for effective practical use) rather than the statistical significance of any reliability indicators.Methods based on correlation coefficients and regression provide an indication of ‘relative reliability’. Since these methods are highly influenced by the range of measured values, researchers should be cautious in: (i) concluding acceptable relative reliability even if a correlation is above 0.9; (ii) extrapolating the results of a test-retest correlation to a new sample of individuals involved in an experiment; and (iii) comparing test-retest correlations between different reliability studies.Methods used to describe ‘absolute reliability’ include the standard error of measurements (SEM), coefficient of variation (CV) and limits of agreement (LOA). These statistics are more appropriate for comparing reliability between different measurement tools in different studies. They can be used in multiple retest studies from ANOVA procedures, help predict the magnitude of a ‘real’ change in individual athletes and be employed to estimate statistical power for a repeated-measures experiment.These methods vary considerably in the way they are calculated and their use also assumes the presence (CV) or absence (SEM) of heteroscedasticity. Most methods of calculating SEM and CV represent approximately 68% of the error that is actually present in the repeated measurements for the ‘average’ individual in the sample. LOA represent the test-retest differences for 95% of a population. The associated Bland-Altman plot shows the measurement error schematically and helps to identify the presence of heteroscedasticity. If there is evidence of heteroscedasticity or non-normality, one should logarithmically transform the data and quote the bias and random error as ratios. This allows simple comparisons of reliability across different measurement tools.It is recommended that sports clinicians and researchers should cite and interpret a number of statistical methods for assessing reliability. We encourage the inclusion of the LOA method, especially the exploration of heteroscedasticity that is inherent in this analysis. We also stress the importance of relating the results of any reliability statistic to ‘analytical goals’ in sports medicine.",1998,106,2953,336,0,4,19,30,28,28,39,59,60,75
baef5c6685cc25d52592b7900857efd1d34a731b,"Evidence-based Healthcare: How to Make Health Policy and Management Decisions, by J. A. Muir Gray, 270 pp, with illus, paper, $29.95, ISBN 0-443-05721-4, New York, NY, Churchill Livingstone, 1997. We develop clinical expertise with bedside training and experience. How well do we integrate this experience with the best available external evidence for the purpose of direct patient care? I suspect that we do not carry out this function very well. Evidence-based Medicine (EBM) is the practice of applying valid evidence and data to a specific clinical question engendered during patient care. Lately, EBM has been on the lips and pen tips of clinicians, perhaps as a close runner-up to the other shibboleths managed care, gag clause, and networking. Is EBM then another mere mantra, a novel paradigm, or a practicable concept to help with ordinary, day-to-day clinical care? I believe it is the last, but you should be",1997,0,3452,195,8,30,55,54,65,105,120,136,155,152
8552f26dc0d4d08717e5d5ae44339f1a06d343b5,"Part 1. General Medicine: Clinical Examination and Making a Diagnosis. General Systemic States. Diseases of the Newborn. Practical Antimicrobial Therapeutics. Diseases of the Alimentary Tract I. Diseases of the Alimentary Tract II. Diseases of the Liver and Pancreas. Diseases of the Cardiovascular System. Diseases of the Blood and Blood-Forming Organs. Diseases of the Respiratory System. Diseases of the Urinary System. Diseases of the Nervous System. Diseases of the Musculoskeletal System. Diseases of the Skin, Conjunctiva and External Ear. Part 2. Special Medicine: Mastitis. Diseases Caused By Bacteria V. Diseases Caused By Viruses and Chlamydia I. Diseases Caused By Viruses and Chlamydia II. Diseases Caused By Rickettsia. Diseases Caused By Algae and Fungi. Diseases Caused By Protozoa. Diseases Caused By Helminth Parasites. Diseases Caused By Arthropod Parasites. Metabolic Diseases. Diseases Caused By Nutritional Deficiencies. Diseases Caused By Physical Agents. Diseases Caused By Inorganic and Farm Chemicals. Diseases Caused By Toxins in Plants, Fungi, Cyanobacteria, Clavibacteria, Insects and Animals. Diseases Caused By Allergy. Diseases Caused By the Inheritance of Undesirable Characters. Specific Diseases of Uncertain Etiology. Conversion Tables. Reference Laboratory Values. Index.",1994,0,3084,241,6,12,12,23,18,17,23,48,43,55
c6b5380f8e79382257fc4315d0b7d891c436b935,"BACKGROUND
Many people use unconventional therapies for health problems, but the extent of this use and the costs are not known. We conducted a national survey to determine the prevalence, costs, and patterns of use of unconventional therapies, such as acupuncture and chiropractic.


METHODS
We limited the therapies studied to 16 commonly used interventions neither taught widely in U.S. medical schools nor generally available in U.S. hospitals. We completed telephone interviews with 1539 adults (response rate, 67 percent) in a national sample of adults 18 years of age or older in 1990. We asked respondents to report any serious or bothersome medical conditions and details of their use of conventional medical services; we then inquired about their use of unconventional therapy.


RESULTS
One in three respondents (34 percent) reported using at least one unconventional therapy in the past year, and a third of these saw providers for unconventional therapy. The latter group had made an average of 19 visits to such providers during the preceding year, with an average charge per visit of $27.60. The frequency of use of unconventional therapy varied somewhat among socio-demographic groups, with the highest use reported by nonblack persons from 25 to 49 years of age who had relatively more education and higher incomes. The majority used unconventional therapy for chronic, as opposed to life-threatening, medical conditions. Among those who used unconventional therapy for serious medical conditions, the vast majority (83 percent) also sought treatment for the same condition from a medical doctor; however, 72 percent of the respondents who used unconventional therapy did not inform their medical doctor that they had done so. Extrapolation to the U.S. population suggests that in 1990 Americans made an estimated 425 million visits to providers of unconventional therapy. This number exceeds the number of visits to all U.S. primary care physicians (388 million). Expenditures associated with use of unconventional therapy in 1990 amounted to approximately $13.7 billion, three quarters of which ($10.3 billion) was paid out of pocket. This figure is comparable to the $12.8 billion spent out of pocket annually for all hospitalizations in the United States.


CONCLUSIONS
The frequency of use of unconventional therapy in the United States is far higher than previously reported. Medical doctors should ask about their patients' use of unconventional therapy whenever they obtain a medical history.",1993,27,4277,155,19,44,77,97,163,209,178,240,228,269
029929987c1321e6880234bf0492689a7a379e38,"In medicine we often want to compare two different methods of measuring some quantity, such as blood pressure, gestational age, or cardiac stroke volume. Sometimes we compare an approximate or simple method with a very precise one. This is a calibration problem, and we shall not discuss it further here. Frequently, however, we cannot regard either method as giving the true value of the quantity being measured. In this case we want to know whether the methods give answers which are, in some sense, comparable. For example, we may wish to see whether a new, cheap and quick method produces answers that agree with those from an established method sufficiently well for clinical purposes. Many such studies, using a variety of statistical techniques, have been reported. Yet few really answer the question “Do the two methods of measurement agree sufficiently closely?” In this paper we shall describe what is usually done, show why this is inappropriate, suggest a better approach, and ask why such studies are done so badly. We will restrict our consideration to the comparison of two methods of measuring a continuous variable, although similar problems can arise with categorical variables.",1983,23,3532,171,1,3,10,17,18,12,18,21,36,33
16399f848bb04faabd7f0107459132cc1c4777b0,"Life is the interplay between structure and energy, yet the role of energy deficiency in human disease has been poorly explored by modern medicine. Since the mitochondria use oxidative phosphorylation (OXPHOS) to convert dietary calories into usable energy, generating reactive oxygen species (ROS) as a toxic by-product, I hypothesize that mitochondrial dysfunction plays a central role in a wide range of age-related disorders and various forms of cancer. Because mitochondrial DNA (mtDNA) is present in thousands of copies per cell and encodes essential genes for energy production, I propose that the delayed-onset and progressive course of the age-related diseases results from the accumulation of somatic mutations in the mtDNAs of post-mitotic tissues. The tissue-specific manifestations of these diseases may result from the varying energetic roles and needs of the different tissues. The variation in the individual and regional predisposition to degenerative diseases and cancer may result from the interaction of modern dietary caloric intake and ancient mitochondrial genetic polymorphisms. Therefore the mitochondria provide a direct link between our environment and our genes and the mtDNA variants that permitted our forbears to energetically adapt to their ancestral homes are influencing our health today.",2005,299,2871,173,11,95,151,161,213,207,203,234,230,220
ee4f3da2b55d75a8e978275fd36eaa10124ea88f,"The purpose of this Position Stand is to provide an overview of issues critical to understanding the importance of exercise and physical activity in older adult populations. The Position Stand is divided into three sections: Section 1 briefly reviews the structural and functional changes that characterize normal human aging, Section 2 considers the extent to which exercise and physical activity can influence the aging process, and Section 3 summarizes the benefits of both long-term exercise and physical activity and shorter-duration exercise programs on health and functional capacity. Although no amount of physical activity can stop the biological aging process, there is evidence that regular exercise can minimize the physiological effects of an otherwise sedentary lifestyle and increase active life expectancy by limiting the development and progression of chronic disease and disabling conditions. There is also emerging evidence for significant psychological and cognitive benefits accruing from regular exercise participation by older adults. Ideally, exercise prescription for older adults should include aerobic exercise, muscle strengthening exercises, and flexibility exercises. The evidence reviewed in this Position Stand is generally consistent with prior American College of Sports Medicine statements on the types and amounts of physical activity recommended for older adults as well as the recently published 2008 Physical Activity Guidelines for Americans. All older adults should engage in regular physical activity and avoid an inactive lifestyle.",2009,238,2916,144,48,91,169,232,240,205,285,231,257,268
34f3877c84af60d0b1029145a6e6ff7bbad6ad76,"In order to stimulate further adaptation toward specific training goals, progressive resistance training (RT) protocols are necessary. The optimal characteristics of strength-specific programs include the use of concentric (CON), eccentric (ECC), and isometric muscle actions and the performance of bilateral and unilateral single- and multiple-joint exercises. In addition, it is recommended that strength programs sequence exercises to optimize the preservation of exercise intensity (large before small muscle group exercises, multiple-joint exercises before single-joint exercises, and higher-intensity before lower-intensity exercises). For novice (untrained individuals with no RT experience or who have not trained for several years) training, it is recommended that loads correspond to a repetition range of an 8-12 repetition maximum (RM). For intermediate (individuals with approximately 6 months of consistent RT experience) to advanced (individuals with years of RT experience) training, it is recommended that individuals use a wider loading range from 1 to 12 RM in a periodized fashion with eventual emphasis on heavy loading (1-6 RM) using 3- to 5-min rest periods between sets performed at a moderate contraction velocity (1-2 s CON; 1-2 s ECC). When training at a specific RM load, it is recommended that 2-10% increase in load be applied when the individual can perform the current workload for one to two repetitions over the desired number. The recommendation for training frequency is 2-3 d x wk(-1) for novice training, 3-4 d x wk(-1) for intermediate training, and 4-5 d x wk(-1) for advanced training. Similar program designs are recommended for hypertrophy training with respect to exercise selection and frequency. For loading, it is recommended that loads corresponding to 1-12 RM be used in periodized fashion with emphasis on the 6-12 RM zone using 1- to 2-min rest periods between sets at a moderate velocity. Higher volume, multiple-set programs are recommended for maximizing hypertrophy. Progression in power training entails two general loading strategies: 1) strength training and 2) use of light loads (0-60% of 1 RM for lower body exercises; 30-60% of 1 RM for upper body exercises) performed at a fast contraction velocity with 3-5 min of rest between sets for multiple sets per exercise (three to five sets). It is also recommended that emphasis be placed on multiple-joint exercises especially those involving the total body. For local muscular endurance training, it is recommended that light to moderate loads (40-60% of 1 RM) be performed for high repetitions (>15) using short rest periods (<90 s). In the interpretation of this position stand as with prior ones, recommendations should be applied in context and should be contingent upon an individual's target goals, physical capacity, and training status.",2009,415,2914,116,89,118,149,181,198,193,196,220,287,278
391adf80e9a43ded3f591b911136876513ee1c39,Introduction biology and pathophysiology of skin disorders presenting in the skin and mucous membranes dermatology and internal medicine diseases due to microbial agents therapeutics paediatric and geriatric dermatology.,1971,0,4224,37,1,7,16,23,25,26,19,21,14,26
aeff86720a66ae6a0ce26ae29c68fbd8dbebfbee,"A NEW paradigm for medical practice is emerging. Evidence-based medicine de-emphasizes intuition, unsystematic clinical experience, and pathophysiologic rationale as sufficient grounds for clinical decision making and stresses the examination of evidence from clinical research. Evidence-based medicine requires new skills of the physician, including efficient literature searching and the application of formal rules of evidence evaluating the clinical literature. An important goal of our medical residency program is to educate physicians in the practice of evidence-based medicine. Strategies include a weekly, formal academic half-day for residents, devoted to learning the necessary skills; recruitment into teaching roles of physicians who practice evidence-based medicine; sharing among faculty of approaches to teaching evidence-based medicine; and providing faculty with feedback on their performance as role models and teachers of evidence-based medicine. The influence of evidencebased medicine on clinical practice and medical education is increasing. CLINICAL SCENARIO A junior medical resident working in a teaching hospital",1992,42,3742,29,0,19,32,48,61,89,93,80,94,99
527bf3363fcbb24c814e47934a5b0f5204131461,"The growth of blood vessels (a process known as angiogenesis) is essential for organ growth and repair. An imbalance in this process contributes to numerous malignant, inflammatory, ischaemic, infectious and immune disorders. Recently, the first anti-angiogenic agents have been approved for the treatment of cancer and blindness. Angiogenesis research will probably change the face of medicine in the next decades, with more than 500 million people worldwide predicted to benefit from pro- or anti-angiogenesis treatments.",2005,32,3160,118,4,75,190,228,238,232,292,225,254,211
f155714ac666017a4d8883c2afbd7589801e5fda,"Part 1: Normal Sleep. Normal Sleep and Its Variations. Phylogeny. Sleep Mechanisms. Physiology in Sleep. Chronobiology. Pharmacology. Psychophysiology and Dreaming. Part 2: Abnormal Sleep. Impact, Presentation, And Diagnosis. Disorders of Chrononbiology. Insomnia. Primary Disorders of Daytime Somnolence. Parasomnias. Sleep Breathing Disorders. Medical Disorders. Psychiatric Disorders. Methodology.",1989,0,4263,38,3,19,23,26,26,30,62,82,72,93
251b1c8bce87bae9ec1e0f542ea9152aa7781b8f,"I might have guessed that a book dedicated to ""H.L. Mencken, Kurt Vonnegut, Jr., Douglas Adams, and the Emperor's New Clothes"" would be fun to read. It was! Readers will sense the authors' enthusiasm for their subject on each page, from the preface to the final chapter. The authors prepared this book for ""users"" rather than ""doers"" of clinical research. Physicians and others who wish to recognize key clinical epidemiologic features of the diagnosis and management of patients will benefit from reading Clinical Epidemiology. Those who wish to conduct actual research studies will need to look elsewhere for a detailed discussion of clinical epidemiologic methodology. In this review, I will mention",1985,0,3553,64,5,15,31,59,58,64,71,95,93,110
418516bbea55b3b66594158d6808be66f05baba2,,1981,0,2676,213,1,1,1,4,4,3,3,3,3,1
2b98f39bfb8abb50813f25596192cf9c9041d523,"OBJECTIVE
This report presents selected estimates of complementary and alternative medicine (CAM) use among U.S. adults and children, using data from the 2007 National Health Interview Survey (NHIS), conducted by the Centers for Disease Control and Prevention's (CDC) National Center for Health Statistics (NCHS). Trends in adult use were assessed by comparing data from the 2007 and 2002 NHIS.


METHODS
Estimates were derived from the Complementary and Alternative Medicine supplements and Core components of the 2007 and 2002 NHIS. Estimates were generated and comparisons conducted using the SUDAAN statistical package to account for the complex sample design.


RESULTS
In 2007, almost 4 out of 10 adults had used CAM therapy in the past 12 months, with the most commonly used therapies being nonvitamin, nonmineral, natural products (17.7%) and deep breathing exercises (12.7%). American Indian or Alaska Native adults (50.3%) and white adults (43.1%) were more likely to use CAM than Asian adults (39.9%) or black adults (25.5%). Results from the 2007 NHIS found that approximately one in nine children (11.8%) used CAM therapy in the past 12 months, with the most commonly used therapies being nonvitamin, nonmineral, natural products (3.9%) and chiropractic or osteopathic manipulation (2.8%). Children whose parent used CAM were almost five times as likely (23.9%) to use CAM as children whose parent did not use CAM (5.1%). For both adults and children in 2007, when worry about cost delayed receipt of conventional care, individuals were more likely to use CAM than when the cost of conventional care was not a worry. Between 2002 and 2007 increased use was seen among adults for acupuncture, deep breathing exercises, massage therapy, meditation, naturopathy, and yoga. CAM use for head or chest colds showed a marked decrease from 2002 to 2007 (9.5% to 2.0%).",2008,46,2675,87,4,59,176,203,278,296,311,307,249,218
66a586cc77c0f01d5f481c644d58e8f1931b120a,"OBJECTIVE
To issue a recommendation on the types and amounts of physical activity needed to improve and maintain health in older adults.


PARTICIPANTS
A panel of scientists with expertise in public health, behavioral science, epidemiology, exercise science, medicine, and gerontology.


EVIDENCE
The expert panel reviewed existing consensus statements and relevant evidence from primary research articles and reviews of the literature.


PROCESS
After drafting a recommendation for the older adult population and reviewing drafts of the Updated Recommendation from the American College of Sports Medicine (ACSM) and the American Heart Association (AHA) for Adults, the panel issued a final recommendation on physical activity for older adults.


SUMMARY
The recommendation for older adults is similar to the updated ACSM/AHA recommendation for adults, but has several important differences including: the recommended intensity of aerobic activity takes into account the older adult's aerobic fitness; activities that maintain or increase flexibility are recommended; and balance exercises are recommended for older adults at risk of falls. In addition, older adults should have an activity plan for achieving recommended physical activity that integrates preventive and therapeutic recommendations. The promotion of physical activity in older adults should emphasize moderate-intensity aerobic activity, muscle-strengthening activity, reducing sedentary behavior, and risk management.",2007,76,2236,118,12,72,128,153,184,191,216,189,221,186
140f7c7d5fafba3fc9911476c6cbc911b35e93a0,"Hydrophilic polymers are the center of research emphasis in nanotechnology because of their perceived “intelligence”. They can be used as thin films, scaffolds, or nanoparticles in a wide range of biomedical and biological applications. Here we highlight recent developments in engineering uncrosslinked and crosslinked hydrophilic polymers for these applications. Natural, biohybrid, and synthetic hydrophilic polymers and hydrogels are analyzed and their thermodynamic responses are discussed. In addition, examples of the use of hydrogels for various therapeutic applications are given. We show how such systems’ intelligent behavior can be used in sensors, microarrays, and imaging. Finally, we outline challenges for the future in integrating hydrogels into biomedical applications.",2006,274,3088,52,5,29,67,120,156,202,231,245,276,263
efa296faa52fd7799002315956f082f631b99ac4,,1998,0,1927,248,33,50,55,58,84,65,85,74,99,81
9b5474dd271db17cf56a64800f2993e1feff7ca0,"PART I General Considerations of Cardiovascular Disease PART II Examination of the Patient PART III Normal and Abnormal Cardiac Function: Heart Failure and Arrhythmias PART IV Hypertensive and Atherosclerotic Cardiovascular Disease PART V Diseases of the Heart, Pericardium and Pulmonary Vascular Bed PART VI Molecular Biology and Genetics PART VII Cardiovascular Disease in Special Populations PART VIII Cardiovascular Disease and Disorders of other Organ Systems",1979,0,3650,12,0,1,6,10,16,16,22,17,21,24
3df1d83247b2865db49966bfcd950672c0d98b26,,1987,0,2846,82,2,2,14,13,21,26,23,41,70,73
4f1bc78d826b558472ddef7d5b19dc78a6e7db55,"The Textbook was written by O.M. Radostits, C.C. Gay, K.V. Hinchcliff and P.D. Constable and produced in 2007 by Mosby Elsevier publisher. Tenth edition textbook of 2,156 pages including 148 instructive tables and 30 illustrations provides a source of detailed information about all important transmissible and nontransmissible diseases of cattle, horse, sheep, pigs and goats. The textbook is divided in two main parts. The first one is dedicated to general medicine and the second one to specific medicine. The general medicine part is subdivided in following chapters: clinical examination and making a diagnosis, general systemic states, diseases of the newborn, practical antimicrobial therapeutics, diseases of the alimentary tract, of the liver and pancreas, of the cardiovascular system, of the hemolymphatic and immune systems, of the respiratory system, of the urinary system, of the nervous system, of the musculoskeletal system, of the skin, conjunctiva and external ear and of the mammary gland. The specific medicine part is subdivided in diseases associated with bacteria, with viruses and Chlamydia, with prions, with Rickettsiales, algae and fungi, with protozoa, with helminth parasites and with arthropod parasites. This part continues with metabolic diseases, diseases associated with nutritional deficiencies, with physical agents, with inorganic and farm chemicals, with toxins in plants, fungi, cyanobacteria, clavibacteria, insects and animals, with allergy, with the inheritance of undesirable characteristics and finally specific diseases of uncertain etiology. At the end there are appendices dealing with conversion factors, reference laboratory values, drug doses for horses and ruminants and drug doses for pigs. Detailed Index required 89 pages. The publication meets the demand not only of undergraduate veterinary students and graduate veterinarians working in the field of large-animal medicine but also of other specialists dealing with livestock husbandry. The book has international scope including most of the diseases occurring in large animals worldwide. It produces up-to-date review of the large-animal medicine. Review of communicable diseases includes etiology, epidemiology, pathogenesis, treatment and control. This edition is giving special emphasis to zoonotic implications, individual diagnostic tests, provision of emergency service to the owners of herds and flocks. The textbook is giving priority attention to food-producing animals and industrial animal agriculture. Necessary information are provided on equine practice and companion animal practice. Very useful are the procedures aimed at the efficiency of livestock production: providing the most economical method of diagnosis and treatment, monitoring animal health and production, recommending specific disease control and prevention programmes, organizing planned herds and flock health programmes, advising on nutrition, breeding and general management practices. The authors give the attention also to animal welfare and food safety. This publication serves as an encyclopedia of modern veterinary medicine supporting through the best possible health the most effective performance of food-producing large-animals under the conditions of developed as well as developing countries.",2007,0,1597,275,6,13,30,53,61,66,100,131,117,133
f14b8ef5a3edc5fdc9613a8a1c5545aa6cb626ad,"This Position Stand provides guidance on fluid replacement to sustain appropriate hydration of individuals performing physical activity. The goal of prehydrating is to start the activity euhydrated and with normal plasma electrolyte levels. Prehydrating with beverages, in addition to normal meals and fluid intake, should be initiated when needed at least several hours before the activity to enable fluid absorption and allow urine output to return to normal levels. The goal of drinking during exercise is to prevent excessive (>2% body weight loss from water deficit) dehydration and excessive changes in electrolyte balance to avert compromised performance. Because there is considerable variability in sweating rates and sweat electrolyte content between individuals, customized fluid replacement programs are recommended. Individual sweat rates can be estimated by measuring body weight before and after exercise. During exercise, consuming beverages containing electrolytes and carbohydrates can provide benefits over water alone under certain circumstances. After exercise, the goal is to replace any fluid electrolyte deficit. The speed with which rehydration is needed and the magnitude of fluid electrolyte deficits will determine if an aggressive replacement program is merited.",2007,10,1797,207,36,62,53,93,111,114,140,150,158,152
dccd557eb3d10b858458c17e0f2e51ac0e622115,"The emerging field of regenerative medicine will require a reliable source of stem cells in addition to biomaterial scaffolds and cytokine growth factors. Adipose tissue represents an abundant and accessible source of adult stem cells with the ability to differentiate along multiple lineage pathways. The isolation, characterization, and preclinical and clinical application of adipose-derived stem cells (ASCs) are reviewed in this article.",2007,160,2058,84,6,43,74,112,165,158,229,193,218,190
ae83ba2c166ca4a6bb19661b998830f8ad05bb57,"Biomaterials have played an enormous role in the success of medical devices and drug delivery systems. We discuss here new challenges and directions in biomaterials research. These include synthetic replacements for biological tissues, designing materials for specific medical applications, and materials for new applications such as diagnostics and array technologies.",2004,78,2697,24,11,50,82,111,136,141,179,204,212,205
2f48a1937b4502fc11b3b0cc0889bcba02d34750,"The factors that cause large individual differences in professional achievement are only partially understood. Nobody becomes an outstanding professional without experience, but extensive experience does not invariably lead people to become experts. When individuals are first introduced to a professional domain after completing their education, they are often overwhelmed and rely on help from others to accomplish their responsibilities. After months or years of experience, they attain an acceptable level of proficiency and are able to work independently. Although everyone in a given domain tends to improve with experience initially, some develop faster than others and continue to improve during ensuing years. These individuals are eventually recognized as experts and masters. In contrast, most professionals reach a stable, average level of performance within a relatively short time frame and maintain this mediocre status for the rest of their careers. The nature of the individual differences that cause the large variability in attained performance is still debated. The most common explanation is that achievement in a given domain is limited by innate factors that cannot be changed through experience and training; hence, limits of attainable performance are determined by one’s basic endowments, such as abilities, mental capacities, and innate talents. Educators with this widely held view of professional development have focused on identifying and selecting students who possess the necessary innate talents that would allow them to reach expert levels with adequate experience. Therefore, the best schools and professional organizations nearly always rely on extensive testing and interviews to find the most talented applicants. This general view also explains age-related declines in professional achievement in terms of the inevitable reductions in general abilities and capacities believed to result from aging. In this article, I propose an alternative framework to account for individual differences in attained professional development, as well as many aspects of age-related decline. This framework is based on the assumption that acquisition of expert performance requires engagement in deliberate practice and that continued deliberate practice is necessary for maintenance of many types of professional performance. In order to contrast this alternative framework with the traditional view, I first describe the account based on innate talent. I then provide a brief review of the evidence on deliberate practice in the acquisition of expert performance in several performance domains, including music, chess, and sports. Finally, I review evidence from the acquisition and maintenance of expert performance in medicine and examine the role of deliberate practice in this domain.",2004,133,2267,85,0,23,44,81,78,95,123,141,166,165
0cf16c3770d1ce1b9b1b4d875741ba51252882f7,"Abstract Ti–Ni-based alloys are quite attractive functional materials not only as practical shape memory alloys with high strength and ductility but also as those exhibiting unique physical properties such as pre-transformation behaviors, which are enriched by various martensitic transformations. The paper starts from phase diagram, structures of martensites, mechanisms of martensitic transformations, premartensitic behavior, mechanism of shape memory and superelastic effects etc., and covers most of the fundamental issues related with the alloys, which include not only martensitic transformations but also diffusional transformations, since the latter greatly affect the former, and are useful to improve shape memory characteristics. Thus the alloy system will serve as an excellent case study of physical metallurgy, as is the case for steels where all kinds of phase transformations are utilized to improve the physical properties. In short this review is intended to give a self-consistent and logical account of key issues on Ti–Ni based alloys from physical metallurgy viewpoint on an up-to-date basis.",2005,345,2950,99,6,37,60,102,100,110,141,134,195,206
c7ca77863c289ecb4bcfdd7e329d90f792d707be,"Physical Metallurgy Principles is intended for use in an introductory course in physical metallurgy and is designed for all engineering students at the junior or senior level. The approach is largely theoretical, but covers all aspects of physical metallurgy and behavior of metals and alloys. The treatment used in this textbook is in harmony with a more fundamental approach to engineering education.",1972,0,2195,50,5,3,3,1,7,9,7,7,8,13
eb4ec95f5ea8aa0b7e54b3b0975494999b113595,"Preface. 1. Introduction. 1.1 Ni-base Alloy Classification. 1.2 History of Nickel and Ni-base Alloys. 1.3 Corrosion Resistance. 1.4 Nickel Alloy Production. 2. Alloying Additions, Phase Diagrams, and Phase Stability. 2.1 Introduction. 2.2 General Influence of Alloying Additions. 2.3 Phase Diagrams for Solid-Solution Alloys. 2.4 Phase Diagrams for Precipitation Hardened Alloys--gamma' Formers. 2.5 Phase Diagrams for Precipitation-Hardened Alloys--gamma"" Formers. 2.6 Calculated Phase Stability Diagrams. 2.7 PHACOMP Phase Stability Calculations. 3. Solid-Solution Strengthened Ni-base Alloys. 3.1 Standard Alloys and Consumables. 3.2 Physical Metallurgy and Mechanical Properties. 3.3 Welding Metallurgy. 3.4 Mechanical Properties of Weldments. 3.5 Weldability. 3.6 Corrosion Resistance. 3.7 Case Studies. 4. Precipitation Strengthened Ni-base Alloys. 4.1 Standard Alloys and Consumables. 4.2 Physical Metallurgy and Mechanical Properties. 4.3 Welding Metallurgy. 4.4 Mechanical Properties of Weldments. 4.5 Weldability. 5. Oxide Dispersion Strengthened Alloys and Nickel Aluminides. 5.1 Oxide Dispersion Strengthened Alloys. 5.2 Nickel Aluminide Alloys. 6. Repair Welding of Ni-base Alloys. 6.1 Solid-Solution Strengthened Alloys. 6.2 Precipitation Strengthened Alloys. 6.3 Single Crystal Superalloys. 7. Dissimilar Welding. 7.1 Application of Dissimilar Welds. 7.2 Influence of Process Parameters on Fusion Zone Composition. 7.3 Carbon, Low Alloys and Stainless Steels. 7.4 Postweld Heat Treatment Cracking in Stainless Steels Welded with Ni-base Filler Metals. 7.5 Super Austenitic Stainless Steels. 7.6 Dissimilar Welds in Ni-base Alloys - Effect on Corrosion Resistance. 7.7 9%Ni Steels. 7.8 Super Duplex Stainless Steels. 7.9 Case Studies. 8. Weldability Testing. 8.1 Introduction. 8.2 The Varestraint Test. 8.3 Modified Cast Pin Tear Test. 8.4 The Sigmajig Test. 8.5 The Hot Ductility Test. 8.6 The Strain-to-Fracture Test. 8.7 Other Weldability Tests. Appendix A Composition of Wrought and Cast Nickel-Base Alloys. Appendix B Composition of Nickel and Nickel Alloy Consumables. Appendix C Corrosion Acceptance Testing Methods. Appendix D Etching Techniques for Ni-base Alloys and Welds. Author Index. Subject Index.",2009,66,699,60,0,3,18,18,47,54,53,71,68,85
2524a882d157961d70b6106459aed3bb442b74c1,"Comprehensive information for the American aluminium industry Collective effort of 53 recognized experts on aluminium and aluminium alloys Joint venture by world renowned authorities-the Aluminium Association Inc. and American Society for Metals. The completely updated source of information on aluminium industry as a whole rather than its individual contributors. this book is an opportunity to gain from The knowledge of the experts working for prestigious companies such as Alcoa, Reynolds Metals Co., Alcan International Ltd., Kaiser Aluminium & Chemical Corp., Martin Marietta Laboratories and Anaconda Aluminium Co. It took four years of diligent work to complete this comprehensive successor to the classic volume, Aluminium, published by ASM in 1967. Contents: Properties of Pure Aluminum Constitution of Alloys Microstructure of Alloys Work Hardening Recovery, Recrystalization and Growth Metallurgy of Heat Treatment and General Principles of Precipitation Hardening Effects of Alloying Elements and Impurities on Properties Corrosion Behaviour Properties of Commercial Casting Alloys Properties of Commercial Wrought Alloys Aluminum Powder and Powder Metallurgy Products.",1984,0,1561,73,0,0,2,7,6,5,3,10,13,14
34058cc7431331f5af8cd63ac3c9df14765c7eac,Preface. 1. Introduction. 2. Phase Diagrams. 3. Alloying Elements and Constitution Diagrams. 4. Martensitic Stainless Steels. 5. Ferritic Stainless Steels. 6. Austenitic Stainless Steels. 7. Duplex Stainless Steels. 8. Precipitation-Hardening Stainless Steels. 9. Dissimilar Welding of Stainless Steels. 10. Weldability Testing. Appendix 1: Nominal Compositions of Stainless Steels. Appendix 2: Etching Techniques for Stainless Steel Welds. Author Index. Subject Index.,2005,0,1025,77,1,3,13,28,27,34,60,56,74,77
d8133fb8e17ed2a5809e03cc43c7914ea49f2b0d,"This practical reference provides thorough and systematic coverage on both basic metallurgy and the practical engineering aspects of metallic material selection and application. Contents includes: Practical information on the engineering properties and applications of steels, cast irons, nonferrous alloys, and metal matrix composites. Concise overviews and practical implications of metallic structure, imperfections, deformation, and phase transformations Process metallurgy of solidification and casting, recovery, recrystallization and grain growth, precipitation hardening Mechanical deformation during processing and in-service properties of fatigue, fracture, and creep. Physical properties and corrosion.",2008,0,567,50,0,1,4,12,16,42,52,57,74,68
eafe94460769dc1dca747eb9d9c9490a6d8ec12f,"Abstract The generation of zinc and zinc alloy coatings on steel is one of the commercially most important processing techniques used to protect steel components exposed to corrosive environments. From a technological standpoint, the principles of galvanizing have remained unchanged since this coating came into use over 200 years ago. However, because of new applications in the automotive and construction industry, a considerable amount of research has recently occurred on all aspects of the galvanizing process and on new types of Zn coatings. This review will discuss the metallurgy of zinc-coated steel from a scientific standpoint to develop relationships to practical applications. Hot-dip zinc coating methods, i.e. batch and continuous processes, will first be reviewed along with Fe–Zn phase equilibria and kinetics. Commercially, the addition of aluminum to the zinc bath results in three important types of coatings, galvanized, galfan and galvalume, and produces complex reactions at the coating/substrate interface. Fe–Zn–Al equilibrium will be reviewed in the light of recent studies of solubility and inhibition layer formation and breakdown. The effect of steel substrate composition on these reactions will also be critically analyzed. The overlay coating formation, or the coating alloy, is specifically chosen for its desired properties. The morphology of the galvanize, galfan and galvalume coating overlays will be reviewed, as well as the effect of heat treatment to produce a galvanneal coating. Finally, the effect of the microstructures of these coatings on the important properties of corrosion, formability, weldability and paintability will be discussed.",2000,64,1046,39,0,2,2,6,13,25,32,48,46,40
5c1af053ef5d859221eab1d0e56418b04d8f9249,"Skripta Fizikalna metalurgija I je sažeti prikaz znanosti o materijalima u kojoj se na znanstvenim i inženjerskim principima tumaci kristalna građa metala, dizajniranje legura i mikrostrukture, te odnos između strukture metala i njegovih mehanickih i fizickih svojstava.",2009,8,395,29,21,25,30,32,25,28,40,25,24,35
abbe5dee41d472e0468ae30e7bc6dc7d53284b2e,"This volume provides a substantial background to microalloyed steels with a wide selection of applications, some of which are very recent. A well-illustrated practical guide, this book acts as a useful source of data and a concise account of the theoretical aspects of the subject. Both academic institutions and the world-wide steel industry will find it indispensable.",1997,0,958,75,0,6,4,10,18,15,16,22,39,32
3e29d4c182490ea94be52b5593f7d218d4782358,"1. Stress and strain 2. Plasticity 3. Strain hardening 4. Plastic instability 5. Temperature and strain-rate dependence 6. Work balance 7. Slab analysis and friction 8. Friction and lubrication 9. Upper-bound analysis 10. Slip-line field analysis 11. Deformation zone geometry 12. Formability 13. Bending 14. Plastic anisotropy 15. Cupping, redrawing and ironing 16. Forming limit diagrams 17. Stamping 18. Hydroforming 19. Other sheet forming operations 20. Formability tests 21. Sheet metal properties.",1993,13,1086,42,9,12,9,14,15,15,9,16,20,17
6b09d20de5ccfcf15453d0f953147fd1e9079587,,1984,0,1147,69,0,0,3,0,2,5,9,8,7,9
b00ac81caf4ad7a151407477ce9f0db123615c71,Table of Contents: Special Considerations Cardiovascular Emergencies Pulmonary Emergencies Gastrointestinal Emergencies Genitourinary Emergencies Vascular Emergencies Neurologic Emergencies Trauma Metabolic and Endocrine Emergencies Dermatologic Emergencies Ophthalmologic Emergencies ENT Emergencies Hematologic and Oncologic Emergencies Musculoskeletal and Rheumatologic Emergencies Psychiatric Emergencies,2003,2,15,1,0,0,0,0,0,0,0,0,0,0
a7da090fdb85b79cd8a52aadc9ff4715814656e5,"One of the problems which has plagued thouse attempting to predict the behavior of capital marcets is the absence of a body of positive of microeconomic theory dealing with conditions of risk/ Althuogh many usefull insights can be obtaine from the traditional model of investment under conditions of certainty, the pervasive influense of risk in finansial transactions has forced those working in this area to adobt models of price behavior which are little more than assertions. A typical classroom explanation of the determinationof capital asset prices, for example, usually begins with a carefull and relatively rigorous description of the process through which individuals preferences and phisical relationship to determine an equilibrium pure interest rate. This is generally followed by the assertion that somehow a market risk-premium is also determined, with the prices of asset adjusting accordingly to account for differences of their risk.",1964,2,17474,1038,0,0,0,0,0,0,0,0,0,0
a95e6d3280e2db04563556fa576fa55465b7a317,"This paper tests the relationship between average return and risk for New York Stock Exchange common stocks. The theoretical basis of the tests is the ""two-parameter"" portfolio model and models of market equilibrium derived from the two-parameter portfolio model. We cannot reject the hypothesis of these models that the pricing of common stocks reflects the attempts of risk-averse investors to hold portfolios that are ""efficient"" in terms of expected value and dispersion of return. Moreover, the observed ""fair game"" properties of the coefficients and residuals of the risk-return regressions are consistent with an ""efficient capital market""--that is, a market where prices of securities",1973,38,13626,1396,0,0,0,0,0,0,0,0,0,0
b470ef87fcf834e0e90bf65b497732cef2376063,"Nose has modified Newtonian dynamics so as to reproduce both the canonical and the isothermal-isobaric probability densities in the phase space of an N-body system. He did this by scaling time (with s) and distance (with V/sup 1/D/ in D dimensions) through Lagrangian equations of motion. The dynamical equations describe the evolution of these two scaling variables and their two conjugate momenta p/sub s/ and p/sub v/. Here we develop a slightly different set of equations, free of time scaling. We find the dynamical steady-state probability density in an extended phase space with variables x, p/sub x/, V, epsilon-dot, and zeta, where the x are reduced distances and the two variables epsilon-dot and zeta act as thermodynamic friction coefficients. We find that these friction coefficients have Gaussian distributions. From the distributions the extent of small-system non-Newtonian behavior can be estimated. We illustrate the dynamical equations by considering their application to the simplest possible case, a one-dimensional classical harmonic oscillator.",1985,0,13404,297,0,0,0,0,0,0,0,0,0,0
cd7fb9e476e7002d5649309661a06c8688058f49,"This paper develops techniques for empirically analyzing demand and supply in differentiated product markets and then applies these techniques to the U.S. automobile industry. The authors' framework enables one to obtain estimates of demand and cost parameters for a class of oligopolistic differentiated products markets. These estimates can be obtained using only widely available product-level and aggregate consumer-level data, and they are consistent with a structural model of equilibrium in an oligopolistic industry. Applying these techniques, the authors obtain parameters for essentially all autos sold over a twenty-year period. Copyright 1995 by The Econometric Society.",1995,53,4984,681,5,9,23,23,26,49,62,83,102,116
0a577ebd728080640ba1f6da20e99cf6e9526c8e,Focuses on a study which examined perfect equilibrium in a bargaining model. Overview of the strategic approach adopted for the study; Details of the bargaining situation used; Discussion on perfect equilibrium. (From Ebsco),1982,22,5274,344,1,7,6,17,20,35,43,52,51,52
f2b954dab9db5e74d7000b6610378c2523119290,"If looking for a ebook by S. R. de Groot and P. Mazur Non-Equilibrium Thermodynamics in pdf form, in that case you come on to the correct site. We furnish full release of this book in ePub, DjVu, txt, doc, PDF formats. You may read by S. R. de Groot and P. Mazur online Non-Equilibrium Thermodynamics or download. In addition, on our website you may reading instructions and diverse artistic eBooks online, either load theirs. We like attract consideration what our site does not store the eBook itself, but we grant ref to website whereat you may load either reading online. So if have necessity to download pdf Non-Equilibrium Thermodynamics by S. R. de Groot and P. Mazur, then you have come on to faithful website. We have Non-Equilibrium Thermodynamics ePub, txt, PDF, DjVu, doc formats. We will be glad if you will be back to us anew.",1963,0,5809,332,6,8,24,31,33,40,46,45,42,50
f9a8f37fa587f4a50dceb4d6a4176ae424e8099f,"Abstract The paper derives a general form of the term structure of interest rates. The following assumptions are made: (A.1) The instantaneous (spot) interest rate follows a diffusion process; (A.2) the price of a discount bond depends only on the spot rate over its term; and (A.3) the market is efficient. Under these assumptions, it is shown by means of an arbitrage argument that the expected rate of return on any bond in excess of the spot rate is proportional to its standard deviation. This property is then used to derive a partial differential equation for bond prices. The solution to that equation is given in the form of a stochastic integral representation. An interpretation of the bond pricing formula is provided. The model is illustrated on a specific case.",1977,14,6023,619,1,4,3,9,2,3,5,10,7,9
f0d65b8633f39cbc40cd482c1a394726c8763fb3,"THE SPHERE of model financial economics encompasses finance, micro investment theory and much of the economics of uncertainty. As is evident from its influence on other branches of economics including public finance, industrial organization and monetary theory, the boundaries of this sphere are both permeable and flexible. The complex interactions of time and uncertainty guarantee intellectual challenge and intrinsic excitement to the study of financial economics. Indeed, the mathematics of the subject contain some of the most interesting applications of probability and optimization theory. But for all its mathematical refinement, the research has nevertheless had a direct and significant influence on practice. ’ It was not always thus. Thirty years ago, finance theory was little more than a collection of anecdotes, rules of thumb, and manipulations of accounting data with an almost exclusive focus on corporate financial management. There is no need in this meeting of the guild to recount the subsequent evolution from this conceptual potpourri to a rigorous economic theory subjected to systematic empirical examination? Nor is there a need on this occasion to document the wide-ranging impact of the research on finance practice.2 I simply note that the conjoining of intrinsic intellectual interest with extrinsic application is a prevailing theme of research in financial economics. The later stages of this successful evolution have however been marked by a substantial accumulation of empirical anomalies; discoveries of theoretical inconsistencies; and a well-founded concern about the statistical power of many of the test methodologies.3 Finance thus finds itself today in the seemingly-paradoxical position of having more questions and empirical puzzles than at the start of its",1987,46,5403,473,1,4,7,11,8,9,7,12,13,26
e3317b468c1b9a0d5d801ad11f12d3bffa6364af,Part 1 Unemployment in the model of balanced growth: the labour market long-run equilibrium and balanced growth adjustment dynamics. Part 2 further ananlysis of the labour market: search intensity and job advertising.,1990,145,3626,342,0,2,7,14,19,20,25,39,50,71
87e8816e5520a37849e368a0770219452715da18,"One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …",1950,3,6513,390,0,1,2,1,1,1,1,1,1,2
bb484ca72e7e04f643014d0f1be8070af2e0f031,"Involuntary unemployment appears to be a persistent feature of many modern labor markets. The presence of such unemployment raises the question of why wages do not fall to clear labor markets. In this paper we show how the information structure of employer-employee relationships, in particular the inability of employers to costlessly observe workers' on-the-job effort, can explain involuntary unemployment as an equilibrium phenomenon. Indeed, we show that imperfect monitoring necessitates unemployment in equilibrium. The intuition behind our result is simple. Under the conventional competitive paradigm, in which all workers receive the market wage and there is no unemployment, the worst that can happen to a worker who shirks on the job is that he is fired. Since he can immediately be rehired, however, he pays no penalty for his misdemeanor. With imperfect monitoring and full employment, therefore, workers will choose to shirk. To induce its workers not to shirk, the firm attempts to pay more than the “going wage”; then, if a worker is caught shirking and is fired, he will pay a penalty. If it pays one firm to raise its wage, however, it will pay all firms to raise their wages. When they all raise their wages, the incentive not to shirk again disappears. But as all firms raise their wages, their demand for labor decreases, and unemployment results. With unemployment, even if all firms pay the same wages, a worker has an incentive not to shirk.",1984,11,4898,215,3,17,23,33,40,45,54,66,51,56
e937fc6b51ab16bfdb3d7cde90a13c7e12e2c641,"A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game. Introduction L. Walras [ 24 ] first formulated the state of the economic system at any point of time as the solution of a system of simultaneous equations representing the demand for goods by consumers, the supply of goods by producers, and the equilibrium condition that supply equal demand on every market. It was assumed that each consumer acts so as to maximize his utility, each producer acts so as to maximize his profit, and perfect competition prevails, in the sense that each producer and consumer regards the prices paid and received as independent of his own choices. Walras did not, however, give any conclusive arguments to show that the equations, as given, have a solution.",1954,29,4256,282,0,0,2,0,3,5,2,2,8,0
5fbba8fcf94492a5902bad624b4f708e50e9c2f1,"A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q0 and frequency ω0 of the instability. Type Is systems (ω0=0, q0≠0) are stationary in time and periodic in space; type IIIo systems (ω0≠0, q0=0) are periodic in time and uniform in space; and type Io systems (ω0≠0, q0≠0) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via ""amplitude equations,"" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the ""phase equation"" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing ""real pattern effects"" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying ""temporal chaos,"" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Benard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future.",1993,918,5264,176,11,52,108,123,127,126,126,138,147,142
921471828f08f6bb7bcef8d2685811af0fb0dac7,"A modified Redlich-Kwong equation of state is proposed. Vapor pressures of pure com- pounds can be closely reproduced by assuming the parameter a in the original equation to be tempera- ture-dependent. With the introduction of the acentric factor as a third parameter, a generalized correla- tion for the modified parameter can be derived. It applies to all nonpolar compounds. With the application of the original generalized mixing rules, the proposed equation can be extended successfully to multicomponent-VLE calculations, for mixtures of nonpolar substances, with the exclusion of carbon dioxide. Less accurate results are obtained for hydrogen-containing mixtures.",1972,6,4598,258,0,1,1,1,7,9,14,8,13,11
12982242bcceff2de817f51fe1ccbcc0cbb926f8,,1969,0,4821,213,0,1,0,6,11,7,8,18,17,16
8692d38cdef40d3ec18e9e8a9b18743b768d039d,"This paper argues that the textbook search and matching model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies in response to shocks of a plausible magnitude. In the United States, the standard deviation of the vacancy-unemployment ratio is almost 20 times as large as the standard deviation of average labor productivity, while the search model predicts that the two variables should have nearly the same volatility. A shock that changes average labor productivity primarily alters the present value of wages, generating only a small movement along a downward-sloping Beveridge curve (unemployment-vacancy locus). A shock to the separation rate generates a counterfactually positive correlation between unemployment and vacancies. In both cases, the model exhibits virtually no propagation.",2005,85,2653,700,47,98,119,107,150,165,207,185,187,209
eda646f4a2d46adf404790e94276ca254285ad01,"This paper investigates the properties of a market for risky assets on the basis of a simple model of general equilibrium of exchange, where individual investors seek to maximize preference functions over expected yield and variance of yield on their port- folios. A theory of market risk premiums is outlined, and it is shown that general equilibrium implies the existence of a so-called ""market line,"" relating per dollar expected yield and standard deviation of yield. The concept of price of risk is discussed in terms of the slope of this line.",1966,9,4373,127,0,1,3,6,10,9,17,16,20,21
e9f31a23580f9b88ba34ca671c438839a068de7a,"A dynamic stochastic model for a competitive industry is developed in which entry, exit, and the growth of firms' output and employment is determined. The paper extends long-run industry equilibrium theory to account for entry, exit, and heterogeneity in the size and growth rate of firms. Conditions under which there is entry and exit in the long run are developed. Cross sectional implications and distributions of profits and value of firms are derived. Comparative statics on the equilibrium size distribution and turnover rates are analyzed. Copyright 1992 by The Econometric Society.",1992,1,2914,265,2,2,1,14,9,12,14,29,40,37
faa25a6c7c7b3ecd221e4fc7b7d6dee131e04e04,"Publisher Summary In recent years, the interest in the problem of brittle fracture and, in particular, in the theory of cracks has grown appreciably in connection with various technical applications. Numerous investigations have been carried out, enlarging in essential points the classical concepts of cracks and methods of analysis. The qualitative features of the problems of cracks, associated with their peculiar nonlinearity as revealed in these investigations, makes the theory of cracks stand out distinctly from the whole range of problems in terms of the theory of elasticity. The chapter presents a unified view of the way basic problems in the theory of equilibrium cracks are formulated and discusses the results obtained thereby. The object of the theory of equilibrium cracks is the study of the equilibrium of solids in the presence of cracks. However, there exists a fundamental distinction between these two problems, The form of a cavity undergoes only slight changes even under a considerable variation in the load acting on a body, while the cracks whose surface also constitutes a part of the body boundary can expand even with small increase of the load to which the body is subjected.",1962,80,4296,170,0,1,0,4,7,12,8,10,10,8
3f94a0fe29c97858ae8143e31ec621911865afda,,1972,3,3022,380,4,7,15,8,11,34,30,35,20,28
8979bdafefff9da97342fbc5cf3f511000306f60,Kinetics of Unireactant Enzymes. Simple Inhibition Systems. Rapid Equilibrium Partial and Mixed--Type Inhibition. Enzyme Activation. Rapid Equilibrium Bireactant and Terreactant Systems. Multisite and Allosteric Enzymes. Multiple Inhibition Analysis. Steady--State Kinetics of Multireactant Enzymes. Isotope Exchange. Effects of pH and Temperature. Appendix. Index.,1975,0,3084,215,0,2,10,10,12,13,13,17,20,19
4ef2492426f6eb90191d9c411b94f5d686f2e618,"Parts I and II deal with the theory of crystal growth, parts III and IV with the form (on the atomic scale) of a crystal surface in equilibrium with the vapour. In part I we calculate the rate of advance of monomolecular steps (i.e. the edges of incomplete monomolecular layers of the crystal) as a function of supersaturation in the vapour and the mean concentration of kinks in the steps. We show that in most cases of growth from the vapour the rate of advance of monomolecular steps will be independent of their crystallographic orientation, so that a growing closed step will be circular. We also find the rate of advance for parallel sequences of steps. In part II we find the resulting rate of growth and the steepness of the growth cones or growth pyramids when the persistence of steps is due to the presence of dislocations. The cases in which several or many dislocations are involved are analysed in some detail; it is shown that they will commonly differ little from the case of a single dislocation. The rate of growth of a surface containing dislocations is shown to be proportional to the square of the supersaturation for low values and to the first power for high values of the latter. Volmer & Schultze’s (1931) observations on the rate of growth of iodine crystals from the vapour can be explained in this way. The application of the same ideas to growth of crystals from solution is briefly discussed. Part III deals with the equilibrium structure of steps, especially the statistics of kinks in steps, as dependent on temperature, binding energy parameters, and crystallographic orientation. The shape and size of a two-dimensional nucleus (i.e. an ‘island* of new monolayer of crystal on a completed layer) in unstable equilibrium with a given supersaturation at a given temperature is obtained, whence a corrected activation energy for two-dimensional nucleation is evaluated. At moderately low supersaturations this is so large that a crystal would have no observable growth rate. For a crystal face containing two screw dislocations of opposite sense, joined by a step, the activation energy is still very large when their distance apart is less than the diameter of the corresponding critical nucleus; but for any greater separation it is zero. Part IV treats as a ‘co-operative phenomenon’ the temperature dependence of the structure of the surface of a perfect crystal, free from steps at absolute zero. It is shown that such a surface remains practically flat (save for single adsorbed molecules and vacant surface sites) until a transition temperature is reached, at which the roughness of the surface increases very rapidly (‘surface melting’). Assuming that the molecules in the surface are all in one or other of two levels, the results of Onsager (1944) for two-dimensional ferromagnets can be applied with little change. The transition temperature is of the order of, or higher than, the melting-point for crystal faces with nearest neighbour interactions in both directions (e.g. (100) faces of simple cubic or (111) or (100) faces of face-centred cubic crystals). When the interactions are of second nearest neighbour type in one direction (e.g. (110) faces of s.c. or f.c.c. crystals), the transition temperature is lower and corresponds to a surface melting of second nearest neighbour bonds. The error introduced by the assumed restriction to two available levels is investigated by a generalization of Bethe’s method (1935) to larger numbers of levels. This method gives an anomalous result for the two-level problem. The calculated transition temperature decreases substantially on going from two to three levels, but remains practically the same for larger numbers.",1951,0,4106,92,1,7,8,6,3,8,10,12,6,7
b56d311ae4f97b1248344ad8a891b8b59af08273,"We explore the determinants of liquidation values of assets, particularly focusing on the potential buyers of assets. When a firm in financial distress needs to sell assets, its industry peers are likely to be experiencing problems themselves, leading to asset sales at prices below value in best use. Such illiquidity makes assets cheap in bad times, and so ex ante is a significant private cost of leverage. We use this focus on asset buyers to explain variation in debt capacity across industries and over the business cycle, as well as the rise in U.S. corporate leverage in the 1980s.",1992,38,2859,173,1,4,19,19,16,22,30,33,49,50
9ead28b73dc3c2329ee2db668f288636b2870196,"It is shown that for allele frequency data a useful measure of the extent of gene flow between a pair of populations is M∘=(1/FST‐1)/4 , which is the estimated level of gene flow in an island model at equilibrium. For DNA sequence data, the same formula can be used if FST is replaced by NST. In a population with restricted dispersal, analytic theory shows that there is a simple relationship between M̂ and geographic distance in both equilibrium and non‐equilibrium populations and that this relationship is approximately independent of mutation rate when the mutation rate is small. Simulation results show that with reasonable sample sizes, isolation by distance can indeed be detected and that, at least in some cases, non‐equilibrium patterns can be distinguished. This approach to analyzing isolation by distance is used for two allozyme data sets, one from gulls and one from pocket gophers.",1993,31,2425,279,0,13,24,51,52,66,64,66,88,84
46fe07c5695229769631ca693658a82da6e9395d,"The concept of a perfect equilibrium point has been introduced in order to exclude the possibility that disequilibrium behavior is prescribed on unreached subgames. (Selten 1965 and 1973). Unfortunately this definition of perfectness does not remove all difficulties which may arise with respect to unreached parts of the game. It is necessary to reexamine the problem of defining a satisfactory non-cooperative equilibrium concept for games in extensive form. Therefore a new concept of a perfect equilibrium point will be introduced in this paper. In retrospect the earlier use of the word ""perfect"" was premature. Therefore a perfect equilibrium point in the old Sense will be called ""subgame perfect"". The new definition of perfectness has the property that a perfect equilibrium point is always subgame perfect but a subgame perfect equilibrium point may not be perfect. It will be shown that every finite extensive game with perfect recall has at least one perfect equilibrium point. Since subgame perfectness cannot be detected in the normal form, it is clear that for the purpose of the investigation of the problem of perfectness, the normal form is an inadequate representation of the extensive form. It will be convenient to introduce an ""agent normal form"" as a more adequate representation of games with perfect recall.",1975,5,3173,158,0,3,4,5,5,13,16,25,20,33
85d493e7b86d5f39e47926b8d52d895bd67a8ff1,"The different roles the attractive and repulsive forces play in forming the equilibrium structure of a Lennard‐Jones liquid are discussed. It is found that the effects of these forces are most easily separated by considering the structure factor (or equivalently, the Fourier transform of the pair‐correlation function) rather than the pair‐correlation function itself. At intermediate and large wave vectors, the repulsive forces dominate the quantitative behavior of the liquid structure factor. The attractions are manifested primarily in the small wave vector part of the structure factor; but this effect decreases as the density increases and is almost negligible at reduced densities higher than 0.65. These conclusions are established by considering the structure factor of a hypothetical reference system in which the intermolecular forces are entirely repulsive and identical to the repulsive forces in a Lennard‐Jones fluid. This reference system structure factor is calculated with the aid of a simple but accurate approximation described herein. The conclusions lead to a very simple prescription for calculating the radial distribution function of dense liquids which is more accurate than that obtained by any previously reported theory. The thermodynamic ramifications of the conclusions are presented in the form of calculations of the free energy, the internal energy (from the energy equation), and the pressure (from the virial equation). The implications of our conclusions to perturbation theories for liquids and to the interpretation of x‐ray scattering experiments are discussed.",1971,18,3574,40,4,11,15,16,20,24,39,17,18,25
f66608476a3cf3c6147823f55fc0ba235234175f,Computer program is described for numerical solution of chemical equilibria in complex systems by using nonlinear algebraic equations. Free-energy minimization technique is used.,1972,37,2939,194,4,5,8,6,10,10,12,19,23,26
879e8d7778c0ab1479339fe29d3cc4ded78fe4e5,"The probability of a configuration is given in classical theory by the Boltzmann formula exp [— V/hT] where V is the potential energy of this configuration. For high temperatures this of course also holds in quantum theory. For lower temperatures, however, a correction term has to be introduced, which can be developed into a power series of h. The formula is developed for this correction by means of a probability function and the result discussed.",1932,0,5250,45,0,0,0,0,1,0,1,1,0,0
9ba558e613372642d3c24357e21f76835859e709,,2007,0,2011,175,101,114,99,112,128,147,143,155,133,157
dca0630c63a5403a8383b4a2abe7f94a28a23eb7,Gibbs Measures.- General Thermodynamic Formalism.- Axiom a Diffeomorphisms.- Ergodic Theory of Axiom a Diffeomorphisms.,1975,43,2432,224,0,1,7,7,6,7,6,18,15,13
87a2272cc9a9abe4556509600073032fa6f01c33,The published experimental data of Hansson and of Mehrbach et al. have been critically compared after adjustment to a common pH scale based upon total hydrogen ion concentration. No significant systematic differences are found within the overall experimental error of the data. The results have been pooled to yield reliable equations that can be used to estimate pK1∗and pK2∗ for seawater media a salinities from 0 to 40 and at temperatures from 2 to 35°C.,1987,16,2628,123,0,1,1,4,4,4,17,5,12,10
2943f194efb591a1231ecea27e1af8a4b4c50827,,1994,0,2145,188,1,4,4,6,10,6,15,13,23,26
7bc19372cdca52dc43c2e8d5a2111161804ef865,"AbstractA number of experiments have been conducted in order to study the equilibria between olivine and basaltic liquids and to try and understand the conditions under which olivine will crystallize. These experiments were conducted with several basaltic compositions over a range of temperature (1150–1300° C) and oxygen fugacity (10−0.68–10−12 atm.) at one atmosphere total pressure. The phases in these experimental runs were analyzed with the electron microprobe and a number of empirical equations relating the composition of olivine and liquid were determined. The distribution coefficient 1
$$K_D = \frac{{(X_{{\text{FeO}}}^{{\text{Ol}}} )}}{{(X_{{\text{FeO}}}^{{\text{Liq}}} )}}\frac{{(X_{{\text{MgO}}}^{{\text{Liq}}} )}}{{(X_{{\text{MgO}}}^{{\text{Ol}}} )}}$$
 relating the partioning of iron and magnesium between olivine and liquid is equal to 0.30 and is independent of temperature. This means that the composition of olivine can be used to determine the magnesium to ferrous iron ratio of the liquid from which it crystallized and conversely to predict the olivine composition which would crystallize from a liquid having a particular magnesium to ferrous iron ratio.A model (saturation surface) is presented which can be used to estimate the effective solubility of olivine in basaltic melts as a function of temperature. This model is useful in predicting the temperature at which olivine and a liquid of a particular composition can coexist at equilibrium.",1970,30,2408,303,0,0,7,19,10,18,19,16,24,26
edf2bdb75e01b30d1f9d165738303081b6f630c3,"Following a recession, the aggregate labor market is slack-employment remains below normal and recruiting efforts of employers, as measured by help-wanted advertising and vacancies, are low. A model of matching friction explains the qualitative responses of the labor market to adverse shocks, but requires implausibly large shocks to account for the magnitude of observed fluctuations. The incorporation of wage stickiness vastly increases the sensitivity of the model to driving forces. I develop a new model of the way that wage stickiness affects unemployment. The stickiness arises in an economic equilibrium and satisfies the condition that no worker-employer pair has an unexploited opportunity for mutual improvement. Sticky wages neither interfere with the efficient formation of employment matches nor cause inefficient job loss. Thus the model provides an answer to the fundamental criticism previously directed at sticky-wage models of fluctuations.",2005,49,1454,273,36,56,88,85,104,102,121,103,92,90
310beac23e27b4d0689bd11338e459d03a1dc3c0,"Recently, a number of authors have argued that the standard search model cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies, given shocks of a plausible magnitude. We use data on the cost of vacancy creation and cyclicality of wages to identify the two key parameters of the model - the value of non-market activity and the bargaining weights. Our calibration implies that the model is, in fact, consistent with the data.",2008,169,1197,287,94,61,85,115,85,81,78,91,82,69
f0f7e780651f1d6189b6c772e5ca0e2355e182d8,"A modified conjugate gradient algorithm for geometry optimization is outlined for use with ab initio MO methods. Since the computation time for analytical energy gradients is approximately the same as for the energy, the optimization algorithm evaluates and utilizes the gradients each time the energy is computed. The second derivative matrix, rather than its inverse, is updated employing the gradients. At each step, a one‐dimensional minimization using a quartic polynomial is carried out, followed by an n‐dimensional search using the second derivative matrix. By suitably controlling the number of negative eigenvalues of the second derivative matrix, the algorithm can also be used to locate transition structures. Representative timing data for optimizations of equilibrium geometries and transition structures are reported for ab initio SCF–MO calculations.",1982,92,2748,12,3,5,22,19,26,23,37,33,19,31
ac81ede712252992b364566fc2f44637e3b0eab3,An improved metal stamped and formed screw is disclosed. The subject screw is stamped and formed from continuous web of metal stock to form a plurality of screws joined by a carrier strip. The thus formed strip of screws can be machine applied to prebored holes and manually withdrawn therefrom and reapplied by conventional means.,1981,21,2815,35,0,2,4,3,2,5,5,4,10,13
98fe960b3f5354a987e33e62ab1de060a42ec9ec,"Economic theorists traditionally banish discussions of information to footnotes. Serious consideration of costs of communication, imperfect knowledge, and the like would, it is believed, complicate without informing. This paper, which analyzes competitive markets in which the characteristics of the commodities exchanged are not fully known to at least one of the parties to the transaction, suggests that this comforting myth is false. Some of the most important conclusions of economic theory are not robust to considerations of imperfect information.",1976,21,2530,91,1,7,8,11,7,15,23,24,26,22
c4db548b1437a0a564468889c4aaf5b0ae53aeb3,"A new suite of 10 programs concerned with equilibrium constants and solution equilibria is described. The suite includes data preparation programs, pretreatment programs, equilibrium constant refinement and post-run analysis. Data preparation is facilitated by a customized data editor. The pretreatment programs include manual trial and error data fitting, speciation diagrams, end-point determination, absorbance error determination, spectral baseline corrections, factor analysis and determination of molar absorbance spectra. Equilibrium constants can be determined from potentiometric data and/or spectrophotometric data. A new data structure is also described in which information on the model and on experimental measurements are kept in separate files.",1996,59,2321,32,0,2,15,23,18,30,28,41,55,54
52524271c3298b5541c4346b18f5e07ac6c4590e,"Research on how organizational systems develop and change is shaped, at every level of analysis, by traditional assumptions about how change works. New theories in several fields are challenging some of the most pervasive of these assumptions, by conceptualizing change as a punctuated equilibrium: an alternation between long periods when stable infrastructures permit only incremental adaptations, and brief periods of revolutionary upheaval. This article compares models from six domains—adult, group, and organizational development, history of science, biological evolution, and physical science—to explicate the punctuated equilibrium paradigm and show its broad applicability for organizational studies. Models are juxtaposed to generate new research questions about revolutionary change in organizational settings: how it is triggered, how systems function during such periods, and how it concludes. The article closes with implications for research and theory.",1991,59,2030,152,3,11,16,26,26,28,26,33,43,56
04228cd8a236c84355c75eeee0fc50203d15776b,"Prior to elections, governments (at all levels) frequently undertake a consumption binge. Taxes are cut, transfers are raised, and government spending is distorted towards highly visible items. The ""political business cycle"" (better be thought of as ""the political budget cycle"") has been intensively examined, at least for the case of national elections. A number of proposals have been advanced for mitigating electoral cycles in fiscal policy. The present paper is the first effort to provide a fully-specified equilibrium framework for analyzing such proposals. A political budget cycle arises here via a multidimensional signaling process, in which incumbent leaders try to convince voters that they have recently been doing an excellent job in administering the government. Efforts to mitigate the cycle can easily prove counterproductive, either by impeding the transmission of information or by inducing politicians to select more costly ways of signaling. The model also indicates new directions for empirical research.",1987,34,1976,128,1,2,6,6,8,13,13,9,17,18
7a58a1995f7fcd05d8820dabf25fd42c53cb422e,"Abstract A suite of divalent metal (Ca, Cd, Ba) carbonates was synthesized over the temperature range 10–40°C by the classical method of slowly bubbling N 2 through a bicarbonate solution. It was discovered that carbonates could be precipitated reproducibly in or out of isotopic equilibrium with the environmental solution by varying the concentrations of bicarbonate and cation. Precipitation rate had little or no influence on the isotopic composition of the product. Relatively high initial concentrations of up to 25 mM in both bicarbonate and cation were prepared by adding solid metal chlorides to solutions of NaHC0 3 . On the basis of results of equilibrium experiments and a new determination of the acid fractionation factor, a new expression is proposed for the oxygen isotope fractionation between calcite and water at low temperatures: 10001nα(Calcite-H 2 O) = 18.03(10 3 T −1 ) − 32.42 where α is the fractionation factor, and T is in kelvins. Combining new data for low-temperature precipitations and the high-temperature equilibrium fractionations published by O'Neil et al. (1969) results in a revised expression for the oxygen isotope fractionation between octavite (CdCO 3 ) and water from 0° to 500°C: 10001nα(CdC0P 3 H 2 O) = 2.7 6 (10 6 T −2 ) − 3.96 The ability to produce nonequilibrium carbonates allowed assessment to be made, for the first time, of the temperature dependence of nonequilibrium stable isotope fractionations in mineral systems. The temperature coefficients of a(carbonate-water) for nonequilibrium divalent metal carbonates are greater than those for equilibrium carbonates, a finding that may bear on the interpretation of analyses of biogenic carbonates forming out of isotopic equilibrium in nature. New determinations of acid fractionation factors (10001nα) at 25°C for calcite (10.44 − 0.10), aragonite (11.01 ± 0.01), and witherite (10.57 − 0.16) are mildly to strongly different from those published by Sharma and Clayton (1965) and point to a control on this fractionation by some physical property of the mineral. Reproducible values for octavite (CdC0 3 ) varied from 11.18 to 13.60 depending on the conditions of preparation of the carbonate. These new values need to be considered in determinations of absolute 18 80 16 60 ratios of international reference standards and in relating analyses of carbonates to those of waters, silicates, and oxides.",1997,25,1954,149,0,5,15,12,20,32,29,39,52,49
8a83305f83a86f7f1ca9fa5e15b22f42dade1a0a,"The authors study a rich class of noncooperative games that includes models of oligopoly competition, macroeconomic coordination failures, arms races, bank runs, technology adoption and diffusion, R&D competition, pretrial bargaining, coordination in teams, and many others. For all these games, the sets of pure strategy Nash equilibria, correlated equilibria, and rationalizable strategies have identical bounds. Also, for a class of models of dynamic adaptive choice behavior that encompasses both best-response dynamics and Bayesian learning, the players' choices lie eventually within the same bounds. These bounds are shown to vary monotonically with certain exogenous parameters. Copyright 1990 by The Econometric Society.",1990,55,1820,203,2,7,10,8,14,17,21,25,26,32
f3ca3e173ea3cfa0f8a74e4c68970e8a281d95eb,,1985,97,1930,151,0,3,7,4,7,5,9,7,12,13
3e3dcdc85b74c1b8c494292cef9eb9f893785718,"This paper develops a continuous time general equilibrium model of a simple but complete economy and uses it to examine the behavior of asset prices. In this model, asset prices and their stochastic properties are determined endogenously. One principal result is a partial differential equation which asset prices must satisfy. The solution of this equation gives the equilibrium price of any asset in terms of the underlying real variables in the economy. IN THIS PAPER, we develop a general equilibrium asset pricing model for use in applied research. An important feature of the model is its integration of real and financial markets. Among other things, the model endogenously determines the stochastic process followed by the equilibrium price of any financial asset and shows how this process depends on the underlying real variables. The model is fully consistent with rational expectations and maximizing behavior on the part of all agents. Our framework is general enough to include many of the fundamental forces affecting asset markets, yet it is tractable enough to be specialized easily to produce specific testable results. Furthermore, the model can be extended in a number of straightforward ways. Consequently, it is well suited to a wide variety of applications. For example, in a companion paper, Cox, Ingersoll, and Ross [7], we use the model to develop a theory of the term structure of interest rates. Many studies have been concerned with various aspects of asset pricing under uncertainty. The most relevant to our work are the important papers on intertemporal asset pricing by Merton [19] and Lucas [16]. Working in a continuous time framework, Merton derives a relationship among the equilibrium expected rates of return on assets. He shows that when investment opportunities are changing randomly over time this relationship will include effects which have no analogue in a static one period model. Lucas considers an economy with homogeneous individuals and a single consumption good which is produced by a number of processes. The random output of these processes is exogenously determined and perishable. Assets are defined as claims to all or a part of the output of a process, and the equilibrium determines the asset prices. Our theory draws on some elements of both of these papers. Like Merton, we formulate our model in continuous time and make full use of the analytical tractability that this affords. The economic structure of our model is somewhat similar to that of Lucas. However, we include both endogenous production and",1985,27,1984,102,4,12,21,24,35,35,37,32,37,29
828be060c75cd9e01f5b2c6aa48508b06c856239,"This paper develops and estimates a dynamic stochastic general equilibrium (DSGE) model with sticky prices and wages for the euro area. The model incorporates various other features such as habit formation, costs of adjustment in capital accumulation and variable capacity utilisation. It is estimated with Bayesian techniques using seven key macro-economic variables: GDP, consumption, investment, prices, real wages, employment and the nominal interest rate. The introduction of ten orthogonal structural shocks (including productivity, labour supply, investment, preference, cost-push and monetary policy shocks) allows for an empirical investigation of the effects of such shocks and of their contribution to business cycle fluctuations in the euro area. Using the estimated model, the paper also analyses the output (real interest rate) gap, defined as the difference between the actual and model-based potential output (real interest rate).",2002,92,1329,239,3,10,37,44,50,72,85,98,118,105
7ceae624bd1d65ccf66e8448a8f902c2415ce62c,"An analysis of the quantitative effects of agency costs in a real business cycle model, showing that these costs can explain why output growth displays positive autocorrelation at short horizons.",1998,32,1487,219,6,9,13,21,31,39,33,34,50,52
a63da98b44996c5c484612bc754205f975c96467,"This book provides a solid foundation and an extensive study for an important class of constrained optimization problems known as Mathematical Programs with Equilibrium Constraints (MPEC), which are extensions of bilevel optimization problems. The book begins with the description of many source problems arising from engineering and economics that are amenable to treatment by the MPEC methodology. Error bounds and parametric analysis are the main tools to establish a theory of exact penalisation, a set of MPEC constraint qualifications and the first-order and second-order optimality conditions. The book also describes several iterative algorithms such as a penalty-based interior point algorithm, an implicit programming algorithm and a piecewise sequential quadratic programming algorithm for MPECs. Results in the book are expected to have significant impacts in such disciplines as engineering design, economics and game equilibria, and transportation planning, within all of which MPEC has a central role to play in the modelling of many practical problems.",1996,0,1780,140,5,13,24,26,22,35,38,45,76,86
3a8322cc9ce071f1fce97d10687d8a64265d29e8,"The theory of inequality and intergenerational mobility presented in this essay assumes that each family maximizes a utility function spanning several generations. Utility depends on the consumption of parents and on the quantity and quality of their children. The income of children is raised when they receive more human and nonhuman capital from their parents. Their income is also raised by their ""endowment"" of genetically determined race, ability, and other characteristics, family reputation and ""connections,"" and knowledge, skills, and goals provided by their family environment. The fortunes of children are linked to their parents not only through investments but also through these endowments acquired from parents (and other family members). The equilibrium income of children is determined by their market and endowed luck, the own income and endowment of parents, and the two parameters, the degree of inheritability and the propensity to invest in children. If these parameters are both less than unity, the distribution of income between families approaches a stationary distribution. The stationary coefficient of variation is greater, the larger the degree of in-heritability and the smaller the propensity to invest in children. Intergenerational mobility measures the effect of a family on the well-being of its children. We show that the family is more important when the degree of inheritability and the propensity to invest are larger. If both these parameters are less than unity, an increase in family income in one generation has negligible effects on the incomes of much later descendants. However, the incomes of children, grandchildren, and other early descendants could significantly increase; indeed, if the sum of these parameters exceeds unity, the changes in income rise for several generations before falling, and the maximum increase in income could exceed the initial increase.",1979,37,1974,179,1,2,6,6,2,8,3,4,10,5
35cca93034be1f68c585d309bc0de963021b86ea,"Abstract Transport phenomena in spatially periodic systems far from thermal equilibrium are considered. The main emphasis is put on directed transport in so-called Brownian motors (ratchets), i.e. a dissipative dynamics in the presence of thermal noise and some prototypical perturbation that drives the system out of equilibrium without introducing a priori an obvious bias into one or the other direction of motion. Symmetry conditions for the appearance (or not) of directed current, its inversion upon variation of certain parameters, and quantitative theoretical predictions for specific models are reviewed as well as a wide variety of experimental realizations and biological applications, especially the modeling of molecular motors. Extensions include quantum mechanical and collective effects, Hamiltonian ratchets, the influence of spatial disorder, and diffusive transport.",2000,839,1751,51,0,7,31,74,82,122,110,111,98,112
0b787672d66ecd0ecd3c1ba4a0b7a740c842dfae,"ing and summarizing knowledge about range dynamics without distorting it. The amount of detail lost in a particular description would depend on how many states and transitions were recognized. We are proposing the state-and-transition formulation because it is a practicable way to organize information for management, not because it follows from theoretical models about dynamics. In consequence, we consider management rather than theoretical criteria should be used in deciding what states to recognize in a given situation. As a general rule, one would distinguish 2 states only if the difference between them represented an important change in the land from the point of view of management. For example, variation due to seasonal phenology of the plants would not normally be subdivided into states, while important changes in the underlying botanical composition would be recognized. It follows that a given rangeland could be described in terms of a greater or lesser number of states and transitions, depending on the nature and objectives of management and on the state of existing knowledge. There would not be a single correct description. Under the state-and-transition formulation, knowledge about a given rangeland should be organized and expressed in the follow-",1989,51,1874,101,0,6,15,19,21,32,39,40,36,46
0fc5d7098267bab662bdc7f455f6119699f24ee9,"A redundant internal coordinate system for optimizing molecular geometries is constructed from all bonds, all valence angles between bonded atoms, and all dihedral angles between bonded atoms. Redundancies are removed by using the generalized inverse of the G matrix; constraints can be added by using an appropriate projector. For minimizations, redundant internal coordinates provide substantial improvements in optimization efficiency over Cartesian and nonredundant internal coordinates, especially for flexible and polycyclic systems. Transition structure searches are also improved when redundant coordinates are used and when the initial steps are guided by the quadratic synchronous transit approach. © 1996 by John Wiley & Sons, Inc.",1996,24,1941,12,5,11,14,21,19,32,39,36,57,65
e8fcba63671d0c88bcc674b47d62dddea83e427b,This paper studies four classic fiscal-policy experiments within a quantitatively restricted neoclassical model. The authors' main findings are as follows: (1) permanent changes in government purchases can lead to short-run and long-run output multipliers that exceed one; (2) permanent changes in government purchases induce larger effects than temporary changes; (3) the financing decision is quantitatively more important than the resource cost of changes in government purchases; and (4) public investment has dramatic effects on private output and investment. These findings stem from important dynamic interactions of capital and labor absent in earlier equilibrium analyses of fiscal policy. Copyright 1993 by American Economic Association.,1990,0,1607,205,0,3,4,5,10,14,13,21,30,24
f7bb8b20c217c81cb446b0bd9c3f6b187de80b6c,"Deviations from Hardy-Weinberg equilibrium (HWE) can indicate inbreeding, population stratification, and even problems in genotyping. In samples of affected individuals, these deviations can also provide evidence for association. Tests of HWE are commonly performed using a simple chi2 goodness-of-fit test. We show that this chi2 test can have inflated type I error rates, even in relatively large samples (e.g., samples of 1,000 individuals that include approximately 100 copies of the minor allele). On the basis of previous work, we describe exact tests of HWE together with efficient computational methods for their implementation. Our methods adequately control type I error in large and small samples and are computationally efficient. They have been implemented in freely available code that will be useful for quality assessment of genotype data and for the detection of genetic association or population stratification in very large data sets.",2005,21,1322,121,7,22,62,82,113,115,104,105,111,91
06b2234a73c812eb01cc1fbc51b83de14107788d,,1964,0,2591,32,0,0,1,2,1,1,7,5,4,7
3e4e98e430a2ed0abd49f027d4d42bb4f422d299,"This paper considers the locational choice of firms in an upstream and a downstream industry. Both industries are imperfectly competitive, with firms subject to increasing returns. There are transport costs between the two locations. Depending on the level of these costs there may be a single equilibrium with production diversified between locations, or multiple equilibria, some of which involve agglomeration at a single location. Typically the forces for agglomeration are greatest at intermediate levels of transport costs. Reducing these costs from a high to an intermediate level will cause agglomeration and consequent divergence of economic structure and income levels; reducing them to a low level may cause the industries to operate in both locations, bringing convergence of structure and income.",1996,0,1476,113,9,13,37,32,40,48,49,72,95,78
577194f578a9abc404fe1d4e406714c2550d7c8e,"Bamboo, an abundant and inexpensive natural resource in Malaysia was used to prepare activated carbon by physiochemical activation with potassium hydroxide (KOH) and carbon dioxide (CO(2)) as the activating agents at 850 degrees C for 2h. The adsorption equilibrium and kinetics of methylene blue dye on such carbon were then examined at 30 degrees C. Adsorption isotherm of the methylene blue (MB) on the activated carbon was determined and correlated with common isotherm equations. The equilibrium data for methylene blue adsorption well fitted to the Langmuir equation, with maximum monolayer adsorption capacity of 454.2mg/g. Two simplified kinetic models including pseudo-first-order and pseudo-second-order equation were selected to follow the adsorption processes. The adsorption of methylene blue could be best described by the pseudo-second-order equation. The kinetic parameters of this best-fit model were calculated and discussed.",2007,24,1240,37,1,30,39,50,61,65,96,97,113,108
087d26e0fa16877c358764f714afac31de699397,"Equilibrium is analyzed for a simple barter model with identical risk-neutral agents where trade is coordinated by a stochastic matching process. It is shown that there are multiple steady-state rational expectations equilibria, with all non-corner solution equilibria inefficient. This implies that an economy with this type of trade friction does not have a unique natural rate of unemployment.",1982,7,1897,170,1,2,1,5,7,18,12,18,26,17
32916bb78af5e6f15f3a8b87a225c1ee59f7741b,"If it is common knowledge that the players in a game are Bayesian utility maximizers who treat uncertainty about other players' actions like any other uncertainty, then the outcome is necessarily a correlated equilibrium. Random strategies appear as an expression of each player's uncertainty about what the others will do, not as the result of willful randomization. Use is made of the common prior assumption, according to which differences in probability assessments by different individuals are due to the different information that they have (where ""information"" may be interpreted broadly, to include experience, upbringing, and genetic makeup). Copyright 1987 by The Econometric Society.",1987,23,1515,184,5,8,13,26,28,26,15,30,19,23
41590083788b7cd293953d930ebf4dff61772ba6,"The capacity of immunity to control and shape cancer, that is, cancer immunoediting, is the result of three processes that function either independently or in sequence: elimination (cancer immunosurveillance, in which immunity functions as an extrinsic tumour suppressor in naive hosts); equilibrium (expansion of transformed cells is held in check by immunity); and escape (tumour cell variants with dampened immunogenicity or the capacity to attenuate immune responses grow into clinically apparent cancers). Extensive experimental support now exists for the elimination and escape processes because immunodeficient mice develop more carcinogen-induced and spontaneous cancers than wild-type mice, and tumour cells from immunodeficient mice are more immunogenic than those from immunocompetent mice. In contrast, the equilibrium process was inferred largely from clinical observations, including reports of transplantation of undetected (occult) cancer from organ donor into immunosuppressed recipients. Herein we use a mouse model of primary chemical carcinogenesis and demonstrate that equilibrium occurs, is mechanistically distinguishable from elimination and escape, and that neoplastic cells in equilibrium are transformed but proliferate poorly in vivo. We also show that tumour cells in equilibrium are unedited but become edited when they spontaneously escape immune control and grow into clinically apparent tumours. These results reveal that, in addition to destroying tumour cells and sculpting tumour immunogenicity, the immune system of a naive mouse can also restrain cancer growth for extended time periods.",2007,35,1217,27,1,55,69,105,102,89,127,88,78,100
2b21e47e893a307d45f007c9f78ea7e2e6e322dd,"A global game is an incomplete information game where the actual payoff structure is determined by a random draw from a given class of games and where each player makes a noisy observation of the selected game. For 2 x 2 games, it is shown that, when the noise vanishes, iterated elimination of dominated strategies in the global game forces the players to conform to J. C. Harsanyi and R. Selten's risk dominance criterion. Copyright 1993 by The Econometric Society.",1993,8,1588,52,0,0,8,5,14,8,6,8,31,51
3bfc2313fa86a1649763898783937a21a3cf8a4c,"The first acidity scale to be established in a pure solvent other than water was the result of the pioneering work of Conant, Wheland, and McEwen in ether or ben~ene .~ During the past 20 years an ion-pair acidity scale covering an ""effective pKa rangefrom about 15 to 40 has been developed in cyclohexylamine (CHA),6 and similar studies in other low-dielectric-constant solvents including 1,2-dimethoxyethane (DME)7a and tetrahydrofuran (THF)7b*c have been carried out. A more limited ion-pair acidity scale has been developed in liquid NH,.7d Also, during this period, acidity scales have been established in the polar non-hydrogenbond-donor (NHBD) solvents dimethyl sulfoxide (Me$0)8 and N-methylpyrrolidin-2-one (NMP)? which have relatively high dielectric constants. The pK,'s measured in these solvents differ from ion-pair pK,'s in that they are absolute, in the sense that they are based on Me2S0 and NMP as the standard states, which allows direct comparisons to be made with H20 and gas-phase pK,'s. A truly absolute acidity scale has been established in the gas phase, which, for the first time, provides intrinsic measures of structural effects free of solvent effects.1° Our purpose in this Account is (a) to discuss briefly acidities in various solvent media, (b) to present a table of representative equilibrium acidity constants in M e 8 0 solution, and (c) to illustrate ways in which these pK, data can be used. In an accompanying Account we compare acidities in Me2S0 solution with intrinsic gas-phase acidities and discuss some of the insights into solvation effects provided thereby. Acidities in H 2 0 and Me2S0. It is important to recognize that pKa values are solvent dependent. The",1988,72,1780,4,0,5,12,13,15,16,24,23,27,22
187e2d614a0f342d7a6677528c7a4f3e3a0bb0ab,"In 1972 ~ and 19852 the Committee on Hearing and Equilibrium of the American Academy of Otolaryngolog3~-Head and Neck Surgery published recommended guidelines for reporting the results of treatment of Meniere's disease. These reports have proved very beneficial to efforts to understand this disorder and its treatment. With advancing knowledge it has become evident that the reporting guidelines could be refined further. In undertaking this review, the Committee established several guiding principles. We wished to establish a distinction between the recording of results and the analysis and interpretation of results. Insofar as is possible, we wanted to retain and integrate the methods recommended in the 1972 and 1985 reports. We wanted guidelines to be ""upwardly compatible"" in the sense of computer software, so that existing data could not only be conserved but analyzed in new ways. Reporting methods should be clearly stated, straightforward to apply, as simple as possible, and usable in a wide range of settings, from multicenter university studies to reviews of personal experience by individual private practitioners. Specialized test equipment should not be required. Methods should facilitate statistical evaluation and comparison of results among studies. The guidelines should encourage reports to reflect disease severity in a meaningful manner.",1995,9,1551,24,1,6,13,18,11,24,45,34,51,50
5a1e3136ac33b0cdcec827c245738f3e8ba0488c,"Because of its toxicity, arsenic is of considerable environmental concern. Its solubility in natural systems is strongly influenced by adsorption at iron oxide surfaces. The objective of this study was to compare the adsorption behavior of arsenite and arsenate on ferrihydrite, under carefully controlled conditions, with regard to adsorption kinetics, adsorption isotherms, and the influence of pH on adsorption. The adsorption reactions were relatively fast, with the reactions almost completed within the first few hours. At relatively high As concentrations, arsenite reacted faster than arsenate with the ferrihydrite, i.e., equilibrium was achieved sooner, but arsenate adsorption was faster at low As concentrations and low pH. Adsorp tion maxima of approximately 0.60 (0.58) and 0.25 (0.16) molAs molFe-1 were achieved for arsenite and arsenate, respectively, at pH 4.6 (pH 9.2 in parentheses). The high arsenite retention, which precludes its retention entirely as surface adsorbed species, indicates the likel...",1998,7,1320,90,1,6,22,25,31,40,33,52,54,54
0be2acd403746056d71f5a2c89c440100bd9127b,"Mendelian randomization (MR) permits causal inference between exposures and a disease. It can be compared with randomized controlled trials. Whereas in a randomized controlled trial the randomization occurs at entry into the trial, in MR the randomization occurs during gamete formation and conception. Several factors, including time since conception and sampling variation, are relevant to the interpretation of an MR test. Particularly important is consideration of the “missingness” of genotypes that can be originated by chance, genotyping errors, or clinical ascertainment. Testing for Hardy-Weinberg equilibrium (HWE) is a genetic approach that permits evaluation of missingness. In this paper, the authors demonstrate evidence of nonconformity with HWE in real data. They also perform simulations to characterize the sensitivity of HWE tests to missingness. Unresolved missingness could lead to a false rejection of causality in an MR investigation of trait-disease association. These results indicate that large-scale studies, very high quality genotyping data, and detailed knowledge of the life-course genetics of the alleles/genotypes studied will largely mitigate this risk. The authors also present a Web program (http://www.oege.org/software/hwe-mr-calc.shtml) for estimating possible missingness and an approach to evaluating missingness under different genetic models.",2009,19,925,27,6,23,43,70,86,112,104,104,87,80
bdd5f1ee849909127b9c91d299082c29071719f0,"Empirical research on cities starts with a spatial equilibrium condition: workers and firms are assumed to be indifferent across space. This condition implies that research on cities is different from research on countries, and that work on places within countries needs to consider population, income and housing prices simultaneously. Housing supply elasticity will determine whether urban success shows up in more people or higher incomes. Urban economists generally accept the existence of agglomeration economies, which exist when productivity rises with density, but estimating the magnitude of those economies is difficult. Some manufacturing firms cluster to reduce the costs of moving goods, but this force no longer appears to be important in driving urban success. Instead, modern cities are far more dependent on the role that density can play in speeding the flow of ideas. Finally, urban economics has some insights to offer related topics such as growth theory, national income accounts, public economics and housing prices.",2009,172,819,47,6,39,49,41,76,64,70,76,98,81
3273eccfed4825e1159a17fc4cb361ce670ad295,"An equilibrium theory of unemployment assumes that firms and workers maximize their payoffs under rational expectations and that wages are determined to exploit the private gains from trade. This book focuses on the modeling of the transitions in and out of unemployment, given the stochastic processes that break up jobs and lead to the formation of new jobs, and on the implications of this approach for macroeconomic equilibrium and for the efficiency of the labor market. This approach to labor market equilibrium and unemployment has been successful in explaining the determinants of the ""natural"" rate of unemployment and new data on job and worker flows, in modeling the labor market in equilibrium business cycle and growth models, and in analyzing welfare policy. The second edition contains two new chapters, one on endogenous job destruction and one on search on the job and job-to-job quitting. The rest of the book has been extensively rewritten and, in several cases, simplified.",2000,0,1167,159,6,16,32,36,44,53,52,76,64,73
4e8419da55942518cb52d59efb0af73cfe1a6e37,,1964,27,2342,13,7,28,43,61,87,104,137,159,146,135
bcb88e0d541ebde4a4d255a0bd67a3917d66b181,"As the area of sampling A increases in an ecologically uniform area, the number of plant and animal species s increases in an approximately logarithmic manner, or s = bAk, (1) where k < 1, as shown most recently in in the detailed analysis of Preston (1962). The same relationship holds for islands, where, as one of us has noted (Wilson, 1961), the parameters b and k vary among taxa. Thus, in the ponerine ants of Melanesia and the Moluccas, k (which might be called the faunal coefficient) is approximately 0.5 where area is measured in square miles; in the Carabidae and herpetofauna of the Greater Antilles and associated islands, 0.3; in the land and freshwater birds of Indonesia, 0.4; and in the islands of the Sahul Shelf (New Guinea and environs), 0.5.",1963,19,2079,82,0,1,6,7,7,6,13,2,3,4
82136b3b3be539cc11182ae80a4c9936a585137a,"This paper considers the prospects for constructing a neoclassical theory of growth and international trade that is consistent with some of the main features of economic development. Three models are considered and compared to evidence: a model emphasizing physical capital accumulation and technological change, a model emphasizing human capital accumulation through schooling, and a model emphasizing specialized human capital accumulation through learning-by-doing.",1988,19,20916,1155,0,0,0,0,0,0,1,0,0,0
dce8146987557735a19771aefa1f027211a2c275,"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. 
Traditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. 
The scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. 
The non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology.",2001,367,17527,825,0,0,1,0,0,2,1,0,11,7
7d35fdb4e676200a61d88fb06a9bbe0bf7cd7c28,Part 1 Newtonian mechanics: experimental facts investigation of the equations of motion. Part 2 Lagrangian mechanics: variational principles Lagrangian mechanics on manifolds oscillations rigid bodies. Part 3 Hamiltonian mechanics: differential forms symplectic manifolds canonical formalism introduction to pertubation theory.,1974,0,10775,661,0,0,0,0,0,0,0,0,0,0
08b67692bc037eada8d3d7ce76cc70994e7c8116,"Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of ""statistical complementarity"" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation.",1957,0,10679,615,0,0,0,0,0,0,0,0,0,0
2d4f6af07a0bb851ecd0f3e2243676518dc555b8,"Ludwig Krinner (Dated: November 5th 2012) Abstract This is a script made with the help of Landau Lifshitz, Book VI [1] on fluid mechanics, that gives a short introduction to basic fluid mechanics. This short script includes, various equations of continuity, Eulers equation for motion of nonviscous fluid, gravitational waves in nonviscous fluids, Navier-Stokes Equation for viscous fluids, viscous flow within a pipe, some turbulence and laminar wake (can be used to calculate the lift of a wing), all along with some basic thermodynamics and vector calculus. Plagiarism: For creation of this script we have closely oriented ourselves towards Laundau Lifschitz Course in theoretical physics Vol 6 Fluid Mechanics. This is no original work! Even though we have tried to write everyting in our own words (in order to have to understand everything), most of the formulas are similar or equal to the book, while some of the steps which seemed too fast are supplemented by own “fill in” calculations. I will not cite anything properly, since this is not an official paper or homework or term paper, and since I basically cite every line of calculation and from the content also every line of text.",1980,4,7139,953,6,4,4,11,6,8,7,6,4,11
e8989d460086966038008eb78ae32a923523ef3b,"Contact problems are central to Solid Mechanics, because contact is the principal method of applying loads to a deformable body and the resulting stress concentration is often the most critical point in the body. Contact is characterized by unilateral inequalities, describing the physical impossibility of tensile contact tractions (except under special circumstances) and of material interpenetration. Additional inequalities and/or non-linearities are introduced when friction laws are taken into account. These complex boundary conditions can lead to problems with existence and uniqueness of quasi-static solution and to lack of convergence of numerical algorithms. In frictional problems, there can also be lack of stability, leading to stick±slip motion and frictional vibrations. If the material is non-linear, the solution of contact problems is greatly complicated, but recent work has shown that indentation of a power-law material by a power law punch is self-similar, even in the presence of friction, so that the complete history of loading in such cases can be described by the (usually numerical) solution of a single problem. Real contacting surfaces are rough, leading to the concentration of contact in a cluster of microscopic actual contact areas. This aects the conduction of heat and electricity across the interface as well as the mechanical contact process. Adequate description of such systems requires a random process or statistical treatment and recent results suggest that surfaces possess fractal properties that can be used to obtain a more ecient mathematical characterization. Contact problems are very sensitive to minor pro®le changes in the contacting bodies and hence are also aected by thermoelastic distortion. Important applications include cases where non-uniform temperatures are associated with frictional heating or the conduction of heat across a non-uniform interface. The resulting coupled thermomechanical problem can be unstable, leading to a rich range of physical phenomena. Other recent developments are also brie ̄y surveyed, including examples of anisotropic materials, elastodynamic problems and fretting fatigue. # 1999 Published by Elsevier Science Ltd. All rights reserved. International Journal of Solids and Structures 37 (2000) 29±43 0020-7683/00/$ see front matter # 1999 Published by Elsevier Science Ltd. All rights reserved. PII: S0020-7683(99 )00075-X www.elsevier.com/locate/ijsolstr * Corresponding author. Tel.: +1-313-936-0406; fax: +1-313-647-3170. E-mail addresses: jbarber@umich.edu (J.R. Barber), m.ciavarella@area.ba.cnr.it (M. Ciavarella) 1 Presently at CNR-IRIS Computational Mechanics of Solids, str. Croce®sso, 2/B 70126, Bari, Italy. A casual survey of the kinds of engineering applications to which the techniques of Solid Mechanics are applied will show that the vast majority of solid bodies are loaded by being pressed against another body. The only alternatives comprise loading of the boundary by  ̄uid pressure or various kinds of body force such as gravitational or magnetic forces, but even in such cases, the reaction force required to maintain equilibrium will almost invariably be provided at a contact interface. When we also recall that contacts between bodies generally constitute stress concentrations and are therefore likely sites for material failure, it is not surprising that Contact Mechanics has occupied a central place in the development of Solid Mechanics over the years and continues to do so today. Additional interest in the subject is generated by the fact that the inevitable roughness of contacting surfaces generates a very complex local structure at which extreme conditions are likely to occur, particularly if sliding takes place, leading to frictional heating and very high local temperatures. Historically, the development of the subject stems from the famous paper of Heinrich Hertz (1882) giving the solution for the frictionless contact of two elastic bodies of ellipsoidal pro®le. Hertz' analysis still forms the basis of the design procedures used in many industrial situations involving elastic contact. Since 1882, the subject of contact mechanics has seen considerable development. Two major threads can be distinguished Ð from a mathematical standpoint, emphasis has been placed on the extension of Hertz' analysis to other geometries and constitutive laws and on the proof of theorems of existence and uniqueness of solution, whereas engineers have focussed on the solution of particular problems in an attempt to understand and in ̄uence phenomena that occur in practical contacting systems, both on the macro and the micro scale. Gladwell (1980) provides a compendious treatment of the various contact geometries that had been treated up to that time, including an invaluable survey of the rich Russian literature on the subject. Johnson (1985) gives an excellent overview of the range of contact problems that have come under consideration and achieves a nice balance between mathematical rigour and engineering practicality. In an attempt to obtain a similar balance, we ®rst revisit the de®ning characteristics of contact mechanics in the mathematical framework of problems involving unilateral inequalities. Particular attention is given to the additional features associated with the presence of friction at the interface, where nonexistence, non-uniqueness and instability of the quasi-static solution can be obtained with suciently high friction coecients. We then introduce the concept of self-similarity, which provides a powerful method for indentation problems even for non-linear materials and other aspects of contact beyond the elastic limit are also discussed. The specialized areas of anisotropic and elastodynamic contact are brie ̄y summarized and the paper concludes with a discussion of recent developments in the characterization and contact of rough surfaces and in thermoelastic contact. 1. Unilateral inequalities The essence of a contact problem lies in the fact that any point on the boundary of each body must either be in contact or not in contact. If it is not in contact, the gap g between it and the other body must be positive ( g>0), whereas if it is in contact, g = 0, by de®nition. A dual relation involves the contact pressure p between the bodies which must be positive ( p>0) where there is contact and zero where there is no contact. The inequalities serve to determine which points will be in contact and which not. If the contact area is prescribed, it can be shown from classical existence and uniqueness proofs that the equations alone are sucient to de®ne the stresses and displacements throughout the bodies, but there is then of course no guarantee that the solution will satisfy the inequalities. Fichera (1964, 1972) proved that the complete problem, including the inequalities, has a unique solution when the material is linear elastic and many related proofs have since been advanced for other classes of contact J.R. Barber, M. Ciavarella / International Journal of Solids and Structures 37 (2000) 29±43 30",1999,128,4600,717,28,54,53,70,94,122,154,173,158,168
5e6a4eda0a3d0d3a8c2864fd3afee4e67026bd40,"exactly solved models in statistical mechanics exactly solved models in statistical mechanics rodney j baxter exactly solved models in statistical mechanics exactly solved models in statistical mechanics flae exactly solved models in statistical mechanics dover books exactly solved models in statistical mechanics dover books exactly solved models in statistical mechanics dover books hatsutori in size 15 gvg7bzbookyo.qhigh literature cited r. j. baxter, exactly solved models in exactly solvable models in statistical mechanics exactly solved models in statistical mechanics dover books okazaki in size 24 vk19j3book.buncivy exactly solved models of statistical mechanics valerio nishizawa in size 11 b4zntdbookntey fukuda in size 13 33oloxbooknhuy yamada in size 19 x6g84ybook.zolay in honour of r j baxter’s 75th birthday arxiv:1608.04899v2 statistical mechanics, threedimensionality and np beautiful models: 70 years of exactly solved quantum many exactly solved models in statistical mechanics (dover solved lattice models: 1944 2010 university of melbourne exactly solved models and beyond: a special issue in the statistical mechanics of the classical two-dimensional faculty of science, p. j. saf ́arik university in ko?sice? a one-dimensional statistical mechanics model with exact statistical mechanics department of physics and astronomy statistical mechanics principles and selected applications graph theory and statistical physics yaroslavvb chapter 4 methods of statistical mechanics ijs thermodynamics and an introduction to thermostatistics potts models and related problems in statistical mechanics methods of quantum field theory in statistical physics statistical mechanics: theory and molecular simulation exactly solvable su(n) mixed spin ladders springer statistical field theory : an introduction to exactly",1982,21,5187,450,1,10,20,38,62,65,83,84,112,119
bc7505aea4e3bbad61df2837227d58c1af1762fd,"Quantum Mechanics for Organic Chemists.By Howard E. Zimmerman. Pp. x + 215. (Academic: New York and London, May 1975.) $16.50; £7.90.",1975,23,6607,575,4,6,1,4,3,4,6,3,3,9
f2dbab1960529561c9432600694607d59d6db694,,1980,0,8047,323,59,64,51,58,64,62,66,80,96,78
f2b196ffd931dfde6f021ab9ad67297147c2e0e3,The basic concepts of quantum mechanics Energy and momentum Schrodinger's equation Angular momentum Perturbation theory Spin The identity of particles The atom The theory of symmetry Polyatomic molecules Motion in a magnetic field Nuclear structure Elastic collisions Mathematical appendices.,1959,1,6522,364,1,2,6,6,1,2,11,9,5,7
f14f8f1470f0a95bf1157bb82da03f0efebcd774,,1965,0,7661,416,0,3,10,9,12,15,16,17,31,31
bc510ce607c277e3e0a06f883ecbe3db8dd807a3,"Equations of Anisotropic Elasticity, Virtual Work Principles, and Variational Methods Fiber-Reinforced Composite Materials Mathematical Preliminaries Equations of Anisotropic Entropy Virtual Work Principles Variational Methods Summary Introduction to Composite Materials Basic Concepts and Terminology Constitutive Equations of a Lamina Transformation of Stresses and Strains Plan Stress Constitutive Relations Classical and First-Order Theories of Laminated Composite Plates Introduction An Overview of Laminated Plate Theories The Classical Laminated Plate Theory The First-Order Laminated Plate Theory Laminate Stiffnesses for Selected Laminates One-Dimensional Analysis of Laminated Composite Plates Introduction Analysis of Laminated Beams Using CLPT Analysis of Laminated Beams Using FSDT Cylindrical Bending Using CLPT Cylindrical Bending Using FSDT Vibration Suppression in Beams Closing Remarks Analysis of Specially Orthotropic Laminates Using CLPT Introduction Bending of Simply Supported Rectangular Plates Bending of Plates with Two Opposite Edges Simply Supported Bending of Rectangular Plates with Various Boundary Conditions Buckling of Simply Supported Plates Under Compressive Loads Buckling of Rectangular Plates Under In-Plane Shear Load Vibration of Simply Supported Plates Buckling and Vibration of Plates with Two Parallel Edges Simply Supported Transient Analysis Closure Analytical Solutions of Rectangular Laminated Plates Using CLPT Governing Equations in Terms of Displacements Admissible Boundary Conditions for the Navier Solutions Navier Solutions of Antisymmetric Cross-Ply Laminates Navier Solutions of Antisymmetric Angle-Ply Laminates The Levy Solutions Analysis of Midplane Symmetric Laminates Transient Analysis Summary Analytical Solutions of Rectangular Laminated Plates Using FSDT Introduction Simply Supported Antisymmetric Cross-Ply Laminated Plates Simply Supported Antisymmetric Angle-Ply Laminated Plates Antisymmetric Cross-Ply Laminates with Two Opposite Edges Simply Supported Antisymmetric Angle-Ply Laminates with Two Opposite Edges Simply Supported Transient Solutions Vibration Control of Laminated Plates Summary Theory and Analysis of Laminated Shells Introduction Governing Equations Theory of Doubly-Curved Shell Vibration and Buckling of Cross-Ply Laminated Circular Cylindrical Shells Linear Finite Element Analysis of Composite Plates and Shells Introduction Finite Element Models of the Classical Plate Theory (CLPT) Finite Element Models of Shear Deformation Plate Theory (FSDT) Finite Element Analysis of Shells Summary Nonlinear Analysis of Composite Plates and Shells Introduction Classical Plate Theory First-Order Shear Deformation Plate Theory Time Approximation and the Newton-Raphson Method Numerical Examples of Plates Functionally Graded Plates Finite Element Models of Laminated Shell Theory Continuum Shell Finite Element Postbuckling Response and Progressive Failure of Composite Panels in Compression Closure Third-Order Theory of Laminated Composite Plates and Shells Introduction A Third-Order Plate Theory Higher-Order Laminate Stiffness Characteristics The Navier Solutions Levy Solutions of Cross-Ply Laminates Finite Element Model of Plates Equations of Motion of the Third-Order Theory of Doubly-Curved Shells Layerwise Theory and Variable Kinematic Model Introduction Development of the Theory Finite Element Model Variable Kinematic Formulations Application to Adaptive Structures Layerwise Theory of Cylindrical Shell Closure Subject Index",1996,17,4683,443,0,0,0,1,2,1,2,18,32,56
3fca038f28d3bd49e7d003a4e6171747dcd5b6ad,"A method is presented in which fracture mechanics is introduced into finite element analysis by means of a model where stresses are assumed to act across a crack as long as it is narrowly opened. This assumption may be regarded as a way of expressing the energy adsorption GC in the energy balance approach, but it is also in agreement with results of tension tests. As a demonstration the method has been applied to the bending of an unreinforced beam, which has led to an explanation of the difference between bending strength and tensile strength, and of the variation in bending strength with beam depth.",1976,5,5522,251,0,1,2,2,3,3,1,7,17,26
1cebe440b99bbcfab994eb9fce8f10e605a7d485,,1961,0,17019,70,0,0,0,0,0,0,0,0,0,1
5c96465c7c5a291b16d134db6bd1dbaa79ad8d1b,"AbstractPROF. R. H. FOWLER'S monumental work on statistical mechanics has, in this the second edition, in his own modest words, been rearranged and brought more up to date. But the new volume is much more than a revision, in that it is explicitly based on quantum mechanics from the outset ; the first dynamical equation found written in the formal presentation is a wave-equation. Prof. Fowler states in justification that although classical mechanics is used to derive the quantum mechanics by a process of generalization, ""once the laws of quantum mechanics have been thus guessed, as they must be before we can discuss the theorems of statistical mechanics, quantized systems naturally come first. In 1935 this attitude hardly needs apology"". In consequence of this, the concluding chapter of the first edition, dealing with quantum statistics, has been incorporated in the new exposition from the start, and what is effectively the opening chapter is now doubled in length. Otherwise there is no change of general structure. But a comparison of the two editions shows that the chapter of the first edition entitled ""Thermionics"" is now more than four times as long, and contains a treatment of the electron theory of metals and of semi-conductors ; that the chapter on dielectrics and magnetic constants has been similarly extended, and now includes an account of ferro-magnetism ; lastly, there is a new concluding chapter on ""co-operative"" and other phenomena. The number of references is almost doubled. It is difficult to over-estimate the amount of work involved in thus re-writing and extending what was already an encyclopædic work.Statistical Mechanics: 
 the Theory of the Properties of Matter in Equilibrium. By Prof. R. H. Fowler. Second edition, revised and enlarged. Pp. x + 864. (Cambridge: At the University Press, 1936.) 50s. net.",1937,0,4931,471,0,0,0,0,0,0,0,0,0,0
1350fb3231b36d942b6ff1dacb8df68a6df9c9b0,"An important achievement of modern experimental fluid mechanics is the invention and development of techniques for the measurement of whole, instantaneous fields of scalars and vectors. These techniques include tomographic interferometry (Hesselink 1988) and planar laser-induced fluorescence for scalars (Hassa et al 1987), and nuclear-magnetic-resonance imaging (Lee et al 1987), planar laser-induced fluorescence, laser-speckle velocimetry, particle-tracking velocimetry, molecular-tracking velocimetry (Miles et al 1989), and particle-image velocimetry for velocity fields. Reviews of these methods can be found in articles by Lauterborn & Vogel (1984), Adrian (1986a), Hesselink (1988), and Dudderar et al (1988), in books written by Merzkirch (1987) and edited by Chiang & Reid (1988) and Gad-el-Hak (1989).",1991,0,3309,258,3,24,48,39,70,65,85,71,73,104
a13bd077717cb28cace6fd97c2fedeab8ba2449c,"Mathematical Foundations of Quantum Mechanics was a revolutionary book that caused a sea change in theoretical physics. Here, John von Neumann, one of the leading mathematicians of the twentieth century, shows that great insights in quantum physics can be obtained by exploring the mathematical structure of quantum mechanics. He begins by presenting the theory of Hermitean operators and Hilbert spaces. These provide the framework for transformation theory, which von Neumann regards as the definitive form of quantum mechanics. Using this theory, he attacks with mathematical rigor some of the general problems of quantum theory, such as quantum statistical mechanics as well as measurement processes. Regarded as a tour de force at the time of publication, this book is still indispensable for those interested in the fundamental issues of quantum mechanics.",1955,0,4318,294,0,1,0,2,2,5,3,9,3,11
2ec88a1138f65a5e6f802547758c495ef69bbc79,1. Fundamental Concepts. 2. Quantum Dynamics. 3. Theory of Angular Momentum. 4. Symmetry in Quantum Mechanics. 5. Approximation Methods. 6. Identical Particles. 7. Scattering Theory. Appendices. Supplements. Bibliography. Index.,1986,0,3577,226,0,4,3,3,4,7,17,18,14,33
8de008afdc3535d09dbbb33e52976a4277ba51d7,"Contents: General results and concepts on invariant sets and attractors.- Elements of functional analysis.- Attractors of the dissipative evolution equation of the first order in time: reaction-diffusion equations.- Fluid mechanics and pattern formation equations.- Attractors of dissipative wave equations.- Lyapunov exponents and dimensions of attractors.- Explicit bounds on the number of degrees of freedom and the dimension of attractors of some physical systems.- Non-well-posed problems, unstable manifolds. lyapunov functions, and lower bounds on dimensions.- The cone and squeezing properties.- Inertial manifolds.- New chapters: Inertial manifolds and slow manifolds the nonselfadjoint case.",1993,4,3674,229,38,46,69,70,73,96,86,79,109,113
c3efe67b570661f6370c40eb8f9fefe36616dcef,,1943,0,6051,483,0,0,0,2,1,2,2,1,0,1
bbdb0b63eb59c30c57042f39a6348137595aa3f1,Introduction Foreward by Tudor Ratiu and Richard Cushman Preliminaries Differential Theory Calculus on Manifolds Analytical Dynamics Hamiltonian and Lagrangian Systems Hamiltonian Systems with Symmetry Hamiltonian-Jacobi Theory and Mathematical Physics An Outline of Qualitative Dynamics Topological Dynamics Differentiable Dynamics Hamiltonian Dynamics Celestial Mechanics The Two-Body Problem The Three-Body Problem.,1967,0,3256,285,2,3,4,14,10,18,8,13,12,29
13e592e40746f0ccb1df7e0b88a751ed9e2a86f9,"This book offers a concise introduction to the angular momentum, one of the most fundamental quantities in all of quantum mechanics. Beginning with the quantization of angular momentum, spin angular momentum, and the orbital angular momentum, the author goes on to discuss the Clebsch-Gordan coefficients for a two-component system. After developing the necessary mathematics, specifically spherical tensors and tensor operators, the author then investigates the 3-""j,"" 6-""j,"" and 9-""j"" symbols. Throughout, the author provides practical applications to atomic, molecular, and nuclear physics. These include partial-wave expansions, the emission and absorption of particles, the proton and electron quadrupole moment, matrix element calculation in practice, and the properties of the symmetrical top molecule.",1957,0,5011,199,2,4,12,18,28,29,25,55,50,64
b6b67088f42337785438e8f40dfa89bb314d8d0d,"Keywords: Mecanique des sols ; Sols non satures Reference Record created on 2004-09-07, modified on 2016-08-08",1993,0,3168,322,0,6,13,17,15,23,22,40,51,57
7033363f0c6c2274cede210b6f86d86ec0f0eed8,,1979,0,3377,311,1,7,10,27,35,31,53,33,32,39
bccaa261b96906eb100d55c8bcbbbd9953e79971,"One of the simplest, and most completely treated, fields of application of quantum mechanics is the theory of atoms with one or two electrons. For hydrogen and the analcgous ions He+, Li++, etc., the calculations can be performed exactly, both in Schrodinger’s nonrelativistic wave mechanics and in Dirac’s relativistic theory of the electron. More specifically, the calculations are exact for a single electron in a fixed Coulomb potential. Hydrogen-like atoms thus furnish an excellent way of testing the validity of quantum mechanics. For such atoms the correction terms due to the motion and structure of atomic nuclei and due to quantum electrodynamic effects are small and can be calculated with high accuracy. Since the energy levels of hydrogen and similar atoms can be investigated experimentally to an astounding degree of accuracy, some accurate tests of the validity of quantum electrodynamics are also possible. Finally, the theory of such atoms in an external electric or magnetic field has also been developed in detail and compared with experiment.",1957,9,3980,190,0,1,4,3,12,19,21,32,22,19
f8034ea2444dc0e672a6176dc09e0cb7dc07dd91,CONTENTS: PHYSICAL PROPERTIES OF SOILS INDEX PROPERTIES OF SOILS HYDRAULIC AND MECHANICAL PROPERTIES OF SOILS DRAINAGE OF SOILS THEORETICAL SOIL MECHANICS HYDRAULICS OF SOILS PLASTIC EQUILIBRIUM IN SOILS SETTLEMENT AND CONTACT PRESSURE PROBLEMS OF DESIGN AND CONSTRUCTION SOIL EXPLORATION EARTH PRESSURE AND STABILITY OF SLOPES FOUNDATIONS SETTLEMENT DUE TO EXCEPTIONAL CAUSES DAMS AND DAM FOUNDATIONS PERFORMANCE OBSERVATIONS.,1948,0,4439,196,0,2,1,0,4,2,1,0,4,6
37cd44ffb93ef295ac1939ce3846f382dbf972c4,"A new molecular mechanics force field, the Universal force field (UFF), is described wherein the force field parameters are estimated using general rules based only on the element, its hybridization, and its connectivity. The force field functional forms, parameters, and generating formulas for the full periodic table are presented",1992,5,6055,94,0,2,13,15,34,42,44,57,64,57
ef9bf044a16ea6e827d8e102591fc7b97d284cd8,"This textbook offers a unified presentation of the concepts and general principles common to all branches of solid and fluid mechanics. It deals with: vectors and tensors, stress, strain and deformation, general principles, constitutive equations, fluid mechanics and linearized theory of elasticity. (TRRL)",1969,0,3576,219,1,0,1,3,6,10,7,17,11,13
fc2607214581153e5a8058dd6760fe703558fdf1,"1. Elements of the physical mechanisms of deformation and fracture 2. Elements of continuum mechanics and thermodynamics 3. Identification and theological classification of real solids 4. Linear elasticity, thermoelasticity and viscoelasticity 5. Plasticity 6. Viscoplasticity 7. Damage mechanics 8. Crack mechanics.",1990,0,3285,168,2,11,16,17,33,30,34,45,67,51
850d1dc5db19e553458136b6cfe5e8e52b83c1bf,,1989,0,3069,346,2,3,5,12,20,14,47,30,31,56
8845cd6af2396b19b8bce7b20646b345758b571f,DEFORMATION OF ENGINEERING MATERIALS. Tensile Response of Materials. Elements of Dislocation Theory. Slip and Twinning in Crystalline Solids. Strengthening Mechanisms in Metals. High-Temperature Deformation Response of Crystalline Solids. Deformation Response of Engineering Plastics. FRACTURE MECHANICS OF ENGINEERING MATERIALS. Fracture: An Overview. Elements of Fracture Mechanics. Transition Temperature Approach to Fracture Control. Microstructural Aspects of Fracture Toughness. Environment-Assisted Cracking. Cyclic Stress and Strain Fatigue. Fatigue Crack Propagation. Analyses of Engineering Failures. Appendices. Indexes.,1976,0,3539,180,2,2,18,9,11,9,21,22,10,13
80350eb7c643fd437b9a720ac2e3e6707eaf7626,"Note: A basic exposition of classical mechanical systems; 2nd edition Reference CAG-BOOK-2008-008 Record created on 2008-11-21, modified on 2017-09-27",1994,1,3017,258,2,18,23,43,72,69,85,114,106,108
41a72efc7febe451e0e8e47589d40f77d56ad3da,"Keywords: Mecanique des roches ; Analyse des contraintes ; Elasticite Reference Record created on 2004-09-07, modified on 2016-08-08",1969,0,4439,123,2,6,9,8,13,17,16,25,20,33
c2837cc05d715c8a3fe95137983049b671571239,Introduction to Vectors and Tensors. Kinematics. The Concept of Stress. Balance Principles. Some Aspects of Objectivity. Hyperelastic Materials. Thermodynamics of Materials. Variational Principles. References. Index.,2000,5,2694,292,3,6,17,24,28,34,58,76,91,126
359f1840c407d7fabe7100bc885b6f15e05ca1ab,"A unified treatment of the mechanics of deformation and acoustic propagation in porous media is presented, and some new results and generalizations are derived. The writer's earlier theory of deformation of porous media derived from general principles of nonequilibrium thermodynamics is applied. The fluid‐solid medium is treated as a complex physical‐chemical system with resultant relaxation and viscoelastic properties of a very general nature. Specific relaxation models are discussed, and the general applicability of a correspondence principle is further emphasized. The theory of acoustic propagation is extended to include anisotropic media, solid dissipation, and other relaxation effects. Some typical examples of sources of dissipation other than fluid viscosity are considered.",1962,16,3205,194,0,1,2,1,1,1,1,1,3,3
5b43d4b435281f204a7a5bf450c88a267ae7093a,"In this paper we develop a new constitutive law for the description of the (passive) mechanical response of arterial tissue. The artery is modeled as a thick-walled nonlinearly elastic circular cylindrical tube consisting of two layers corresponding to the media and adventitia (the solid mechanically relevant layers in healthy tissue). Each layer is treated as a fiber-reinforced material with the fibers corresponding to the collagenous component of the material and symmetrically disposed with respect to the cylinder axis. The resulting constitutive law is orthotropic in each layer. Fiber orientations obtained from a statistical analysis of histological sections from each arterial layer are used. A specific form of the law, which requires only three material parameters for each layer, is used to study the response of an artery under combined axial extension, inflation and torsion. The characteristic and very important residual stress in an artery in vitro is accounted for by assuming that the natural (unstressed and unstrained) configuration of the material corresponds to an open sector of a tube, which is then closed by an initial bending to form a load-free, but stressed, circular cylindrical configuration prior to application of the extension, inflation and torsion. The effect of residual stress on the stress distribution through the deformed arterial wall in the physiological state is examined.The model is fitted to available data on arteries and its predictions are assessed for the considered combined loadings. It is explained how the new model is designed to avoid certain mechanical, mathematical and computational deficiencies evident in currently available phenomenological models. A critical review of these models is provided by way of background to the development of the new model.",2000,75,2635,287,2,4,17,34,39,43,60,94,91,130
c2d79dfb430e7067989cc82633b5f299c8e9abe6,"The Young's modulus, strength, and toughness of nanostructures are important to proposed applications ranging from nanocomposites to probe microscopy, yet there is little direct knowledge of these key mechanical properties. Atomic force microscopy was used to determine the mechanical properties of individual, structurally isolated silicon carbide (SiC) nanorods (NRs) and multiwall carbon nanotubes (MWNTs) that were pinned at one end to molybdenum disulfide surfaces. The bending force was measured versus displacement along the unpinned lengths. The MWNTs were about two times as stiff as the SiC NRs. Continued bending of the SiC NRs ultimately led to fracture, whereas the MWNTs exhibited an interesting elastic buckling process. The strengths of the SiC NRs were substantially greater than those found previously for larger SiC structures, and they approach theoretical values. Because of buckling, the ultimate strengths of the stiffer MWNTs were less than those of the SiC NRs, although the MWNTs represent a uniquely tough, energy-absorbing material.",1997,25,4304,86,1,24,47,54,64,102,142,192,186,277
b75768ea79096244631a7e65bbda869f529daed3,"In this article the principles of the field operation and manipulation (FOAM) C++ class library for continuum mechanics are outlined. Our intention is to make it as easy as possible to develop reliable and efficient computational continuum-mechanics codes: this is achieved by making the top-level syntax of the code as close as possible to conventional mathematical notation for tensors and partial differential equations. Object-orientation techniques enable the creation of data types that closely mimic those of continuum mechanics, and the operator overloading possible in C++ allows normal mathematical symbols to be used for the basic operations. As an example, the implementation of various types of turbulence modeling in a FOAM computational-fluid-dynamics code is discussed, and calculations performed on a standard test case, that of flow around a square prism, are presented. To demonstrate the flexibility of the FOAM library, codes for solving structures and magnetohydrodynamics are also presented with a...",1998,61,3044,200,1,2,6,1,3,2,5,10,19,14
dc7e9b66582d21e5b2ea16a9aa45f3ced91241f2,"Contents: Introduction.- The Mechanics of Lagrange.- The Mechanics of Hamilton and Jacobi.- Integrable Systems.- The Three-Body Problem: Moon-Earth-Sun.- Three Methods of Section.- Periodic Orbits.- The Surface of Solution.- Models of the Galaxy and of Small Molecules.- Soft Chaos and the KAM Theorem.- Entropy and Other Measures of Chaos.- The Anisotropic Kepler Problem.- The Transition From Classical to Quantum Mechanics.- The New World of Quantum Mechanics.- The Quantization of Integrable Systems.- Wave Functions in Classically Chaotic Systems.- The Energy Spectrum of a Classically Chaotic System.- The Trace Formula.- The Diamagnetic Kepler Problem.- Motion on a Surface of Constant Negative Curvature.- Scattering Problems, Coding and Multifractal Invariant Measures.- References.- Index.",1990,0,3124,171,1,7,48,72,104,153,156,159,165,170
726acea4c546a72d0a157f19859bbcd65c0bf394,"Quantum mechanics can speed up a range of search applications over unsorted data. For example, imagine a phone directory containing $N$ names arranged in completely random order. To find someone's phone number with a probability of 50%, any classical algorithm (whether deterministic or probabilistic) will need to access the database a minimum of $0.5N$ times. Quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. By properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. As a result, the desired phone number can be obtained in only $O(\sqrt{N})$ accesses to the database.",1997,20,3157,128,9,39,56,80,103,121,98,115,142,149
a0be400e9820f60f1e87e9c5e3a7510de36609c9,,1985,0,4860,59,0,3,4,10,13,21,21,27,26,39
1cc920998208f988a873dbbfa0315274d0b51b57,1. Introduction. 2. Spatial Descriptions and Transformations. 3. Manipulator Kinematics. 4. Inverse Manipulator Kinematics. 5. Jacobians: Velocities and Static Forces. 6. Manipulator Dynamics. 7. Trajectory Generation. 8. Manipulator Mechanism Design. 9. Linear Control of Manipulators. 10. Nonlinear Control of Manipulators. 11. Force Control of Manipulators. 12. Robot Programming Languages and Systems. 13. Off-Line Programming Systems.,1986,0,3650,95,4,10,29,60,41,79,66,71,65,81
ac75c8f5d0013e322e1c5e9ac7c689e1e4aac20c,"Preface - Introduction - PART I: PHYSICAL PRINCIPLES - Mechanical Forces - Mass, Stiffness, and Damping of Proteins - Thermal Forces and Diffusion - Chemical Forces - Polymer Mechanics - PART II: CYTOSKELETON - Structures of Cytoskeletal Filaments - Mechanics of the Cytoskeleton - Polymerization of Cytoskeletal Filaments - Force Generation by Cytoskeletal Filaments - Active Polymerization - PART III: MOTOR PROTEINS - Structures of Motor Proteins - Speeds of Motors - ATP Hydrolysis - Steps and Forces - Motility Models: From Crossbridges to Motion - Afterword - Appendix - Bibliography - Index",2001,8,2553,205,6,48,74,104,155,167,159,179,155,163
9a5038fe1c547708bdfb7ef8e89cf1f74145c716,"- Forces in Joints, - Skeletal Biology, - Analysis of Bone Remodeling, - Mechanical Properties of Bone, - Fatigue and Fracture Resistance of Bone, - Mechanical Adaptation of the Skeleton, - Synovial Joint Mechanics, - Mechanical Properties of Ligament and Tendon",1998,775,495,50,0,0,0,0,0,0,0,0,0,0
0ceb798602f4a466b1e3c943a73a892c25d8abbc,"Perturbation Methods in Fluid MechanicsBy Milton Van Dyke. (Applied Mathematics and Mechanics: an International Series of Monographs, Vol. 8.) Pp. x + 229. (New York: Academic Press, Inc.; London: Academic Press, Inc. (London), Ltd., 1964.) 56s.",1965,0,3063,188,3,21,33,26,41,41,53,63,46,48
8e5b0ab2a89c9e1677da4344409521d62af37058,,1991,0,2314,247,10,14,29,25,24,26,34,31,38,33
3dcd8eeee529e3cf8ee8ffea2b46ba84e00c1403,,1969,0,3458,67,5,27,29,50,37,35,41,43,53,46
1ac4ec57a37423e18471d8cb87faf9a15bf0a596,"Abstract T he macroscopic elastic moduli of two-phase composites are estimated by a method that takes account of the inhomogeneity of stress and strain in a way similar to the Hershey-Kroner theory of crystalline aggregates. The phases may be arbitrarily aeolotropic and in any concentrations, but are required to have the character of a matrix and effectively ellipsoidal inclusions. Detailed results arc given for an isotropic dispersion of spheres.",1965,9,3090,126,1,1,3,2,3,5,4,2,3,6
cddd3b14440835151ab6bf17dd7bdb2535bef776,Preface Acknowledgments List of symbols 1. Brittle fracture of rock 2. Rock friction 3. Mechanics of faulting 4. Mechanics of earthquakes 5. The seismic cycle 6. Seismotectonics 7. Earthquake prediction and hazard analysis References Index.,1990,935,2709,93,3,9,35,36,49,48,79,91,83,74
042ad33902b18a6cc9310441d5853afbc17bc5e4,"Keywords: CFD ; numerique ; transfert de chaleur ; ecoulement Reference Record created on 2005-11-18, modified on 2016-08-08",1984,0,3087,51,1,6,12,21,33,49,53,63,76,76
eb11c570127272630f711f272b29d78125d1ca6a,R J Baxter 1982 London: Academic xii + 486 pp price £43.60 Over the past few years there has been a growing belief that all the twodimensional lattice statistical models will eventually be solved and that it will be Professor Baxter who solves them. Baxter has inherited the mantle of Onsager who started the process by solving exactly the two-dimensional Ising model in 1944.,1983,0,2135,317,1,0,0,3,8,3,3,7,2,22
6222d3a6e2673d0c3ee208d3915c8e0f82293d35,"Cellular automata are used as simple mathematical models to investigate self-organization in statistical mechanics. A detailed analysis is given of ''elementary'' cellular automata consisting of a sequence of sites with values 0 or 1 on a line, with each site evolving deterministically in discrete time steps according to p definite rules involving the values of its nearest neighbors. With simple initial configurations, the cellular automata either tend to homogeneous states, or generate self-similar patterns with fractal dimensions approx. =1.59 or approx. =1.69. With ''random'' initial configurations, the irreversible character of the cellular automaton evolution leads to several self-organization phenomena. Statistical properties of the structures generated are found to lie in two universality classes, independent of the details of the initial state or the cellular automaton rules. More complicated cellular automata are briefly considered, and connections with dynamical systems theory and the formal theory of computation are discussed.",1983,117,2660,128,1,21,16,38,45,25,49,62,54,39
ef6729b6aee8d2975724e0f0b138be0d5124773e,"sorption results9 revealed that the iron surface was mostly covered by promoter oxides of AI, Ca, and K. Postreaction XPS results also revealed a C( Is) XPS peak of weak to moderate intensity centered at 284.1-283.7 eV. This binding energy approaches those (ca. 283.5 eV) reported for iron cat bide^.^^*'^ More convincing evidence for carbide formation was obtained from TPHT results collected after reaction studies like those displayed in Figure 1 in which methane was the only product. After reaction at temperatures below 340 OC, only small amounts of reactive carbon could be distinguished with maximum methane desorption rates near 300 OC. However, for higher reaction temperatures, large amounts of methane were produced with a maximum rate just above 400 OC. Since XPS results revealed only small amounts of carbonaceous residue on top of the catalyst surface, this reactive carbon must be associated with carbiding of the catalyst. Consequently, it appears that the active carbon incorporation catalyst is carbided iron. This conclusion is well supported by bulk carbon to iron stoichiometries of 0.1-0.25 estimated from the TPHT peak areas which were adequate to represent 40-60'36 conversion to bulk carbides such as Fe,C or FeSC2. Moreover, preliminary results from studies using bona fide iron carbides have shown similar catalytic b e h a ~ i o r . ~",1990,0,2889,67,0,10,13,25,34,53,37,51,57,68
cbd17c0169ff3390ddcebbdc37c6665ef4243137,"Matter is commonly found in the form of materials. Analytical mechanics turned its back upon this fact, creating the centrally useful but abstract concepts of the mass point and the rigid body, in which matter manifests itself only through its inertia, independent of its constitution; “modern” physics likewise turns its back, since it concerns solely the small particles of matter, declining to face the problem of how a specimen made up of such particles will behave in the typical circumstances in which we meet it. Materials, however, continue to furnish the masses of matter we see and use from day to day: air, water, earth, flesh, wood, stone, steel, concrete, glass, rubber, ... All are deformable. A theory aiming to describe their mechanical behavior must take heed of their deformability and represent the definite principles it obeys.",1992,647,2307,191,23,19,14,21,21,11,18,16,24,36
c00dd9a342196aecd0bcd7af562e62226c7c39b1,"Abstract A local symmetric weak form (LSWF) for linear potential problems is developed, and a truly meshless method, based on the LSWF and the moving least squares approximation, is presented for solving potential problems with high accuracy. The essential boundary conditions in the present formulation are imposed by a penalty method. The present method does not need a “finite element mesh”, either for purposes of interpolation of the solution variables, or for the integration of the “energy”. All integrals can be easily evaluated over regularly shaped domains (in general, spheres in three-dimensional problems) and their boundaries. No post-smoothing technique is required for computing the derivatives of the unknown variable, since the original solution, using the moving least squares approximation, is already smooth enough. Several numerical examples are presented in the paper. In the example problems dealing with Laplace & Poisson's equations, high rates of convergence with mesh refinement for the Sobolev norms ||·||0 and ||·||1 have been found, and the values of the unknown variable and its derivatives are quite accurate. In essence, the present meshless method based on the LSWF is found to be a simple, efficient, and attractive method with a great potential in engineering applications.",1998,10,2217,123,0,6,16,28,42,57,72,71,92,88
6d5204a744332d11da9a43f6414adc29769a699e,,2001,0,1913,235,83,82,88,100,79,93,71,103,102,115
5448660b5bf49a4989c686b45cdf1777f0bd1c5a,,1988,0,2581,134,0,6,44,27,20,30,33,72,49,70
9e8bac0fc1546f601208035bc322c88116d80243,"An integrated molecular modeling system for designing and studying organic and bioorganic molecules and their molecular complexes using molecular mechanics is described. The graphically controlled, atom‐based system allows the construction, display and manipulation of molecules and complexes having as many as 10,000 atoms and provides interactive, state‐of‐the‐art molecular mechanics on any subset of up to 1,000 atoms. The system semiautomates the graphical construction and analysis of complex structures ranging from polycyclic organic molecules to biopolymers to mixed molecular complexes. We have placed emphasis on providing effective searches of conformational space by a number of different methods and on highly optimized molecular mechanics energy calculations using widely used force fields which are supplied as external files. Little experience is required to operate the system effectively and even novices can use it to carry out sophisticated modeling operations. The software has been designed to run on Digital Equipment Corporation VAX computers interfaced to a variety of graphics devices ranging from inexpensive monochrome terminals to the sophisticated graphics displays of the Evans & Sutherland PS300 series.",1990,25,2709,35,8,31,56,63,72,98,104,127,112,126
6fa1157ed98d90212053726120f125390faae1bf,,1931,0,4872,242,1,0,1,4,1,1,2,0,2,1
d68ff1226018b9ca1c1a0d16672fe951dede95d9,Basics or How the Theory Works.- Historical Background and Physical Motivations.- Learning with Boltzmann-Gibbs Statistical Mechanics.- Generalizing What We Learnt: Nonextensive Statistical Mechanics.- Foundations or Why the Theory Works.- Stochastic Dynamical Foundations of~Nonextensive Statistical Mechanics.- Deterministic Dynamical Foundations of Nonextensive Statistical Mechanics.- Generalizing Nonextensive Statistical Mechanics.- Applications or What for the Theory Works.- Thermodynamical and Nonthermodynamical Applications.- Last (But Not Least).- Final Comments and Perspectives.,2009,0,1509,159,52,89,115,141,113,157,146,125,123,128
487c109929fc1f82c74ae9b88d70ff231f603c85,"The overall mechanics of fold-and-thrust belts and accretionary wedges along compressive plate boundaries is considered to be analogous to that of a wedge of soil or snow in front of a moving bulldozer. The material within the wedge deforms until a critical taper is attained, after which it slides stably, continuing to grow at constant taper as additional material is encountered at the toe. The critical taper is the shape for which the wedge is on the verge of failure under horizontal compression everywhere, including the basal decollement. A wedge of less than critical taper will not slide when pushed but will deform internally, steepening its surface slope until the critical taper is attained. Common silicate sediments and rocks in the upper 10-15 km of the crust have pressure-dependent brittle compressive strengths which can be approximately represented by the empirical Coulomb failure criterion, modified to account for the weakening effects of pore fluid pressure. A simple analytical theory that predicts the critical tapers of subaerial and submarine Coulomb wedges is developed and tested quantitatively in three ways: First, laboratory model experiments with dry sand match the theory. Second, the known surface slope, basal dip, and pore fluid pressures in the active fold-and-thrust belt of western Taiwan are used to determine the effective coefficient of internal friction within the wedge,/x = 1.03, consistent with Byerlee's empirical law of sliding friction,/at, = 0.85, on the base. This excess of internal strength over basal friction suggests that although the Taiwan wedge is highly deformed by imbricate thrusting, it is not so pervasively fractured that frictional sliding is always possible on surfaces of optimum orientation. Instead, the overall internal strength apparently is controlled by frictional sliding along suboptimally oriented planes and by the need to fracture some parts of the observed geometrically complex structure for continued deformation. Third, using the above values of/at, and/x, we predict Hubbert-Rubey fluid pressure ratios X = Xt, for a number of other active subaerial and submarine accretionary wedges based on their observed tapers, finding values everywhere in excess of hydrostatic. These predicted overpressures are reasonable in light of petroleum drilling experience in general and agree with nearby fragmentary well data in specific wedges where they are available. The pressure-dependent Coulomb wedge theory developed here is expected to break down if the decollement exhibits pressure-independent plastic behavior because of either temperature or rock type. The effects of this breakdown are observed in the abrupt decrease in taper where wedge thicknesses exceed about 15 km, which is the predicted depth of the brittle-plastic transition in quartz-rich rocks for typical geothermal gradients. We conclude that fold-and-thrust belts and accretionary wedges have the mechanics of bulldozer wedges in compression and that normal laboratory fracture and frictional strengths are appropriate to mountain-building processes in the upper crust, above the brittle-plastic transition.",1983,63,2327,146,6,10,15,24,22,37,51,54,52,60
312c23a35beb067492b21d1ad5b04570485a00a3,"Non-relativistic quantum mechanics is formulated here in a different way. It is, however, mathematically equivalent to the familiar formulation. In quantum mechanics the probability of an event which can happen in several different ways is the absolute square of a sum of complex contributions, one from each alternative way. The probability that a particle will be found to have a path x(t) lying somewhere within a region of space time is the square of a sum of contributions, one from each path in the region. The contribution from a single path is postulated to be an exponential whose (imaginary) phase is the classical action (in units of ℏ) for the path in question. The total contribution from all paths reaching x, t from the past is the wave function ψ(x, t). This is shown to satisfy Schroedinger's equation. The relation to matrix and operator algebra is discussed. Applications are indicated, in particular to eliminate the coordinates of the field oscillators from the equations of quantum electrodynamics.",1948,0,3126,125,0,1,1,3,4,1,4,9,10,6
c6b9a95f672b1ef0a2b3d4eae61c46696e157958,"Thermodynamics, fundamentals conditions for equilibrium and stability statistical mechanics non-interacting (ideal) systems statistical mechanical theory of phase transitions Monte Carlo method in statistical mechanics classical fluids statistical mechanics of non-equilibrium systems.",1987,0,2346,110,0,0,7,7,18,19,26,34,43,33
b86ac859932ebdab1b2c281c95c9fb7b6f8766ec,"The demonstrations of von Neumann and others, that quantum mechanics does not permit a hidden variable interpretation, are reconsidered. It is shown that their essential axioms are unreasonable. It is urged that in further examination of this problem an interesting axiom would be that mutually distant systems are independent of one another.",1966,6,2688,157,0,2,4,6,3,3,5,8,6,4
6583be285aef2087223d19e890c8d94aa8a89805,,1952,0,3106,119,0,3,1,1,1,2,0,2,1,0
3aa3518ca0ed8d1346447145c9688d9e9bdce1ae,"1 Phenomenological Aspects of Damage.- 1.1 Physical Nature of the Solid State and Damage.- 1.1.1 Atoms, Elasticity and Damage.- 1.1.2 Slips, Plasticity and Irreversible Strains.- 1.1.3 Scale of the Phenomena of Strain and Damage.- 1.1.4 Different Manifestations of Damage.- 1.1.5 Exercise on Micrographic Observations.- 1.2 Mechanical Representation of Damage.- 1.2.1 One-Dimensional Surface Damage Variable.- 1.2.2 Effective Stress Concept.- 1.2.3 Strain Equivalence Principle.- 1.2.4 Coupling Between Strains and Damage Rupture Criterion Damage Threshold.- 1.2.5 Exercise on the Micromechanics of the Effective Damage Area.- 1.3 Measurement of Damage.- 1.3.1 Direct Measurements.- 1.3.2 Variation of the Elasticity Modulus.- 1.3.3 Variation of the Microhardness.- 1.3.4 Other Methods.- 1.3.5 Exercise on Measurement of Damage by the Stress Amplitude Drop.- 2 Thermodynamics and Micromechanics of Damage.- 2.1 Three-Dimensional Analysis of Isotropic Damage.- 2.1.1 Thermodynamical Variables, State Potential.- 2.1.2 Damage Equivalent Stress Criterion.- 2.1.3 Potential of Dissipation.- 2.1.4 Strain-Damage Coupled Constitutive Equations.- 2.1.5 Exercise on the Identification of Material Parameters.- 2.2 Analysis of Anisotropic Damage.- 2.2.1 Geometrical Definition of a Second-Order Damage Tensor.- 2.2.2 Thermodynamical Definition of a Fourth-Order Damage Tensor.- 2.2.3 Energetic Definition of a Double Scalar Variable.- 2.2.4 Exercise on Anisotropic Damage in Proportional Loading.- 2.3 Micromechanics of Damage.- 2.3.1 Brittle Isotropie Damage.- 2.3.2 Ductile Isotropie Damage.- 2.3.3 Anisotropie Damage.- 2.3.4 Microcrack Closure Effect, Unilateral Conditions.- 2.3.5 Damage Localization and Instability.- 2.3.6 Exercise on the Fiber Bundle System.- 3 Kinetic Laws of Damage Evolution.- 3.1 Unified Formulation of Damage Laws.- 3.1.1 General Properties and Formulation.- 3.1.2 Stored Energy Damage Threshold.- 3.1.3 Three-Dimensional Rupture Criterion.- 3.1.4 Case of Elastic-Perfectly Plastic and Damageable Materials.- 3.1.5 Identification of the Material Parameters.- 3.1.6 Exercise on Identification by a Low Cycle Test.- 3.2 Brittle Damage of Metals, Ceramics, Composites and Concrete.- 3.2.1 Pure Brittle Damage.- 3.2.2 Quasi-brittle Damage.- 3.2.3 Exercise on the Influence of the Triaxiality on Rupture.- 3.3 Ductile and Creep Damage of Metals and Polymers.- 3.3.1 Ductile Damage.- 3.3.2 Exercises on the Fracture Limits in Metal Forming.- 3.3.3 Creep Damage.- 3.3.4 Exercise on Isochronous Creep Damage Curves.- 3.4 Fatigue Damage.- 3.4.1 Low Cycle Fatigue.- 3.4.2 Exercise on Creep Fatigue Interaction.- 3.4.3 High Cycle Fatigue.- 3.4.4 Exercise on Damage Accumulation.- 3.5 Damage of Interfaces.- 3.5.1 Continuity of the Stress and Strain Vectors.- 3.5.2 Strain Surface Energy Release Rate.- 3.5.3 Kinetic Law of Debonding Damage Evolution.- 3.5.4 Simplified Model.- 3.5.5 Exercise on a Debonding Criterion for Interfaces.- 3.6 Table of Material Parameters.- 4 Analysis of Crack Initiation in Structures.- 4.1 Stress-Strain Analysis.- 4.1.1 Stress Concentrations.- 4.1.2 Neuter's Method.- 4.1.3 Finite Element Method.- 4.1.4 Exercise on the Stress Concentration Near a Hole.- 4.2 Uncoupled Analysis of Crack Initiation.- 4.2.1 Determination of the Critical Point(s).- 4.2.2 Integration of the Kinetic Damage Law.- 4.2.3 Exercise on Fatigue Crack Initiation Near a Hole.- 4.3 Locally Coupled Analysis.- 4.3.1 Localization of Damage.- 4.3.2 Postprocessing of Damage Growth.- 4.3.3 Description and Listing of the Postprocessor DAMAGE 90.- 4.3.4 Exercises Using the DAMAGE 90 Postprocessor.- 4.4 Fully Coupled Analysis.- 4.4.1 Initial Strain Hardening and Damage.- 4.4.2 Example of a Calculation Using the Finite Element Method.- 4.4.3 Growth of Damaged Zones and Macrocracks.- 4.4.4 Exercise on Damage Zone at a Crack Tip.- 4.5 Statistical Analysis with Microdefects.- 4.5.1 Initial Defects.- 4.5.2 Case of Brittle Materials.- 4.5.3 Case of Quasi Brittle Materials.- 4.5.4 Case of Ductile Materials.- 4.5.5 Volume Effect.- 4.5.6 Effect of Stress Heterogeneity.- 4.5.7 Exercise on Bending Fatigue of a Beam.- History of International Damage Mechanics Conferences.- Authors and Subject Index.",1992,0,2141,121,1,2,11,16,29,26,61,44,59,72
96648eb0a740d7bb807518d49d793618683e584b,"A model of isotropic ductile plastic damage based on a continuum damage variable, on the effective stress concept and on thermodynamics is derived. The damage is linear with equivalent strain and shows a large influence of triaxiality by means of a damage equivalent stress. Identification for several metals is made by means of elasticity modulus change induced by damage. A comparison with the McClintock and Rice-Tracey models and with some experiments is presented for the influence of triaxiality on the strain to rupture.",1985,22,2014,135,2,3,15,6,9,12,16,14,22,25
8f4c4dd51fdef827a29660505379122910b14e9e,"In this text the authors develop a propagator theory of Dirac particles, photons, and Klein-Gordon mesons and per- form a series of calculations designed to illustrate various useful techniques and concepts in electromagnetic, weak, and strong interactions. these include defining and implementing the renormalization program and evaluating effects of radia- tive corrections, such as the Lamb shift, in low-order calculations. The necessary background for the book is pro- vided by a course in nonrelativistic quantum mechanics at the general level of Schiff's text, QUANTUM MECHANICS.",1965,0,2566,153,0,2,7,6,11,17,12,27,20,14
f43660d1f7ea3bba4ef18b56d83f84909bf01b88,1. Brownian Motion and Langevin equations 2. Fokker-Planck equations 3. Master equations 4. Reaction rates 5. Kinetic models 6. Quantum dynamics 7. Linear response theory 8. Projection operators 9. Nonlinear problems 10. The paradoxes of irreversibility Appendices,2001,0,1753,118,17,19,36,49,63,74,63,87,113,107
a7e4d45f1adab6c0e6ddaccdabbbc086d9547ef9,,1996,0,1586,213,1,6,10,28,40,31,34,43,33,54
8ba14021ad49e5248f06a6c8ffa8826dc0d5701e,"In the past ten years, the ideas of supersymmetry have been profitably applied to many nonrelativistic quantum mechanical problems. In particular, there is now a much deeper understanding of why certain potentials are analytically solvable and an array of powerful new approximation methods for handling potentials which are not exactly solvable. In this report, we review the theoretical formulation of supersymmetric quantum mechanics and discuss many applications. Exactly solvable potentials can be understood in terms of a few basic ideas which include supersymmetric partner potentials, shape invariance and operator transformations. Familiar solvable potentials all have the property of shape invariance. We describe new exactly solvable shape invariant potentials which include the recently discovered self-similar potentials as a special case. The connection between inverse scattering, isospectral potentials and supersymmetric quantum mechanics is discussed and multisoliton solutions of the KdV equation are constructed. Approximation methods are also discussed within the framework of supersymmetric quantum mechanics and in particular it is shown that a supersymmetry inspired WKB approximation is exact for a class of shape invariant potentials. Supersymmetry ideas give particularly nice results for the tunneling rate in a double well potential and for improving large N expansions. We also discuss the problem of a charged Dirac particle in an external magnetic field and other potentials in terms of supersymmetric quantum mechanics. Finally, we discuss structures more general than supersymmetric quantum mechanics such as parasupersymmetric quantum mechanics in which there is a symmetry between a boson and a para-fermion of order p.",1994,565,2017,67,3,6,12,30,23,38,48,55,63,60
895860c6083736508d2541900cdf0960eb11592f,"The design, implementation, and capabilities of an extensible visualization system, UCSF Chimera, are discussed. Chimera is segmented into a core that provides basic services and visualization, and extensions that provide most higher level functionality. This architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features. Two unusual extensions are presented: Multiscale, which adds the ability to visualize large‐scale molecular assemblies such as viral coats, and Collaboratory, which allows researchers to share a Chimera session interactively despite being at separate locales. Other extensions include Multalign Viewer, for showing multiple sequence alignments and associated structures; ViewDock, for screening docked ligand orientations; Movie, for replaying molecular dynamics trajectories; and Volume Viewer, for display and analysis of volumetric data. A discussion of the usage of Chimera in real‐world situations is given, along with anticipated future directions. Chimera includes full user documentation, is free to academic and nonprofit users, and is available for Microsoft Windows, Linux, Apple Mac OS X, SGI IRIX, and HP Tru64 Unix from http://www.cgl.ucsf.edu/chimera/. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1605–1612, 2004",2004,70,28253,1711,0,0,0,0,1,0,0,0,0,0
d45eaee8b2e047306329e5dbfc954e6dd318ca1e,"This paper gives an overview of ROS, an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and briefly overview some of the available application software which uses ROS.",2009,10,7534,1468,13,90,194,363,487,623,680,800,903,947
6a17ebeeb80cd696bc83a288f1a77ddfc1467079,"Das Buch behandelt die Systemidentifizierung in dem theoretischen Bereich, der direkte Auswirkungen auf Verstaendnis und praktische Anwendung der verschiedenen Verfahren zur Identifizierung hat. Da ...",1987,0,20133,1859,0,0,0,0,0,0,0,1,0,0
915e96dd02d6ca3c989070791b02830fce6d6f63,"A new software suite, called Crystallography & NMR System (CNS), has been developed for macromolecular structure determination by X-ray crystallography or solution nuclear magnetic resonance (NMR) spectroscopy. In contrast to existing structure-determination programs, the architecture of CNS is highly flexible, allowing for extension to other structure-determination methods, such as electron microscopy and solid-state NMR spectroscopy. CNS has a hierarchical structure: a high-level hypertext markup language (HTML) user interface, task-oriented user input files, module files, a symbolic structure-determination language (CNS language), and low-level source code. Each layer is accessible to the user. The novice user may just use the HTML interface, while the more advanced user may use any of the other layers. The source code will be distributed, thus source-code modification is possible. The CNS language is sufficiently powerful and flexible that many new algorithms can be easily implemented in the CNS language without changes to the source code. The CNS language allows the user to perform operations on data structures, such as structure factors, electron-density maps, and atomic properties. The power of the CNS language has been demonstrated by the implementation of a comprehensive set of crystallographic procedures for phasing, density modification and refinement. User-friendly task-oriented input files are available for nearly all aspects of macromolecular structure determination by X-ray crystallography and solution NMR.",1998,61,15365,1217,0,0,1,0,1,0,17,743,1262,1225
7a6142cfa79cc01ceced5e144bd0e01a0f241a74,,2009,43,7886,2012,377,456,500,626,706,655,723,661,657,644
45b98fcf47aa90099d3c921f68c3404af98d7b56,,2002,0,17518,724,0,0,0,0,0,0,0,1,1,2
1c2040faf07594e60bd7dd339cf363a867416ef8,"Part I: Characteristics of Modern Power Systems. Introduction to the Power System Stability Problem. Part II: Synchronous Machine Theory and Modelling. Synchronous Machine Parameters. Synchronous Machine Representation in Stability Studies. AC Transmission. Power System Loads. Excitation in Stability Studies. Prime Mover and Energy Supply Systems. High-Voltage Direct-Current Transmission. Control of Active Power and Reactive Power. Part III: Small Signal Stability. Transient Stability. Voltage Stability. Subsynchronous Machine Representation in Stability Studies. AC Transmission. Power System Loads. Excitation in Stability Studies. Prime Mover and Energy Supply Systems, High-Voltage Direct-Current Transmission. Control of Active Power and Reactive Power. Part III: Small Signal Stability. Transient Stability. Voltage Stability. Subsynchronous Oscillations. Mid-Term and Long-Term Stability. Methods of Improving System Stability.",1994,12,11037,1366,0,0,0,0,0,0,0,0,0,1
0095b6bb7c92f5deeffa8a311b80f75e680325eb,"The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-line in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. >",1993,95,14333,1318,0,0,0,0,0,0,0,0,0,0
839c64b86d978baf5180381c975c0947eacdb7ba,Preface 1. Introduction Overview A Brief History of LMIs in Control Theory Notes on the Style of the Book Origin of the Book 2. Some Standard Problems Involving LMIs. Linear Matrix Inequalities Some Standard Problems Ellipsoid Algorithm Interior-Point Methods Strict and Nonstrict LMIs Miscellaneous Results on Matrix Inequalities Some LMI Problems with Analytic Solutions 3. Some Matrix Problems. Minimizing Condition Number by Scaling Minimizing Condition Number of a Positive-Definite Matrix Minimizing Norm by Scaling Rescaling a Matrix Positive-Definite Matrix Completion Problems Quadratic Approximation of a Polytopic Norm Ellipsoidal Approximation 4. Linear Differential Inclusions. Differential Inclusions Some Specific LDIs Nonlinear System Analysis via LDIs 5. Analysis of LDIs: State Properties. Quadratic Stability Invariant Ellipsoids 6. Analysis of LDIs: Input/Output Properties. Input-to-State Properties State-to-Output Properties Input-to-Output Properties 7. State-Feedback Synthesis for LDIs. Static State-Feedback Controllers State Properties Input-to-State Properties State-to-Output Properties Input-to-Output Properties Observer-Based Controllers for Nonlinear Systems 8. Lure and Multiplier Methods. Analysis of Lure Systems Integral Quadratic Constraints Multipliers for Systems with Unknown Parameters 9. Systems with Multiplicative Noise. Analysis of Systems with Multiplicative Noise State-Feedback Synthesis 10. Miscellaneous Problems. Optimization over an Affine Family of Linear Systems Analysis of Systems with LTI Perturbations Positive Orthant Stabilizability Linear Systems with Delays Interpolation Problems The Inverse Problem of Optimal Control System Realization Problems Multi-Criterion LQG Nonconvex Multi-Criterion Quadratic Problems Notation List of Acronyms Bibliography Index.,1998,173,11571,1282,0,0,0,0,1,3,34,407,473,566
7793cdaf86e9bd3663f965f779a040f0a3073803,"Myeloid-derived suppressor cells (MDSCs) are a heterogeneous population of cells that expand during cancer, inflammation and infection, and that have a remarkable ability to suppress T-cell responses. These cells constitute a unique component of the immune system that regulates immune responses in healthy individuals and in the context of various diseases. In this Review, we discuss the origin, mechanisms of expansion and suppressive functions of MDSCs, as well as the potential to target these cells for therapeutic benefit.",2009,149,5165,463,53,193,330,434,459,496,470,460,474,453
ecdd0f2d494ea181792ed0eb40900a5d2786f9c4,,2009,0,5981,1309,0,0,0,0,0,0,0,5,29,1564
5252e82f5f3b1d6633e2e4560653385af7323768,"An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS.",1996,62,10956,808,0,1,1,0,0,3,0,0,80,330
e04ecae6cba2bce685cff811906c17474c31a8a3,"SummaryThe NMRPipe system is a UNIX software environment of processing, graphics, and analysis tools designed to meet current routine and research-oriented multidimensional processing requirements, and to anticipate and accommodate future demands and developments. The system is based on UNIX pipes, which allow programs running simultaneously to exchange streams of data under user control. In an NMRPipe processing scheme, a stream of spectral data flows through a pipeline of processing programs, each of which performs one component of the overall scheme, such as Fourier transformation or linear prediction. Complete multidimensional processing schemes are constructed as simple UNIX shell scripts. The processing modules themselves maintain and exploit accurate records of data sizes, detection modes, and calibration information in all dimensions, so that schemes can be constructed without the need to explicitly define or anticipate data sizes or storage details of real and imaginary channels during processing. The asynchronous pipeline scheme provides other substantial advantages, including high flexibility, favorable processing speeds, choice of both all-in-memory and disk-bound processing, easy adaptation to different data formats, simpler software development and maintenance, and the ability to distribute processing tasks on multi-CPU computers and computer networks.",1995,99,12827,453,0,0,0,0,0,0,0,0,0,0
120c17ffdb8d9112d0820f12d94722e091b92f92,"Many plant-associated microbes are pathogens that impair plant growth and reproduction. Plants respond to infection using a two-branched innate immune system. The first branch recognizes and responds to molecules common to many classes of microbes, including non-pathogens. The second responds to pathogen virulence factors, either directly or through their effects on host targets. These plant immune systems, and the pathogen molecules to which they respond, provide extraordinary insights into molecular recognition, cell biology and evolution across biological kingdoms. A detailed understanding of plant immune function will underpin crop improvement for food, fibre and biofuels production.",2006,117,8980,809,5,155,248,390,401,477,585,621,716,701
92936ad88a5412bc48b86900da94b08fb7a3eefb,"Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.",2006,54,5335,411,9,32,82,182,336,447,490,595,631,613
593619c2a69391454eae1f5ebe75fb8fc7e77e9d,"The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.",1978,7,9541,499,7,15,24,13,37,43,34,48,50,75
4af77aafa93f810e403461e5ee911287aa16d76e,"A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks.",1986,20,9119,461,3,16,44,86,152,164,212,208,287,326
bb01353f818ca226b53433163893efc56c3df32d,"The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF)-based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable location-aware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy.",2000,36,8416,939,18,41,70,138,237,348,377,446,474,558
fee623ce8cb964a53cc1849007758aa5a32490b7,"A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanism--the mirror-neuron mechanism--that appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language.",2004,142,6789,408,10,82,166,256,351,428,450,483,570,552
fbf58da8ebd072bb1fcb43683a2bf6490fe79c31,"Plasticity and functional polarization are hallmarks of the mononuclear phagocyte system. Here we review emerging key properties of different forms of macrophage activation and polarization (M1, M2a, M2b, M2c), which represent extremes of a continuum. In particular, recent evidence suggests that differential modulation of the chemokine system integrates polarized macrophages in pathways of resistance to, or promotion of, microbial pathogens and tumors, or immunoregulation, tissue repair and remodeling.",2004,94,4964,371,1,29,77,108,150,176,194,263,319,341
a3223f1fa1718bf3913753cabda44d854392287b,,2002,0,8918,660,41,68,84,157,237,344,390,470,609,628
ab5716fcabcdf2a1359d7140913e140287190cf0,"The fourth edition of the World Health Organization (WHO) classification of tumours of the central nervous system, published in 2007, lists several new entities, including angiocentric glioma, papillary glioneuronal tumour, rosette-forming glioneuronal tumour of the fourth ventricle, papillary tumour of the pineal region, pituicytoma and spindle cell oncocytoma of the adenohypophysis. Histological variants were added if there was evidence of a different age distribution, location, genetic profile or clinical behaviour; these included pilomyxoid astrocytoma, anaplastic medulloblastoma and medulloblastoma with extensive nodularity. The WHO grading scheme and the sections on genetic profiles were updated and the rhabdoid tumour predisposition syndrome was added to the list of familial tumour syndromes typically involving the nervous system. As in the previous, 2000 edition of the WHO ‘Blue Book’, the classification is accompanied by a concise commentary on clinico-pathological characteristics of each tumour type. The 2007 WHO classification is based on the consensus of an international Working Group of 25 pathologists and geneticists, as well as contributions from more than 70 international experts overall, and is presented as the standard for the definition of brain tumours to the clinical oncology and cancer research communities world-wide.",2007,73,11361,223,0,0,0,5,582,806,872,967,1166,1130
cb4368eab957ded0ce8c587c4fda165a26bc5a22,"This paper introduces the ant colony system (ACS), a distributed algorithm that is applied to the traveling salesman problem (TSP). In the ACS, a set of cooperating agents called ants cooperate to find good solutions to TSPs. Ants cooperate using an indirect form of communication mediated by a pheromone they deposit on the edges of the TSP graph while building solutions. We study the ACS by running experiments to understand its operation. The results show that the ACS outperforms other nature-inspired algorithms such as simulated annealing and evolutionary computation, and we conclude comparing ACS-3-opt, a version of the ACS augmented with a local search procedure, to some of the best performing algorithms for symmetric and asymmetric TSPs.",1997,72,7445,724,14,26,44,49,65,111,98,186,209,299
b7ebcec93f35e52ef14a41c9c9d55fa132987473,"Abstract In the first of two papers on MAGMA , a new system for computational algebra, we present the MAGMA language, outline the design principles and theoretical background, and indicate its scope and use. Particular attention is given to the constructors for structures, maps, and sets.",1997,17,5714,644,9,11,30,38,57,53,61,78,83,121
4e9ec92a90c5d571d2f1d496f8df01f0a8f38596,"A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.",2008,80,4446,727,0,1,1,4,11,30,66,83,121,234
895ba2d6b9c6eae6d61c48e842a57d05e95b42c8,"Representing the first volume in the fourth edition series of the World Health Organization (WHO) Classification of Tumours, this book provides a welcome mix of old and new. Similar to prior versions, it opens with a summary of the recently revised WHO Classification of Tumours of the Central Nervous System (CNS), the remainder of the book being dedicated to a comprehensive yet succinct presentation of the most current knowledge relative to each individual tumor and familial tumor syndrome. The 73 contributing authors have likewise adopted a familiar standardized format with the following subheadings: definition, grading, incidence, age and sex distribution, localization, clinical features, neuroimaging, macroscopy, histopathology, proliferation, genetic susceptibility, genetics, histogenesis, and prognostic and predictive factors. Although a fair number of images have been recycled from previous editions, the majority is new and includes more than 400 full-color photomicrographs and macroscopic images, as well as numerous neuroimages, informative diagrams and charts. A number of tumor entities new to this version of the WHO Classification are explored in detail, including pilomyxoid astrocytoma, atypical choroid plexus papilloma, angiocentric glioma, extraventricular neurocytoma, papillary glioneuronal tumor, rosetteforming glioneuronal tumor of the fourth ventricle, papillary tumor of the pineal region, pituicytoma, and spindle cell oncocytoma of the adenohypophysis. Perhaps the most noticeable improvement comes by way of a voluminous expansion in the genetics sections of the majority of tumor categories. This update parallels the recent explosion of research utilizing high-resolution genome screening and other molecular techniques. The authors have done an outstanding job in distilling the information housed in over 2,500 cited references into a readerfriendly authoritative reference of CNS neoplasia. In summation, the current edition of the WHO Classification of Tumours of the Central Nervous System will serve as an indispensable textbook for all of those involved in the diagnosis and management of patients with tumors of the CNS, and will make a valuable addition to libraries in pathology, radiology, oncology, and neurosurgery departments.",2007,0,4321,324,14,112,212,263,387,435,428,442,495,388
62d36f23580ae0c822ebc7de69ae603d85441bfc,"The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the morphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.",1986,57,5014,476,7,4,9,14,27,28,33,35,30,54
88779104d045f4f319d17184368ee12431a0ce82,"Face perception, perhaps the most highly developed visual skill in humans, is mediated by a distributed neural system in humans that is comprised of multiple, bilateral regions. We propose a model for the organization of this system that emphasizes a distinction between the representation of invariant and changeable aspects of faces. The representation of invariant aspects of faces underlies the recognition of individuals, whereas the representation of changeable aspects of faces, such as eye gaze, expression, and lip movement, underlies the perception of information that facilitates social communication. The model is also hierarchical insofar as it is divided into a core system and an extended system. The core system is comprised of occipitotemporal regions in extrastriate visual cortex that mediate the visual analysis of faces. In the core system, the representation of invariant aspects is mediated more by the face-responsive region in the fusiform gyrus, whereas the representation of changeable aspects is mediated more by the face-responsive region in the superior temporal sulcus. The extended system is comprised of regions from neural systems for other cognitive functions that can be recruited to act in concert with the regions in the core system to extract meaning from faces.",2000,102,4221,417,5,36,55,77,85,89,124,165,168,230
634c9fde5f1c411e4487658ac738dcf18d98ea8d,"Information theory has recently been employed to specify more precisely than has hitherto been possible man's capacity in certain sensory, perceptual, and perceptual-motor functions (5, 10, 13, 15, 17, 18). The experiments reported in the present paper extend the theory to the human motor system. The applicability of only the basic concepts, amount of information, noise, channel capacity, and rate of information transmission, will be examined at this time. General familiarity with these concepts as formulated by recent writers (4, 11,20, 22) is assumed. Strictly speaking, we cannot study man's motor system at the behavioral level in isolation from its associated sensory mechanisms. We can only analyze the behavior of the entire receptor-neural-effector system. However, by asking 51 to make rapid and uniform responses that have been highly overlearned, and by holding all relevant stimulus conditions constant with the exception of those resulting from 5""s own movements, we can create an experimental situation in which it is reasonable to assume that performance is limited primarily by the capacity of the motor system. The motor system in the present case is defined as including the visual and proprioceptive feedback loops that permit S to monitor his own activity. The information capacity of the motor system is specified by its ability to produce consistently one class of movement from among several alternative movement classes. The greater the number of alternative classes, the greater is the information capacity of a particular type of response. Since measurable aspects of motor responses, such as their force, direction, and amplitude, are continuous variables, their information capacity is limited only by the amount of statistical variability, or noise, that is characteristic of repeated efforts to produce the same response. The information capacity of the motor Editor's Note. This article is a reprint of an original work published in 1954 in the Journal of Experimental Psychology, 47, 381391.",1954,28,7604,848,0,2,1,1,4,2,1,3,1,3
770b40c2c84bf7a3541b5c7fdd9fbd8842740156,A new total knee rating system has been developed by The Knee Society to provide an up-to-date more stringent evaluation form. The system is subdivided into a knee score that rates only the knee joint itself and a functional score that rates the patient's ability to walk and climb stairs. The dual rating system eliminates the problem of declining knee scores associated with patient infirmity.,1989,0,4341,368,0,0,3,6,12,9,17,32,43,47
571dd9820e128c44f6fb6d09f2d17eb021b2af8f,,1996,0,7975,439,1,23,94,173,271,342,432,534,618,610
b39b864606b8c0cf8e223e4d67554503ef33e010,"Ever since Richard Stone (1954) first estimated a system of demand equations derived explicitly from consumer theory, there has been a continuing search for alternative specifications and functional forms. Many models have been proposed, but perhaps the most important in current use, apart from the original linear expenditure system, are the Rotterdam model (see Henri Theil, 1965, 1976; Anton Barten) and the translog model (see Laurits Christensen, Dale Jorgenson, and Lawrence Lau; Jorgenson and Lau). Both of these models have been extensively estimated and have, in addition, been used to test the homogeneity and symmetry restrictions of demand theory. In this paper, we propose and estimate a new model which is of comparable generality to the Rotterdam and translog models but which has considerable advantages over both. Our model, which we call the Almost Ideal Demand System (AIDS), gives an arbitrary first-order approximation to any demand system; it satisfies the axioms of choice exactly; it aggregates perfectly over consumers without invoking parallel linear Engel curves; it has a functional form which is consistent with known household-budget data; it is simple to estimate, largely avoiding the need for non-linear estimation; and it can be used to test the restrictions of homogeneity and symmetry through linear restrictions on fixed parameters. Although many of these desirable properties are possessed by one or other of the Rotterdam or translog models, neither possesses all of them simultaneously. In Section I of the paper, we discuss the theoretical specification of the AIDS and justify the claims in the previous paragraph. In Section II, the model is estimated on postwar British data and we use our results to test the homogeneity and symmetry restrictions. Our results are consistent with earlier findings in that both sets of restrictions are decisively rejected. We also find that imposition of homogeneity generates positive serial correlation in the errors of those equations which reject the restrictions most strongly; this suggests that the now standard rejection of homogeneity in demand analysis may be due to insufficient attention to the dynamic aspects of consumer behavior. Finally, in Section III, we offer a summary and conclusions. We believe that the results of this paper suggest that the AIDS is to be recommended as a vehicle for testing, extending, and improving conventional demand analysis. This does not imply that the system, particularly in its simple static form, is to be regarded as a fully satisfactory explanation of consumers' behavior. Indeed, by proposing a demand system which is superior to its predecessors, we hope to be able to reveal more clearly the problems and potential solutions associated with the usual approach.",1980,27,4512,431,4,3,12,16,32,24,18,35,42,48
71b9b9c5d7fd73986af47a6515dcbab246f3aea5,"Despite multiple disparate prognostic risk analysis systems for evaluating clinical outcome for patients with myelodysplastic syndrome (MDS), imprecision persists with such analyses. To attempt to improve on these systems, an International MDS Risk Analysis Workshop combined cytogenetic, morphological, and clinical data from seven large previously reported risk-based studies that had generated prognostic systems. A global analysis was performed on these patients, and critical prognostic variables were re-evaluated to generate a consensus prognostic system, particularly using a more refined bone marrow (BM) cytogenetic classification. Univariate analysis indicated that the major variables having an impact on disease outcome for evolution to acute myeloid leukemia were cytogenetic abnormalities, percentage of BM myeloblasts, and number of cytopenias; for survival, in addition to the above, variables also included age and gender. Cytogenetic subgroups of outcome were as follows: ""good"" outcomes were normal, -Y alone, del(5q) alone, del(20q) alone; ""poor"" outcomes were complex (ie, > or = 3 abnormalities) or chromosome 7 anomalies; and ""intermediate"" outcomes were other abnormalities. Multivariate analysis combined these cytogenetic subgroups with percentage of BM blasts and number of cytopenias to generate a prognostic model. Weighting these variables by their statistical power separated patients into distinctive subgroups of risk for 25% of patients to undergo evolution to acute myeloid leukemia, with: low (31% of patients), 9.4 years; intermediate-1 (INT-1; 39%), 3.3 years; INT-2 (22%), 1.1 years; and high (8%), 0.2 year. These features also separated patients into similar distinctive risk groups for median survival: low, 5.7 years; INT-1, 3.5 years; INT-2, 1.2 years; and high, 0.4 year. Stratification for age further improved analysis of survival. Compared with prior risk-based classifications, this International Prognostic Scoring System provides an improved method for evaluating prognosis in MDS. This classification system should prove useful for more precise design and analysis of therapeutic trials in this disease.",1997,58,4016,364,10,43,60,80,90,110,92,121,143,170
89662b1305051451a9ab3ece083961921092a063,"Abstract Lack of user acceptance has long been an impediment to the success of new information systems. The present research addresses why users accept or reject information systems and how user acceptance is affected by system design features. The technology acceptance model (TAM) specifies the causal relationships between system design features, perceived usefulness, perceived ease of use, attitude toward using, and actual usage behavior. Attitude theory from psychology provides the rationale for hypothesized model relationships, and validated measures were used to operationalize model variables. A field study of 112 users regarding two end-user systems was conducted to test the hypothesized model. TAM fully mediated the effects of system characteristics on usage behavior, accounting for 36% of the variance in usage. Perhaps the most striking finding was that perceived usefulness was 50% more influential than ease of use in determining usage, underscoring the importance of incorporating the appropriate functional capabilities in new systems. Overall, TAM provides an informative representation of the mechanisms by which design choices influence user acceptance, and should therefore be helpful in applied contexts for forecasting and evaluating user acceptance of information technology. Implications for future research and practice are discussed.",1993,0,3836,453,2,2,9,10,18,14,24,26,27,51
7b532c5c43e4903b46d9ebff8be7e878ff3aad57,"New information regarding neuronal circuits that control food intake and their hormonal regulation has extended our understanding of energy homeostasis, the process whereby energy intake is matched to energy expenditure over time. The profound obesity that results in rodents (and in the rare human case as well) from mutation of key signalling molecules involved in this regulatory system highlights its importance to human health. Although each new signalling pathway discovered in the hypothalamus is a potential target for drug development in the treatment of obesity, the growing number of such signalling molecules indicates that food intake is controlled by a highly complex process. To better understand how energy homeostasis can be achieved, we describe a model that delineates the roles of individual hormonal and neuropeptide signalling pathways in the control of food intake and the means by which obesity can arise from inherited or acquired defects in their function.",2000,142,5981,317,39,151,224,225,291,279,313,330,309,333
20996231727b9d11d66ed092efef86054024214c,"Techniques to determine changing system complexity from data are evaluated. Convergence of a frequently used correlation dimension algorithm to a finite value does not necessarily imply an underlying deterministic model or chaos. Analysis of a recently developed family of formulas and statistics, approximate entropy (ApEn), suggests that ApEn can classify complex systems, given at least 1000 data values in diverse settings that include both deterministic chaotic and stochastic processes. The capability to discern changing complexity from such a relatively small amount of data holds promise for applications of ApEn in a variety of contexts.",1991,5,4509,351,1,4,6,7,13,17,24,48,33,41
2a16fe9680001fcade8b4ccf04ab458028dee80e,"Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.",2000,47,3894,366,12,30,117,204,278,379,370,368,320,312
e5616898a40b7c7356f7ea6bf49d16227b07abfe,,2003,6,5024,488,4,34,57,67,113,178,246,360,368,436
56c16d9e2a5270ba6b1d83271e2c10916591968d,"Publisher Summary This chapter presents a general theoretical framework of human memory and describes the results of a number of experiments designed to test specific models that can be derived from the overall theory. This general theoretical framework categorizes the memory system along two major dimensions. The first categorization distinguishes permanent, structural features of the system from control processes that can be readily modified or reprogrammed at the will of the subject. The second categorization divides memory into three structural components: the sensory register, the short-term store, and the long-term store. Incoming sensory information first enters the sensory register, where it resides for a very brief period of time, then decays and is lost. The short-term store is the subject's working memory; it receives selected inputs from the sensory register and also from long-term store. The chapter also discusses the control processes associated with the sensory register. The term control process refers to those processes that are not permanent features of memory, but are instead transient phenomena under the control of the subject; their appearance depends on several factors such as instructional set, the experimental task, and the past history of the subject.",1968,92,6111,322,12,32,61,55,47,66,79,71,54,64
418373ac211f44e50ad148e7e1368a51babfca01,"for competition that is based on information, their ability to exploit intangible assets has become far more decisive than their ability to invest in and manage physical assets. Several years ago, in recognition of this change, we introduced a concept we called the balanced scorecard. The balanced scorecard supplemented traditional fi nancial measures with criteria that measured performance from three additional perspectives – those of customers, internal business processes, and learning and growth. (See the exhibit “Translating Vision and Strategy: Four Perspectives.”) It therefore enabled companies to track fi nancial results while simultaneously monitoring progress in building the capabilities and acquiring the intangible assets they would need for future growth. The scorecard wasn’t Editor’s Note: In 1992, Robert S. Kaplan and David P. Norton’s concept of the balanced scorecard revolutionized conventional thinking about performance metrics. By going beyond traditional measures of fi nancial performance, the concept has given a generation of managers a better understanding of how their companies are really doing. These nonfi nancial metrics are so valuable mainly because they predict future fi nancial performance rather than simply report what’s already happened. This article, fi rst published in 1996, describes how the balanced scorecard can help senior managers systematically link current actions with tomorrow’s goals, focusing on that place where, in the words of the authors, “the rubber meets the sky.” Using the Balanced Scorecard as a Strategic Management System",1996,0,5312,443,9,32,38,63,79,98,94,128,132,170
0683fa33bb1ebb0b65b3bb6e8b0ea6272c3b5dce,"To address the need for a standardized system to classify the gross motor function of children with cerebral palsy, the authors developed a five‐level classification system analogous to the staging and grading systems used in medicine. Nominal group process and Delphi survey consensus methods were used to examine content validity and revise the classification system until consensus among 48 experts (physical therapists, occupational therapists, and developmental pediatricians with expertise in cerebral palsy) was achieved. Interrater reliability (k) was 0.55 for children less than 2 years of age and 0.75 for children 2 to 12 years of age. The classification system has application for clinical practice, research, teaching, and administration.",1997,15,4614,320,0,6,7,21,39,55,59,96,112,151
902be51a420f734b6ab033fe4cea69189a6297c4,"Introductory immunology textbook for medical students, advanced undergraduates, and graduate students.",1996,0,4560,398,11,35,55,73,83,109,132,155,176,184
eb51cb223fb17995085af86ac70f765077720504,"We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users.",2002,11,3209,423,11,56,80,112,163,230,229,278,284,282
3e7bf1c10181e243d5f4a69c4b845a2bdddf434a,"Enterprise systems present a new model of corporate computing. They allow companies to replace their existing information systems, which are often incompatible with one another, with a single, integrated system. By streamlining data flows throughout an organization, these commercial software packages, offered by vendors like SAP, promise dramatic gains in a company's efficiency and bottom line. It's no wonder that businesses are rushing to jump on the ES bandwagon. But while these systems offer tremendous rewards, the risks they carry are equally great. Not only are the systems expensive and difficult to implement, they can also tie the hands of managers. Unlike computer systems of the past, which were typically developed in-house with a company's specific requirements in mind, enterprise systems are off-the-shelf solutions. They impose their own logic on a company's strategy, culture, and organization, often forcing companies to change the way they do business. Managers would do well to heed the horror stories of failed implementations. FoxMeyer Drug, for example, claims that its system helped drive it into bankruptcy. Drawing on examples of both successful and unsuccessful ES projects, the author discusses the pros and cons of implementing an enterprise system, showing how a system can produce unintended and highly disruptive consequences. Because of an ES's profound business implications, he cautions against shifting responsibility for its adoption to technologists. Only a general manager will be able to mediate between the imperatives of the system and the imperatives of the business.",1998,0,3721,430,4,37,86,88,108,143,161,176,218,211
b1f3590c426f50d8320977f646479d4490e94f13,"Classification systems are necessary in order to provide a framework in which to scientifically study the etiology, pathogenesis, and treatment of diseases in an orderly fashion. In addition, such systems give clinicians a way to organize the health care needs of their patients. The last time scientists and clinicians in the field of periodontology and related areas agreed upon a classification system for periodontal diseases was in 1989 at the World Workshop in Clinical Periodontics. Subsequently, a simpler classification was agreed upon at the 1st European Workshop in Periodontology. These classification systems have been widely used by clinicians and research scientists throughout the world. Unfortunately, the 1989 classification had many shortcomings, including: (1) considerable overlap in disease categories, (2) absence of a gingival disease component, (3) inappropriate emphasis on age of onset of disease and rates of progression, and (4) inadequate or unclear classification criteria. The 1993 European classification lacked the detail necessary for adequate characterization of the broad spectrum of periodontal diseases encountered in clinical practice. The need for a revised classification system for periodontal diseases was emphasized during the 1996 World Workshop in Periodontics. In 1997 the American Academy of Periodontology responded to this need and formed a committee to plan and organize an international workshop to revise the classification system for periodontal diseases. The proceedings in this volume are the result of this reclassification effort. The process involved development by the Organizing Committee of an outline for a new classification and identification of individuals to write state-of-the-science reviews for each of the items on the outline. The reviewers were encouraged to depart from the preliminary outline if there were data to support any modifications. On October 30-November 2, 1999, the International Workshop for a Classification of Periodontal Diseases and Conditions was held and a new classification was agreed upon (Figure 1). This paper summarizes how the new classification for periodontal diseases and conditions presented in this volume differs from the classification system developed at the 1989 World Workshop in Clinical Periodontics. In addition, an analysis of the rationale is provided for each of the modifications and changes.",1999,33,4401,232,2,4,26,44,47,67,91,102,102,144
7c95d8fe969693dd71573dd803493d0869a915bd,"A model of the neuropsychology of anxiety is proposed. The model is based in the first instance upon an analysis of the behavioural effects of the antianxiety drugs (benzodiazepines, barbiturates, and alcohol) in animals. From such psychopharmacologi-cal experiments the concept of a “behavioural inhibition system” (BIS) has been developed. This system responds to novel stimuli or to those associated with punishment or nonreward by inhibiting ongoing behaviour and increasing arousal and attention to the environment. It is activity in the BIS that constitutes anxiety and that is reduced by antianxiety drugs. The effects of the antianxiety drugs in the brain also suggest hypotheses concerning the neural substrate of anxiety. Although the benzodiazepines and barbiturates facilitate the effects of γ-aminobutyrate, this is insufficient to explain their highly specific behavioural effects. Because of similarities between the behavioural effects of certain lesions and those of the antianxiety drugs, it is proposed that these drugs reduce anxiety by impairing the functioning of a widespread neural system including the septo-hippocampal system (SHS), the Papez circuit, the prefrontal cortex, and ascending monoaminergic and cholinergic pathways which innervate these forebrain structures. Analysis of the functions of this system (based on anatomical, physiological, and behavioural data) suggests that it acts as a comparator: it compares predicted to actual sensory events and activates the outputs of the BIS when there is a mismatch or when the predicted event is aversive. Suggestions are made as to the functions of particular pathways within this overall brain system. The resulting theory is applied to the symptoms and treatment of anxiety in man, its relations to depression, and the personality of individuals who are susceptible to anxiety or depression.",1982,369,4399,239,62,22,33,104,51,59,51,55,95,93
0bd6e70ab6c2d131bbda7ef24d552709e6b6455b,"This book provides a basic treatment of one of the most widely used operations research tools: discrete-event simulation. Prerequisites are calculus, probability theory, and elementary statistics. Contents, abridged: Introduction to discrete-event system simulation. Mathematical and statistical models. Random numbers. Analysis of simulation data. Index.",1995,0,4045,264,26,53,49,44,48,47,75,86,110,148
53a7cf6bf8568c660240c080125e55836d507098,"During the First International EEG Congress, London in 1947, it was recommended that Dr. Herbert H. Jasper study methods to standardize the placement of electrodes used in EEG (Jasper 1958). A report with recommendations was to be presented to the Second International Congress in Paris in 1949. The electrode placement systems in use at various centers were found to be similar, with only minor differences, although their designations, letters and numbers were entirely different. Dr. Jasper established some guidelines which would be established in recommending a speci®c system to the federation and these are listed below.",1999,4,6060,213,117,102,112,109,141,179,188,194,216,210
a6d38c2f78a12b92464ef95b89d0567e01262631,"Grey System theory was initiated in 1982 [7]. As far as information is concerned, the systems which lack information, such as structure message, operation mechanism and behaviour document, are referred to as Grey Systems. For example, the human body, agriculture, economy, etc., are Grey Systems. Usually, on the grounds of existing grey relations, grey elements, grey numbers (denoted by 8 ) one can identify which Grey System is, where ""grey"" means poor, incomplete, uncertain, etc. The goal of Grey Systeni and its applications is to bridge the gap existing between social science and natural science. Thus, one can say that the Grey System theory is interdisciplinary, cutting across a variety of specialized fields, and it is evident that Grey System theory stands the test of time since 1982. As the case stands, the developn~ent of the Grey System-as well as theoretical topic-is coupled with clear applications of the theory in assorted fields. The conccept of the Grey System, in its theory and successful application, is now well known in China. The application fields of the Grey System involve agriculture [23, 77-81, 911, ecology [59], economy [61, 102, 103, 1041, meteorology [58, 74,911, medicine [55, 891, history [63, 641, geography [I], industry [61, earthquake [73, 87, 881, geology [76, 1 191, hydrology [98, 1 121, irrigation strategy [261, military affairs, sports [116], traffic [67], management [30, 97, 1051, material science [82, 831, environment [ 1081, biological protection [69,70], judicial system [loo], etc. Projects which have been successfully completed with the Grey System theory and its applications are as follows: 1. Regional econonlic planning for several provinces in China; 2. To forecast yields of grain for some provinces in China:",1989,239,3896,279,1,1,0,0,0,1,4,4,8,11
5f8994ef56c66605c45c9a5e727c6cb1a3af28c1,ion “101” An Introduction for New Abstractors,2002,3,6575,254,104,130,183,231,287,256,277,328,330,354
9b88e58fa34269c31a0f8fe823f2bf96824fadc4,"This paper presents the form and validation results of APACHE II, a severity of disease classification system. APACHE II uses a point score based upon initial values of 12 routine physiologic measurements, age, and previous health status to provide a general measure of severity of disease. An increasing score (range 0 to 71) was closely correlated with the subsequent risk of hospital death for 5815 intensive care admissions from 13 hospitals. This relationship was also found for many common diseases.When APACHE II scores are combined with an accurate description of disease, they can prognostically stratify acutely ill patients and assist investigators comparing the success of new or differing forms of therapy. This scoring index can be used to evaluate the use of hospital resources and compare the efficacy of intensive care in different hospitals or over time.",1985,1,4957,230,1,1,1,7,5,12,7,8,17,14
9aa87ac3947fc59e58eb3eef2fad1b27709a74cc,"It is shown that the free energy of a volume V of an isotropic system of nonuniform composition or density is given by : NV∫V [f 0(c)+κ(▿c)2]dV, where NV is the number of molecules per unit volume, ▿c the composition or density gradient, f 0 the free energy per molecule of a homogeneous system, and κ a parameter which, in general, may be dependent on c and temperature, but for a regular solution is a constant which can be evaluated. This expression is used to determine the properties of a flat interface between two coexisting phases. In particular, we find that the thickness of the interface increases with increasing temperature and becomes infinite at the critical temperature Tc , and that at a temperature T just below Tc the interfacial free energy σ is proportional to (T c −T) 3 2 . The predicted interfacial free energy and its temperature dependence are found to be in agreement with existing experimental data. The possibility of using optical measurements of the interface thickness to provide an additional check of our treatment is briefly discussed.",1958,16,7571,211,1,4,4,4,7,6,2,4,9,8
a27087636a2708771f654fa175f406c63a9272be,"A description of the ab initio quantum chemistry package GAMESS is presented. Chemical systems containing atoms through radon can be treated with wave functions ranging from the simplest closed‐shell case up to a general MCSCF case, permitting calculations at the necessary level of sophistication. Emphasis is given to novel features of the program. The parallelization strategy used in the RHF, ROHF, UHF, and GVB sections of the program is described, and detailed speecup results are given. Parallel calculations can be run on ordinary workstations as well as dedicated parallel machines. © John Wiley & Sons, Inc.",1993,131,15509,135,0,0,0,0,0,0,0,0,0,0
1ed876aa17c32dfb8ca7b41ed506a44e976be6ed,"The framework of a national land use and land cover classification system is presented for use with remote sensor data. The classification system has been developed to meet the needs of Federal and State agencies for an up-to-date overview of land use and land cover throughout the country on a basis that is uniform in categorization at the more generalized first and second levels and that will be receptive to data from satellite and aircraft remote sensors. The pro-posed system uses the features of existing widely used classification systems that are amenable to data derived from re-mote sensing sources. It is intentionally left open-ended so that Federal, regional, State, and local agencies can have flexibility in developing more detailed land use classifications at the third and fourth levels in order to meet their particular needs and at the same time remain compatible with each other and the national system. Revision of the land use classification system as presented in US Geological Survey Circular 671 was undertaken in order to incorporate the results of extensive testing and review of the categorization and definitions.",1976,90,3909,226,2,9,8,7,8,13,13,11,8,23
35c0183e9940feb567b4417115be8460bc127cfa,"From the Publisher: 
An extensive revision of the author's highly successful text, this third edition of Linear System Theory and Design has been made more accessible to students from all related backgrounds. After introducing the fundamental properties of linear systems, the text discusses design using state equations and transfer functions. The two main objectives of the text are to: use simple and efficient methods to develop results and design procedures; enable students to employ the results to carry out design. Striking a balance between theory and applications, Linear System Theory and Design, 3/e, is ideal for use in advanced undergraduate/first-year graduate courses in linear systems and multivariable system design in electrical, mechanical, chemical, and aeronautical engineering departments. It assumes a working knowledge of linear algebra and the Laplace transform and an elementary knowledge of differential equations.",1995,0,3664,232,83,83,75,62,67,69,65,73,105,108
c3777a30ff0679b56f712324c93ad1b0ecb4ed46,"PROTEIN-protein interactions between two proteins have generally been studied using biochemical techniques such as crosslinking, co-immunoprecipitation and co-fractionation by chromatography. We have generated a novel genetic system to study these interactions by taking advantage of the properties of the GAL4 protein of the yeast Saccharomyces cerevisiae. This protein is a transcriptional activator required for the expression of genes encoding enzymes of galactose utilization1. It consists of two separable and functionally essential domains: an N-terminal domain which binds to specific DNA sequences (UASG); and a C-terminal domain containing acidic regions, which is necessary to activate transcription2,3. We have generated a system of two hybrid proteins containing parts of GAL4: the GAL4 DNA-binding domain fused to a protein 'X' and a GAL4 activating region fused to a protein 'Y'. If X and Y can form a protein-protein complex and reconstitute proximity of the GAL4 domains, tran-scription of a gene regulated by UASG occurs. We have tested this system using two yeast proteins that are known to interactSNF1 and SNF4. High transcriptional activity is obtained only when both hybrids are present in a cell. This system may be applicable as a general method to identify proteins that interact with a known protein by the use of a simple galactose selection.",1989,17,6131,199,0,5,5,23,50,104,196,254,330,332
007524794d49bebca5845722054e459a86d8b785,"Mobile users' data rate and quality of service are limited by the fact that, within the duration of any given call, they experience severe variations in signal attenuation, thereby necessitating the use of some type of diversity. In this two-part paper, we propose a new form of spatial diversity, in which diversity gains are achieved via the cooperation of mobile users. Part I describes the user cooperation strategy, while Part II (see ibid., p.1939-48) focuses on implementation issues and performance analysis. Results show that, even though the interuser channel is noisy, cooperation leads not only to an increase in capacity for both users but also to a more robust system, where users' achievable rates are less susceptible to channel variations.",2003,38,6553,177,18,86,204,323,469,599,707,784,644,600
0dcd97c4e2a91e6b8f8ea8ac2f5da393a49a96ab,"This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.",2000,34,4252,182,7,57,84,135,204,274,280,350,329,344
a5c333a7fbca00121d9221c107d2a82d3b582b71,"Solar photospheric and meteoritic CI chondrite abundance determinations for all elements are summarized and the best currently available photospheric abundances are selected. The meteoritic and solar abundances of a few elements (e.g., noble gases, beryllium, boron, phosphorous, sulfur) are discussed in detail. The photospheric abundances give mass fractions of hydrogen (X ¼ 0:7491), helium (Y ¼ 0:2377), and heavy elements (Z ¼ 0:0133), leading to Z=X ¼ 0:0177, which is lower than the widely used Z=X ¼ 0:0245 from previous compilations. Recent results from standard solar models considering helium and heavy-element settling imply that photospheric abundances and mass fractions are not equal to protosolar abundances (representative of solar system abundances). Protosolar elemental and isotopic abundances are derived from photospheric abundances by considering settling effects. Derived protosolar mass fractions are X0 ¼ 0:7110, Y0 ¼ 0:2741, and Z0 ¼ 0:0149. The solar system and photospheric abundance tables are used to compute self-consistent sets of condensation temperatures for all elements. Subject headings: astrochemistry — meteors, meteoroids — solar system: formation — Sun: abundances — Sun: photosphere",2003,96,3163,739,10,44,98,123,144,132,165,174,147,192
2b78a95be01f0d7f9faa592d81ce40e4330c1afc,"Second in a series of publications from the Institute of Medicine's Quality of Health Care in America project Today's health care providers have more research findings and more technology available to them than ever before. Yet recent reports have raised serious doubts about the quality of health care in America. Crossing the Quality Chasm makes an urgent call for fundamental change to close the quality gap. This book recommends a sweeping redesign of the American health care system and provides overarching principles for specific direction for policymakers, health care leaders, clinicians, regulators, purchasers, and others. In this comprehensive volume the committee offers: A set of performance expectations for the 21st century health care system. A set of 10 new rules to guide patient-clinician relationships. A suggested organizing framework to better align the incentives inherent in payment and accountability with improvements in quality. Key steps to promote evidence-based practice and strengthen clinical information systems. Analyzing health care organizations as complex systems, Crossing the Quality Chasm also documents the causes of the quality gap, identifies current practices that impede quality care, and explores how systems approaches can be used to implement change.",2001,0,13771,65,0,0,0,1,0,0,0,2,428,753
dc139f901c869f80b54b41f89d5b7f35c7dfa3c7,"Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >",1995,11,4293,179,9,88,183,237,295,272,248,281,241,240
94f1cbb8e88f524efe2a9034e25c1f83a6b2abf3,Illustration de trois fonctions principales qui sont predominantes dans l'etude de l'intervention de l'attention dans les processus cognitifs: 1) orientation vers des evenements sensoriels; 2) detection des signaux par processus focal; 3) maintenir la vigilance en etat d'alerte,1990,87,5550,190,11,29,93,90,111,131,115,154,159,156
983bee5c523b2b99f36a027772e67c1c2499ab31,"The adhesive interactions of cells with other cells and with the extracellular matrix are crucial to all developmental processes, but have a central role in the functions of the immune system throughout life. Three families of cell-surface molecules regulate the migration of lymphocytes and the interactions of activated cells during immune responses.",1990,158,6415,162,19,229,443,509,508,443,485,397,426,322
bcfe59f68a5b6883db9497374cba8cafacc355f4,"Never ever burnt out to improve your knowledge by reviewing book. Currently, we present you an exceptional reading e-book entitled International System For Human Cytogenetic Nomenclature panamabustickets.com Study Group has writer this book completely. So, just review them online in this click switch and even download them to allow you check out anywhere. Still confused ways to review? Find them and also make choice for data format in pdf, ppt, zip, word, rar, txt, and also kindle. i international legal protection of human rights ohchr 2 international legal protection of human rights in armed conflict this publication provides a thorough legal analysis and guidance to state authorities, human rights and humanitarian actors and others",1978,0,7421,128,0,7,19,25,43,47,41,74,83,122
13241a844c714549c173e239714ae020386172e3,"The authors describe a model of autobiographical memory in which memories are transitory mental constructions within a self-memory system (SMS). The SMS contains an autobiographical knowledge base and current goals of the working self. Within the SMS, control processes modulate access to the knowledge base by successively shaping cues used to activate autobiographical memory knowledge structures and, in this way, form specific memories. The relation of the knowledge base to active goals is reciprocal, and the knowledge base ""grounds"" the goals of the working self. It is shown how this model can be used to draw together a wide range of diverse data from cognitive, social, developmental, personality, clinical, and neuropsychological autobiographical memory research.",2000,307,3150,301,7,26,41,51,76,64,96,114,142,161
17cc1d6be6ed398af518fef32ba4e37a389c9dbb,"A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.",1992,17,4254,171,6,19,29,25,31,25,43,75,72,102
536f48a2755969d2bc269302ae2698f9496a4619,"Dendritic cells are a system of antigen presenting cells that function to initiate several immune responses such as the sensitization of MHC-restricted T cells, the rejection of organ transplants, and the formation of T-dependent antibodies. Dendritic cells are found in many nonlymphoid tissues but can migrate via the afferent lymph or the blood stream to the T-dependent areas of lymphoid organs. In skin, the immunostimulatory function of dendritic cells is enhanced by cytokines, especially GM-CSF. After foreign proteins are administered in situ, dendritic cells are a principal reservoir of immunogen. In vitro studies indicate that dendritic cells only process proteins for a short period of time, when the rate of synthesis of MHC products and content of acidic endocytic vesicles are high. Antigen processing is selectively dampened after a day in culture, but the capacity to stimulate responses to surface bound peptides and mitogens remains strong. Dendritic cells are motile, and efficiently cluster and activate T cells that are specific for stimuli on the cell surface. High levels of MHC class-I and -II products and several adhesins, such as ICAM-1 and LFA-3, likely contribute to these functions. Therefore dendritic cells are specialized to mediate several physiologic components of immunogenicity such as the acquisition of antigens in tissues, the migration to lymphoid organs, and the identification and activation of antigen-specific T cells. The function of these presenting cells in immunologic tolerance is just beginning to be studied.",1991,64,4908,141,4,43,106,98,169,206,283,339,327,335
3b6ea979601bbddf3cf36125d8c4e2432ec139a9,"Revisions in stage grouping of the TNM subsets (T=primary tumor, N=regional lymph nodes, M=distant metastasis) in the International System for Staging Lung Cancer have been adopted by the American Joint Committee on Cancer and the Union Internationale Contre le Cancer. These revisions were made to provide greater specificity for identifying patient groups with similar prognoses and treatment options with the least disruption of the present classification: T1N0M0, stage IA; T2N0M0, stage IB; T1N1M0, stage IIA; T2N1M0 and T3N0M0, stage IIB; and T3N1M0, T1N2M0, T2N2M0, T3N2M0, stage IIIA. The TNM subsets in stage IIIB-T4 any N M0, any T N3M0, and in stage IV-any T any N M1, remain the same. Analysis of a collected database representing all clinical, surgical-pathologic, and follow-up information for 5,319 patients treated for primary lung cancer confirmed the validity of the TNM and stage grouping classification schema.",1997,17,4590,166,4,37,100,184,219,257,282,316,325,339
fac9db01f3db66d937854101cdcad5f4d9c28013,"NF-kappa B is a ubiquitous transcription factor. Nevertheless, its properties seem to be most extensively exploited in cells of the immune system. Among these properties are NF-kappa B's rapid posttranslational activation in response to many pathogenic signals, its direct participation in cytoplasmic/nuclear signaling, and its potency to activate transcription of a great variety of genes encoding immunologically relevant proteins. In vertebrates, five distinct DNA binding subunits are currently known which might extensively heterodimerize, thereby forming complexes with distinct transcriptional activity, DNA sequence specificity, and cell type- and cell stage-specific distribution. The activity of DNA binding NF-kappa B dimers is tightly controlled by accessory proteins called I kappa B subunits of which there are also five different species currently known in vertebrates. I kappa B proteins inhibit DNA binding and prevent nuclear uptake of NF-kappa B complexes. An exception is the Bcl-3 protein which in addition can function as a transcription activating subunit in th nucleus. Other I kappa B proteins are rather involved in terminating NF-kappa B's activity in the nucleus. The intracellular events that lead to the inactivation of I kappa B, i.e. the activation of NF-kappa B, are complex. They involve phosphorylation and proteolytic reactions and seem to be controlled by the cells' redox status. Interference with the activation or activity of NF-kappa B may be beneficial in suppressing toxic/septic shock, graft-vs-host reactions, acute inflammatory reactions, acute phase response, and radiation damage. The inhibition of NF-kappa B activation by antioxidants and specific protease inhibitors may provide a pharmacological basis for interfering with these acute processes.",1994,52,4643,156,8,104,159,237,341,381,314,334,273,216
8e60816d0db257d2e20e88a5d63d73c2e21e716e,"Metallic, oxygen-deficient compounds in the Ba−La−Cu−O system, with the composition BaxLa5−xCu5O5(3−y) have been prepared in polycrystalline form. Samples withx=1 and 0.75,y>0, annealed below 900°C under reducing conditions, consist of three phases, one of them a perovskite-like mixed-valent copper compound. Upon cooling, the samples show a linear decrease in resistivity, then an approximately logarithmic increase, interpreted as a beginning of localization. Finally an abrupt decrease by up to three orders of magnitude occurs, reminiscent of the onset of percolative superconductivity. The highest onset temperature is observed in the 30 K range. It is markedly reduced by high current densities. Thus, it results partially from the percolative nature, bute possibly also from 2D superconducting fluctuations of double perovskite layers of one of the phases present.",1986,19,8937,64,0,569,840,654,503,414,320,261,224,115
27f02bf3e3fcd3435c3305c51c7491f73ae468e6,"It is shown that, particularly for terrestrial cellular telephony, the interference-suppression feature of CDMA (code division multiple access) can result in a many-fold increase in capacity over analog and even over competing digital techniques. A single-cell system, such as a hubbed satellite network, is addressed, and the basic expression for capacity is developed. The corresponding expressions for a multiple-cell system are derived. and the distribution on the number of users supportable per cell is determined. It is concluded that properly augmented and power-controlled multiple-cell CDMA promises a quantum increase in current cellular capacity. >",1991,4,3201,183,21,93,81,182,160,205,173,186,169,222
5f0a10c781c208e9da913b5d173d48b717ac0869,,1993,0,3420,199,17,21,20,27,37,38,33,83,70,84
e59f56e30f1eb7578a8e2ea1140c0765ddb1e024,"The decision support system for agrotechnology transfer (DSSAT) has been in use for the last 15 years by researchers worldwide. This package incorporates models of 16 different crops with software that facilitates the evaluation and application of the crop models for different purposes. Over the last few years, it has become increasingly difficult to maintain the DSSAT crop models, partly due to fact that there were different sets of computer code for different crops with little attention to software design at the level of crop models themselves. Thus, the DSSAT crop models have been re-designed and programmed to facilitate more efficient incorporation of new scientific advances, applications, documentation and maintenance. The basis for the new DSSAT cropping system model (CSM) design is a modular structure in which components separate along scientific discipline lines and are structured to allow easy replacement or addition of modules. It has one Soil module, a Crop Template module which can simulate different crops by defining species input files, an interface to add individual crop models if they have the same design and interface, a Weather module, and a module for dealing with competition for light and water among the soil, plants, and atmosphere. It is also designed for incorporation into various application packages, ranging from those that help researchers adapt and test the CSM to those that operate the DSSAT-CSM to simulate production over time and space for different purposes. In this paper, we describe this new DSSAT-CSM design as well as approaches used to model the primary scientific components (soil, crop, weather, and management). In addition, the paper describes data requirements and methods used for model evaluation. We provide an overview of the hundreds of published studies in which the DSSAT crop models have been used for various applications. The benefits of the new, re-designed DSSAT-CSM will provide considerable opportunities to its developers and others in the scientific community for greater cooperation in interdisciplinary research and in the application of knowledge to solve problems at field, farm, and higher levels.",2003,265,2964,270,11,23,29,50,62,78,79,123,125,161
c67818b2ce1410ba61f29e1f77412fe23c69f346,"Abstract Ant System, the first Ant Colony Optimization algorithm, showed to be a viable method for attacking hard combinatorial optimization problems. Yet, its performance, when compared to more fine-tuned algorithms, was rather poor for large instances of traditional benchmark problems like the Traveling Salesman Problem. To show that Ant Colony Optimization algorithms could be good alternatives to existing algorithms for hard combinatorial optimization problems, recent research in this area has mainly focused on the development of algorithmic variants which achieve better performance than Ant System. In this paper, we present MAX – MIN Ant System ( MM AS ), an Ant Colony Optimization algorithm derived from Ant System. MM AS differs from Ant System in several important aspects, whose usefulness we demonstrate by means of an experimental study. Additionally, we relate one of the characteristics specific to MM AS — that of using a greedier search than Ant System — to results from the search space analysis of the combinatorial optimization problems attacked in this paper. Our computational results on the Traveling Salesman Problem and the Quadratic Assignment Problem show that MM AS is currently among the best performing algorithms for these problems.",2000,156,2682,334,4,10,24,29,68,65,114,146,154,185
ca2949898b9a139f7b9129d414b3c1cafe915b61,"Abstract We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder's traffic transits. We give an overview of the system's design, which emphasizes high-speed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an `event engine' that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a `policy script interpreter' that interprets event handlers written in a specialized language used to express a site's security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the six applications integrated into it so far: Finger, FTP, Portmapper, Ident, Telnet and Rlogin. The system is publicly available in source code form.",1998,54,2798,258,5,19,23,39,57,78,108,146,175,159
d70e50a2cf1adb0a8de34e28de9cde267b931e26,"OBJECTIVE
This article defines stress and related concepts and reviews their historical development. The notion of a stress system as the effector of the stress syndrome is suggested, and its physiologic and pathophysiologic manifestations are described. A new perspective on human disease states associated with dysregulation of the stress system is provided.


DATA SOURCES
Published original articles from human and animal studies and selected reviews. Literature was surveyed utilizing MEDLINE and the Index Medicus.


STUDY SELECTION
Original articles from the basic science and human literature consisted entirely of controlled studies based on verified methodologies and, with the exception of the most recent studies, replicated by more than one laboratory. Many of the basic science and clinical studies had been conducted in our own laboratories and clinical research units. Reviews cited were written by acknowledged leaders in the fields of neurobiology, endocrinology, and behavior.


DATA EXTRACTION
Independent extraction and cross-referencing by the authors.


DATA SYNTHESIS
Stress and related concepts can be traced as far back as written science and medicine. The stress system coordinates the generalized stress response, which takes place when a stressor of any kind exceeds a threshold. The main components of the stress system are the corticotropin-releasing hormone and locus ceruleus-norepinephrine/autonomic systems and their peripheral effectors, the pituitary-adrenal axis, and the limbs of the autonomic system. Activation of the stress system leads to behavioral and peripheral changes that improve the ability of the organism to adjust homeostasis and increase its chances for survival. There has been an exponential increase in knowledge regarding the interactions among the components of the stress system and between the stress system and other brain elements involved in the regulation of emotion, cognitive function, and behavior, as well as with the axes responsible for reproduction, growth, and immunity. This new knowledge has allowed association of stress system dysfunction, characterized by sustained hyperactivity and/or hypoactivity, to various pathophysiologic states that cut across the traditional boundaries of medical disciplines. These include a range of psychiatric, endocrine, and inflammatory disorders and/or susceptibility to such disorders.


CONCLUSIONS
We hope that knowledge from apparently disparate fields of science and medicine integrated into a working theoretical framework will allow generation and testing of new hypotheses on the pathophysiology and diagnosis of, and therapy for, a variety of human illnesses reflecting systematic alterations in the principal effectors of the generalized stress response. We predict that pharmacologic agents capable of altering the central apparatus that governs the stress response will be useful in the treatment of many of these illnesses.",1992,130,3661,126,1,27,41,62,73,76,93,88,113,108
1ea0d689587486473b802ec166d7c052041421d7,"Vasculature O.U. Scremin, Cerebral Vascular System. Spinal Cord and Peripheral Nervous System C. Molander and G. Grant, Spinal Cord Cytoarchitecture. A. Ribeiro-da-Silva, Substantia Gelantinosa of Spinal Cord. G. Grant, Primary Afferent Projections to the Spinal Cord. D.J. Tracey, Ascending and Descending Pathways in the Spinal Cord. G. Gabella, Autonomic Nervous System. Brainstem and Cerebellum C.B. Saper, CentralAutonomic System. G. Holstege, The Basic, Somatic, and Emotional Components of the Motor System in Mammals. B.E. Jones, Reticular Formation: Cytoarchitecture, Transmitters, and Projections. A.J. Beitz, Periaqueductal Gray. G. Aston-Jones, M.T. Shipley, and R. Grzanna, The Locus Coeruleus, A5 and A7 Noradrenergic Cell Groups. J.H. Fallon and S.E. Loughlin, Substantia Nigra. J.B. Travers, Oromotor Nuclei. G. Holstege, B.F.M. Blok, and G.J. ter Horst, Brain Stem Systems Involved in the Blink Reflex, Feeding Mechanisms, and Micturition. T.J.H. Ruigrok and F. Cella, Precerebellar Nuclei and Red Nucleus. J. Voogd, Cerebellum. Forebrain R.B. Simerly, Anatomical Substrates of Hypothalamic Integration. W.E. Armstrong, Hypothalamic Supraoptic and Paraventricular Nuclei. B.J. Oldfield and M.J. McKinley, Circumventricular Organs. R.L. Jakab and C. Leranth, Septum. D.G. Amaral and M.P. Witter, Hippocampal Formation. G.F. Alheid, J.S. de Olmos, and C.A. Beltramino, Amygdala and Extended Amygdala. L. Heimer, D.S. Zahm, and G.F. Alheid, Basal Ganglia. J.L. Price, Thalamus. K. Zilles and A. Wree, Cortex: Areal and Laminar Structure. Sensory Systems D.J. Tracey and P.M.E. Waite, Somatosensory System. P.M.E. Waite and D.J. Tracey, Trigeminal Sensory System. W.D. Willis, K.N. Westlund, and S.M. Carlton, Pain. R. Norgren, Gustatory System. J.A. Rubertone, W.R. Mehler, and J. Voogd, The Vestibular Nuclear Complex. W.R. Webster, Auditory System. A.J. Sefton and B. Dreher, Visual System. M.T. Shipley, J.H. McLean, and M. Ennis, Olfactory System. Neurotransmitters G. Halliday, A. Harding, and G. Paxinos, Serotonin and Tachykinin Systems. S.E. Loughlin, F.M. Leslie, and J.H. Fallon, Endogenous Opioid Systems. L.L. Butcher, Cholinergic Neurons and Networks. O.P. Ottersen, O.P. Hjelle, K.K. Osen, and J.H. Laake, Amino Acid Transmitters. Development S.A. Bayer and J. Altman, Neurogenesis and Neuronal Migration. S.A. Bayer and J. Altman, Principles of Neurogenesis, Neuronal Migration, and Neural Circuit Formation. Subject Index.",1985,0,4358,106,0,2,23,20,20,29,33,35,26,24
e1077944f6a4ca988f3635708cb23b92fb99ccd7,"We present two new hybrid meta exchange- correlation functionals, called M06 and M06-2X. The M06 functional is parametrized including both transition metals and nonmetals, whereas the M06-2X functional is a high-nonlocality functional with double the amount of nonlocal exchange (2X), and it is parametrized only for nonmetals.The functionals, along with the previously published M06-L local functional and the M06-HF full-Hartree–Fock functionals, constitute the M06 suite of complementary functionals. We assess these four functionals by comparing their performance to that of 12 other functionals and Hartree–Fock theory for 403 energetic data in 29 diverse databases, including ten databases for thermochemistry, four databases for kinetics, eight databases for noncovalent interactions, three databases for transition metal bonding, one database for metal atom excitation energies, and three databases for molecular excitation energies. We also illustrate the performance of these 17 methods for three databases containing 40 bond lengths and for databases containing 38 vibrational frequencies and 15 vibrational zero point energies. We recommend the M06-2X functional for applications involving main-group thermochemistry, kinetics, noncovalent interactions, and electronic excitation energies to valence and Rydberg states. We recommend the M06 functional for application in organometallic and inorganometallic chemistry and for noncovalent interactions.",2008,283,17647,201,0,0,0,0,0,0,0,2,147,1921
cffe5f6bcababfb9cd99079cf83e90a9bb516383,"The term apoptosis is proposed for a hitherto little recognized mechanism of controlled cell deletion, which appears to play a complementary but opposite role to mitosis in the regulation of animal cell populations. Its morphological features suggest that it is an active, inherently programmed phenomenon, and it has been shown that it can be initiated or inhibited by a variety of environmental stimuli, both physiological and pathological.The structural changes take place in two discrete stages. The first comprises nuclear and cytoplasmic condensation and breaking up of the cell into a number of membrane-bound, ultrastructurally well-preserved fragments. In the second stage these apoptotic bodies are shed from epithelial-lined surfaces or are taken up by other cells, where they undergo a series of changes resembling in vitro autolysis within phagosomes, and are rapidly degraded by lysosomal enzymes derived from the ingesting cells.Apoptosis seems to be involved in cell turnover in many healthy adult tissues and is responsible for focal elimination of cells during normal embryonic development. It occurs spontaneously in untreated malignant neoplasms, and participates in at least some types of therapeutically induced tumour regression. It is implicated in both physiological involution and atrophy of various tissues and organs. It can also be triggered by noxious agents, both in the embryo and adult animal.",1972,75,15075,358,0,0,0,0,0,1,0,3,1,2
583e1e985b915610f556cad4860619c4bac8cc4b,"Abstract A photo-induced cyclic peroxidation in isolated chloroplasts is described. In an osmotic buffered medium, chloroplasts upon illumination produce malondialdehyde (MDA)—a decomposition product of tri-unsaturated fatty acid hydroperoxides—bleach endogenous chlorophyll, and consume oxygen. These processes show ( a ) no reaction in the absence of illumination; ( b ) an initial lag phase upon illumination of 10–20 minutes duration; ( c ) a linear phase in which the rate is proportional to the square root of the light intensity; ( d ) cessation of reaction occurring within 3 minutes after illumination ceases; and ( e ) a termination phase after several hours of illumination. The kinetics of the above processes fit a cyclic peroxidation equation with velocity coefficients near those for chemical peroxidation. The stoichiometry of MDA/O 2 = 0.02, and O 2 Chl bleached = 6.9 correlates well with MDA production efficiency in other biological systems and with the molar ratio of unsaturated fatty acids to chlorophyll. The energies of activation for the lag and linear phases are 17 and 0 kcal/mole, respectively, the same as that for autoxidation. During the linear phase of oxygen uptake the dependence upon temperature and O 2 concentration indicates that during the reaction, oxygen tension at the site of peroxidation is 100-fold lower than in the aqueous phase. It is concluded that isolated chloroplasts upon illumination can undergo a cyclic peroxidation initiated by the light absorbed by chlorophyll. Photoperoxidation results in a destruction of the chlorophyll and tri-unsaturated fatty acids of the chloroplast membranes.",1968,21,7240,256,1,2,2,2,3,2,3,8,6,3
056ded69eecde9a755f8ad4c4abf7eed549388e8,"INTRODUCTION
Astronauts soaring through space modules with the grace of birds seems counterintuitive. How do they adapt to the weightless environment? Previous spaceflights have shown that astronauts in orbit adapt their motor strategies to each change in their gravitational environment. During adaptation, performance is degraded and can lead to mission-threatening injuries. If adaptation can occur before a mission, productivity during the mission might improve, minimizing risk. The goal is to combine kinetic and kinematic data to examine translational motions during microgravity adaptations.


METHODS
Experiments were performed during parabolic flights aboard NASA's C-9. Five subjects used their legs to push off from a sensor, landing on a target 3.96 m (13 ft) away. The sensor quantified the kinetics during contact, while four cameras recorded kinematics during push-off. Joint torques were calculated for a subset of traverses (N = 50) using the forces, moments, and joint angles.


RESULTS
During the 149 traverses, the average peak force exerted onto the sensor was 224.6 +/- 74.6 N, with peak values ranging between 65.8-461.9 N. Two types of force profiles were observed, some having single, strong peaks (N = 64) and others having multiple, weaker peaks (N = 86).


CONCLUSIONS
The force data were consistent with values recorded previously in sustained microgravity aboard Mir and the Space Shuttle. A training program for astronauts might be designed to encourage fine-control motions (i.e., multiple, weaker peaks) as these reduce the risk of injury and increase controllability. Additionally, a kinematic and kinetic sensor suite was successfully demonstrated in the weightless environment onboard the C-9 aircraft.",2009,0,5485,0,202,233,213,940,1159,1596,921,1,0,0
7d10232c90bf60ac69b2e99f3deb8fb177ddeed2,"Abstract An analysis is made of the process whereby diffusion effects can cause the precipitation of grains of a second phase in a supersaturated solid solution. The kinetics of this type of grain growth are examined in detail. Some grains grow, only to be later dissolved; others increase in size and incorporate further grains that they encounter in so doing. This latter phenomenon of coalescence is discussed in a new “kinetic” approximation. Formulae are given for the asymptotic grain size distribution, for the number of grains per unit volume and for the supersaturation as a function of time. The effects of anisotropy, strain, crystalline order and the finite size of the specimen are allowed for. It is pointed out that for a material that can be said to be “supersaturated with vacancies”, the discussion can be applied to the vacancies as solute “atoms” which cluster together to form internal cavities. The practical case of a real, finite crystal is here important, because the vacancies can in general also escape to the surface. A special analysis is made of this example, and the results are applied to the theory of sintering.",1961,1,6197,130,0,0,1,2,3,6,3,5,13,12
665c4ff4e6479348893fa8764e1c52a5959c114b,"Laboratory investigations show that rates of adsorption of persistent organic compounds on granular carbon are quite low. Intraparticle diffusion of solute appears to control the rate of uptake, thus the rate is partially a function of the pore size distribution of the adsorbent, of the molecular size and configuration of the solute, and of the relative electrokinetic properties of adsorbate and adsorbent. Systemic factors such as temperature and pH will influence the rates of adsorption; rates increase with increasing temperature and decrease with increasing pH. The effect of initial concentration of solute is of considerable significance, the rate of uptake being a linear function of the square-root of concentration within the range of experimentation. Relative reaction rates also vary reciprocally with the square of the diameter of individual carbon particle for a given weight of carbon. Based on the findings of the research, fluidized-bed operation is suggested as an efficient means of using adsorption for treatment of waters and waste waters.",1963,0,5563,214,0,0,2,0,1,1,0,1,1,1
03403d7469b3fa7fe672c861d3e53589d1b28791,"The theory of the kinetics of phase change is developed with the experimentally supported assumptions that the new phase is nucleated by germ nuclei which already exist in the old phase, and whose number can be altered by previous treatment. The density of germ nuclei diminishes through activation of some of them to become growth nuclei for grains of the new phase, and ingestion of others by these growing grains. The quantitative relations between the density of germ nuclei, growth nuclei, and transformed volume are derived and expressed in terms of a characteristic time scale for any given substance and process. The geometry and kinetics of a crystal aggregate are studied from this point of view, and it is shown that there is strong evidence of the existence, for any given substance, of an isokinetic range of temperatures and concentrations in which the characteristic kinetics of phase change remains the same. The determination of phase reaction kinetics is shown to depend upon the solution of a function...",1939,13,8549,65,0,1,1,0,1,0,0,0,0,0
14fd4ea9fae9173f518bc29aac107ee1aa26b49e,"We present a new local density functional, called M06-L, for main-group and transition element thermochemistry, thermochemical kinetics, and noncovalent interactions. The functional is designed to capture the main dependence of the exchange-correlation energy on local spin density, spin density gradient, and spin kinetic energy density, and it is parametrized to satisfy the uniform-electron-gas limit and to have good performance for both main-group chemistry and transition metal chemistry. The M06-L functional and 14 other functionals have been comparatively assessed against 22 energetic databases. Among the tested functionals, which include the popular B3LYP, BLYP, and BP86 functionals as well as our previous M05 functional, the M06-L functional gives the best overall performance for a combination of main-group thermochemistry, thermochemical kinetics, and organometallic, inorganometallic, biological, and noncovalent interactions. It also does very well for predicting geometries and vibrational frequencies. Because of the computational advantages of local functionals, the present functional should be very useful for many applications in chemistry, especially for simulations on moderate-sized and large systems and when long time scales must be addressed.",2006,313,3342,37,2,7,31,36,65,136,184,246,291,320
878b775577fe08d47260b781dddd74b16b748e08,"Following upon the general theory in Part I, a considerable simplification is here introduced in the treatment of the case where the grain centers of the new phase are randomly distributed. Also, the kinetics of the main types of crystalline growth, such as result in polyhedral, plate‐like and lineal grains, are studied. A relation between the actual transformed volume V and a related extended volume V1 ex is derived upon statistical considerations. A rough approximation to this relation is shown to lead, under the proper conditions, to the empirical formula of Austin and Rickett. The exact relation is used to reduce the entire problem to the determination of V1 ex, in terms of which all other quantities are expressed. The approximate treatment of the beginning of transformation in the isokinetic range is shown to lead to the empirical formula of Krainer and to account quantitatively for certain relations observed in recrystallization phenomena. It is shown that the predicted shapes for isothermal transfo...",1940,4,6654,55,0,1,0,0,0,0,0,0,0,1
8979bdafefff9da97342fbc5cf3f511000306f60,Kinetics of Unireactant Enzymes. Simple Inhibition Systems. Rapid Equilibrium Partial and Mixed--Type Inhibition. Enzyme Activation. Rapid Equilibrium Bireactant and Terreactant Systems. Multisite and Allosteric Enzymes. Multiple Inhibition Analysis. Steady--State Kinetics of Multireactant Enzymes. Isotope Exchange. Effects of pH and Temperature. Appendix. Index.,1975,0,3084,215,0,2,10,10,12,13,13,17,20,19
9e4c2c2c7ecdc0a4fcc19c88b6c0e4bfb09a4864,,1991,0,3211,131,3,18,18,44,46,69,68,86,68,93
97e84e177ca0946644eaa521399d8a616263abf3,,1957,13,8622,40,0,0,1,0,1,5,1,2,3,4
a252446c4054d5b000dcb80cf5ec0b50afa99580,"As part of a series of evaluated sets, rate constants and photochemical cross sections compiled by the NASA Panel for Data Evaluation are provided. The primary application of the data is in the modeling of stratospheric processes, with particular emphasis on the ozone layer and its possible perturbation by anthropogenic and natural phenomena. Copies of this evaluation are available from the Jet Propulsion Laboratory.",1985,12,3406,108,4,7,12,12,9,21,72,83,94,124
6c399c082c0085f97b000bffade60cad00bb9f60,"The theory of the preceding papers is generalized and the notation simplified. A cluster of molecules in a stable phase surrounded by an unstable phase is itself unstable until a critical size is reached, though for statistical reasons a distribution of such clusters may exist. Beyond the critical size, the cluster tends to grow steadily. The designation ``nuclei'' or ``grains'' is used according as the clusters are below or above the critical size. It is shown that a comprehensive description of the phenomena of phase change may be summarized in Phase Change, Grain Number and Microstructure Formulas or Diagrams, giving, respectively, the transformed volume, grain, and microstructure densities as a function of time, temperature, and other variables. To facilitate the deduction of formulas for these densities the related densities of the ``extended'' grain population are introduced. The extended population is that system of interpenetrating volumes that would obtain if the grains granulated and grew throug...",1941,3,5037,39,0,0,1,0,0,0,0,0,0,0
3d0933aa3de5854503f55770df771e4b7b7f873e,"We present a new hybrid meta exchange-correlation functional, called M05-2X, for thermochemistry, thermochemical kinetics, and noncovalent interactions. We also provide a full discussion of the new M05 functional, previously presented in a short communication. The M05 functional was parametrized including both metals and nonmetals, whereas M05-2X is a high-nonlocality functional with double the amount of nonlocal exchange (2X) that is parametrized only for nonmetals. In particular, M05 was parametrized against 35 data values, and M05-2X is parametrized against 34 data values. Both functionals, along with 28 other functionals, have been comparatively assessed against 234 data values:  the MGAE109/3 main-group atomization energy database, the IP13/3 ionization potential database, the EA13/3 electron affinity database, the HTBH38/4 database of barrier height for hydrogen-transfer reactions, five noncovalent databases, two databases involving metal-metal and metal-ligand bond energies, a dipole moment database, a database of four alkyl bond dissociation energies of alkanes and ethers, and three total energies of one-electron systems. We also tested the new functionals and 12 others for eight hydrogen-bonding and stacking interaction energies in nucleobase pairs, and we tested M05 and M05-2X and 19 other functionals for the geometry, dipole moment, and binding energy of HCN-BF3, which has recently been shown to be a very difficult case for density functional theory. We tested eight functionals for four more alkyl bond dissociation energies, and we tested 12 functionals for several additional bond energies with varying amounts of multireference character. On the basis of all the results for 256 data values in 18 databases in the present study, we recommend M05-2X, M05, PW6B95, PWB6K, and MPWB1K for general-purpose applications in thermochemistry, kinetics, and noncovalent interactions involving nonmetals and we recommend M05 for studies involving both metallic and nonmetallic elements. The M05 functional, essentially uniquely among the functionals with broad applicability to chemistry, also performs well not only for main-group thermochemistry and radical reaction barrier heights but also for transition-metal-transition-metal interactions. The M05-2X functional has the best performance for thermochemical kinetics, noncovalent interactions (especially weak interaction, hydrogen bonding, π···π stacking, and interactions energies of nucleobases), and alkyl bond dissociation energies and the best composite results for energetics, excluding metals.",2006,148,2650,28,11,27,65,130,140,214,263,244,247,220
e4dbd52d06832df3c6e812b02749fadd20fd3e39,Part 1 Equilibria: fundamentals of pure component adsorption equilibria practical approaches of pure component adsorption equilibria pure component adsorption in microporous solids multicomponent adsorption equilibria heterogeneous adsorption equilibria. Part 2 Kinetics: fundamentals of diffusion and adsorption in porous media diffusion and adsorption in porous media - Maxwell-Stefan approach analysis of adsorption kinetics in a single homogeneous particle analysis of adsorption kinetics in a zeolite particle analysis of adsorption kinetics in a heterogeneous particle. Part 3 Measurement techniques: time lag analysis in diffusion and adsorption in porous media analysis of steady state and transient diffusion cells adsorption and diffusion measurement by a chromatography method kinetics of a batch adsorber.,1998,0,2068,167,0,1,20,22,42,36,59,62,49,74
d64c71c6adc11b135c09d1fb6a29193d7d83ffe9,"A pseudo-second order rate equation describing the kinetics of sorption of divalent metal ions onto sphagnum moss peat at diAerent initial metal ion concentrations and peat doses has been developed. The kinetics of sorption were followed based on the amounts of metal sorbed at various time intervals. Results show that sorption (chemical bonding) might be rate-limiting in the sorption of divalent metal ions onto peat during agitated batch contact time experiments. The rate constant, the equilibrium sorption capacity and the initial sorption rate were calculated. From these parameters, an empirical model for predicting the sorption capacity of metal ions sorbed was derived. # 2000 Elsevier Science Ltd. All rights reserved",2000,11,2511,40,2,9,9,27,42,74,57,71,113,149
d64f64829ca2b04ec3fab50d58b7f5c83c95c1a3,"Stochastic chemical kinetics describes the time evolution of a well-stirred chemically reacting system in a way that takes into account the fact that molecules come in whole numbers and exhibit some degree of randomness in their dynamical behavior. Researchers are increasingly using this approach to chemical kinetics in the analysis of cellular systems in biology, where the small molecular populations of only a few reactant species can lead to deviations from the predictions of the deterministic differential equations of classical chemical kinetics. After reviewing the supporting theory of stochastic chemical kinetics, I discuss some recent advances in methods for using that theory to make numerical simulations. These include improvements to the exact stochastic simulation algorithm (SSA) and the approximate explicit tau-leaping procedure, as well as the development of two approximate strategies for simulating systems that are dynamically stiff: implicit tau-leaping and the slow-scale SSA.",2007,47,1689,125,15,54,72,116,130,131,137,138,115,146
e6c08e887f683af6f0bacd512764aa042d11595e,"The discovery of ammonia oxidation by mesophilic and thermophilic Crenarchaeota and the widespread distribution of these organisms in marine and terrestrial environments indicated an important role for them in the global nitrogen cycle. However, very little is known about their physiology or their contribution to nitrification. Here we report oligotrophic ammonia oxidation kinetics and cellular characteristics of the mesophilic crenarchaeon ‘Candidatus Nitrosopumilus maritimus’ strain SCM1. Unlike characterized ammonia-oxidizing bacteria, SCM1 is adapted to life under extreme nutrient limitation, sustaining high specific oxidation rates at ammonium concentrations found in open oceans. Its half-saturation constant (Km = 133 nM total ammonium) and substrate threshold (≤10 nM) closely resemble kinetics of in situ nitrification in marine systems and directly link ammonia-oxidizing Archaea to oligotrophic nitrification. The remarkably high specific affinity for reduced nitrogen (68,700 l per g cells per h) of SCM1 suggests that Nitrosopumilus-like ammonia-oxidizing Archaea could successfully compete with heterotrophic bacterioplankton and phytoplankton. Together these findings support the hypothesis that nitrification is more prevalent in the marine nitrogen cycle than accounted for in current biogeochemical models.",2009,67,1228,127,2,50,113,96,112,100,102,119,111,109
132bea6311f0ce960cd7de9219a7da0f6989d66b,"Fluorescence quenching rate constants, kq, ranging from 106 to 2 × 1010 M−1 sec−1, of more than 60 typical electron donor-acceptor systems have been measured in de-oxygenated acetonitrile and are shown to be correlated with the free enthalpy change, ΔG23, involved in the actual electron transfer process 
 
in the encounter complex and varying between + 5 and −60 kcal/mole. The correlation which is based on the mechanism of adiabatic outer-sphere electron transfer requires ΔG≠23, the activation free enthalpy of this process to be a monotonous function of ΔG23 and allows the calculation of rate constants of electron transfer quenching from spectroscopic and electrochemical data. 
 
A detailed study of some systems where the calculated quenching constants differ from the experimental ones by several orders of magnitude revealed that the quenching mechanism operative in these cases was hydrogen-atom rather than electron transfer. 
 
The conditions under which these different mechanisms apply and their consequences are discussed.",1970,25,2835,17,0,2,4,6,6,10,13,13,27,18
8af645645302948eb875ba9f34f848560d7ea3ed,to Kinetics and Many-Body Theory.- Boltzmann Equation.- Numerical Solutions of the Boltzmann Equation.- Equilibrium Green Function Theory.- Nonequilibrium Many-Body Theory.- Contour-Ordered Green Functions.- Basic Quantum Kinetic Equations.- Boltzmann Limit.- Gauge Invariance.- Quantum Distribution Functions.- Quantum Transport in Semiconductors.- Linear Transport.- Field-Dependent Green Functions.- Optical Absorption in Intense THz Fields.- Transport in Mesoscopic Semiconductor Structures.- Time-Dependent Phenomena.- Theory of Ultrafast Kinetics in Laser-Excited Semiconductors.- Optical Free-Carrier Interband Kinetics in Semiconductors.- Interband Quantum Kinetics with LO-Phonon Scattering.- Two-Pulse Spectroscopy.- Coulomb Quantum Kinetics in a Dense Electron-Hole Plasma.- The Buildup of Screening.- Femtosecond Four-Wave Mixing with Dense Plasmas.,2004,0,1619,118,70,88,93,86,121,111,102,104,96,102
ecc4155c6567b6c9ff61d7e699581385e2e3b466,"Fluorescence photobleaching recovery (FPR) denotes a method for measuring two-dimensional lateral mobility of fluorescent particles, for example, the motion of fluorescently labeled molecules in approximately 10 mum2 regions of a single cell surface. A small spot on the fluorescent surface is photobleached by a brief exposure to an intense focused laser beam, and the subsequent recovery of the fluorescence is monitored by the same, but attenuated, laser beam. Recovery occurs by replenishment of intact fluorophore in the bleached spot by lateral transport from the surrounding surface. We present the theoretical basis and some practical guidelines for simple, rigorous analysis of FPR experiments. Information obtainable from FPR experiments includes: (a) identification of transport process type, i.e. the admixture of random diffusion and uniform directed flow; (b) determination of the absolute mobility coefficient, i.e. the diffusion constant and/or flow velocity; and (c) the fraction of total fluorophore which is mobile. To illustrate the experimental method and to verify the theory for diffusion, we describe some model experiments on aqueous solutions of rhodamine 6G.",1976,17,2330,162,2,5,12,14,21,32,29,22,30,25
7667d74b712a1ad8567a83e519b2c0343f39e22a,"The oxidation of organic and inorganic compounds during ozonation can occur via ozone or OH radicals or a combination thereof. The oxidation pathway is determined by the ratio of ozone and OH radical concentrations and the corresponding kinetics. A huge database with several hundred rate constants for ozone and a few thousand rate constants for OH radicals is available. Ozone is an electrophile with a high selectivity. The second-order rate constants for oxidation by ozone vary over 10 orders of magnitude, between < 0.1 M(-1)s(-1) and about 7 x 10(9) M(-1)s(-1). The reactions of ozone with drinking-water relevant inorganic compounds are typically fast and occur by an oxygen atom transfer reaction. Organic micropollutants are oxidized with ozone selectively. Ozone reacts mainly with double bonds, activated aromatic systems and non-protonated amines. In general, electron-donating groups enhance the oxidation by ozone whereas electron-withdrawing groups reduce the reaction rates. Furthermore, the kinetics of direct ozone reactions depend strongly on the speciation (acid-base, metal complexation). The reaction of OH radicals with the majority of inorganic and organic compounds is nearly diffusion-controlled. The degree of oxidation by ozone and OH radicals is given by the corresponding kinetics. Product formation from the ozonation of organic micropollutants in aqueous systems has only been established for a few compounds. It is discussed for olefines, amines and aromatic compounds.",2003,194,1553,106,3,7,23,43,45,52,75,82,112,94
b1cb818493644eca95bbbeff3a73b30104646252,,1961,0,2937,24,0,3,11,8,3,14,12,22,27,29
8ab3cb75edb599cb90f122ddd50cfaf3424ef75c,"We describe the use of gel electrophoresis in studies of equilibrium binding, site distribution, and kinetics of protein-DNA interactions. The method, which we call protein distribution analysis, is simple, sensitive and yields thermodynamically rigorous results. It is particularly well suited to studies of simultaneous binding of several proteins to a single nucleic acid. In studies of the lac repressor-operator interaction, we found that binding to the so-called third operator site (03) is 15-18 fold weaker than operator binding, and that the binding reactions with the first and third operators are uncoupled, implying that there is no communication between the sites. Pseudo-first order dissociation kinetics of the repressor-203 bp operator complex were found to be temperature sensitive, with delta E of 80 kcal mol-1 above 29 degrees C and 26 kcal mol-1 below. The half life of the complex (5 min at 21 degrees C) is shorter than that reported for very high molecular weight operator-containing DNAs, but longer than values reported for much shorter fragments. The binding of lac repressor core to DNA could not be detected by this technique: the maximum binding constant consistent with this finding is 10(5) M-1.",1981,0,2075,40,0,2,9,12,15,36,97,108,155,152
0df70a70a760c4900df850ddef14028b16e596b6,"The kinetics and mechanism of methylene blue adsorption on commercial activated carbon (CAC) and indigenously prepared activated carbons from bamboo dust, coconut shell, groundnut shell, rice husk, and straw, have been studied. The effects of various experimental parameters have been investigated using a batch adsorption technique to obtain information on treating effluents from the dye industry. The extent of dye removal increased with decrease in the initial concentration of the dye and particle size of the adsorbent and also increased with increase in contact time, amount of adsorbent used and the initial pH of the solution. Adsorption data were modeled using the Freundlich and Langmuir adsorption isotherms and first order kinetic equations. The kinetics of adsorption were found to be first order with regard to intra-particle diffusion rate. The adsorption capacities of indigenous activated carbons have been compared with that of the commercial activated carbon. The results indicate that such carbons could be employed as low cost alternatives to commercial activated carbon in wastewater treatment for the removal of colour and dyes.",2001,17,1647,36,0,0,5,19,30,54,57,60,71,79
bd54c3596ae468c928bbc783aa43bc58c11ab974,"The theory of phase-ordering dynamics that is the growth of order through domain coarsening when a system is quenched from the homogeneous phase into a broken-symmetry phase, is reviewed, with the emphasis on recent developments. Interest will focus on the scaling regime that develops at long times after the quench.How can one determine the growth laws that describe the time dependence of characteristic length scales, and what can be said about the form of the associated scaling functions? Particular attention will be paid to systems described by more complicated order parameters than the simple scalars usually considered, for example vector and tensor ®elds. The latter are needed, for example, to describe phase ordering in nematic liquid crystals, on which there have been a number of recent experiments. The study of topological defects (domain walls, vortices, strings and monopoles) provides a unifying framework for discussing coarsening in these diA erent systems.",1994,301,1631,78,1,6,16,24,31,33,41,39,58,68
985f76317c3d17aa3ac7c346d29d972d19b55db1,"Intrinsic rate equations were derived for the steam reforming of methane, accompanied by water-gas shift on a Ni/MgAl2O4 catalyst. A large number of detailed reaction mechanisms were considered. Thermodynamic analysis helped in reducing the number of possible mechanisms. Twenty one sets of three rate equations were retained and subjected to model discrimination and parameter estimation. The parameter estimates in the best model are statistically significant and thermodynamically consistent.",1989,8,1766,83,0,4,4,4,5,4,4,3,8,5
a0405e8b39e24a9c37d3dd67f8f6bfe5aff0ac1f,,1965,24,2455,38,0,1,8,3,3,3,3,4,9,11
ab7dd811be154081aa8c7e778a0ecde7fbac17a8,"Protein levels have been shown to vary substantially between individual cells in clonal populations. In prokaryotes, the contribution to such fluctuations from the inherent randomness of gene expression has largely been attributed to having just a few transcripts of the corresponding mRNAs. By contrast, eukaryotic studies tend to emphasize chromatin remodeling and burst-like transcription. Here, we study single-cell transcription in Escherichia coli by measuring mRNA levels in individual living cells. The results directly demonstrate transcriptional bursting, similar to that indirectly inferred for eukaryotes. We also measure mRNA partitioning at cell division and correlate mRNA and protein levels in single cells. Partitioning is approximately binomial, and mRNA-protein correlations are weaker earlier in the cell cycle, where cell division has recently randomized the relative concentrations. Our methods further extend protein-based approaches by counting the integer-valued number of transcript with single-molecule resolution. This greatly facilitates kinetic interpretations in terms of the integer-valued random processes that produce the fluctuations.",2005,73,1367,83,1,32,56,55,80,78,101,88,108,88
9043b736c593390f4389409f8051c95b75e1de97,"Nicotine is of importance as the addictive chemical in tobacco, pharmacotherapy for smoking cessation, a potential medication for several diseases, and a useful probe drug for phenotyping cytochrome P450 2A6 (CYP2A6). We review current knowledge about the metabolism and disposition kinetics of nicotine, some other naturally occurring tobacco alkaloids, and nicotine analogs that are under development as potential therapeutic agents. The focus is on studies in humans, but animal data are mentioned when relevant to the interpretation of human data. The pathways of nicotine metabolism are described in detail. Absorption, distribution, metabolism, and excretion of nicotine and related compounds are reviewed. Enzymes involved in nicotine metabolism including cytochrome P450 enzymes, aldehyde oxidase, flavin-containing monooxygenase 3, amine N-methyltransferase, and UDP-glucuronosyltransferases are represented, as well as factors affecting metabolism, such as genetic variations in metabolic enzymes, effects of diet, age, gender, pregnancy, liver and kidney diseases, and racial and ethnic differences. Also effects of smoking and various inhibitors and inducers, including oral contraceptives, on nicotine metabolism are discussed. Due to the significance of the CYP2A6 enzyme in nicotine clearance, special emphasis is given to the effects and population distributions of CYP2A6 alleles and the regulation of CYP2A6 enzyme.",2005,549,1279,100,14,47,57,56,56,65,75,87,108,89
5a1e3136ac33b0cdcec827c245738f3e8ba0488c,"Because of its toxicity, arsenic is of considerable environmental concern. Its solubility in natural systems is strongly influenced by adsorption at iron oxide surfaces. The objective of this study was to compare the adsorption behavior of arsenite and arsenate on ferrihydrite, under carefully controlled conditions, with regard to adsorption kinetics, adsorption isotherms, and the influence of pH on adsorption. The adsorption reactions were relatively fast, with the reactions almost completed within the first few hours. At relatively high As concentrations, arsenite reacted faster than arsenate with the ferrihydrite, i.e., equilibrium was achieved sooner, but arsenate adsorption was faster at low As concentrations and low pH. Adsorp tion maxima of approximately 0.60 (0.58) and 0.25 (0.16) molAs molFe-1 were achieved for arsenite and arsenate, respectively, at pH 4.6 (pH 9.2 in parentheses). The high arsenite retention, which precludes its retention entirely as surface adsorbed species, indicates the likel...",1998,7,1320,90,1,6,22,25,31,40,33,52,54,54
9643b8e34043edd982ac64ee2ea7ec0b81d9852e,"Three major models (from Tofts, Larsson, and Brix) for collecting and analyzing dynamic MRI gadolinium‐diethylenetriamine penta‐acetic acid (Gd‐DTPA) data are examined. All models use compartments representing the blood plasma and the abnormal extravascular extracellular space (EES), and they are intercompatible. All measure combinations of three parameters: (1) kpsρ is the influx volume transfer constant (min−1), or permeability surface area product per unit volume of tissue, between plasma and EES; (2) ve is the volume of EES space per unit volume of tissue (0 < ve < 1); and (3) kep, the efflux rate constant (min−1), is the ratio of the first two parameters (kep = kpsρ/ve). The ratio kep is the simplest to measure, requiring only signal linearity with Gd tracer concentration or, alternatively, a measurement of T1 before injection of Gd (T10). To measure the physiologic parameters kpsρ and ve separately requires knowledge of T10 and of the tissue relaxivity R1 (≈︁ in vitro value).",1997,61,1431,88,3,10,15,24,8,30,34,35,54,56
7238deb8a30f94c0a5a2c64d9123095e78267aa6,"DNA is increasingly being used as the engineering material of choice for the construction of nanoscale circuits, structures, and motors. Many of these enzyme-free constructions function by DNA strand displacement reactions. The kinetics of strand displacement can be modulated by toeholds, short single-stranded segments of DNA that colocalize reactant DNA molecules. Recently, the toehold exchange process was introduced as a method for designing fast and reversible strand displacement reactions. Here, we characterize the kinetics of DNA toehold exchange and model it as a three-step process. This model is simple and quantitatively predicts the kinetics of 85 different strand displacement reactions from the DNA sequences. Furthermore, we use toehold exchange to construct a simple catalytic reaction. This work improves the understanding of the kinetics of nucleic acid reactions and will be useful in the rational design of dynamic DNA and RNA circuits and nanodevices.",2009,73,933,36,2,17,24,48,57,84,96,85,102,109
7cecac43cad599979f6d7fb75c85ddbe66c9e866,"Nicotine underlies tobacco addiction, influences tobacco use patterns, and is used as a pharmacological aid to smoking cessation. The absorption, distribution and disposition characteristics of nicotine from tobacco and medicinal products are reviewed. Nicotine is metabolized primarily by the liver enzymes CYP2A6, UDPglucuronosyltransferase (UGT), and flavin-containing monooxygenase (FMO). In addition to genetic factors, nicotine metabolism is influenced by diet and meals, age, sex, use of estrogen-containing hormone preparations, pregnancy and kidney disease, other medications, and smoking itself. Substantial racial/ethnic differences are observed in nicotine metabolism, which are likely influenced by both genetic and environmental factors. The most widely used biomarker of nicotine intake is cotinine, which may be measured in blood, urine, saliva, hair, or nails. The current optimal plasma cotinine cut-point to distinguish smokers from non-smokers in the general US population is 3 ng ml(-1). This cut-point is much lower than that established 20 years ago, reflecting less secondhand smoke exposure due to clear air policies and more light or occasional smoking.",2009,150,965,63,6,24,36,32,59,71,87,93,91,110
3a3d221aa1043a58f005b9ab6c581c5814634807,"Basic Principles of Chemical Kinetics Introduction to Enzyme Kinetics ""Alternative"" Enzymes Practical Aspects of Kinetics Deriving Steady-state Rate Equations Reversible Inhibition and Activation Tight-binding and Irreversible Inhibitors Reactions of More than One Substrate Use of Isotopes for Studying Enzyme Mechanisms Effect of pH on Enzyme Activity Temperature Effects on Enzyme Activity Regulation of Enzyme Activity Multienzyme Systems Fast Reactions Estimation of Kinetic Constants Standards for Reporting Enzymology Data Solutions and Notes to Problems Index",1979,0,1816,98,0,1,11,12,14,13,13,16,19,21
28713d928f1f9654917e819d737dd126cf32b50b,"Dissecting Amyloid Formation Amyloid fibrils are associated with clinical disorders ranging from Alzheimer's disease to type II diabetes. Their self-assembly can be described by a master equation that takes into account nucleation-dependent polymerization and fragmentation. Knowles et al. (p. 1533) now present an analytical solution to the master equation, which shows that amyloid growth kinetics is often limited by the fragmentation rate rather than by the rate of primary nucleation. In addition, the results reveal relationships between system properties (scaling laws) that provide mechanistic insight not only into amyloid growth, but also into related self-assembly processes. The growth kinetics of amyloid fibrils and related self-assembly phenomena are revealed by analytical theory. We present an analytical treatment of a set of coupled kinetic equations that governs the self-assembly of filamentous molecular structures. Application to the case of protein aggregation demonstrates that the kinetics of amyloid growth can often be dominated by secondary rather than by primary nucleation events. Our results further reveal a range of general features of the growth kinetics of fragmenting filamentous structures, including the existence of generic scaling laws that provide mechanistic information in contexts ranging from in vitro amyloid growth to the in vivo development of mammalian prion diseases.",2009,31,871,31,1,28,58,71,81,73,88,76,77,88
5b8d72c87266ac5cff1fa3ac9a5619354eb0ae7f,"Abstract The kinetics of glide at constant structure and the kinetics of structure evolution are correlated on the basis of various experimental observations in pure f.c.c. mono- and polycrystals. Two regimes of behavior are identified. In the initial regime, the Cottrell-Stokes law is satisfied, hardening is athermal, and a single structure parameter is adequate. With increasing importance of dynamic recovery, be it at large strains or at high temperatures, all of these simple assumptions break down. However, the proportionality between the flow stress and the square-root of the dislocation density holds, to a good approximation, over the entire regime; mild deviations arc primarily ascribed to differences between the various experimental techniques used. A phenomenological model is proposed, which incorporates the rate of dynamic recovery into the flow kinetics. It has been successful in matching many experimental data quantitatively.",1981,29,1799,43,0,2,6,2,5,15,11,5,7,7
bbb35e715579a067091eb0feb5dd912b2ee31dcc,Empirical Treatment of Reaction Rates. Experimental Methods and Treatment of Data. Elementary Processes: Molecular Collisions. Elementary Processes: Potential Energy Surfaces and Transition--State Theory. Simple Gas--Phase Reactions--Interplay of Theory and Experiment. Reactions in Solution. Complex Reactions. Homogeneous Catalysts. Chain Reactions. Photochemistry. Appendix.,1961,0,2047,40,7,9,10,13,17,12,23,19,22,25
87f942443f71277d7f1919e0824dcdb843f85fe1,"Abstract This paper describes a simple yet sensitive laboratory procedure that was developed to provide detailed information on the fermentation kinetics of ruminant feeds. In principle, the technique is similar to other in vitro digestibility procedures using ground particulate substrates, anaerobic media and a rumen fluid inoculum. It differs, however, in that incubations are conducted in gas-tight culture bottles, thus enabling gases to accumulate in the head-space as the fermentation proceeds. A pressure transducer connected to a digital readout voltmeter and gas-tight syringe assembly is then used to measure and release the accumulated gas pressures from the incubating culture bottles. By repeating this gas-measurement, gas-release procedure at regular intervals during the fermentation, it is possible to construct gas accumulation profiles for feeds by summation of regression-corrected gas volumes. These profiles are then described using a recently derived growth function developed to characterise gas production profiles. Results obtained establish the pressure transducer as a suitable tool for determining the fermentation kinetics of ruminant feeds and ranking them with respect to their in vitro fermentability.",1994,30,1378,76,0,12,21,17,29,27,20,17,20,19
dd4164b2dff46cb997266bb7ef0b4057b7a06755,"Abstract Chaotic dynamics can be considered as a physical phenomenon that bridges the regular evolution of systems with the random one. These two alternative states of physical processes are, typically, described by the corresponding alternative methods: quasiperiodic or other regular functions in the first case, and kinetic or other probabilistic equations in the second case. What kind of kinetics should be for chaotic dynamics that is intermediate between completely regular (integrable) and completely random (noisy) cases? What features of the dynamics and in what way should they be represented in the kinetics of chaos? These are the subjects of this paper, where the new concept of fractional kinetics is reviewed for systems with Hamiltonian chaos. Particularly, we show how the notions of dynamical quasi-traps, Poincare recurrences, Levy flights, exit time distributions, phase space topology prove to be important in the construction of kinetics. The concept of fractional kinetics enters a different area of applications, such as particle dynamics in different potentials, particle advection in fluids, plasma physics and fusion devices, quantum optics, and many others. New characteristics of the kinetics are involved to fractional kinetics and the most important are anomalous transport, superdiffusion, weak mixing, and others. The fractional kinetics does not look as the usual one since some moments of the distribution function are infinite and fluctuations from the equilibrium state do not have any finite time of relaxation. Different important physical phenomena: cooling of particles and signals, particle and wave traps, Maxwell's Demon, etc. represent some domains where fractional kinetics proves to be valuable.",2002,173,1229,32,3,26,46,89,72,82,78,59,70,62
1907fe052cd717a7ee3484c937025d0968cba2d3,,1969,0,1912,98,3,4,10,8,23,13,17,24,18,24
99ca73a0e103dca19d945f97a83fe318741ac9b4,"Abstract The intraparticle diffusion model (IPD) proposed by Weber and Morris has been widely applied for the analysis of adsorption kinetics. In this work, the characteristic curves based on this model were plotted with various initial adsorption factors (Ri). Four zones of the initial adsorption according to Ri value from 0 to 1 were classified; that is, approaching completely initial adsorption (zone 4), strongly initial adsorption (zone 3), intermediately initial adsorption (zone 2), and weakly initial adsorption (zone 1). Activated carbons with micropore volume fraction of 0.537 and 0.686 were prepared from oil-palm shells by steam activation. Based on the standard deviations, the kinetics of the adsorption of tannic acid (TA), methylene blue (MB), phenol, and 4-chlorophenol (4-CP) on activated carbons could be best described by intraparticle diffusion model. The initial adsorption of TA and MB belonged to zone 2, and that of phenol and 4-CP mostly belonged to zone 3. Nearly 80% of the 86 adsorption systems surveyed belonged to zones 2 and 3, indicating that the Ri value was smaller when the carbon with smaller particle and steam-activated carbon was used.",2009,62,771,12,1,15,17,22,33,39,39,68,70,85
993249be5ea76b470046b13eec5e6979d0fc8c0f,"28, 303 (1956). Heilbron, I. M., Kamm, E. D., Owens, W. M., J . Chem. SOC. 1926, 1631. Mer, O., Riiegg, R., Chopard-ditJean, L., Bernhard, K., Helv. Chim. Acta 39, 897 (195s). (7) Karrer, P., Helfenstein, A., Ibid., 14, 78 (1931). (8) Sax, K. J., Stross, F. H., J . Org. Chew 22, 1251 (1957). (9) Schmitt, J., Ann. 547, 115 (1941). (10) Trippett, S., Chem. & Ind. (London) 1956,80. (11) Tsujimoto, M., Ind. Ena. Chem. 8. . . 889 (191s). '",1957,2,1597,237,0,0,0,0,0,0,0,0,0,0
e97c8469ff95e4e873d4c9bb1bc32ffd79a5039c,Abstract Non-isothermal kinetics of the process of nucleation and its growth are derived by extending Avrami's equation. Kinetic analysis of the thermoanalytical data of the process is also described and applied to DSC curves of crystallization obtained by cooling poly(ethylene terephthalate) at constant rates. Crystallization is thought to proceed by nuclei being formed randomly and growing in three-dimensions.,1971,16,1841,31,0,1,1,0,0,0,4,1,4,1
287c5eb5857102bff5f566ed9ed723d25a960328,"Objective. To establish a clinically relevant list with explicit criteria for pharmacologically inappropriate prescriptions in general practice for elderly people ≥70 years. Design. A three-round Delphi process for validating the clinical relevance of suggested criteria (n = 37) for inappropriate prescriptions to elderly patients. Setting. A postal consensus process undertaken by a panel of specialists in general practice, clinical pharmacology, and geriatrics. Main outcome measures. The Norwegian General Practice (NORGEP) criteria, a relevance-validated list of drugs, drug dosages, and drug combinations to be avoided in the elderly (≤70 years) patients. Results. Of the 140 invited panellists, 57 accepted to participate and 47 completed all three rounds of the Delphi process. The panellists reached consensus that 36 of the 37 suggested criteria were clinically relevant for general practice. Relevance of three of the criteria was rated significantly higher in Round 3 than in Round 1. At the end of the Delphi process, a significant difference between the different specialist groups’ scores was seen for only one of the 36 criteria. Conclusion. The NORGEP criteria may serve as rules of thumb for general practitioners (GPs) related to their prescribing practice for elderly patients, and as a tool for evaluating the quality of GPs’ prescribing in settings where access to clinical information for individual patients is limited, e.g. in prescription databases and quality improvement interventions.",2009,48,216,18,0,7,13,25,18,23,21,24,18,25
71de3ae53b4ad8557f6b203b3d9e555a66bbb651,"The epidemiology of immune thrombocytopenic purpura (ITP) is not well‐characterised in the general population. This study described the incidence and survival of ITP using the UK population‐based General Practice Research Database (GPRD). ITP patients first diagnosed in 1990–2005 were identified in the GPRD. Overall incidence rates (per 100 000 person‐years) and rates by age, sex, and calendar periods were calculated. Survival analysis was conducted using the Kaplan‐Meier and proportional hazard methods. A total of 1145 incident ITP patients were identified. The crude incidence was 3·9 (95% confidence interval [CI]: 3·7–4·1). Overall average incidence was statistically significantly higher in women (4·4, 95% CI: 4·1–4·7) compared to men (3·4; 95% CI: 3·1–3·7). Among men, incidence was bimodal with peaks among ages under 18 and between 75–84 years. The hazard ratio for death among ITP patients was 1·6 (95% CI: 1·3–1·9) compared to age‐ and sex‐matched comparisons. During follow‐up 139 cases died, of whom 75 had a computerised plausible cause of death. Death was related to bleeding in 13% and infection in 19% of these 75. In conclusion, ITP incidence varies with age and is higher in women than men. This potentially serious medical condition is associated with increased mortality in the UK.",2009,49,195,11,2,12,14,17,16,18,22,15,22,14
f8e5bbe7e6f67ad4f829e21c90a1d288a13e9825,"Ageing of the population in western societies and the rising costs of health and social care are refocusing health policy on health promotion and disability prevention among older people. However, efforts to identify at-risk groups of older people and to alter the trajectory of avoidable problems associated with ageing by early intervention or multidisciplinary case management have been largely unsuccessful. This paper argues that this failure arises from the dominance in primary care of a managerial perspective on health care for older people, and proposes instead the adoption of a clinical paradigm based on the concept of frailty. Frailty, in its simplest definition, is vulnerability to adverse outcomes. It is a dynamic concept that is different from disability and easy to overlook, but also easy to identify using heuristics (rules of thumb) and to measure using simple scales. Conceptually, frailty fits well with the biopsychosocial model of general practice, offers practitioners useful tools for patient care, and provides commissioners of health care with a clinical focus for targeting resources at an ageing population.",2009,72,198,3,3,10,16,19,20,10,22,17,20,14
e3283da7acdf880647c34b48e2951058abeba8f3,Objective: To evaluate the management of cardiovascular disease (CVD) risk in Australian general practice.,2009,31,116,2,2,10,4,14,10,11,16,6,9,11
91e0c86e47e15bee8a3b6203c7ffd650a3afc75f,"BACKGROUND
Although studies are available on patients' ideas, concerns, and expectations in primary care, there is a scarcity of studies that explore the triad of ideas, concerns, and expectations (ICE) in general practice consultations and the impact on medication prescribing.


AIM
To evaluate the presence of ICE and its relation to medication prescription.


DESIGN OF STUDY
Cross-sectional study.


SETTING
Thirty-six GP teaching practices affiliated with the University of Ghent, in Flanders, Belgium.


METHOD
Participants were all patients consulting on 30 May 2005, and their doctors. Reasons for an encounter (consultation or home visit) with new and follow-up contacts, the identification of ICE, and the prescription of medication were recorded by 36 trainee GPs undergoing observational training. The study included 613 consultations.


RESULTS
One, two, or three of the ICE components were expressed in 38.5%, 24.4%, and 20.1% (n = 236, 150, 123) of contacts respectively. On the other hand, in 17.0% (104/613) of all contacts, and in 22% (77/350) of the new contact reasons, no ICE was voiced, and the GPs operated without knowing this information about the patients. Mean number of ICE components per doctor and per contact was 1.54 (standard deviation = 0.54). A logistic regression analysis of the 350 new contacts showed that the presence of concerns (P = 0.037, odds ratio [OR] 1.73, 95% confidence interval [CI] = 1.03 to 2.9), and expectations (P = 0.009, OR = 2.0, 95% CI = 1.2 to 3.4) was associated with not prescribing new medication (dichotomised into the categories present/absent); however, other patient, doctor, and student variables were not significantly associated with medication prescription.


CONCLUSION
An association was found between the presence of concerns and/or expectations, and less medication prescribing. The data suggest that exploring ICE components may lead to fewer new medication prescriptions.",2009,45,93,1,6,4,3,3,6,9,8,5,8,9
9397e3d9a5556de8be5ed16bedfaf3d2e78d054c,"BACKGROUND
The extent to which a fear of needles influences health decisions remains largely unexplored. This study investigated the prevalence of fear of needles in a southeast Queensland community, described associated symptoms, and highlighted health care avoidance tendencies of affected individuals.


METHODS
One hundred and seventy-seven participants attending an outer urban general practice responded to a questionnaire on fear of needles, symptoms associated with needles and its influence on their use of medical care.


RESULTS
Twenty-two percent of participants reported a fear of needles. Affected participants were more likely than participants with no fear to report vasovagal symptoms, have had a previous traumatic needle experience (46.2 vs. 16.4%, p<0.001) and avoid medical treatment involving needles (20.5 vs. 2.3%, p<0.001).


DISCUSSION
Fear of needles is common and is associated with health care avoidance. Health professionals could better identify and manage patients who have a fear of needles by recognising associated vasovagal symptoms and past traumatic experiences.",2009,16,116,5,1,1,3,5,5,16,11,18,15,15
ade4d85d0937c9445fc88a96feab1af55e785f19,"BACKGROUND
Health literacy is the ability to understand and interpret the meaning of health information in written, spoken or digital form and how this motivates people to embrace or disregard actions relating to health.


OBJECTIVE
This article aims to describe the concept of health literacy, its importance and its applications in the general practice setting.


DISCUSSION
Australia trails behind other western countries in practical applications of health literacy. Health literacy underpins the efficiency of consultations, health promotion efforts, and self management programs. Recognition of the health literacy status of individuals allows use of appropriate communication tools. This can save time and effort and improve patient satisfaction and health outcomes.",2009,23,119,5,2,4,11,12,8,11,12,7,6,10
f7d7a2c03932683dfc56baf3627f37c6c165aef3,"BackgroundGeneral practitioners sometimes base clinical decisions on gut feelings alone, even though there is little evidence of their diagnostic and prognostic value in daily practice. Research to validate the determinants and to assess the test properties of gut feelings requires precise and valid descriptions of gut feelings in general practice which can be used as a reliable measuring instrument. Research question: Can we obtain consensus on descriptions of two types of gut feelings: a sense of alarm and a sense of reassurance?MethodsQualitative research including a Delphi consensus procedure with a heterogeneous sample of 27 Dutch and Belgian GPs or ex-GPs involved in academic educational or research programmes.ResultsAfter four rounds, we found 70% or greater agreement on seven of the eleven proposed statements. A ""sense of alarm"" is defined as an uneasy feeling perceived by a GP as he/she is concerned about a possible adverse outcome, even though specific indications are lacking: There's something wrong here. This activates the diagnostic process by stimulating the GP to formulate and weigh up working hypotheses that might involve a serious outcome. A ""sense of alarm"" means that, if possible, the GP needs to initiate specific management to prevent serious health problems. A ""sense of reassurance"" is defined as a secure feeling perceived by a GP about the further management and course of a patient's problem, even though the doctor may not be certain about the diagnosis: Everything fits in.ConclusionThe sense of alarm and the sense of reassurance are well-defined concepts. These descriptions enable us to operationalise the concept of gut feelings in further research.",2009,21,77,4,0,11,5,3,5,4,4,5,9,9
d29c2d9d76def6f0789b16d7fc9227dc0e08c3e2,"Over the past 5 years, general practice in the UK has undergone major change. Starting with the introduction of the new GMS contract in 2004, it has continued apace with the establishment of Postgraduate Medical Education Training Board, a GP training curriculum, and nMRCGP. The NHS is developing very differently in the four countries of the UK. Regulation of the profession is under review, and a system of relicensing, recertification, and revalidation is being introduced. The Essence project, initiated by RCGP Scotland in conjunction with International Futures Forum 4 years ago is a constructive response to these changes. It has included learning journeys, a discussion day for GPs, and commissioned short pieces of 100 words from GPs and patients. From an analysis of these, some characteristics of the essence of general practice have been defined. These include key roles and core personal qualities for GPs. It is argued that general practice has important and unique advantages - trust, coordination, continuity, flexibility, universal coverage, and leadership - which mean that it should continue to be central to the development of primary care throughout the UK.",2009,55,53,1,9,8,7,7,2,7,3,2,4,2
f66a0d9cfd4f5eb5d586f53732cfe2bb38b471cb,"BACKGROUND
In Australia, most medical students graduate without a firm career choice, with this decision being made during their early postgraduate years. Strategies addressing the current lack of meaningful exposure to general practice during these formative prevocational years are likely to be the most effective in increasing the proportion and number of entrants to general practice.


OBJECTIVE
This review summarises the influences of medical student selection criteria, curriculum, geographical location, timing and duration of general practice exposure and experience, prevocational experience, and vocational training, on an eventual choice of general practice as a career.


DISCUSSION
These are important influences on the complex process of career choice. Much research has focused on isolated interventions at one point along the pipeline. Varied and conflicting conclusions emerge from individual studies. In complex systems it is hard to understand the influence of an isolated intervention without looking at the system as a whole.",2009,28,60,2,0,2,12,13,4,6,4,8,2,3
b612a64c54e09baa3f094940bd5aa9870fd42df9,"BackgroundSchizophrenia patients frequently develop somatic co-morbidity. Core tasks for GPs are the prevention and diagnosis of somatic diseases and the provision of care for patients with chronic diseases. Schizophrenia patients experience difficulties in recognizing and coping with their physical problems; however GPs have neither specific management policies nor guidelines for the diagnosis and treatment of somatic co-morbidity in schizophrenia patients. This paper systematically reviews the prevalence and treatment of somatic co-morbidity in schizophrenia patients in general practice.MethodsThe MEDLINE, EMBASE, PsycINFO data-bases and the Cochrane Library were searched and original research articles on somatic diseases of schizophrenia patients and their treatment in the primary care setting were selected.ResultsThe results of this search show that the incidence of a wide range of diseases, such as diabetes mellitus, the metabolic syndrome, coronary heart diseases, and COPD is significantly higher in schizophrenia patients than in the normal population. The health of schizophrenic patients is less than optimal in several areas, partly due to their inadequate help-seeking behaviour. Current GP management of such patients appears not to take this fact into account. However, when schizophrenic patients seek the GP's help, they value the care provided.ConclusionSchizophrenia patients are at risk of undetected somatic co-morbidity. They present physical complaints at a late, more serious stage. GPs should take this into account by adopting proactive behaviour. The development of a set of guidelines with a clear description of the GP's responsibilities would facilitate the desired changes in the management of somatic diseases in these patients.",2009,50,115,0,4,8,14,9,13,9,9,9,10,10
6b25ca281526bf0a5fb955ad2617662d4e52718b,"BackgroundWith increasing rates of chronic disease associated with lifestyle behavioural risk factors, there is urgent need for intervention strategies in primary health care. Currently there is a gap in the knowledge of factors that influence the delivery of preventive strategies by General Practitioners (GPs) around interventions for smoking, nutrition, alcohol consumption and physical activity (SNAP). This qualitative study explores the delivery of lifestyle behavioural risk factor screening and management by GPs within a 45–49 year old health check consultation. The aims of this research are to identify the influences affecting GPs' choosing to screen and choosing to manage SNAP lifestyle risk factors, as well as identify influences on screening and management when multiple SNAP factors exist.MethodsA total of 29 audio-taped interviews were conducted with 15 GPs and one practice nurse over two stages. Transcripts from the interviews were thematically analysed, and a model of influencing factors on preventive care behaviour was developed using the Theory of Planned Behaviour as a structural framework.ResultsGPs felt that assessing smoking status was straightforward, however some found assessing alcohol intake only possible during a formal health check. Diet and physical activity were often inferred from appearance, only being assessed if the patient was overweight. The frequency and thoroughness of assessment were influenced by the GPs' personal interests and perceived congruence with their role, the level of risk to the patient, the capacity of the practice and availability of time. All GPs considered advising and educating patients part of their professional responsibility. However their attempts to motivate patients were influenced by perceptions of their own effectiveness, with smoking causing the most frustration. Active follow-up and referral of patients appeared to depend on the GPs' orientation to preventive care, the patient's motivation, and cost and accessibility of services to patients.ConclusionGeneral practitioner attitudes, normative influences from both patients and the profession, and perceived external control factors (time, cost, availability and practice capacity) all influence management of behavioural risk factors. Provider education, community awareness raising, support and capacity building may improve the uptake of lifestyle modification interventions.",2009,22,180,13,0,11,10,23,30,20,24,12,9,15
6624a88676463a96f3b67c7731d0ea8695f5a8df,"Traditionally, professional expertise has been judged by length of experience, reputation, and perceived mastery of knowledge and skill. Unfortunately, recent research demonstrates only a weak relationship between these indicators of expertise and actual, observed performance. In fact, observed performance does not necessarily correlate with greater professional experience. Expert performance can, however, be traced to active engagement in deliberate practice (DP), where training (often designed and arranged by their teachers and coaches) is focused on improving particular tasks. DP also involves the provision of immediate feedback, time for problem-solving and evaluation, and opportunities for repeated performance to refine behavior. In this article, we draw upon the principles of DP established in other domains, such as chess, music, typing, and sports to provide insight into developing expert performance in medicine.",2008,41,1184,52,2,9,30,46,66,99,99,109,110,107
7cfad0fc9e5413aa34324b776c027e5c1aa89b39,"Outline of a Theory of Practice is recognized as a major theoretical text on the foundations of anthropology and sociology. Pierre Bourdieu, a distinguished French anthropologist, develops a theory of practice which is simultaneously a critique of the methods and postures of social science and a general account of how human action should be understood. With his central concept of the habitus, the principle which negotiates between objective structures and practices, Bourdieu is able to transcend the dichotomies which have shaped theoretical thinking about the social world. The author draws on his fieldwork in Kabylia (Algeria) to illustrate his theoretical propositions. With detailed study of matrimonial strategies and the role of rite and myth, he analyses the dialectical process of the 'incorporation of structures' and the objectification of habitus, whereby social formations tend to reproduce themselves. A rigorous consistent materialist approach lays the foundations for a theory of symbolic capital and, through analysis of the different modes of domination, a theory of symbolic power.",1972,0,22475,1640,0,0,0,0,0,0,0,0,0,0
fe001eb4659b6fefa8974780bcfb8afc0eaa0e42,,1899,0,99,0,0,0,0,0,0,0,0,0,0,0
2f3d7e9a29b58ec04a0d52257984011b1154962d,"AbstractBackgroundThe impact of high physician workload and job stress on quality and outcomes of healthcare delivery is not clear. Our study explored whether high workload and job stress were associated with lower performance in general practices in the Netherlands.MethodsSecondary analysis of data from 239 general practices, collected in practice visits between 2003 to 2006 in the Netherlands using a comprehensive set of measures of practice management. Data were collected by a practice visitor, a trained non-physician observer using patients questionnaires, doctors and staff. For this study we selected five measures of practice performance as outcomes and six measures of GP workload and job stress as predictors. A total of 79 indicators were used out of the 303 available indicators. Random coefficient regression models were applied to examine associations.Results and discussionWorkload and job stress are associated with practice performance.
Workload: Working more hours as a GP was associated with more positive patient experiences of accessibility and availability (b = 0.16). After list size adjustment, practices with more GP-time per patient scored higher on GP care (b = 0.45). When GPs provided more than 20 hours per week per 1000 patients, patients scored over 80% on the Europep questionnaire for quality of GP care.
Job stress: High GP job stress was associated with lower accessibility and availability (b = 0.21) and insufficient practice management (b = 0.25). Higher GP commitment and more satisfaction with the job was associated with more prevention and disease management (b = 0.35).ConclusionProviding more time in the practice, and more time per patient and experiencing less job stress are all associated with perceptions by patients of better care and better practice performance. Workload and job stress should be assessed by using list size adjusted data in order to realise better quality of care. Organisational development using this kind of data feedback could benefit both patients and GP.",2009,48,92,1,0,4,4,7,7,6,8,9,9,10
5a9847dc44598142d4552f6f4e4d6b30b31c43c0,"Primary care spirometry services can be provided by trained primary care staff, peripatetic specialist services, or through referral to hospital-based or laboratory spirometry. The first of these options is the focus of this Standards Document. It aims to provide detailed information for clinicians, managers and healthcare commissioners on the key areas of quality required for diagnostic spirometry in primary care--including training requirements and quality assurance. These proposals and recommendations are designed to raise the standard of spirometry and respiratory diagnosis in primary care and to provide the impetus for debate, improvement and maintenance of quality for diagnostic (rather than screening) spirometry performed in primary care. This document should therefore challenge current performance and should constitute an aspirational guide for delivery of this service.",2009,170,185,4,5,17,21,24,19,14,15,19,11,9
c2f2e32233712d0fc3ca57a401464600ca6e28e0,"BACKGROUND AND AIMS
Clinical inertia is considered a major barrier to better care. We assessed its prevalence, predictors and associations with the intermediate outcomes of diabetes care.


MATERIALS AND METHODS
Baseline and follow-up data of a Dutch randomized controlled trial on the implementation of a locally adapted guideline were used. The study involved 30 general practices and 1283 patients. Treatment targets differed between study groups [HbA1c <or= 8.0% and blood pressure (BP) < 140/85% versus HbA1c <or= 8.5% and BP < 150/85]. Clinical inertia was defined as the failure to intensify therapy when indicated. A complete medication profile of all participating patients was obtained.


RESULTS
In the intervention and control group, the percentages of patients with poor diabetes or lipid control who did not receive treatment intensification were 45% and 90%, approximately. More control group patients with BP levels above target were confronted with inertia (72.7% versus 63.3%, P < 0.05). In poorly controlled hypertensive patients, inertia was associated with the height of systolic BP at baseline [adjusted odds ratio (OR) 0.98, 95% confidence interval (CI) 0.98-0.99] and the frequency of BP control (adjusted OR 0.89, 95% CI 0.81-0.99). If a practice nurse managed these patients, clinical inertia was less common (adjusted OR 0.12, 95% CI 0.02-0.91). In both study groups, cholesterol decreased significantly more in patients who received proper treatment intensification.


CONCLUSION
GPs were more inclined to control blood glucose levels than BP or cholesterol levels. Inertia in response to poorly controlled high BP was less common if nurses assisted GPs.",2009,68,91,3,1,2,13,4,22,7,9,4,9,4
3641e66e6e0bc275ccf1ad277880a9611df92fdc,"‘There are few things we should keenly desire if we really knew what we wanted.’ Francois de la Rochefoucauld (French writer 1613–1680) Social prescribing is about expanding the range of options available to GP and patient as they grapple with a problem. Where that problem has its origins in socioeconomic deprivation or long-term psychosocial issues, it is easy for both patient and GP to feel overwhelmed and reluctant to open what could turn out to be a can of worms. Settling for a short-term medical fix may be pragmatic but can easily become a conspiracy of silence which confirms the underlying sense of defeat. Can or should we try to do more during the precious minutes of a GP consultation?

Where there are psychosocial issues GPs do suggest social avenues, such as visiting a Citizens Advice Bureau for financial problems, or a dance class for exercise and loneliness, but without a supportive framework this tends to be a token action. The big picture difficulty with leaving underlying psychosocial problems largely hidden in the consulting room is the medicalisation of society's ills. This ranges from using antidepressants for the misery of a difficult life, to the complex pharmaceutical regimes prescribed to patients with obesity and type 2 diabetes. This sort of medicalisation may help immediate problems (including driving the economy through jobs in the healthcare industries) but it is not enough if our society is to have a sustainable future.

Another way of looking at this is in terms of choice. The consumerist type of choice of provider beloved of the government, is what Canadian philosopher Charles Taylor calls ‘weak evaluation’.1 By this he means a utilitarian ‘weighing-up’ of generally short-term consequences of a choice. These choices represent ‘second-order desires’, such as to feel more cheerful, or to relieve a …",2009,31,118,1,0,0,1,2,6,3,12,8,8,19
e26b2804e4d12e0391893c496d46fdbd832dd1a2,"1. Introduction Designing PCR and sequencing primers are essential activities for molecular biologists around the world. This chapter assumes acquaintance with the principles and practice of PCR, as outlined in, for example, refs. 1–4. Primer3 is a computer program that suggests PCR primers for a variety of applications, for example to create STSs (sequence tagged sites) for radiation hybrid mapping (5), or to amplify sequences for single nucleotide polymor-phism discovery (6). Primer3 can also select single primers for sequencing reactions and can design oligonucleotide hybridization probes. In selecting oligos for primers or hybridization probes, Primer3 can consider many factors. These include oligo melting temperature, length, GC content , 3′ stability, estimated secondary structure, the likelihood of annealing to or amplifying undesirable sequences (for example interspersed repeats), the likelihood of primer–dimer formation between two copies of the same primer, and the accuracy of the source sequence. In the design of primer pairs Primer3 can consider product size and melting temperature, the likelihood of primer– dimer formation between the two primers in the pair, the difference between primer melting temperatures, and primer location relative to particular regions of interest or to be avoided.",2000,21,16090,1332,0,0,1,0,0,0,2,1,2,8
e0ed44b682c1ec1684909eb9c7c06fd659aa7c74,"Objective: To investigate and compare the prevalence, comorbidities and management of gout in practice in the UK and Germany. Methods: A retrospective analysis of patients with gout, identified through the records of 2.5 million patients in UK general practices and 2.4 million patients attending GPs or internists in Germany, using the IMS Disease Analyzer. Results: The prevalence of gout was 1.4% in the UK and Germany. Obesity was the most common comorbidity in the UK (27.7%), but in Germany the most common comorbidity was diabetes (25.9%). The prevalence of comorbidities tended to increase with serum uric acid (sUA) levels. There was a positive correlation between sUA level and the frequency of gout flares. Compared with those in whom sUA was <360 μmol/l (<6 mg/dl), odds ratios for a gout flare were 1.33 and 1.37 at sUA 360–420 μmol/l (6–7 mg/dl), and 2.15 and 2.48 at sUA >530 μmol/l ( >9 mg/dl) in the UK and Germany, respectively (p<0.01). Conclusions: The prevalence of gout in practice in the UK and Germany in the years 2000–5 was 1.4%, consistent with previous UK data for 1990–9. Chronic comorbidities were common among patients with gout and included conditions associated with an increased risk for cardiovascular disease, such as obesity, diabetes and hypertension. The importance of regular monitoring of sUA in order to tailor gout treatment was highlighted by data from this study showing that patients with sUA levels ⩾360 μmol/l (⩾6 mg/dl) had an increased risk of gout flares.",2007,47,453,26,2,4,21,23,26,43,50,49,46,47
036d7b6b85f93a2b071c0354cbc0f811a1919829,"CONTEXT
People with severe mental illness (SMI) appear to have an elevated risk of death from cardiovascular disease, but results regarding cancer mortality are conflicting.


OBJECTIVE
To estimate this excess mortality and the contribution of antipsychotic medication, smoking, and social deprivation.


DESIGN
Retrospective cohort study.


SETTING
United Kingdom's General Practice Research Database. Patients Two cohorts were compared: people with SMI diagnoses and people without such diagnoses. Main Outcome Measure Mortality rates for coronary heart disease (CHD), stroke, and the 7 most common cancers in the United Kingdom.


RESULTS
A total of 46 136 people with SMI and 300 426 without SMI were selected for the study. Hazard ratios (HRs) for CHD mortality in people with SMI compared with controls were 3.22 (95% confidence interval [CI], 1.99-5.21) for people 18 through 49 years old, 1.86 (95% CI, 1.63-2.12) for those 50 through 75 years old, and 1.05 (95% CI, 0.92-1.19) for those older than 75 years. For stroke deaths, the HRs were 2.53 (95% CI, 0.99-6.47) for those younger than 50 years, 1.89 (95% CI, 1.50-2.38) for those 50 through 75 years old, and 1.34 (95% CI, 1.17-1.54) for those older than 75 years. The only significant result for cancer deaths was an unadjusted HR for respiratory tumors of 1.32 (95% CI, 1.04-1.68) for those 50 to 75 years old, which lost statistical significance after controlling for smoking and social deprivation. Increased HRs for CHD mortality occurred irrespective of sex, SMI diagnosis, or prescription of antipsychotic medication during follow-up. However, a higher prescribed dose of antipsychotics predicted greater risk of mortality from CHD and stroke.


CONCLUSIONS
This large community sample demonstrates that people with SMI have an increased risk of death from CHD and stroke that is not wholly explained by antipsychotic medication, smoking, or social deprivation scores. Rates of nonrespiratory cancer mortality were not raised. Further research is required concerning prevention of this mortality, including cardiovascular risk assessment, monitoring of antipsychotic medication, and attention to diet and exercise.",2007,36,566,31,12,32,36,45,63,46,46,37,53,36
0c2d5f55d7b9e566f3c2e2784e191b49d59fdcff,"Background There is evidence that the prevalence of common mental disorders varies across Europe. Aims To compare prevalence of common mental disorders in general practice attendees in six European countries. Method Unselected attendees to general practices in the UK, Spain, Portugal, Slovenia, Estonia and The Netherlands were assessed for major depression, panic syndrome and other anxiety syndrome. Prevalence of DSM–IV major depression, other anxiety syndrome and panic syndrome was compared between the UK and other countries after taking account of differences in demographic factors and practice consultation rates. Results Prevalence was estimated in 2344 men and 4865 women. The highest prevalence for all disorders occurred in the UK and Spain, and lowest in Slovenia and The Netherlands. Men aged 30–50 and women aged 18–30 had the highest prevalence of major depression; men aged 40–60 had the highest prevalence of anxiety, and men and women aged 40–50 had the highest prevalence of panic syndrome. Demographic factors accounted for the variance between the UK and Spain but otherwise had little impact on the significance of observed country differences. Conclusions These results add to the evidence for real differences between European countries in prevalence of psychological disorders and show that the burden of care on general practitioners varies markedly between countries.",2008,33,261,11,7,13,14,27,21,20,22,22,28,32
b0a15b2a986f058ec01e7b43eea7b1b5b35a9441,"CONTEXT
Strategies for prevention of depression are hindered by lack of evidence about the combined predictive effect of known risk factors.


OBJECTIVES
To develop a risk algorithm for onset of major depression.


DESIGN
Cohort of adult general practice attendees followed up at 6 and 12 months. We measured 39 known risk factors to construct a risk model for onset of major depression using stepwise logistic regression. We corrected the model for overfitting and tested it in an external population.


SETTING
General practices in 6 European countries and in Chile.


PARTICIPANTS
In Europe and Chile, 10 045 attendees were recruited April 2003 to February 2005. The algorithm was developed in 5216 European attendees who were not depressed at recruitment and had follow-up data on depression status. It was tested in 1732 patients in Chile who were not depressed at recruitment. Main Outcome Measure DSM-IV major depression.


RESULTS
Sixty-six percent of people approached participated, of whom 89.5% participated again at 6 months and 85.9%, at 12 months. Nine of the 10 factors in the risk algorithm were age, sex, educational level achieved, results of lifetime screen for depression, family history of psychological difficulties, physical health and mental health subscale scores on the Short Form 12, unsupported difficulties in paid or unpaid work, and experiences of discrimination. Country was the tenth factor. The algorithm's average C index across countries was 0.790 (95% confidence interval [CI], 0.767-0.813). Effect size for difference in predicted log odds of depression between European attendees who became depressed and those who did not was 1.28 (95% CI, 1.17-1.40). Application of the algorithm in Chilean attendees resulted in a C index of 0.710 (95% CI, 0.670-0.749).


CONCLUSION
This first risk algorithm for onset of major depression functions as well as similar risk algorithms for cardiovascular events and may be useful in prevention of depression.",2008,71,147,5,1,6,8,16,9,15,12,12,11,7
3be75b6ed1658b431ccd7d95ebcb4a3321942449,"BackgroundThis study was carried out to compare the HRQoL of patients in general practice with differing chronic diseases with the HRQoL of patients without chronic conditions, to evaluate the HRQoL of general practice patients in Germany compared with the HRQoL of the general population, and to explore the influence of different chronic diseases on patients' HRQoL, independently of the effects of multiple confounding variables.MethodsA cross-sectional questionnaire survey including the SF-36, the EQ-5D and demographic questions was conducted in 20 general practices in Germany. 1009 consecutive patients aged 15–89 participated. The SF-36 scale scores of general practice patients with differing chronic diseases were compared with those of patients without chronic conditions. Differences in the SF-36 scale/summary scores and proportions in the EQ-5D dimensions between patients and the general population were analyzed. Independent effects of chronic conditions and demographic variables on the HRQoL were analyzed using multivariable linear regression and polynomial regression models.ResultsThe HRQoL for general practice patients with differing chronic diseases tended to show more physical than mental health impairments compared with the reference group of patients without. Patients in general practice in Germany had considerably lower SF-36 scores than the general population (P < 0.001 for all) and showed significantly higher proportions of problems in all EQ-5D dimensions except for the self-care dimension (P < 0.001 for all). The mean EQ VAS for general practice patients was lower than that for the general population (69.2 versus 77.4, P < 0.001). The HRQoL for general practice patients in Germany seemed to be more strongly affected by diseases like depression, back pain, OA of the knee, and cancer than by hypertension and diabetes.ConclusionGeneral practice patients with differing chronic diseases in Germany had impaired quality of life, especially in terms of physical health. The independent impacts on the HRQoL were different depending on the type of chronic disease. Findings from this study might help health professionals to concern more influential diseases in primary care from the patient's perspective.",2008,55,122,7,0,3,9,17,7,11,14,12,12,4
8cdc8ed40279a869624c0796e9319ea1850af3b6,"AIMS
The purpose of this study was to describe the demographic and employment characteristics of Australian practice nurses and explore the relationship between these characteristics and the nurses' role.


BACKGROUND
Nursing in general practice is an integral component of primary care and chronic disease management in the United Kingdom and New Zealand, but in Australia it is an emerging specialty and there is limited data on the workforce and role.


DESIGN
National postal survey embedded in a sequential mixed method design.


METHODS
284 practice nurses completed a postal survey during 2003-2004. Descriptive statistics and factor analysis were utilized to analyse the data.


RESULTS
Most participants were female (99%), Registered Nurses (86%), employed part-time in a group practice, with a mean age of 45.8 years, and had a hospital nursing certificate as their highest qualification (63%). The tasks currently undertaken by participants and those requiring further education were inversely related (R2 = -0.779). Conversely, tasks perceived to be appropriate for a practice nurse and those currently undertaken by participants were positively related (R2 = 0.8996). There was a mismatch between the number of participants who perceived that a particular task was appropriate and those who undertook the task. This disparity was not completely explained by demographic or employment characteristics. Extrinsic factors such as legal and funding issues, lack of space and general practitioner attitudes were identified as barriers to role expansion.


CONCLUSION
Practice nurses are a clinically experienced workforce whose skills are not optimally harnessed to improve the care of the growing number of people with chronic and complex conditions. Relevance to clinical practice. Study data reveal a need to overcome the funding, regulatory and interprofessional barriers that currently constrain the practice nurse role. Expansion of the practice nurse role is clearly a useful adjunct to specialist management of chronic and complex disease, particularly within the context of contemporary policy initiatives.",2008,28,133,7,2,12,4,3,8,10,11,9,4,14
44655b06c43cd1a05cec8c15dd83c3a7b4f08f30,"The emergence of healthcare assistants (HCAs) in general practice raises questions about roles and responsibilities, patients' acceptance, cost-effectiveness, patient safety and delegation, training and competence, workforce development, and professional identity. There has been minimal research into the role of HCAs and their experiences, as well as those of other staff working with HCAs in general practice. Lessons may be learned from their role and evidence of their effectiveness in hospital settings. Such research highlights blurred and contested role boundaries and threats to professional identity, which have implications for teamwork, quality of patient care, and patient safety. In this paper it is argued that transferability of evidence from hospital settings to the context of general practice cannot be assumed. Drawing on the limited research in general practice, the challenges and benefits of developing the HCA role in general practice are discussed. It is suggested that in the context of changing skill-mix models, viewing roles as fluid and dynamic is more helpful and reflective of individuals' experiences than endeavouring to impose fixed role boundaries. It is concluded that HCAs can make an increasingly useful contribution to the skill mix in general practice, but that more research and evaluation are needed to inform their training and development within the general practice team.",2008,52,85,4,2,4,4,9,8,7,6,12,6,6
fbff227595f056558eca8c081f11838ece184113,"BackgroundContinual quality improvement in primary care is an international priority. In the United Kingdom, the major initiative for improving quality of care is the Quality and Outcomes Framework (QoF) of the 2004 GP contract. Although the primary focus of the QoF is on clinical care, it is acknowledged that a comprehensive assessment of quality also requires valid and reliable measurement of the patient perspective, so financial incentives are included in the contract for general practices to survey patients' views. One questionnaire specified for use in the QoF is the General Practice Assessment Questionnaire (GPAQ). This paper describes the development of the GPAQ (with post-consultation and postal versions) and presents a preliminary examination of the psychometric properties of the questionnaire.MethodsDescription of scale development and preliminary analysis of psychometric characteristics (internal reliability, factor structure), based on a large dataset of routinely collected GPAQ surveys (n = 190,038 responses to the consultation version of GPAQ and 20,309 responses to the postal version) from practices in the United Kingdom during the 2005–6 contract year.ResultsRespondents tend to report generally favourable ratings. Responses were particularly skewed on the GP communication scale, though no more so than for other questionnaires in current use in the UK for which data were available. Factor analysis identified 2 factors that clearly relate to core concepts in primary care quality ('access' and 'interpersonal care') that were common to both version of the GPAQ. The other factors related to 'enablement' in the post-consultation version and 'nursing care' in the postal version.ConclusionThis preliminary evaluation indicates that the scales of the GPAQ are internally reliable and that the items demonstrate an interpretable factor structure. Issues concerning the distributions of GPAQ responses are discussed. Potential further developments of the item content for the GPAQ are also outlined.",2008,53,97,1,2,10,5,4,12,8,7,11,7,10
424f859cb27e3c10d1c5dd17d5da6196668e2327,"Abstract Objective: To assess the effectiveness of a home exercise programme of strength and balance retraining exercises in reducing falls and injuries in elderly women. Design: Randomised controlled trial of an individually tailored programme of physical therapy in the home (exercise group, n=116) compared with the usual care and an equal number of social visits (control group, n=117). Setting: 17 general practices in Dunedin, New Zealand. Subjects: Women aged 80 years and older living in the community and registered with a general practice in Dunedin. Main outcome measures: Number of falls and injuries related to falls and time between falls during one year of follow up; changes in muscle strength and balance measures after six months. Results: After one year there were 152 falls in the control group and 88 falls in the exercise group. The mean (SD) rate of falls was lower in the exercise than the control group (0.87 (1.29) v 1.34 (1.93) falls per year respectively; difference 0.47; 95% confidence interval 0.04 to 0.90). The relative hazard for the first four falls in the exercise group compared with the control group was 0.68 (0.52 to 0.90). The relative hazard for a first fall with injury in the exercise group compared with the control group was 0.61 (0.39 to 0.97). After six months, balance had improved in the exercise group (difference between groups in change in balance score 0.43 (0.21 to 0.65). Conclusions: An individual programme of strength and balance retraining exercises improved physical function and was effective in reducing falls and injuries in women 80 years and older. Key messages Modifiable risk factors for falls in elderly people have been well defined; they include loss of muscle strength and impaired balance A programme to improve strength and balance in women aged 80 years and older can be set up safely with four home visits from a physiotherapist This programme reduced falls and moderate injuries appreciably over the subsequent year in Dunedin, New Zealand The benefit was most noticeable in elderly people who fell often",1997,23,1076,54,2,5,20,29,44,50,59,58,60,49
b8b96b418ca0302bbc1a50f7e1bd8f8ca5ca37b4,"BackgroundObesity has become a global pandemic, considered the sixth leading cause of mortality by the WHO. As gatekeepers to the health system, General Practitioners are placed in an ideal position to manage obesity. Yet, very few consultations address weight management. This study aims to explore reasons why patients attending General Practice appointments are not engaging with their General Practitioner (GP) for weight management and their perception of the role of the GP in managing their weight.MethodsIn February 2006, 367 participants aged between 17 and 64 were recruited from three General Practices in Melbourne to complete a waiting room self – administered questionnaire. Questions included basic demographics, the role of the GP in weight management, the likelihood of bringing up weight management with their GP and reasons why they would not, and their nominated ideal person to consult for weight management. Physical measurements to determine weight status were then completed. The statistical methods included means and standard deviations to summarise continuous variables such as weight and height. Sub groups of weight and questionnaire answers were analysed using the χ2 test of significant differences taking p as < 0.05.ResultsThe population sample had similar obesity co-morbidity rates to the National Heart Foundation data. 74% of patients were not likely to bring up weight management when they visit their GP. Negative reasons were time limitation on both the patient's and doctor's part and the doctor lacking experience. The GP was the least likely person to tell a patient to lose weight after partner, family and friends. Of the 14% that had been told by their GP to lose weight, 90% had cardiovascular obesity related co-morbidities. GPs (15%) were 4th in the list of ideal persons to manage weight after personal trainerConclusionPatients do not have confidence in their GPs for weight management, preferring other health professionals who may lack evidence based training. Concurrently, GPs target only those with obesity related co-morbidities. Further studies evaluating GPs' opinions about weight management, effective strategies that can be implemented in primary care and the co-ordination of the team approach need to be done.",2008,28,67,5,0,0,1,11,4,11,9,8,2,4
96ffb5cfa0cb6fd6ccb22ddb7ee98dd90b86df49,"Objective: To develop a taxonomy describing patient safety events in general practice from reports submitted by a random representative sample of general practitioners (GPs), and to determine proportions of reported event types. Design: 433 reports received by the Threats to Australian Patient Safety (TAPS) study were analysed by three investigating GPs, classifying event types contained. Agreement between investigators was recorded as the taxonomy developed. Setting and participants: 84 volunteers from a random sample of 320 GPs, previously shown to be representative of 4666 GPs in New South Wales, Australia. Main outcome measures: Taxonomy, agreement of investigators coding, proportions of error types. Results: A three-level taxonomy resulted. At the first level, errors relating to the processes of healthcare (type 1; n = 365 (69.5%)) were more common than those relating to deficiencies in the knowledge and skills of health professionals (type 2; n = 160 (30.5%)). At the second level, five type 1 themes were identified: healthcare systems (n = 112 (21.3%)); investigations (n = 65 (12.4%)); medications (n = 107 (20.4%)); other treatments (n = 13 (2.5%)); and communication (n = 68 (12.9%)). Two type 2 themes were identified: diagnosis (n = 62 (11.8%)) and management (n = 98 (18.7%)). The third level comprised 35 descriptors of the themes. Good inter-coder agreement was demonstrated with an overall κ score of 0.66. A least two out of three investigators independently agreed on event classification in 92% of cases. Conclusions: The proposed taxonomy for reported events in general practice provides a comprehensible tool for clinicians describing threats to patient safety, and could be built into reporting systems to remove difficulties arising from coder interpretation of events.",2008,37,79,4,3,4,6,7,5,6,3,16,4,10
625f20bafb88207c9b7ab998c44b3e000003e720,"BACKGROUND
In general practice, depression is often not recognized. As treatment of depression is effective, screening has been proposed as one solution to combat this 'hidden morbidity'. The results of screening programmes for depression, however, are inconsistent and most studies do not show a positive effect on patient outcomes. Patients do not always accept this diagnosis and hence do not receive proper treatment. Nothing is known about the tendency of those patients who screen positive for depression to accept treatment for their 'disclosed' disorder.


OBJECTIVE
In this study, we aimed to better understand the views of patients who screened positive in a screening programme for depression.


METHODS
We performed a qualitative study with semi-structured in-depth interviews with 17 patients. These adult patients (nine females), all suffering from major depressive disorder, were disclosed by a screening programme for depression performed within 11 Dutch general practices. The transcripts were independently analysed by two researchers using MAXqda2.


RESULTS
All patients appreciated the active way in which they were approached for screening. Fifteen of the 17 patients recognized the depressive symptoms but nine of them did not accept the diagnosis. The first explanation for resistance to the diagnosis of depression is fear of stigmatization and scepticism about the usefulness of labelling. Secondly, patients experienced their depressive symptoms as a normal and transitory reaction to adversity. Thirdly, patients had doubts about the necessity and effectiveness of treatment. Depressive symptoms, such as feelings of guilt, self-depreciation and fatigue, hamper help-seeking behaviour.


CONCLUSIONS
We conclude that some patients with undisclosed depression, who took the trouble of going through a complete screening programme, felt aversion to being diagnosed as having depression. In the context of screening for depression, we recommend that the patients' view on depression be elicited before diagnosing and offering treatment.",2008,43,46,2,1,2,2,8,4,4,1,7,2,5
379ffd011c11510b0ea4bf6b1940f7f5603af6c6,"The services of ecological systems and the natural capital stocks that produce them are critical to the functioning of the Earth's life-support system. They contribute to human welfare, both directly and indirectly, and therefore represent part of the total economic value of the planet. We have estimated the current economic value of 17 ecosystem services for 16 biomes, based on published studies and a few original calculations. For the entire biosphere, the value (most of which is outside the market) is estimated to be in the range of US$16-54 trillion (1012) per year, with an average of US$33 trillion per year. Because of the nature of the uncertainties, this must be considered a minimum estimate. Global gross national product total is around US$18 trillion per year.",1997,57,15924,796,0,0,0,0,0,0,0,0,1,0
13c3630c03bec0c3e443ad5a3dc6d1951db74c20,"Status of this Memo This memo provides information for the Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This document defines an architecture for implementing scalable service differentiation in the Internet. This architecture achieves scalability by aggregating traffic classification state which is conveyed by means of IP-layer packet marking using the DS field [DSFIELD]. Packets are classified and marked to receive a particular per-hop forwarding behavior on nodes along their path. Sophisticated classification, marking, policing, and shaping operations need only be implemented at network boundaries or hosts. Network resources are allocated to traffic streams by service provisioning policies which govern how traffic is marked and conditioned upon entry to a differentiated services-capable network, and how that traffic is forwarded within that network. A wide variety of services can be implemented on top of these building blocks.",1998,14,4114,339,7,121,220,322,407,404,381,357,324,254
505ae0232a6b59786ce8ed61af98573c68f69b92,"Salespeople involved in the marketing of complex services often perform the role of “relationship manager.” It is, in part, the quality of the relationship between the salesperson and the customer ...",1990,45,4378,306,2,1,9,21,29,33,54,39,39,54
d73a3014f2fbe3c02cfe4884bc6f5bbc0990e557,"The Second European Edition of Services Marketing: Integrating Customer Focus Across the Firm by Wilson, Zeithaml, Bitner and Gremler uniquely focuses on the development of customer relationships through quality service. Reflecting the increasing importance of the service economy, Services Marketing is the only text that put the customer's experience of services at the centre of its approach. The core theories, concepts and frameworks are retained, and specifically the gaps model, a popular feature of the book. The text moves from the foundations of services marketing before introducing the gaps model and demonstrating its application to services marketing. In the second edition, the book takes on more European and International contexts to reflect the needs of courses, lecturers and students. The second edition builds on the wealth of European and International examples, cases, and research in the first edition, offering more integration of European content. It has also be fully updated with the latest research to ensure that it continues to be seen as the text covering the very latest services marketing thinking. In addition, the cases section has been thoroughly examined and revised to offer a range of new case studies with a European and global focus. The online resources have also been fully revised and updated providing an excellent package of support for lecturers and students.",1996,0,4803,271,0,0,1,0,3,20,23,41,54,69
698afd7515d01f72e028927d829e1c1d22019f87,"The Semantic Web should enable greater access not only to content but also to services on the Web. Users and software agents should be able to discover, invoke, compose, and monitor Web resources offering particular services and having particular properties. As part of the DARPA Agent Markup Language program, we have begun to develop an ontology of services, called DAML-S, that will make these functionalities possible. In this paper we describe the overall structure of the ontology, the service profile for advertising services, and the process model for the detailed description of the operation of services. We also compare DAML-S with several industry efforts to define standards for characterizing services on the Web.",2001,83,3213,330,10,78,114,136,234,265,300,320,300,267
9438aa83eb8218b7e6e3891ad7bc2b388e35bc33,"In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic “virtual organizations” formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, “The Anatomy of the Grid,” by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration—within and across organizational domains. This is a DRAFT document and continues to be revised. The latest version can be found at http://www.globus.org/research/papers/ogsa.pdf. Please send comments to foster@mcs.anl.gov, carl@isi.edu, jnick@us.ibm.com, tuecke@mcs.anl.gov Physiology of the Grid 2",2002,84,3620,238,141,427,550,551,436,326,231,230,166,131
1b74223af38112ee2814688b1c4c96f597c1ff43,,1983,0,3573,477,1,2,0,1,4,3,2,5,10,3
70f5ad30aabb1de547b34d5aa4167dd9bb8d0957,"This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support real- time as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation.",1994,27,3979,194,6,39,50,78,96,141,217,279,280,326
c9253ce46a869d921a304c30ca604820089c8d9d,"processes are rarely used. The most common scenario is to use them as a template to define executable processes. Abstract processes can be used to replace sets of rules usually expressed in natural language, which is often ambiguous. In this book, we will first focus on executable processes and come back to abstract processes in Chapter 4. 21 This material is copyright and is licensed for the sole use by Encarnacion Bellido on 20th February 2006 Via Alemania, 10, bajos, , Palma de Mallorca, Baleares, 07006",2004,3,3752,193,390,523,566,483,413,283,236,172,116,97
eb9e615ca97b3901f6f312f4dfa11095d0688592,"Abstract An increasing amount of information is being collected on the ecological and socio-economic value of goods and services provided by natural and semi-natural ecosystems. However, much of this information appears scattered throughout a disciplinary academic literature, unpublished government agency reports, and across the World Wide Web. In addition, data on ecosystem goods and services often appears at incompatible scales of analysis and is classified differently by different authors. In order to make comparative ecological economic analysis possible, a standardized framework for the comprehensive assessment of ecosystem functions, goods and services is needed. In response to this challenge, this paper presents a conceptual framework and typology for describing, classifying and valuing ecosystem functions, goods and services in a clear and consistent manner. In the following analysis, a classification is given for the fullest possible range of 23 ecosystem functions that provide a much larger number of goods and services. In the second part of the paper, a checklist and matrix is provided, linking these ecosystem functions to the main ecological, socio–cultural and economic valuation methods.",2002,50,3795,123,7,15,18,45,61,84,87,119,194,232
1c90a7392994300df01fed1801a41fa0c9693ca2,"Relationship marketing is an old idea but a new focus now at the forefront of services marketing practice and academic research. The impetus for its development has come from the maturing of services marketing with the emphasis on quality, increased recognition of potential benefits for the firm and the customer, and technological advances. Accelerating interest and active research are extending the concept to incorporate newer, more sophisticated viewpoints. Emerging perspectives explored here include targeting profitable customers, using the strongest possible strategies for customer bonding, marketing to employees and other stakeholders, and building trust as a marketing tool. Although relationship marketing is developing, more research is needed before it reaches maturity. A baker’s dozen of researchable questions suggests some future directions.",1995,21,3161,284,1,11,21,26,41,59,56,72,71,84
1bfb01fc10cf40d307f5d9b99e9b94de3fb85685,"Processes serve a descriptive role, with more than one use case. One such use case might be to describe the observable behavior of some or all of the services offered by an Executable Process. Another use case would be to define a process template that embodies domain-specific best practices. Such a process template would capture essential process logic in a manner compatible with a design-time representation, while excluding execution details to be completed when mapping to an Executable Process. Regardless of the specific use case and purpose, all Abstract Processes share a common syntactic base. They have different requirements for the level of opacity and restrictions on which parts of a process definition may be omitted or hidden. Tailored uses of Abstract Processes have different effects on the consistency constraints and on the semantics of that process. Some of these required constraints are not enforceable by the XML Schema. A common base specifies the features that define the syntactic universe of Abstract Processes. Given this common base, a usage profile provides the necessary specializations and semantics based on Executable WS-BPEL for a particular use of an Abstract Process. As mentioned above it is possible to use WS-BPEL to define an Executable Business Process. While a WS-BPEL Abstract Process definition is not required to be fully specified, the language effectively defines a portable execution format for business processes that rely exclusively on Web Service resources and XML data. Moreover, such processes execute and interact with their partners in a consistent way regardless of the supporting platform or programming model used by the implementation of the hosting environment. The continuity of the basic conceptual model between Abstract and Executable Processes in WSBPEL makes it possible to export and import the public aspects embodied in Abstract Processes as process or role templates while maintaining the intent and structure of the observable behavior. This applies even where private implementation aspects use platform dependent functionality.",2007,50,3133,200,211,360,423,412,359,310,248,240,146,97
9ea11c0728473937644be8dda349a34d32f656a1,"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.",2004,59,2872,297,7,51,109,202,193,263,297,248,267,238
3cf9e452adceb66ead134d9377ae8155016aa259,"Companies that want to improve their service quality should take a cue from manufacturing and focus on their own kind of scrap heap: customers who won't come back. Because that scrap heap can be every bit as costly as broken parts and misfit components, service company managers should strive to reduce it. They should aim for ""zero defections""--keeping every customer they can profitably serve. As companies reduce customer defection rates, amazing things happen to their financials. Although the magnitude of the change varies by company and industry, the pattern holds: profits rise sharply. Reducing the defection rate just 5% generates 85% more profits in one bank's branch system, 50% more in an insurance brokerage, and 30% more in an auto-service chain. And when MBNA America, a Delaware-based credit card company, cut its 10% defection rate in half, profits rose a whopping 125%. But defection rates are not just a measure of service quality; they are also a guide for achieving it. By listening to the reasons why customers defect, managers learn exactly where the company is falling short and where to direct their resources. Staples, the stationery supplies retailer, uses feedback from customers to pinpoint products that are priced too high. That way, the company avoids expensive broad-brush promotions that pitch everything to everyone. Like any important change, managing for zero defections requires training and reinforcement. Great-West Life Assurance Company pays a 50% premium to group health-insurance brokers that hit customer-retention targets, and MBNA America gives bonuses to departments that hit theirs.",1990,0,5927,136,0,10,16,17,40,47,42,56,89,88
1d109c5b293c59983f388de75faeb2b49abb606e,"Life itself as well as the entire human economy depends on goods and services provided by earth's natural systems. The processes of cleansing, recycling, and renewal, along with goods such as seafood, forage, and timber, are worth many trillions of dollars annually, and nothing could live without them. Yet growing human impacts on the environment are profoundly disrupting the functioning of natural systems and imperiling the delivery of these services.Nature's Services brings together world-renowned scientists from a variety of disciplines to examine the character and value of ecosystem services, the damage that has been done to them, and the consequent implications for human society. Contributors including Paul R. Ehrlich, Donald Kennedy, Pamela A. Matson, Robert Costanza, Gary Paul Nabhan, Jane Lubchenco, Sandra Postel, and Norman Myers present a detailed synthesis of our current understanding of a suite of ecosystem services and a preliminary assessment of their economic value. Chapters consider: major services including climate regulation, soil fertility, pollination, and pest control philosophical and economic issues of valuation case studies of specific ecosystems and services implication of recent findings and steps that must be taken to address the most pressing concerns Nature's Services represents one of the first efforts by scientists to provide an overview of the many benefits and services that nature offers to people and the extent to which we are all vitally dependent on those services. The book enhances our understanding of the value of the natural systems that surround us and can play an essential role in encouraging greater efforts to protect the earth's basic life-support systems before it is too late. -- publisher's description",1998,0,3671,124,34,55,58,60,100,83,82,101,139,147
5541ab96c84e9fbe56c0be28631b682608ca9b40,"This letter is in response to your two Citizen Petitions dated November 17, 1994 and May 13, 2008, requesting that the Food and Drug Administration (FDA or the Agency) require a cancer warning on cosmetic talc products. Your 1994 Petition requests that all cosmetic talc bear labels with a warning such as ""Talcum powder causes cancer in laboratory animals. Frequent talc application in the female genital area increases the risk of ovarian cancer."" Additionally, your 2008 Petition requests that cosmetic talcum powder products bear labels with a prominent warning such as: ""Frequent talc application in the female genital area is responsible for major risks of ovarian cancer."" Further, both of your Petitions specifically request, pursuant to 21 CFR 1 0.30(h)(2), a hearing for you to present scientific evidence in support of this petition.",1999,141,9316,33,52,92,115,158,177,225,227,279,299,330
22f33f9d67edbc1ab9a8da239bfe7a464337db24,"This new edition of Ann Bowling's well-known and highly respected text has been thoroughly revised and updated to reflect key methodological developments in health research. It is a comprehensive, easy to read, guide to the range of methods used to study and evaluate health and health services. It describes the concepts and methods used by the main disciplines involved in health research, including: demography, epidemiology, health economics, psychology and sociology.The research methods described cover the assessment of health needs, morbidity and mortality trends and rates, costing health services, sampling for survey research, cross-sectional and longitudinal survey design, experimental methods and techniques of group assignment, questionnaire design, interviewing techniques, coding and analysis of quantitative data, methods and analysis of qualitative observational studies, and types of unstructured interviewing. With new material on topics such as cluster randomization, utility analyses, patients' preferences, and perception of risk, the text is aimed at students and researchers of health and health services. It has also been designed for health professionals and policy makers who have responsibility for applying research findings in practice, and who need to know how to judge the value of that research",1997,9,2685,210,0,5,11,17,33,43,55,73,100,69
4db2daec1fb9c1e8c49d0c549ea2c3af940485ba,"Abstract The concept of ecosystems services has become an important model for linking the functioning of ecosystems to human welfare. Understanding this link is critical for a wide-range of decision-making contexts. While there have been several attempts to come up with a classification scheme for ecosystem services, there has not been an agreed upon, meaningful and consistent definition for ecosystem services. In this paper we offer a definition of ecosystem services that is likely to be operational for ecosystem service research and several classification schemes. We argue that any attempt at classifying ecosystem services should be based on both the characteristics of the ecosystems of interest and a decision context for which the concept of ecosystem services is being mobilized. Because of this there is not one classification scheme that will be adequate for the many contexts in which ecosystem service research may be utilized. We discuss several examples of how classification schemes will be a function of both ecosystem and ecosystem service characteristics and the decision-making context.",2009,62,2451,155,19,80,102,157,195,237,263,275,244,231
a556ba6f4ae669b253de9a4a7cfa25f3d7b58742,"Pfam is a database of protein families that currently contains 7973 entries (release 18.0). A recent development in Pfam has enabled the grouping of related families into clans. Pfam clans are described in detail, together with the new associated web pages. Improvements to the range of Pfam web tools and the first set of Pfam web services that allow programmatic access to the database and associated tools are also presented. Pfam is available on the web in the UK (), the USA (), France () and Sweden ().",2005,17,2185,207,3,76,350,472,315,180,134,98,81,69
251b03a94b29698df843c0f5c2ffbf2d17fd213e,"The question of the existence of competition among auditors has been the subject of considerable discussion in recent years. More specifically, the ""Big Eight"" firms as a group have been accused of monopolizing the market for audits {Staff Study of the Subcommittee on Reports, Accounting and Management of the Senate Committee on Government Operations [1977]). However, evidence on the issue is scanty and typically anecdotal (e.g., Bernstein [1978]). The evidence of the Staff Study itself is limited to concentration statistics, with the allegations relying on what has come to be called the ""concentration doctrine"" (Demsetz [1973]). According to this doctrine, supplier concentration is a reliable indicator of supplier behavior and performance. In this paper, I provide evidence from a test of the hypothesis that price competition prevails throughout the market for the audits of publicly held companies, irrespective of the share of a market segment which is serviced by the Big Eight firms. The evidence is based on an examination of a sample cross-section of audit fees.",1980,11,2265,399,0,2,2,2,5,1,5,2,8,1
1f101fbd343044499ba2bab28d05d1eaa268e7dd,"Management literature is almost unanimous in suggesting to manufacturers that they should integrate services into their core product offering. The literature, however, is surprisingly sparse in describing to what extent services should be integrated, how this integration should be carried out, or in detailing the challenges inherent in the transition to services. Reports on a study of 11 capital equipment manufacturers developing service offerings for their products. Focuses on identifying the dimensions considered when creating a service organization in the context of a manufacturing firm, and successful strategies to navigate the transition. Analysis of qualitative data suggests that the transition involves a deliberate developmental process to build capabilities as firms shift the nature of the relationship with the product end‐users and the focus of the service offering. The report concludes identifying implications of our findings for further research and practitioners.",2003,40,2178,265,5,13,37,22,43,62,111,118,155,152
1065f1c73c538a8d4b017af1825967e1fab1bf52,"Advances in sensing and tracking technology enable location-based applications but they also create significant privacy risks. Anonymity can provide a high degree of privacy, save service users from dealing with service providers’ privacy policies, and reduce the service providers’ requirements for safeguarding private information. However, guaranteeing anonymous usage of location-based services requires that the precise location information transmitted by a user cannot be easily used to re-identify the subject. This paper presents a middleware architecture and algorithms that can be used by a centralized location broker service. The adaptive algorithms adjust the resolution of location information along spatial or temporal dimensions to meet specified anonymity constraints based on the entities who may be using location services within a given area. Using a model based on automotive traffic counts and cartographic material, we estimate the realistically expected spatial resolution for different anonymity constraints. The median resolution generated by our algorithms is 125 meters. Thus, anonymous location-based requests for urban areas would have the same accuracy currently needed for E-911 services; this would provide sufficient resolution for wayfinding, automated bus routing services and similar location-dependent services.",2003,47,2364,188,8,20,45,63,97,112,151,148,150,178
7d32e764672117c2d5bce2c69dee737c6e88b719,"The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that location of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL.We propose a solution based on DAML-S, a DAML-based language for service description, and we show how service capabilities are presented in the Profile section of a DAML-S description and how a semantic match between advertisements and requests is performed.",2002,20,2572,165,17,105,179,200,251,274,249,229,230,167
1741947f385ae50bbb50c879655ada49a45860ba,"Internet-delivered e-services are increasingly being made available to consumers; however, little is known about how consumers evaluate them for potential adoption. Past Technology Adoption Research has focused primarily on the positive utility gains attributable to system adoption. This research extends that approach to include measures of negative utility (potential losses) attributable to e-service adoption. Drawing from Perceived Risk Theory, specific risk facets were operationalized, integrated, and empirically tested within the Technology Acceptance Model resulting in a proposed e-services adoption model. Results indicated that e-services adoption is adversely affected primarily by performance-based risk perceptions, and perceived ease of use of the e-service reduced these risk concerns. Implications of integrating perceived risk into the proposed e-services adoption model are discussed.",2002,67,2025,186,1,8,8,18,30,43,37,65,72,96
3649fdff0b991e7a5bec42318809c3d50f660690,"Like many other incipient technologies, Web services are still surrounded by a substantial level of noise. This noise results from the always dangerous combination of wishful thinking on the part of research and industry and of a lack of clear understanding of how Web services came to be. On the one hand, multiple contradictory interpretations are created by the many attempts to realign existing technology and strategies with Web services. On the other hand, the emphasis on what could be done with Web services in the future often makes us lose track of what can be really done with Web services today and in the short term. These factors make it extremely difficult to get a coherent picture of what Web services are, what they contribute, and where they will be applied.Alonso and his co-authors deliberately take a step back. Based on their academic and industrial experience with middleware and enterprise application integration systems, they describe the fundamental concepts behind the notion of Web services and present them as the natural evolution of conventional middleware, necessary to meet the challenges of the Web and of B2B application integration. Rather than providing a reference guide or a ""how to write your first Web service"" kind of book, they discuss the main objectives of Web services, the challenges that must be faced to achieve them, and the opportunities that this novel technology provides. Established, as well as recently proposed, standards and techniques (e.g., WSDL, UDDI, SOAP, WS-Coordination, WS-Transactions, and BPEL), are then examined in the context of this discussion in order to emphasize their scope, benefits, and shortcomings. Thus, the book is ideally suited both for professionals considering the development of application integration solutions and for research and students interesting in understanding and contributing to the evolution of enterprise application technologies.",2009,0,1860,138,215,176,164,137,100,108,79,42,33,18
fbfae405e9144274063149c40566da25055dd855,The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models—Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.,1990,55,2487,127,7,9,27,34,46,44,36,28,46,53
0531bb2ecef7a42909719310b23757740ded95f1,"This research examines the benefits customers receive as a result of engaging in long-term relational exchanges with service firms. Findings from two studies indicate that consumer relational benefits can be categorized into three distinct benefit types: confidence, social, and special treatment benefits. Confidence benefits are received more and rated as more important than the other relational benefits by consumers, followed by social and special treatment benefits, respectively. Responses segmented by type of service business show a consistent pattern with respect to customer rankings of benefit importance. Management implications for relational strategies and future research implications of the findings are discussed.",1998,49,2332,168,2,9,28,37,35,48,57,60,75,99
f1b109e548e0ec8434ae5c1495553bc1ff82c4c8,"Aims of the Paper 
The principal aims of this paper are (1) to increase professional health workers’ knowledge of selected research findings and theory so that they may better understand why and under what conditions people take action to prevent, detect and diagnose disease; and (2) to increase awareness among qualified behavioral scientists about the kinds of behavioral research opportunities and needs that exist in public health. 
 
A matter of personal philosophy of the author is that the goal of understanding and predicting behavior should appropriately precede the goal of attempting to persuade people to modify their health practices, even though behavior can sometimes be changed in a planned way without clear understanding of its original causes. Efforts to modify behavior will ultimately be more successful if they grow out of an understanding of causal processes. Accordingly, primary attention will here be given to an effort to understand why people behave as they do. Only then will brief consideration be given to problems of how to persuade people to use health services.",1966,67,2710,239,0,1,1,7,7,5,12,12,17,12
681b6891fca9fab4b6c968ebe6b889fd5d60db7c,"OBJECTIVE
To provide practical strategies for conducting and evaluating analyses of qualitative data applicable for health services researchers. DATA SOURCES AND DESIGN: We draw on extant qualitative methodological literature to describe practical approaches to qualitative data analysis. Approaches to data analysis vary by discipline and analytic tradition; however, we focus on qualitative data analysis that has as a goal the generation of taxonomy, themes, and theory germane to health services research.


PRINCIPLE FINDINGS
We describe an approach to qualitative data analysis that applies the principles of inductive reasoning while also employing predetermined code types to guide data analysis and interpretation. These code types (conceptual, relationship, perspective, participant characteristics, and setting codes) define a structure that is appropriate for generation of taxonomy, themes, and theory. Conceptual codes and subcodes facilitate the development of taxonomies. Relationship and perspective codes facilitate the development of themes and theory. Intersectional analyses with data coded for participant characteristics and setting codes can facilitate comparative analyses.


CONCLUSIONS
Qualitative inquiry can improve the description and explanation of complex, real-world phenomena pertinent to health services research. Greater understanding of the processes of qualitative data analysis can be helpful for health services researchers as they use these methods themselves or collaborate with qualitative researchers from a wide range of disciplines.",2007,72,2266,52,3,23,29,52,68,99,122,146,216,242
a2bfe963ce2168cecaad21d2e66059662e996f70,"This article examines some advantages and disadvantages of conducting online survey research. It explores current features, issues, pricing, and limitations associated with products and services, such as online questionnaire features and services to facilitate the online survey process, such as those offered by web survey businesses. The review shows that current online survey products and services can vary considerably in terms of available features, consumer costs, and limitations. It is concluded that online survey researchers should conduct a careful assessment of their research goals, research timeline, and financial situation before choosing a specific product or service.",2006,48,2105,130,8,15,27,45,68,102,114,139,157,184
72e0802c917104b36a396135bcb5876494abe00e,"This document defines a notation for specifying business process behavior based on Web Services. This notation is called Business Process Execution Language for Web Services (abbreviated to BPEL4WS in the rest of this document). Processes in BPEL4WS export and import functionality by using Web Service interfaces exclusively. Business processes can be described in two ways. Executable business processes model actual behavior of a participant in a business interaction. Business protocols, in contrast, use process descriptions that specify the mutually visible message exchange behavior of each of the parties involved in the protocol, without revealing their internal behavior. The process descriptions for business protocols are called abstract processes. BPEL4WS is meant to be used to model the behavior of both executable and abstract processes. BPEL4WS provides a language for the formal specification of business processes and business interaction protocols. By doing so, it extends the Web services interaction model and enables it to support business transactions. BPEL4WS defines an interoperable integration model that should facilitate the expansion of automated process integration in both the intracorporate and the business-to-business spaces. Status of this Document This is an initial public draft release of the BPEL4WS specification. We anticipate a number of extensions to the feature set of BPEL4WS that are discussed briefly at the end of the document. BPEL4WS represents a convergence of the ideas in the XLANG and WSFL specifications. Both XLANG and WSFL are superseded by the BPEL4WS specification. BPEL4WS and related specifications are provided as-is and for review and evaluation only. BEA, IBM and Microsoft hope to solicit your contributions and suggestions in the near future. BEA, IBM and Microsoft make no warrantees or representations regarding the specifications in any manner whatsoever.",2003,0,2127,161,112,187,282,364,345,247,145,120,98,54
0bd82f5b93c8e2e52d584c3c9bd126fee243e61d,"Humanity is increasingly urban, but continues to depend on Nature for its survival. Cities are dependent on the ecosystems beyond the city limits, but also benefit from internal urban ecosystems. The aim of this paper is to analyze the ecosystem services generated by ecosystems within the urban area. ‘Ecosystem services’ refers to the benefits human populations derive from ecosystems. Seven different urban ecosystems have been identified: street trees; lawns:parks; urban forests; cultivated land; wetlands; lakes:sea; and streams. These systems generate a range of ecosystem services. In this paper, six local and direct services relevant for Stockholm are addressed: air filtration, micro climate regulation, noise reduction, rainwater drainage, sewage treatment, and recreational and cultural values. It is concluded that the locally generated ecosystem services have a substantial impact on the quality-of-life in urban areas and should be addressed in land-use planning. © 1999 Elsevier Science B.V. All rights reserved.",1999,39,2194,94,0,2,4,3,7,4,27,23,43,45
85a4155ee7d04554df0821d28a7a1fe0469beda3,"Concern is growing about the consequences of biodiversity loss for ecosystem functioning, for the provision of ecosystem services, and for human well being. Experimental evidence for a relationship between biodiversity and ecosystem process rates is compelling, but the issue remains contentious. Here, we present the first rigorous quantitative assessment of this relationship through meta-analysis of experimental work spanning 50 years to June 2004. We analysed 446 measures of biodiversity effects (252 in grasslands), 319 of which involved primary producer manipulations or measurements. Our analyses show that: biodiversity effects are weaker if biodiversity manipulations are less well controlled; effects of biodiversity change on processes are weaker at the ecosystem compared with the community level and are negative at the population level; productivity-related effects decline with increasing number of trophic links between those elements manipulated and those measured; biodiversity effects on stability measures ('insurance' effects) are not stronger than biodiversity effects on performance measures. For those ecosystem services which could be assessed here, there is clear evidence that biodiversity has positive effects on most. Whilst such patterns should be further confirmed, a precautionary approach to biodiversity management would seem prudent in the meantime.",2006,148,2169,93,4,32,83,100,108,134,171,178,205,201
1a15e7ee67b3e34fc7277341a0574daaac60af27,"Human-dominated marine ecosystems are experiencing accelerating loss of populations and species, with largely unknown consequences. We analyzed local experiments, long-term regional time series, and global fisheries data to test how biodiversity loss affects marine ecosystem services across temporal and spatial scales. Overall, rates of resource collapse increased and recovery potential, stability, and water quality decreased exponentially with declining diversity. Restoration of biodiversity, in contrast, increased productivity fourfold and decreased variability by 21%, on average. We conclude that marine biodiversity loss is increasingly impairing the ocean's capacity to provide food, maintain water quality, and recover from perturbations. Yet available data suggest that at this point, these trends are still reversible.",2006,102,2405,39,4,71,128,140,153,177,163,186,150,156
a258380b26971406230aaf017bb3e3d473d1c849,"Excellent service is the foundation for services marketing, contend Leonard Berry and A. Parasuraman in this companion volume to ""Delivering Quality Service."" Building on eight years of research, the authors develop a model for understanding the relationship between quality and marketing in services and offer dozens of practical insights into ways to improve services marketing. They argue that superior service cannot be manufactured in a factory, packaged, and delivered intact to customers. Though an innovative service concept may give a company an initial edge, superior quality is vital to sustaining success. Berry and Parasuraman show that inspired leadership, a customer-minded corporate culture, an excellent service-system design, and effective use of technology and information are crucial to superior service quality and services marketing. When a company's service is excellent, customers are more likely to perceive value in transactions, spread favorable word-of-mouth impressions, and respond positively to employee-cross-selling efforts. The authors point out that a service company that does relatively little pre-sales marketing but is truly dedicated to delivering excellent quality service will have greater marketing effectiveness, higher customer retention, and more sales to existing customers than a company that emphasizes pre-sale marketing but falls short during actual service delivery. The focus of any company, they insist, must be customer satisfaction through integration of service quality throughout the entire system. Filled with examples, stories, and insights from senior executives, Berry and Parasuraman's new framework for effective marketing servicescontains the key to high-performance services marketing.",1991,0,2280,117,0,3,13,24,35,31,34,52,41,51
0a954386b36717010e3b07391da43916afa72ad1,"Abstract. Electronic government, or e‐government, increases the convenience and accessibility of government services and information to citizens. Despite the benefits of e‐government – increased government accountability to citizens, greater public access to information and a more efficient, cost‐effective government – the success and acceptance of e‐government initiatives, such as online voting and licence renewal, are contingent upon citizens’ willingness to adopt this innovation. In order to develop ‘citizen‐centred’ e‐government services that provide participants with accessible, relevant information and quality services that are more expedient than traditional ‘brick and mortar’ transactions, government agencies must first understand the factors that influence citizen adoption of this innovation. This study integrates constructs from the Technology Acceptance Model, Diffusions of Innovation theory and web trust models to form a parsimonious yet comprehensive model of factors that influence citizen adoption of e‐government initiatives. The study was conducted by surveying a broad diversity of citizens at a community event. The findings indicate that perceived ease of use, compatibility and trustworthiness are significant predictors of citizens’ intention to use an e‐government service. Implications of this study for research and practice are presented.",2005,57,1853,201,4,20,23,58,106,103,128,146,133,142
baebed6da6c8a7ddda676fb8b0d98d5d2168f219,"The Millennium Ecosystem Assessment (MA) introduced a new framework for analyzing social–ecological systems that has had wide influence in the policy and scientific communities. Studies after the MA are taking up new challenges in the basic science needed to assess, project, and manage flows of ecosystem services and effects on human well-being. Yet, our ability to draw general conclusions remains limited by focus on discipline-bound sectors of the full social–ecological system. At the same time, some polices and practices intended to improve ecosystem services and human well-being are based on untested assumptions and sparse information. The people who are affected and those who provide resources are increasingly asking for evidence that interventions improve ecosystem services and human well-being. New research is needed that considers the full ensemble of processes and feedbacks, for a range of biophysical and social systems, to better understand and manage the dynamics of the relationship between humans and the ecosystems on which they rely. Such research will expand the capacity to address fundamental questions about complex social–ecological systems while evaluating assumptions of policies and practices intended to advance human well-being through improved ecosystem services.",2009,79,1745,75,29,75,101,119,178,189,207,167,160,154
a20cd573efefe1eac59f51faf28e0fbc18d69e35,"Over the past decade, efforts to value and protect ecosystem services have been promoted by many as the last, best hope for making conservation mainstream – attractive and commonplace worldwide. In theory, if we can help individuals and institutions to recognize the value of nature, then this should greatly increase investments in conservation, while at the same time fostering human well-being. In practice, however, we have not yet developed the scientific basis, nor the policy and finance mechanisms, for incorporating natural capital into resource- and land-use decisions on a large scale. Here, we propose a conceptual framework and sketch out a strategic plan for delivering on the promise of ecosystem services, drawing on emerging examples from Hawai‘i. We describe key advances in the science and practice of accounting for natural capital in the decisions of individuals, communities, corporations, and governments.",2009,112,1667,78,15,48,81,106,151,187,193,142,168,159
3283f3ca81ff395643ca49722d522e3606eca006,"Nature provides a wide range of benefits to people. There is increasing consensus about the importance of incorporating these “ecosystem services” into resource management decisions, but quantifying the levels and values of these services has proven difficult. We use a spatially explicit modeling tool, Integrated Valuation of Ecosystem Services and Tradeoffs (InVEST), to predict changes in ecosystem services, biodiversity conservation, and commodity production levels. We apply InVEST to stakeholder-defined scenarios of land-use/land-cover change in the Willamette Basin, Oregon. We found that scenarios that received high scores for a variety of ecosystem services also had high scores for biodiversity, suggesting there is little tradeoff between biodiversity conservation and ecosystem services. Scenarios involving more development had higher commodity production values, but lower levels of biodiversity conservation and ecosystem services. However, including payments for carbon sequestration alleviates this tradeoff. Quantifying ecosystem services in a spatially explicit manner, and analyzing tradeoffs between them, can help to make natural resource decisions more effective, efficient, and defensible.",2009,85,1881,75,28,71,97,136,191,196,182,185,178,189
f81183ede9e5455e82bad824718540fce6f2d1a7,"Ecosystem management that attempts to maximize the production of one ecosystem service often results in substantial declines in the provision of other ecosystem services. For this reason, recent studies have called for increased attention to development of a theoretical understanding behind the relationships among ecosystem services. Here, we review the literature on ecosystem services and propose a typology of relationships between ecosystem services based on the role of drivers and the interactions between services. We use this typology to develop three propositions to help drive ecological science towards a better understanding of the relationships among multiple ecosystem services. Research which aims to understand the relationships among multiple ecosystem services and the mechanisms behind these relationships will improve our ability to sustainably manage landscapes to provide multiple ecosystem services.",2009,70,1550,100,2,20,42,70,106,113,147,177,177,158
caf04b76596bf1da40ec442edb6060dcf3927180,,2001,0,2451,15,22,34,56,56,75,74,105,108,132,167
a54438cc19171b4ba9380ba9a6443532e82357c0,"Payments for environmental services (PES) are part of a new and more direct conservation paradigm, explicitly recognizing the need to bridge the interests of landowners and outsiders. Eloquent theoretical assessments have praised the absolute advantages of PES over traditional conservation approaches. Some pilot PES exist in the tropics, but many field practitioners and prospective service buyers and sellers remain skeptical about the concept. This paper aims to help demystify PES for non-economists, starting with a simple and coherent definition of the term. It then provides practical ‘how-to' hints for PES design. It considers the likely niche for PES in the portfolio of conservation approaches. This assessment is based on a literature review, combined with field observations from research in Latin America and Asia. It concludes that service users will continue to drive PES, but their willingness to pay will only rise if schemes can demonstrate clear additionality vis-a-vis carefully established baselines, if trust-building processes with service providers are sustained, and PES recipients' livelihood dynamics is better understood. PES best suits intermediate and/or projected threat scenarios, often in marginal lands with moderate conservation opportunity costs. People facing credible but medium-sized environmental degradation are more likely to become PES recipients than those living in relative harmony with Nature. The choice between PES cash and in-kind payments is highly context-dependent. Poor PES recipients are likely to gain from participation, though their access might be constrained and non-participating landless poor could lose out. PES is a highly promising conservation approach that can benefit buyers, sellers and improve the resource base, but it is unlikely to completely outstrip other conservation instruments.",2005,83,1607,196,10,19,62,70,98,102,121,133,156,139
916743e3f5d7ac258aa4af94b7b588414f0f0dd9,"More and more corporations throughout the world are adding value to their core corporate offerings through services. The trend is pervading almost all industries, is customer demand-driven, and perceived by corporations as sharpening their competitive edges. Modern corporations are increasingly offering fuller market packages or ""bundles"" of customer-focussed combinations of goods, services, support, self-service, and knowledge. But services are beginning to dominate. This movement is termed the ""servitization of business"" by authors Sandra Vandermerwe and Juan Rada, and is clearly a powerful new feature of total market strategy being adopted by the best companies. It is leading to new relationships between them and their customers. Giving many real-life examples, the authors assess the main motives driving corporations to servitization, and point out that its cumulative effects are changing the competitive dynamics in which managers will have to operate. The special challenge for top managers is how to blend services into the overall strategies of the company.",1988,0,2006,119,0,2,1,1,1,0,5,4,1,2
bce78fc8dc981e231293be5d13348a1e03b10efa,"Differentiated services enhancements to the Internet protocol are intended to enable scalable service discrimination in the Internet without the need for per-flow state and signaling at every hop. A variety of services may be built from a small, well-defined set of building blocks which are deployed in network nodes. The services may be either end-to-end or intra-domain; they include both those that can satisfy quantitative performance requirements (e.g., peak bandwidth) and those based on relative performance (e.g., ""class"" differentiation). Services can be constructed by a combination of:",1998,10,1939,100,11,94,133,162,157,160,114,113,114,94
737337d2d4761d966b3c8b9176d51ecab6a186d1,"This paper advocates consistently defined units of account to measure the contributions of nature to human welfare. We argue that such units have to date not been defined by environmental accounting advocates and that the term ""ecosystem services"" is too ad hoc to be of practical use in welfare accounting. We propose a definition, rooted in economic principles, of ecosystem service units. A goal of these units is comparability with the definition of conventional goods and services found in GDP and the other national accounts. We illustrate our definition of ecological units of account with concrete examples. We also argue that these same units of account provide an architecture for environmental performance measurement by governments, conservancies, and environmental markets.",2006,51,1810,94,7,16,34,41,78,121,144,158,180,187
2f40193b0dff1a6015b8b3229cdfcbe889ad9e16,"CONTEXT
The US military has conducted population-level screening for mental health problems among all service members returning from deployment to Afghanistan, Iraq, and other locations. To date, no systematic analysis of this program has been conducted, and studies have not assessed the impact of these deployments on mental health care utilization after deployment.


OBJECTIVES
To determine the relationship between combat deployment and mental health care use during the first year after return and to assess the lessons learned from the postdeployment mental health screening effort, particularly the correlation between the screening results, actual use of mental health services, and attrition from military service.


DESIGN, SETTING, AND PARTICIPANTS
Population-based descriptive study of all Army soldiers and Marines who completed the routine postdeployment health assessment between May 1, 2003, and April 30, 2004, on return from deployment to Operation Enduring Freedom in Afghanistan (n = 16,318), Operation Iraqi Freedom (n = 222,620), and other locations (n = 64,967). Health care utilization and occupational outcomes were measured for 1 year after deployment or until leaving the service if this occurred sooner.


MAIN OUTCOME MEASURES
Screening positive for posttraumatic stress disorder, major depression, or other mental health problems; referral for a mental health reason; use of mental health care services after returning from deployment; and attrition from military service.


RESULTS
The prevalence of reporting a mental health problem was 19.1% among service members returning from Iraq compared with 11.3% after returning from Afghanistan and 8.5% after returning from other locations (P<.001). Mental health problems reported on the postdeployment assessment were significantly associated with combat experiences, mental health care referral and utilization, and attrition from military service. Thirty-five percent of Iraq war veterans accessed mental health services in the year after returning home; 12% per year were diagnosed with a mental health problem. More than 50% of those referred for a mental health reason were documented to receive follow-up care although less than 10% of all service members who received mental health treatment were referred through the screening program.


CONCLUSIONS
Combat duty in Iraq was associated with high utilization of mental health services and attrition from military service after deployment. The deployment mental health screening program provided another indicator of the mental health impact of deployment on a population level but had limited utility in predicting the level of mental health services that were needed after deployment. The high rate of using mental health services among Operation Iraqi Freedom veterans after deployment highlights challenges in ensuring that there are adequate resources to meet the mental health needs of returning veterans.",2006,32,1868,110,23,59,93,133,133,178,179,146,160,155
8aae732e07c9d14f08ccf31eb9b742683de19595,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity; large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior. We present an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.",2001,46,1724,117,31,86,161,177,186,177,149,122,100,112
bdcb0b64e9142fa934831a13e94375bbca0b71f6,"After initial interviews with 20,291 adults in the National Institute of Mental Health Epidemiologic Catchment Area Program, we estimated prospective 1-year prevalence and service use rates of mental and addictive disorders in the US population. An annual prevalence rate of 28.1% was found for these disorders, composed of a 1-month point prevalence of 15.7% (at wave 1) and a 1-year incidence of new or recurrent disorders identified in 12.3% of the population at wave 2. During the 1-year follow-up period, 6.6% of the total sample developed one or more new disorders after being assessed as having no previous lifetime diagnosis at wave 1. An additional 5.7% of the population, with a history of some previous disorder at wave 1, had an acute relapse or suffered from a new disorder in 1 year. Irrespective of diagnosis, 14.7% of the US population in 1 year reported use of services in one or more component sectors of the de facto US mental and addictive service system. With some overlap between sectors, specialists in mental and addictive disorders provided treatment to 5.9% of the US population, 6.4% sought such services from general medical physicians, 3.0% sought these services from other human service professionals, and 4.1% turned to the voluntary support sector for such care. Of those persons with any disorder, only 28.5% (8.0 per 100 population) sought mental health/addictive services. Persons with specific disorders varied in the proportion who used services, from a high of more than 60% for somatization, schizophrenia, and bipolar disorders to a low of less than 25% for addictive disorders and severe cognitive impairment. Applications of these descriptive data to US health care system reform options are considered in the context of other variables that will determine national health policy.",1993,34,1988,80,7,24,51,55,64,66,74,86,67,111
b756107971906fc319a94da0b247e04607b63225,"Execution Language Working Draft 01, 16 22 October 23 November 2003 Document identifier: wsbpel-specification-draft-01 (XML, HTML, PDF) Location: http://www.oasis-open.org/apps/org/workgroup/wsbpel/ Editors: Ben Bloch <ben_b54@hotmail.com> Francisco Curbera, IBM <curbera@us.ibm.com> Yaron Goland, BEA <ygoland@bea.com> Neelakantan Kartha, Sterling Commerce <N_Kartha@stercomm.com> Canyang Kevin Liu, SAP <kevin.liu@sap.com> Satish Thatte, Microsoft <satisht@microsoft.com> Prasad Yendluri, webMethods <pyendluri@webmethods.com> Editor’s Notes – KevinL – list needs to be updated to include all editors Contributors:",2009,15,1202,120,174,137,125,110,96,77,53,31,30,18
4e547d5df2ef5de3fa2f91db0f11c93f4682a2a8,"Abstract In this article we focus on the vital ecological services provided by insects. We restrict our focus to services provided by “wild” insects; we do not include services from domesticated or mass-reared insect species. The four insect services for which we provide value estimates—dung burial, pest control, pollination, and wildlife nutrition—were chosen not because of their importance but because of the availability of data and an algorithm for their estimation. We base our estimations of the value of each service on projections of losses that would accrue if insects were not functioning at their current level. We estimate the annual value of these ecological services provided in the United States to be at least $57 billion, an amount that justifies greater investment in the conservation of these services.",2006,99,1420,143,7,19,39,51,61,62,93,107,106,115
6435a4806428a18bbeb046668e6794d2bba47c34,"""Every developer working with the Web needs to read this book."" -- David Heinemeier Hansson, creator of the Rails framework ""RESTful Web Services finally provides a practical roadmap for constructing services that embrace the Web, instead of trying to route around it."" -- Adam Trachtenberg, PHP author and EBay Web Services Evangelist You've built web sites that can be used by humans. But can you also build web sites that are usable by machines? That's where the future lies, and that's what RESTful Web Services shows you how to do. The World Wide Web is the most popular distributed application in history, and Web services and mashups have turned it into a powerful distributed computing platform. But today's web service technologies have lost sight of the simplicity that made the Web successful. They don't work like the Web, and they're missing out on its advantages. This book puts the ""Web"" back into web services. It shows how you can connect to the programmable web with the technologies you already use every day. The key is REST, the architectural style that drives the Web. This book: Emphasizes the power of basic Web technologies -- the HTTP application protocol, the URI naming standard, and the XML markup languageIntroduces the Resource-Oriented Architecture (ROA), a common-sense set of rules for designing RESTful web servicesShows how a RESTful design is simpler, more versatile, and more scalable than a design based on Remote Procedure Calls (RPC)Includes real-world examples of RESTful web services, like Amazon's Simple Storage Service and the Atom Publishing ProtocolDiscusses web service clients for popular programming languagesShows how to implement RESTful services in three popular frameworks -- Ruby on Rails, Restlet (for Java), and Django (for Python)Focuses on practical issues: how to design and implement RESTful web services and clients This is the first book that applies the REST design philosophy to real web services. It sets down the best practices you need to make your design a success, and the techniques you need to turn your design into working code. You can harness the power of the Web for programmable applications: you just have to work with the Web instead of against it. This book shows you how.",2007,0,1411,113,5,61,90,132,167,137,146,172,129,109
3741254fcde051b5512143b1a01b57385ca87c44,The diversity of the service sector makes it difficult to come up with managerially useful generalizations concerning marketing practice in service organizations. This article argues for a focus on specific categories of services and proposes five schemes for classifying services in ways that transcend narrow industry boundaries. In each instance insights are offered into how the nature of the service might affect the marketing task.,1983,19,1999,81,0,3,6,6,9,8,9,17,12,17
ce2d38350ef4dad58589ac3de3cdaad1645818a8,"The U.S. Preventive Services Task Force (USPSTF/Task Force) represents one of several efforts to take a more evidence-based approach to the development of clinical practice guidelines. As methods have matured for assembling and reviewing evidence and for translating evidence into guidelines, so too have the methods of the USPSTF. This paper summarizes the current methods of the third USPSTF, supported by the Agency for Healthcare Research and Quality (AHRQ) and two of the AHRQ Evidence-based Practice Centers (EPCs). The Task Force limits the topics it reviews to those conditions that cause a large burden of suffering to society and that also have available a potentially effective preventive service. It focuses its reviews on the questions and evidence most critical to making a recommendation. It uses analytic frameworks to specify the linkages and key questions connecting the preventive service with health outcomes. These linkages, together with explicit inclusion criteria, guide the literature searches for admissible evidence. Once assembled, admissible evidence is reviewed at three strata: (1) the individual study, (2) the body of evidence concerning a single linkage in the analytic framework, and (3) the body of evidence concerning the entire preventive service. For each stratum, the Task Force uses explicit criteria as general guidelines to assign one of three grades of evidence: good, fair, or poor. Good or fair quality evidence for the entire preventive service must include studies of sufficient design and quality to provide an unbroken chain of evidence-supported linkages, generalizable to the general primary care population, that connect the preventive service with health outcomes. Poor evidence contains a formidable break in the evidence chain such that the connection between the preventive service and health outcomes is uncertain. For services supported by overall good or fair evidence, the Task Force uses outcomes tables to help categorize the magnitude of benefits, harms, and net benefit from implementation of the preventive service into one of four categories: substantial, moderate, small, or zero/negative. The Task Force uses its assessment of the evidence and magnitude of net benefit to make a recommendation, coded as a letter: from A (strongly recommended) to D (recommend against). It gives an I recommendation in situations in which the evidence is insufficient to determine net benefit. The third Task Force and the EPCs will continue to examine a variety of methodologic issues and document work group progress in future communications.",2001,47,1687,88,7,37,50,67,82,93,91,103,114,130
7ba11566fec38a35edf9626d2ac48891db32a831,The purpose of this article is to lay the foundations of a theory that can be used to interpret innovation processes in the service sector. The hypothesis underpinning this article is based on Lancaster's definition of the product (in both manufacturing and services) as a set of service characteristics. The article follows the example of those who have sought to apply Lancaster's work to technological phenomena. Various modes of innovation in the service sectors are highlighted and illustrated.,1997,49,1863,120,0,8,6,19,14,28,23,21,36,46
a14750ba068c664b213c88508037e55283427664,"Increasingly, computing addresses collaboration, data sharing, and interaction modes that involve distributed resources, resulting in an increased focus on the interconnection of systems both within and across enterprises. These evolutionary pressures have led to the development of Grid technologies. The authors' work focuses on the nature of the services that respond to protocol messages. Grid provides an extensible set of services that can be aggregated in various ways to meet the needs of virtual organizations, which themselves can be defined in part by the services they operate and share.",2002,18,1872,72,33,189,251,279,215,172,134,122,109,90
56ce6e97f2090df9e0a8ea2d581017a853b45122,This article compares problems and strategies cited in the services marketing literature with those reported by actual service suppliers in a study conducted by the authors. Discussion centers on several broad themes that emerge from this comparison and on guidelines for future work in services marketing.,1985,66,2117,39,0,5,5,7,17,17,19,13,28,17
462570ca88f5ea8da5a3d46b480b378dd4bf8382,"Assessing Ecological Restoration In the wake of the Millennium Ecosystem Assessment, the analysis of ecosystem services, and their relationship to biodiversity, has become one of the most rapidly developing research themes in environmental science. At the same time, ecological restoration is widely being implemented as a response to environmental degradation and biodiversity loss. Rey Benayas et al. (p. 1121, published online 30 July) link these themes in a meta-analysis of the impacts of ecological restoration actions on provision of ecosystem services and biodiversity conservation. The analysis of 89 published restoration projects worldwide establishes that ecological restoration does, in general, have positive impacts on both biodiversity and provision of ecosystem services. These effects are especially marked in the tropics. Thus, ecological restoration actions may indeed deliver benefits, both in terms of biodiversity conservation and supporting human livelihoods. Restoration, biodiversity, and ecosystem services are positively linked in a wide range of ecosystem types across the globe. Ecological restoration is widely used to reverse the environmental degradation caused by human activities. However, the effectiveness of restoration actions in increasing provision of both biodiversity and ecosystem services has not been evaluated systematically. A meta-analysis of 89 restoration assessments in a wide range of ecosystem types across the globe indicates that ecological restoration increased provision of biodiversity and ecosystem services by 44 and 25%, respectively. However, values of both remained lower in restored versus intact reference ecosystems. Increases in biodiversity and ecosystem service measures after restoration were positively correlated. Results indicate that restoration actions focused on enhancing biodiversity should support increased provision of ecosystem services, particularly in tropical terrestrial biomes.",2009,17,1199,46,1,35,57,74,99,113,109,93,115,135
5b3f813531dad9413f145e911c9042972ac65ce3,"The physician's degree of resourcefulness, i.e., the ability to deal skillfully and promptly with new situations, is important for changing the health behaviors of patients within the constraints of a brief office visit. This quality, however, was in short supply among 15 primary care physicians selected for their interest in preventive medicine. The physicians tended to rely on a single approach for changing specific health behaviors of patients, restricted referrals to community services and other health specialists, relied almost exclusively on fear for motivating patients and expressed considerable pessimism about changing the health behaviors of older patients. The physicians uniformly reported that their inadequate education and the lack of reimbursement influenced how they counseled their patients. A good place to begin to rectify this situation is the required reading of the Guide to Clinical Preventive Services for medical students and residents, and continuing education opportunities for practic...",1993,14,2089,0,21,11,18,36,82,113,122,169,173,165
618343bee669d361102b3f68ad3e586d308a2ed2,"This research investigated the relationship between three elements – core service quality, relational service quality‐ and perceived value – and customer satisfaction and future intentions across four services. The results revealed that core service quality (the promise) and perceived value were the most important drivers of customer satisfaction with relational service quality (the delivery) a significant but less important driver. A direct link between customer satisfaction and future intentions was established. The relative importance of the three drivers of satisfaction varied among services. Specifically, the importance of core service quality and perceived value was reversed depending on the service. A major conclusion was that both perceived value and service quality dimensions should be incorporated into customer satisfaction models to provide a more complete picture of the drivers of satisfaction.",2000,38,1742,72,0,3,11,13,19,22,36,59,64,74
d9c7bb541e25d5ff295456c33cb7eebb2df49220,,1961,0,12165,474,0,0,0,0,0,0,0,0,0,0
de6a1fcf1dc0d45fc9cd0ade75b44ddadd46a5e5,"Programmed cell death (PCD) plays a key role in developmental biology and in maintenance of the steady state in continuously renewing tissues. Currently, its existence is inferred mainly from gel electrophoresis of a pooled DNA extract as PCD was shown to be associated with DNA fragmentation. Based on this observation, we describe here the development of a method for the in situ visualization of PCD at the single-cell level, while preserving tissue architecture. Conventional histological sections, pretreated with protease, were nick end labeled with biotinylated poly dU, introduced by terminal deoxy- transferase, and then stained using avidin-conjugated peroxidase. The reaction is specific, only nuclei located at positions where PCD is expected are stained. The initial screening includes: small and large intestine, epidermis, lymphoid tissues, ovary, and other organs. A detailed analysis revealed that the process is initiated at the nuclear periphery, it is relatively short (1-3 h from initiation to cell elimination) and that PCD appears in tissues in clusters. The extent of tissue-PCD revealed by this method is considerably greater than apoptosis detected by nuclear morphology, and thus opens the way for a variety of studies.",1992,55,9547,316,0,22,117,301,482,728,792,786,810,682
453808d1964a003ab6386becd1e7f33b60dfc876,"A structure refinement method is described which does not use integrated neutron powder intensities, single or overlapping, but employs directly the profile intensities obtained from step-scanning measurements of the powder diagram. Nuclear as well as magnetic structures can be refined, the latter only when their magnetic unit cell is equal to, or a multiple of, the nuclear cell. The least-squares refinement procedure allows, with a simple code, the introduction of linear or quadratic constraints between the parameters.",1969,1,12343,223,0,0,0,0,0,0,0,0,0,0
f55b0476eae94742e77590fb2cb8bb54b661b95b,"Hundreds of small RNAs of ∼22 nucleotides, collectively named microRNAs (miRNAs), have been discovered recently in animals and plants. Although their functions are being unravelled, their mechanism of biogenesis remains poorly understood. miRNAs are transcribed as long primary transcripts (pri-miRNAs) whose maturation occurs through sequential processing events: the nuclear processing of the pri-miRNAs into stem-loop precursors of ∼70 nucleotides (pre-miRNAs), and the cytoplasmic processing of pre-miRNAs into mature miRNAs. Dicer, a member of the RNase III superfamily of bidentate nucleases, mediates the latter step, whereas the processing enzyme for the former step is unknown. Here we identify another RNase III, human Drosha, as the core nuclease that executes the initiation step of miRNA processing in the nucleus. Immunopurified Drosha cleaved pri-miRNA to release pre-miRNA in vitro. Furthermore, RNA interference of Drosha resulted in the strong accumulation of pri-miRNA and the reduction of pre-miRNA and mature miRNA in vivo. Thus, the two RNase III proteins, Drosha and Dicer, may collaborate in the stepwise processing of miRNAs, and have key roles in miRNA-mediated gene regulation in processes such as development and differentiation.",2003,35,4910,236,11,104,147,170,208,237,244,291,369,367
0c9bb579d8ad6ac987f7a16b66ddace671fc57c5,"The affine rank minimization problem consists of finding a matrix of minimum rank that satisfies a given system of linear equality constraints. Such problems have appeared in the literature of a diverse set of fields including system identification and control, Euclidean embedding, and collaborative filtering. Although specific instances can often be solved with specialized algorithms, the general affine rank minimization problem is NP-hard because it contains vector cardinality minimization as a special case. 
 
In this paper, we show that if a certain restricted isometry property holds for the linear transformation defining the constraints, the minimum-rank solution can be recovered by solving a convex optimization problem, namely, the minimization of the nuclear norm over the given affine space. We present several random ensembles of equations where the restricted isometry property holds with overwhelming probability, provided the codimension of the subspace is sufficiently large. 
 
The techniques used in our analysis have strong parallels in the compressed sensing framework. We discuss how affine rank minimization generalizes this preexisting concept and outline a dictionary relating concepts from cardinality minimization to those of rank minimization. We also discuss several algorithmic approaches to minimizing the nuclear norm and illustrate our results with numerical examples.",2007,114,3212,258,1,16,81,111,143,172,260,283,324,364
44de88c46c07f6c9cedc51b716447cdedb126e84,The liquid drop model the shell model rotation and single-particle motion nuclear forces the Hartree-Fock method pairing correlations and superfluid nuclei the generalized single-particle model (HFB theory) harmonic vibrations boson expansion methods the generator coordinate method restoration of broken symmetries the time dependent Hartree-Fock method (TDHF) semiclassical methods in nuclear physics. Appendices: angular momentum algebra in the laboratory and the body-fixed system electromagnetic moments and transitions second quantization density matrices theorems concerning product wave functions many-body green's functions.,2004,0,3313,301,132,124,120,135,205,169,182,174,183,195
1a489853b1c8184e9c9844388185deaa0b7e6a0b,List of notation Introduction The dynamics of nuclear spin systems Manipulation of nuclear spin Hamiltonians One-dimensional Fourier spectroscopy Multiple-quantum transitions Two-dimensional Fourier spectroscopy Two-dimensional separation of interactions Two-dimensional correlation methods based on coherence transfer Dynamic processes studied by two-dimensional exchange spectroscopy Nuclear magnetic resonance imaging References Index.,1987,0,4852,256,26,65,104,134,104,120,102,129,104,123
edb43f4deb76e55991b12bcfcf0b204c03a2e759,"Adaptive thermogenesis is an important component of energy homeostasis and a metabolic defense against obesity. We have cloned a novel transcriptional coactivator of nuclear receptors, termed PGC-1, from a brown fat cDNA library. PGC-1 mRNA expression is dramatically elevated upon cold exposure of mice in both brown fat and skeletal muscle, key thermogenic tissues. PGC-1 greatly increases the transcriptional activity of PPARgamma and the thyroid hormone receptor on the uncoupling protein (UCP-1) promoter. Ectopic expression of PGC-1 in white adipose cells activates expression of UCP-1 and key mitochondrial enzymes of the respiratory chain, and increases the cellular content of mitochondrial DNA. These results indicate that PGC-1 plays a key role in linking nuclear receptors to the transcriptional program of adaptive thermogenesis.",1998,123,3486,242,15,46,65,68,83,105,120,127,143,142
35f66c646fa8aadc444209ebfffdf6a2d65a6c67,"Media discourse and public opinion are treated as two parallel systems of constructing meaning. This paper explores their relationship by analyzing the discourse on nuclear power in four general audience media: television news coverage, newsmagazine accounts, editorial cartoons, and syndicated opinion columns. The analysis traces the careers of different interpretive packages on nuclear power from 1945 to the present. This media discourse, it is argued, is an essential context for understanding the formation of public opinion on nuclear power. More specifically, it helps to account for such survey results as the decline in support for nuclear power before Three Mile Island, a rebound after a burst of media publicity has died out, the gap between general support for nuclear power and support for a plant in one's own community, and the changed relationship of age to support for nuclear power from 1950 to the present.",1989,40,3995,197,2,6,14,17,26,18,23,38,21,30
da5241530df59c86691b381328b761559c7e2ddc,"David J. Mangelsdorf,’ Carl Thummel,2 Miguel Beato,3 Peter Herrlich,4 Giinther Schiitq5 Kazuhiko Umesono,6 Bruce Blumberg,’ Philippe Kastner,’ Manuel Mark,* Pierre Chambon,8 and Ronald M. Evan&‘* ‘Howard Hughes Medical Institute University of Texas Southwestern Medical Center Dallas, Texas 75235-9050 *Howard Hughes Medical Institute University of Utah Salt Lake City, Utah 84112 31nstutut fiir Molekularbiologie und Tumorforschung 35037 Marburg Federal Republic of Germany 4Forschungszentrum Karlsruhe lnstitut Genetik 76021 Karlsruhe Federal Republic of Germany 5Deutsches Krebsforschungszentrum 69120 Heidelberg Federal Republic of Germany GAdvanced Institute of Science and Technology Graduate School of Biological Sciences Nara 630-01 Japan ‘The Salk Institute for Biological Studies La Jolla, California 92037-5800 Blnstitut de Genetique et de Biologie Moleculaire et Cellulaire Centre National de la Recherche Scientifique lnstitut National de la Sante et de la Recherche M6dicale 67404 lllkirch Cedex Strasbourg France gHoward Hughes Medical Institute The Salk Institute for Biological Studies La Jolla, California 92037-5800",1995,26,6679,142,2,75,272,332,338,396,364,386,323,313
df8129946bebfa82dbe357e096035293fe814f44,"In chronic inflammatory diseases, such as asthma, rheumatoid arthritis, inflammatory bowel disease, and psoriasis, several cytokines recruit activated immune and inflammatory cells to the site of lesions, thereby amplifying and perpetuating the inflammatory state.1 These activated cells produce many other mediators of inflammation. What causes these diseases is still a mystery, but the disease process results from an interplay of genetic and environmental factors. Genes, such as those for atopy in asthma and for HLA antigens in rheumatoid arthritis and inflammatory bowel disease, may determine a patient's susceptibility to the disease and the disease's severity, but environmental factors, often unknown, . . .",1997,52,4590,134,13,116,185,231,248,232,255,258,239,253
bffc48ecb411dfe846e96d4880dbd3ae65e58b52,"The monoclonal antibody Ki-67 detects a nuclear antigen that is present only in proliferating cells. The aim of the present investigation was to clarify whether the Ki-67 nuclear antigen is restricted in its expression to certain phases of the cell cycle. All experiments consistently showed that the Ki-67 nuclear antigen is present in S, G2, and M phase, but is absent in G0. However, the results concerning Ki-67 antigen expression in G1 phase varied: cells passing the early events of mitogen triggered transition from G0 to G1, i.e., G1T and first G1A, lacked the Ki-67 nuclear antigen, whereas G1 cells after mitosis were constantly Ki-67-positive. This result suggests that after mitosis cells might not follow the same metabolic pathways as G0 cells do when entering G1 for the first time. Therefore, we suggest that the early stages of mitogen stimulation represent initial sequences of proliferation and not parts of the cell cycle. Because our data show that the Ki-67 nuclear antigen is present throughout the cell cycle, immunostaining with monoclonal antibody Ki-67 provides a reliable means of rapidly evaluating the growth fraction of normal and neoplastic human cell populations.",1984,0,4038,96,1,7,20,31,49,86,92,105,108,90
3ddfc1491ef19d4d3a2a5d41ef599593997a6075,"Nuclear resonance techniques involving free precession are examined, and, in particular, a convenient variation of Hahn's spin-echo method is described. This variation employs a combination of pulses of different intensity or duration (""90-degree"" and ""180-degree"" pulses). Measurements of the transverse relaxation time ${T}_{2}$ in fluids are often severely compromised by molecular diffusion. Hahn's analysis of the effect of diffusion is reformulated and extended, and a new scheme for measuring ${T}_{2}$ is described which, as predicted by the extended theory, largely circumvents the diffusion effect. On the other hand, the free precession technique, applied in a different way, permits a direct measurement of the molecular self-diffusion constant in suitable fluids. A measurement of the self-diffusion constant of water at 25\ifmmode^\circ\else\textdegree\fi{}C is described which yields $D=2.5(\ifmmode\pm\else\textpm\fi{}0.3)\ifmmode\times\else\texttimes\fi{}{10}^{\ensuremath{-}5}$ ${\mathrm{cm}}^{2}$/sec, in good agreement with previous determinations. An analysis of the effect of convection on free precession is also given. A null method for measuring the longitudinal relaxation time ${T}_{1}$, based on the unequal-pulse technique, is described.",1954,0,4876,160,1,1,2,9,3,14,14,14,10,19
087d3b45c4d528d7f13f16f79710ffcb13ed295f,"A spin echo method adapted to the measurement of long nuclear relaxation times (T2) in liquids is described. The pulse sequence is identical to the one proposed by Carr and Purcell, but the rf of the successive pulses is coherent, and a phase shift of 90° is introduced in the first pulse. Very long T2 values can be measured without appreciable effect of diffusion.",1958,6,4609,137,0,6,3,4,0,6,5,8,8,5
e91170f4090115a90d7de1f3287c70743fb10788,"Posttranslational modifications of histone N-terminal tails impact chromatin structure and gene transcription. While the extent of histone acetylation is determined by both acetyltransferases and deacetylases, it has been unclear whether histone methylation is also regulated by enzymes with opposing activities. Here, we provide evidence that LSD1 (KIAA0601), a nuclear homolog of amine oxidases, functions as a histone demethylase and transcriptional corepressor. LSD1 specifically demethylates histone H3 lysine 4, which is linked to active transcription. Lysine demethylation occurs via an oxidation reaction that generates formaldehyde. Importantly, RNAi inhibition of LSD1 causes an increase in H3 lysine 4 methylation and concomitant derepression of target genes, suggesting that LSD1 represses transcription via histone demethylation. The results thus identify a histone demethylase conserved from S. pombe to human and reveal dynamic regulation of histone methylation by both histone methylases and demethylases.",2004,83,2710,245,0,53,104,108,112,88,150,159,151,170
c1c58d715e912af0ac8928f68f5c2c19f7e41d1d,,1946,6,4323,113,0,0,1,2,0,1,2,1,4,2
97a431e3d5990a74970417527e0d162527594e84,"I. Introduction II. Molecular Aspects A. PPAR isotypes: identity, genomic organization and chromosomal localization B. DNA binding properties C. PPAR ligand-binding properties D. Alternative pathways for PPAR activation E. PPAR-mediated transactivation properties III. Physiological Aspects A. Differential expression of PPAR mRNAs B. PPAR target genes and functions in fatty acid metabolism C. PPARs and control of inflammatory responses D. PPARs and atherosclerosis E. PPARs and the development of the fetal epidermal permeability barrier F. PPARs, carcinogenesis, and control of the cell cycle IV. Conclusions",1999,486,2971,162,2,28,80,146,123,166,150,207,195,215
34fa2f4ec751a302329ac952048fbfe3c84c9ebd,"The nuclear factor NF-kappaB pathway has long been considered a prototypical proinflammatory signaling pathway, largely based on the role of NF-kappaB in the expression of proinflammatory genes including cytokines, chemokines, and adhesion molecules. In this article, we describe how genetic evidence in mice has revealed complex roles for the NF-kappaB in inflammation that suggest both pro- and anti-inflammatory roles for this pathway. NF-kappaB has long been considered the ""holy grail"" as a target for new anti-inflammatory drugs; however, these recent studies suggest this pathway may prove a difficult target in the treatment of chronic disease. In this article, we discuss the role of NF-kappaB in inflammation in light of these recent studies.",2009,82,2418,104,1,3,40,32,49,80,170,214,318,369
2537db606c213073cb182d7f0170736006fe8ac0,"The exchange of energy between a system of nuclear spins immersed in a strong magnetic field, and the heat reservoir consisting of the other degrees of freedom (the ""lattice"") of the substance containing the magnetic nuclei, serves to bring the spin system into equilibrium at a finite temperature. In this condition the system can absorb energy from an applied radiofrequency field. With the absorption of energy, however, the spin temperature tends to rise and the rate of absorption to decrease. Through this ""saturation"" effect, and in some cases by a more direct method, the spin-lattice relaxation time ${T}_{1}$ can be measured. The interaction among the magnetic nuclei, with which a characteristic time $T_{2}^{}{}_{}{}^{\ensuremath{'}}$ is associated, contributes to the width of the absorption line. Both interactions have been studied in a variety of substances, but with the emphasis on liquids containing hydrogen.",1948,0,4017,9,7,10,12,11,13,20,15,22,28,34
d65895c35b0d698043b7779b7bc80a8ab99105b3,,1946,0,4638,63,3,3,5,7,4,5,1,3,6,4
b26d845e7dcfee0b6f9f236440477be4166711c8,"FIRST EXPERIMENTS IN JET. Results obtained from JET since June 1983 are described which show that this large tokamak behaves in a similar manner to smaller tokamaks, but with correspondingly improved plasma parameters. Long-duration hydrogen and deuterium plasmas (>10 s) have been obtained with electron temperatures reaching > 4 keV for power dissipations < 3 MW and with * Euratom-IPP Association, Institut fur Plasmaphysik, Garching, Federal Republic of Germany. ** Euratom-ENEA Association, Centro di Frascati, Italy. *** Euratom-UKAEA Association, Culham Laboratory, Abingdon, Oxfordshire, United Kingdom. **** University of Dusseldorf, Dusseldorf, Federal Republic of Germany. + Euratom-Ris0 Association, Ris<f> National Laboratory, Roskilde, Denmark. ++ Euratom-CNR Association, Istituto di Física del Plasma, Milan, Italy. +++ Imperial College of Science and Technology, University of London, London, United Kingdom. ++++ Euratom-FOM Association, FOM Instituut voor Plasmafysica,. Nieuwegein, Netherlands. ® Euratom-Suisse Association, Centre de Recherches en Physique des Plasmas, Lausanne, Switzerland.",1987,65,3499,17,185,176,174,228,213,238,197,196,202,178
e226b8871e0b0378a2e31b17d6091e3a359997a5,"MicroRNAs (miRNAs) are initially expressed as long transcripts that are processed in the nucleus to yield approximately 65-nucleotide (nt) RNA hairpin intermediates, termed pre-miRNAs, that are exported to the cytoplasm for additional processing to yield mature, approximately 22-nt miRNAs. Here, we demonstrate that human pre-miRNA nuclear export, and miRNA function, are dependent on Exportin-5. Exportin-5 can bind pre-miRNAs specifically in vitro, but only in the presence of the Ran-GTP cofactor. Short hairpin RNAs, artificial pre-miRNA analogs used to express small interfering RNAs, also depend on Exportin-5 for nuclear export. Together, these findings define an additional cellular cofactor required for miRNA biogenesis and function.",2003,37,2924,85,0,52,70,128,108,145,185,220,221,219
0ed98a8eb9d766e857a32222cc394ee4c85aea0b,"A battery of monoclonal antibodies (mAbs) against brain cell nuclei has been generated by repeated immunizations. One of these, mAb A60, recognizes a vertebrate nervous system- and neuron-specific nuclear protein that we have named NeuN (Neuronal Nuclei). The expression of NeuN is observed in most neuronal cell types throughout the nervous system of adult mice. However, some major cell types appear devoid of immunoreactivity including cerebellar Purkinje cells, olfactory bulb mitral cells, and retinal photoreceptor cells. NeuN can also be detected in neurons in primary cerebellar cultures and in retinoic acid-stimulated P19 embryonal carcinoma cells. Immunohistochemically detectable NeuN protein first appears at developmental timepoints which correspond with the withdrawal of the neuron from the cell cycle and/or with the initiation of terminal differentiation of the neuron. NeuN is a soluble nuclear protein, appears as 3 bands (46-48 x 10(3) M(r)) on immunoblots, and binds to DNA in vitro. The mAb crossreacts immunohistochemically with nervous tissue from rats, chicks, humans, and salamanders. This mAb and the protein recognized by it serve as an excellent marker for neurons in the central and peripheral nervous systems in both the embryo and adult, and the protein may be important in the determination of neuronal phenotype.",1992,56,2242,215,0,1,4,4,16,22,26,34,44,63
923f057495e725086b6bdf856516e661c74d6c0e,"AN image of an object may be defined as a graphical representation of the spatial distribution of one or more of its properties. Image formation usually requires that the object interact with a matter or radiation field characterized by a wavelength comparable to or smaller than the smallest features to be distinguished, so that the region of interaction may be restricted and a resolved image generated.",1973,4,3233,89,0,4,5,10,7,14,17,18,29,40
ef44c55026bcef618e42e7669a47412a2d8552e6,"Quantum computers promise to exceed the computational efficiency of ordinary classical machines because quantum algorithms allow the execution of certain tasks in fewer steps. But practical implementation of these machines poses a formidable challenge. Here I present a scheme for implementing a quantum-mechanical computer. Information is encoded onto the nuclear spins of donor atoms in doped silicon electronic devices. Logical operations on individual spins are performed using externally applied electric fields, and spin measurements are made using currents of spin-polarized electrons. The realization of such a computer is dependent on future refinements of conventional silicon electronics.",1998,47,2730,68,11,34,61,90,136,133,149,164,163,140
45803a2386946716508fb9091a723d21474bbeb7,"MicroRNAs (miRNAs), which function as regulators of gene expression in eukaryotes, are processed from larger transcripts by sequential action of nuclear and cytoplasmic ribonuclease III–like endonucleases. We show that Exportin-5 (Exp5) mediates efficient nuclear export of short miRNA precursors (pre-miRNAs) and that its depletion by RNA interference results in reduced miRNA levels. Exp5 binds correctly processed pre-miRNAs directly and specifically, in a Ran guanosine triphosphate–dependent manner, but interacts only weakly with extended pre-miRNAs that yield incorrect miRNAs when processed by Dicer in vitro. Thus, Exp5 is key to miRNA biogenesis and may help coordinate nuclear and cytoplasmic processing steps.",2004,27,2609,62,61,78,120,120,142,166,192,200,207,230
769decf5a22b2081bced264e9b83187e7f805230,"Nuclear factor-κB (NF-κB) transcription factors and the signalling pathways that activate them are central coordinators of innate and adaptive immune responses. More recently, it has become clear that NF-κB signalling also has a critical role in cancer development and progression. NF-κB provides a mechanistic link between inflammation and cancer, and is a major factor controlling the ability of both pre-neoplastic and malignant cells to resist apoptosis-based tumour-surveillance mechanisms. NF-κB might also regulate tumour angiogenesis and invasiveness, and the signalling pathways that mediate its activation provide attractive targets for new chemopreventive and chemotherapeutic approaches.",2006,70,2505,41,23,154,187,193,207,191,229,196,194,184
58c0730ba6cfbfa19720795a0b12669559e12646,"Bile acids are essential for the solubilization and transport of dietary lipids and are the major products of cholesterol catabolism. Results presented here show that bile acids are physiological ligands for the farnesoid X receptor (FXR), an orphan nuclear receptor. When bound to bile acids, FXR repressed transcription of the gene encoding cholesterol 7alpha-hydroxylase, which is the rate-limiting enzyme in bile acid synthesis, and activated the gene encoding intestinal bile acid-binding protein, which is a candidate bile acid transporter. These results demonstrate a mechanism by which bile acids transcriptionally regulate their biosynthesis and enterohepatic transport.",1999,19,2204,103,19,73,66,86,84,87,88,87,81,93
a7dd81dd04155ad8663fc01811419ed5c74886e7,"CRM1 is distantly related to receptors that mediate nuclear protein import and was previously shown to interact with the nuclear pore complex. Overexpression of CRM1 in Xenopus oocytes stimulates Rev and U snRNA export from the nucleus. Conversely, leptomycin B, a cytotoxin that is shown to bind to CRM1 protein, specifically inhibits the nuclear export of Rev and U snRNAs. In vitro, CRM1 forms a leptomycin B-sensitive complex involving cooperative binding of both RanGTP and the nuclear export signal (NES) from either the Rev or PKI proteins. We conclude that CRM1 is an export receptor for leucine-rich nuclear export signals and discuss a model for the role of RanGTP in CRM1 function and in nuclear export in general.",1997,144,2091,159,9,120,159,134,156,124,105,108,100,107
dbbe9374b0fec379043b2e5658ae760ea3675b60,"The possibilities for the extension of spectroscopy to two dimensions are discussed. Applications to nuclear magnetic resonance are described. The basic theory of two‐dimensional spectroscopy is developed. Numerous possible applications are mentioned and some of them treated in detail, including the elucidation of energy level diagrams, the observation of multiple quantum transitions, and the recording of high‐resolution spectra in inhomogenous magnetic fields. Experimental results are presented for some simple spin systems.",1976,51,2822,50,5,21,17,24,34,39,54,84,92,111
2de6969cb9ea245746d8c7df519c3ae361e21f9d,"Nuclear receptors regulate gene expression by direct activation of target genes and inhibition of AP-1. Here we report that, unexpectedly, activation by nuclear receptors requires the actions of CREB-binding protein (CBP) and that inhibition of AP-1 activity is the apparent result of competition for limiting amounts of CBP/p300 in cells. Utilizing distinct domains, CBP directly interacts with the ligand-binding domain of multiple nuclear receptors and with the p160 nuclear receptor coactivators, which upon cloning have proven to be variants of the SRC-1 protein. Because CBP represents a common factor, required in addition to distinct coactivators for function of nuclear receptors, CREB, and AP-1, we suggest that CBP/p300 serves as an integrator of multiple signal transduction pathways within the nucleus.",1996,130,2195,100,31,197,250,256,237,177,174,135,112,79
229b1687fc6120ba1d6ce543408c9d4558b228ed,"A short sequence of amino acids including Lys-128 is required for the normal nuclear accumulation of wild-type and deleted forms of SV40 large T antigen. A cytoplasmic large T mutant that lacks sequences from around Lys-128 localizes to the nucleus if the missing sequence is attached to its amino terminus. The implication that the sequence element around Lys-128 acts as an autonomous signal capable of specifying nuclear location was tested directly by transferring it to the amino termini of beta-galactosidase and of pyruvate kinase, normally a cytoplasmic protein. Sequences that included the putative signal induced each of the fusion proteins to accumulate completely in the nucleus but had no discernible effect when Lys-128 was replaced by Thr. By reducing the size of the transposed sequence we conclude that Pro-Lys-Lys-Lys-Arg-Lys-Val can act as a nuclear location signal. The sequence may represent a prototype of similar sequences in other nuclear proteins.",1984,30,2328,128,0,19,39,33,60,61,66,76,80,61
75fa5f82fa58637cd93af6703467e7e2c3cc629e,"Abstract We tabulate the atomic mass excesses and nuclear ground-state deformations of 8979 nuclei ranging from 16O to A = 339. The calculations are based on the finite-range droplet macroscopic model and the folded-Yukawa single-particle microscopic model. Relative to our 1981 mass table the current results are obtained with an improved macroscopic model, an improved pairing model with a new form for the effective-interaction pairing gap, and minimization of the ground-state energy with respect to additional shape degrees of freedom. The values of only nine constants are determined directly from a least-squares adjustment to the ground-state masses of 1654 nuclei ranging from 16O to 263106 and to 28 fission-barrier heights. The error of the mass model is 0.669 MeV for the entire region of nuclei considered, but is only 0.448 MeV for the region N ≥ 65.",1993,0,2473,34,1,4,15,40,46,32,44,50,58,77
cb52b4b6fd95b44c6c1a5f476fd3b7eeb0d510a4,"We have identified a 50-nucleotide enhancer from the human erythropoietin gene 3'-flanking sequence which can mediate a sevenfold transcriptional induction in response to hypoxia when cloned 3' to a simian virus 40 promoter-chloramphenicol acetyltransferase reporter gene and transiently expressed in Hep3B cells. Nucleotides (nt) 1 to 33 of this sequence mediate sevenfold induction of reporter gene expression when present in two tandem copies compared with threefold induction when present in a single copy, suggesting that nt 34 to 50 bind a factor which amplifies the induction signal. DNase I footprinting demonstrated binding of a constitutive nuclear factor to nt 26 to 48. Mutagenesis studies revealed that nt 4 to 12 and 19 to 23 are essential for induction, as substitutions at either site eliminated hypoxia-induced expression. Electrophoretic mobility shift assays identified a nuclear factor which bound to a probe spanning nt 1 to 18 but not to a probe containing a mutation which eliminated enhancer function. Factor binding was induced by hypoxia, and its induction was sensitive to cycloheximide treatment. We have thus defined a functionally tripartite, 50-nt hypoxia-inducible enhancer which binds several nuclear factors, one of which is induced by hypoxia via de novo protein synthesis.",1992,27,2364,86,0,7,13,21,31,51,50,48,59,56
e16d88ae17b0551bf719f30e2c0006cad1cc5471,"Nuclear receptors (NR) comprise a family of transcription factors that regulate gene expression in a liganddependent manner. Members of the NR superfamily include receptors for steroid hormones, such as estrogens (ER) and glucocorticoids (GR), receptors for nonsteroidal ligands, such as thyroid hormones (TR) and retinoic acid (RAR), as well as receptors that bind diverse products of lipid metabolism, such as fatty acids and prostaglandins (for review, see Beato et al. 1995; Chambon 1995; Mangelsdorf and Evans 1995). The NR superfamily also includes a large number of so-called orphan receptors for which regulatory ligands have not been identified (Mangelsdorf and Evans 1995). Although many orphan receptors are likely to be regulated by small-molecular-weight ligands, other mechanisms of regulation, such as phosphorylation (Hammer et al. 1999; Tremblay et al. 1999) have also proven to be of importance. Remarkably, the sequence of the Caenorhabditis elegans genome has revealed the presence of >200 members of the NR family, suggesting a critical role of these proteins in environmental adaptation (Sluder et al. 1999). Although mammalian genomes are unlikely to contain such a large complement of these factors, >24 distinct classes of NR have been identified in humans, and these factors exert diverse roles in the regulation of growth, development, and homeostasis. Based on their importance in biology and medicine, as well as the relatively simple mechanism of regulation, NR represent one of the most intensively studied and best-understood classes of transcription factors at the molecular level. Members of the NR family regulate transcription by several mechanisms (Fig. 1). Nuclear receptors can activate or repress target genes by binding directly to DNA response elements as homoor heterodimers or by binding to other classes of DNA-bound transcription factors. A subset of NRs, including TR and RAR, can actively repress target genes in the presence or absence of ligand binding, and many NR have been demonstrated to inhibit transcription in a ligand-dependent manner by antagonizing the transcriptional activities of other classes of transcription factors. These activities have been linked to interactions with general classes of molecules that appear to serve coactivator or corepressor function. In this review, we will discuss recent progress concerning the molecular mechanisms by which NR cofactor interactions serve to activate or repress transcription.",2000,235,2179,94,65,152,193,161,183,140,147,129,106,121
bd54727c7c33fcd6e0481c8c9db988dd7fcce4d3,"The production of a mouse monoclonal antibody, Ki‐67, is described. The Ki‐67 antibody recognized a nuclear antigen present in proliferating cells, but absent in resting cells. Immunostainings with Ki‐67 revealed nuclear reactivity in cells of germinal centres of cortical follicles, cortical thymocytes, neck cells of gastrointestinal mucosa, un‐differentiated spermatogonia and cells of a number of human cell lines. The Ki‐67 antibody did not react with cells known to be in a resting stage, such as lymphocytes, monocytes, parietal cells and Paneth's cells of gastrointestinal mucosa, hepatocytes, renal cells, mature sperm cells, brain cells, etc. Expression of the antigen recognized by Ki‐67 could be induced in peripheral blood lymphocytes after stimulation with phytohaemagglutinin, whereas it disappeared from HL‐60 cells stimulated with phorbol esters to differentiate into mature macrophages in a resting stage. These findings suggest that Ki‐67 is directed against a nuclear antigen associated with cell proliferation. A first series of immunostainings of tumour biopsies indicated that Ki‐67 may be a potent tool for easy and quick evaluation of the proportion of proliferating cells in a tumour.",1983,18,2596,51,1,3,6,17,39,58,74,74,98,92
12bde4bc647fdd1986c8e363b0bde742e9767a9e,"The binding of lipophilic hormones, retinoids and vitamins to members of the nuclear-receptor superfamily modifies the DNA-binding and transcriptional properties of these receptors, resulting in the activation or repression of target genes,. Ligand binding induces conformational changes in nuclear receptors and promotes their association with a diverse group of nuclear proteins, including SRC-1/p160 (refs 3-5), TIF-2/GRIP-1 (refs 6, 7) and CBP/p300 (refs 4, 5, 8, 9) which function as co-activators of transcription, and RIP-140 (ref. 10), TIF-1 (ref. 11) and TRIP-1/SUG-1 (refs 12, 13) whose functions are unclear. Here we report that a short sequence motif LXXLL (where L is leucine and X is any amino acid) present in RIP-140, SRC-1 and CBP is necessary and sufficient to mediate the binding of these proteins to liganded nuclear receptors. We show that the ability of SRC-1 to bind the oestrogen receptor and enhance its transcriptional activity is dependent upon the integrity of the LXXLL motifs and on key hydrophobic residues in a conserved helix (helix 12) of the oestrogen receptor that are required for its ligand-induced activation function. We propose that the LXXLL motif is a signature sequence that facilitates the interaction of different proteins with nuclear receptors, and is thus a defining feature of a new family of nuclear proteins.",1997,27,2095,98,15,88,132,151,126,140,136,131,109,111
d8c307a8b6218b968c9e898d0077007b4ed9cb58,"CYCLIC AMP-regulated gene expression frequently involves a DNA element known as the cAMP-regulated enhancer (CRE)14. Many transcription factors bind to this element, including the protein CREB5,6, which is activated as a result of phosphorylation by protein kinase A7. This modification stimulates interaction with one or more of the general transcription factors or, alternatively, allows recruitment of a co-activator. Here we report that CREB phosphorylated by protein kinase A binds specifically to a nuclear protein of Mr 265K which we term CBP (for CREB-binding protein). Fusion of a heterologous DNA-binding domain to the amino terminus of CBP enables the chimaeric protein to function as a protein kinase A-regulated transcriptional activator. We propose that CBP may participate in cAMP-regulated gene expression by interacting with the activated phosphorylated form of CREB.",1993,17,2025,137,1,17,50,81,100,132,136,136,129,96
dc0eeff2fa397ffa428c308a51f0280f980c5f4f,"The expression of genes is regulated at many levels. Perhaps the area in which least is known is how nuclear organization influences gene expression. Studies of higher-order chromatin arrangements and their dynamic interactions with other nuclear components have been boosted by recent technical advances. The emerging view is that chromosomes are compartmentalized into discrete territories. The location of a gene within a chromosome territory seems to influence its access to the machinery responsible for specific nuclear functions, such as transcription and splicing. This view is consistent with a topological model for gene regulation.",2001,137,2081,75,14,71,83,120,91,112,114,84,107,114
64f814ad0aeac192b62ba73440505b91b0fde6aa,"Cholesterol, fatty acids, fat-soluble vitamins, and other lipids present in our diets are not only nutritionally important but serve as precursors for ligands that bind to receptors in the nucleus. To become biologically active, these lipids must first be absorbed by the intestine and transformed by metabolic enzymes before they are delivered to their sites of action in the body. Ultimately, the lipids must be eliminated to maintain a normal physiological state. The need to coordinate this entire lipid-based metabolic signaling cascade raises important questions regarding the mechanisms that govern these pathways. Specifically, what is the nature of communication between these bioactive lipids and their receptors, binding proteins, transporters, and metabolizing enzymes that links them physiologically and speaks to a higher level of metabolic control? Some general principles that govern the actions of this class of bioactive lipids and their nuclear receptors are considered here, and the scheme that emerges reveals a complex molecular script at work.",2001,29,1984,78,0,68,148,132,112,118,115,117,127,133
b94d4b94697c338303bbbf61dfd0aed0eb7e2b01,"Nuclear factor-kappaB (NF-kappaB) transcription factors and the signalling pathways that activate them are central coordinators of innate and adaptive immune responses. More recently, it has become clear that NF-kappaB signalling also has a critical role in cancer development and progression. NF-kappaB provides a mechanistic link between inflammation and cancer, and is a major factor controlling the ability of both pre-neoplastic and malignant cells to resist apoptosis-based tumour-surveillance mechanisms. NF-kappaB might also regulate tumour angiogenesis and invasiveness, and the signalling pathways that mediate its activation provide attractive targets for new chemopreventive and chemotherapeutic approaches.",2006,0,1739,70,11,78,109,118,148,144,175,170,154,157
0eb465a0e0b7bfe884a38ac01b6dca199aeb0a10,"Nuclear DNA contents of more than 100 important plant species were measured by flow cytometry of isolated nuclei stained with propidium iodide.Arabidopsis exhibits developmentally regulated multiploidy and has a 2C nuclear DNA content of 0.30 pg (145 Mbp/1C), twice the value usually cited. The 2C value for rice is only about three times that ofArabidopsis. Tomato has a 2C value of about 2.0 pg, larger than commonly cited. This survey identified several horticultural crops in a variety of families with genomes only two or three times as large asArabidopsis; these include several fruit trees (a pricot, cherry, mango, orange, papaya, and peach). The small genome sizes of rice and the horticultural plants should facilitate molecular studies of these crops.",1991,11,1387,147,2,0,0,1,0,0,0,0,0,0
0f4e2da695c292f373d8fa9917c06a3cfd03750f,"Abstract A method is described for obtaining pure absorption phase spectra in four quadrants in a two-dimensional nuclear magnetic resonance spin exchange experiment. It is shown that phase correction results in a substantial increase in resolution and discrimination while maintaining a signal-to-noise ratio comparable to that of the usual magnitude spectrum. Experimental results are presented for the application of the method to a biological macromolecule, the bovine pancreatic trypsin inhibitor.",1982,8,2348,24,0,8,20,34,43,68,89,79,94,96
7ec947261f5a3eabdaddb8e53d58a36b986c4e71,"We describe the next generation general purpose Evaluated Nuclear Data File, ENDF/B-VII.0, of recommended nuclear data for advanced nuclear science and technology applications. The library, released by the U.S. Cross Section Evaluation Working Group (CSEWG) in December 2006, contains data primarily for reactions with incident neutrons, protons, and photons on almost 400 isotopes, based on experimental data and theory predictions. The principal advances over the previous ENDF/B-VI library are the following: (1) New cross sections for U, Pu, Th, Np and Am actinide isotopes, with improved performance in integral validation criticality and neutron transmission benchmark tests; (2) More precise standard cross sections for neutron reactions on H, 6 Li, 10 B, Au and for 235,238 U fission, developed by a collaboration with the IAEA and the OECD/NEA Working Party on Evaluation Cooperation (WPEC); (3) Improved thermal neutron scattering; (4) An extensive set of neutron cross sections on fission products developed through a WPEC collaboration; (5) A large suite of photonuclear reactions; (6) Extension of many neutron- and proton-induced evaluations up to 150 MeV; (7) Many new light nucleus neutron and proton reactions; (8) Post-fission beta-delayed photon decay spectra; (9) New radioactive decay data; (10) New methods for uncertainties and covariances, together with covariance evaluations for some sample cases; and (11) New actinide fission energy deposition. The paper provides an overview of this library, consisting of 14 sublibraries in the same ENDF-6 format as the earlier ENDF/B-VI library. We describe each of the 14 sublibraries, focusing on neutron reactions. Extensive validation, using radiation transport codes to simulate measured critical assemblies, show major improvements: (a) The long-standing underprediction of low enriched uranium thermal assemblies is removed; (b) The 238 U and 208 Pb reflector biases in fast systems are largely removed; (c) ENDF/B-VI.8 good agreement for simulations of thermal high-enriched uranium assemblies is preserved; (d) The underprediction of fast criticality of 233,235 U and 239 Pu assemblies is removed; and (e) The intermediate spectrum critical assemblies are predicted more accurately. We anticipate that the new library will play an important role in nuclear technology applications, including transport simulations supporting national security, nonproliferation, advanced reactor and fuel cycle concepts, criticality safety, fusion, medicine, space applications, nuclear astrophysics, and nuclear physics facility design. The ENDF/B-VII.0 library is archived at the National Nuclear Data Center, BNL, and can be retrieved from www.nndc.bnl.gov .",2006,357,1749,49,1,45,92,94,122,160,160,146,136,121
6e55367892ecefd53a91da8d4bc6ecbfeb58212c,"To characterize proteins that bind to the immunoglobulin (Ig) heavy chain and the kappa light chain enhancers, an electrophoretic mobility shift assay with end-labeled DNA fragments was used. Three binding proteins have been found. One is NF-A, a factor found in all tested cell types that binds to the octamer sequence found upstream of all Ig variable region gene segments and to the same octamer in the heavy chain enhancer. The second, also ubiquitous, protein binds to a sequence in both the heavy chain and the kappa enhancers that was previously shown to be protected from methylation in vivo. Other closely related sites do not compete for this binding, implying a restriction enzyme-like binding specificity. The third protein binds to a sequence in the kappa enhancer (and to an identical sequence in the SV40 enhancer) and is restricted in its occurrence to B cells.",1986,56,2351,29,2,59,75,90,112,74,78,54,40,69
f5ea73ac4d01358f31898a17990f275da1e6ec63,"Bile acids regulate the transcription of genes that control cholesterol homeostasis through molecular mechanisms that are poorly understood. Physiological concentrations of free and conjugated chenodeoxycholic acid, lithocholic acid, and deoxycholic acid activated the farnesoid X receptor (FXR; NR1H4), an orphan nuclear receptor. As ligands, these bile acids and their conjugates modulated interaction of FXR with a peptide derived from steroid receptor coactivator 1. These results provide evidence for a nuclear bile acid signaling pathway that may regulate cholesterol homeostasis.",1999,24,1879,87,17,59,59,73,72,71,78,77,57,81
6c04f07678f8d8d73b3a4ea99e2f502be3745e8c,Principles of imaging Introductory nuclear magnetic resonance The influence of magnetic field gradients High resolution k-space imaging k-space microscopy in biology and minerals science The measurement of motion using spin echoes Structural imaging using q-space Spatially heterogeneous motion and dynamic NMR microscopy Elements of the NMR microscope.,1991,0,1956,70,1,5,18,37,49,47,87,61,73,81
fd87cdf251331483521363c01cb0e97701b7775a,"Reprogramming of somatic cells to a pluripotent embryonic stem cell-like state has been achieved by nuclear transplantation of a somatic nucleus into an enucleated egg and most recently by introducing defined transcription factors into somatic cells. Nuclear reprogramming is of great medical interest, as it has the potential to generate a source of patient-specific cells. Here, we review strategies to reprogram somatic cells to a pluripotent embryonic state and discuss our understanding of the molecular mechanisms of reprogramming based on recent insights into the regulatory circuitry of the pluripotent state.",2008,154,1369,66,74,173,147,145,141,136,112,85,95,68
65c2b00e9f64c1dbec79e1d826892db82bd76776,"A cDNA encoding a protein that binds retinoic acid with high affinity has been cloned. The protein is homologous to the receptors for steroid hormones, thyroid hormones and vitamin D3, and appears to be a retinoic acid-inducible {Tans-acting enhancer factor, suggesting that the molecular mechanisms of the effect ofretinoids (vitamin A) on embryonic development, differentiation and tumour cell growth are similar to those described for other members of this nuclear receptor family.",1987,0,2017,23,0,76,139,165,175,174,140,106,109,81
98241074cd3f61d0fb5a7c05e7ccbaa612d6d502,"Bile acids repress the transcription of cytochrome P450 7A1 (CYP7A1), which catalyzes the rate-limiting step in bile acid biosynthesis. Although bile acids activate the farnesoid X receptor (FXR), the mechanism underlying bile acid-mediated repression of CYP7A1 remained unclear. We have used a potent, nonsteroidal FXR ligand to show that FXR induces expression of small heterodimer partner 1 (SHP-1), an atypical member of the nuclear receptor family that lacks a DNA-binding domain. SHP-1 represses expression of CYP7A1 by inhibiting the activity of liver receptor homolog 1 (LRH-1), an orphan nuclear receptor that is known to regulate CYP7A1 expression positively. This bile acid-activated regulatory cascade provides a molecular basis for the coordinate suppression of CYP7A1 and other genes involved in bile acid biosynthesis.",2000,72,1588,126,8,34,67,69,77,78,85,80,62,75
7ddc092390aab9d7a3664012423b713b7d4ce4bc,"Nuclear receptor coregulators are coactivators or corepressors that are required by nuclear receptors for efficient transcripitonal regulation. In this context, we define coactivators, broadly, as molecules that interact with nuclear receptors and enhance their transactivation. Analogously, we refer to nuclear receptor corepressors as factors that interact with nuclear receptors and lower the transcription rate at their target genes. Most coregulators are, by definition, rate limiting for nuclear receptor activation and repression, but do not significantly alter basal transcription. Recent data have indicated multiple modes of action of coregulators, including direct interactions with basal transcription factors and covalent modification of histones and other proteins. Reflecting this functional diversity, many coregulators exist in distinct steady state precomplexes, which are thought to associate in promoter-specific configurations. In addition, these factors may function as molecular gates to enable integration of diverse signal transduction pathways at nuclear receptor-regulated promoters. This review will summarize selected aspects of our current knowledge of the cellular and molecular biology of nuclear receptor coregulators.",1999,233,1787,74,10,106,131,175,153,156,101,115,85,87
0b508aec761ffd3e2ed492fef6c74618aa2276f8,"Medical uses of radiation have grown very rapidly over the past decade, and, as of 2007, medical uses represent the largest source of exposure to the U.S. population. Most physicians have difficulty assessing the magnitude of exposure or potential risk. Effective dose provides an approximate indicator of potential detriment from ionizing radiation and should be used as one parameter in evaluating the appropriateness of examinations involving ionizing radiation. The purpose of this review is to provide a compilation of effective doses for radiologic and nuclear medicine procedures. Standard radiographic examinations have average effective doses that vary by over a factor of 1000 (0.01-10 mSv). Computed tomographic examinations tend to be in a more narrow range but have relatively high average effective doses (approximately 2-20 mSv), and average effective doses for interventional procedures usually range from 5-70 mSv. Average effective dose for most nuclear medicine procedures varies between 0.3 and 20 mSv. These doses can be compared with the average annual effective dose from background radiation of about 3 mSv.",2008,188,1320,29,6,38,73,85,114,114,117,131,117,104
571a43427f773ecb5a4d218f92967ec23375ef7e,"Nuclear targeting sequences are essential for the transport of proteins into the nucleus. The seven-amino-acid nuclear targeting sequence of the SV40 large T antigen has been regarded as the model; however, many nuclear targeting sequences appear to be more complex. We suggest in this review that, despite this diversity, a consensus bipartite motif can be identified.",1991,32,1846,81,0,30,69,75,111,121,158,154,142,126
b661561760003aded401f585a66cec0c906915ab,"NUCLEAR transfer has been used in mammals as both a valuable tool in embryological studies1 and as a method for the multiplication of 'elite' embryos2–4. Offspring have only been reported when early embryos, or embryo-derived cells during primary culture, were used as nuclear donors5,6. Here we provide the first report, to our knowledge, of live mammalian offspring following nuclear transfer from an established cell line. Lambs were born after cells derived from sheep embryos, which had been cultured for 6 to 13 passages, were induced to quiesce by serum starvation before transfer of their nuclei into enucleated oocytes. Induction of quiescence in the donor cells may modify the donor chromatin structure to help nuclear reprogramming and allow development. This approach will provide the same powerful opportunities for analysis and modification of gene function in livestock species that are available in the mouse through the use of embryonic stem cells7.",1996,33,1794,67,19,45,66,82,77,84,99,97,95,47
bb92e666b524f0a50e13c35e165cce8c28ceb9e5,"A new technique of free-space simulation has been developed for solving unbounded electromagnetic problems with the finite-difference time-domain method. Referred to as PML, the new technique is based on the use of an absorbing layer especially designed to absorb without reflection the electromagnetic waves. The first part of the paper presents the theory of the PML technique. The second part is devoted to numerical experiments and to numerical comparisons with the previously used techniques of free-space simulation. These comparisons show that the PML technique works better than the others in all cases; using it allows us to obtain a higher accuracy in some problems and a release of computational requirements in some others.",1994,10,9341,609,5,45,127,170,187,195,216,193,217,277
f6c711b66fa0a060f8ed604af8a2a47c10daef48,Introduction.- The Helmholtz Equation.- Direct Acoustic Obstacle Scattering.- III-Posed Problems.- Inverse Acoustic Obstacle Scattering.- The Maxwell Equations.- Inverse Electromagnetic Obstacle Scattering.- Acoustic Waves in an Inhomogeneous Medium.- Electromagnetic Waves in an Inhomogeneous Medium.- The Inverse Medium Problem.-References.- Index,1992,29,4513,450,0,8,10,19,24,46,70,69,80,74
22132078b8dc885bb3cef2434b5476b6dc566cf7,"The dependence of the dielectric constant, at frequencies between 1 MHz and 1 GHz, on the volumetric water content is determined empirically in the laboratory. The effect of varying the texture, bulk density, temperature, and soluble salt content on this relationship was also determined. Time-domain reflectometry (TDR) was used to measure the dielectric constant of a wide range of granular specimens placed in a coaxial transmission line. The water or salt solution was cycled continuously to or from the specimen, with minimal disturbance, through porous disks placed along the sides of the coaxial tube. 
 
Four mineral soils with a range of texture from sandy loam to clay were tested. An empirical relationship between the apparent dielectric constant Ka and the volumetric water content θv, which is independent of soil type, soil density, soil temperature, and soluble salt content, can be used to determine θv, from air dry to water saturated, with an error of estimate of 0.013. Precision of θv to within ±0.01 from Ka can be obtained with a calibration for the particular granular material of interest. An organic soil, vermiculite, and two sizes of glass beads were also tested successfully. The empirical relationship determined here agrees very well with other experimenters' results, which use a wide range of electrical techniques over the frequency range of 20 MHz and 1 GHz and widely varying soil types. The results of applying the TDR technique on parallel transmission lines in the field to measure θv versus depth are encouraging.",1980,19,4524,402,1,3,2,3,6,6,6,4,10,8
395b44480e3cc49f5b536d37ab11a819698c83e3,"The electric field integral equation (EFIE) is used with the moment method to develop a simple and efficient numerical procedure for treating problems of scattering by arbitrarily shaped objects. For numerical purposes, the objects are modeled using planar triangular surfaces patches. Because the EFIE formulation is used, the procedure is applicable to both open and closed surfaces. Crucial to the numerical formulation is the development of a set of special subdomain-type basis functions which are defined on pairs of adjacent triangular patches and yield a current representation free of line or point charges at subdomain boundaries. The method is applied to the scattering problems of a plane wave illuminated flat square plate, bent square plate, circular disk, and sphere. Excellent correspondence between the surface current computed via the present method and that obtained via earlier approaches or exact formulations is demonstrated in each case.",1980,21,4867,392,0,1,1,3,8,9,7,10,15,9
da3872f2955bf01550bca2f5c199a99765be6087,"Using the freedom of design that metamaterials provide, we show how electromagnetic fields can be redirected at will and propose a design strategy. The conserved fields—electric displacement field D, magnetic induction field B, and Poynting vector B—are all displaced in a consistent manner. A simple illustration is given of the cloaking of a proscribed volume of space to exclude completely all electromagnetic fields. Our work has relevance to exotic lens design and to the cloaking of objects from electromagnetic fields.",2006,61,6777,146,34,145,307,363,459,509,544,560,573,572
68ad7ddfeb078558aedf5f69600ba799baec3b2b,"A new type of metallic electromagnetic structure has been developed that is characterized by having high surface impedance. Although it is made of continuous metal, and conducts dc currents, it does not conduct ac currents within a forbidden frequency band. Unlike normal conductors, this new surface does not support propagating surface waves, and its image currents are not phase reversed. The geometry is analogous to a corrugated metal surface in which the corrugations have been folded up into lumped-circuit elements, and distributed in a two-dimensional lattice. The surface can be described using solid-state band theory concepts, even though the periodicity is much less than the free-space wavelength. This unique material is applicable to a variety of electromagnetic problems, including new kinds of low-profile antennas.",1999,67,3836,194,3,18,27,51,91,92,153,130,159,212
370a6f60b2ae303d0a6d7cdf5e3ab9ccd6769120,Foreword to the Revised Edition. Preface. Fundamental Concepts. Introduction to Waves. Some Theorems and Concepts. Plane Wave Functions. Cylindrical Wave Functions. Spherical Wave Functions. Perturbational and Variational Techniques. Microwave Networks. Appendix A: Vector Analysis. Appendix B: Complex Permittivities. Appendix C: Fourier Series and Integrals. Appendix D: Bessel Functions. Appendix E: Legendre Functions. Bibliography. Index.,1961,0,4960,307,0,2,2,6,10,7,13,19,16,7
0ea9c798317f23df8bda748e7bdb2608aa4967bd,"In this paper, we discuss some interesting properties of the electromagnetic potentials in the quantum domain. We shall show that, contrary to the conclusions of classical mechanics, there exist effects of potentials on charged particles, even in the region where all the fields (and therefore the forces on the particles) vanish. We shall then discuss possible experiments to test these conclusions; and, finally, we shall suggest further possible developments in the interpretation of the potentials.",1959,0,4611,127,0,1,4,3,1,2,2,3,11,6
9880699907547b7f4bcbad05a3c466c425d5395f,"A recently published theory has suggested that a cloak of invisibility is in principle possible, at least over a narrow frequency band. We describe here the first practical realization of such a cloak; in our demonstration, a copper cylinder was “hidden” inside a cloak constructed according to the previous theoretical prescription. The cloak was constructed with the use of artificially structured metamaterials, designed for operation over a band of microwave frequencies. The cloak decreased scattering from the hidden object while at the same time reducing its shadow, so that the cloak and object combined began to resemble empty space.",2006,56,5927,106,11,122,303,308,367,389,456,457,466,467
d741a75c8b6a9046477d2c9b935de66040a0a463,"THE SCATTERING OF ELECTROMAGNETIC WAVES FROM ROUGH SURFACES PDF Are you looking for the scattering of electromagnetic waves from rough surfaces Books? Now, you will be happy that at this time the scattering of electromagnetic waves from rough surfaces PDF is available at our online library. With our complete resources, you could find the scattering of electromagnetic waves from rough surfaces PDF or just found any kind of Books for your readings everyday.",1963,0,3478,162,1,3,5,12,7,10,14,18,17,22
6908a6e13a1aff03007d8478129403dbcbd6f348,"Scalp electric potentials (electroencephalograms) and extracranial magnetic fields (magnetoencephalograms) are due to the primary (impressed) current density distribution that arises from neuronal postsynaptic processes. A solution to the inverse problem--the computation of images of electric neuronal activity based on extracranial measurements--would provide important information on the time-course and localization of brain function. In general, there is no unique solution to this problem. In particular, an instantaneous, distributed, discrete, linear solution capable of exact localization of point sources is of great interest, since the principles of linearity and superposition would guarantee its trustworthiness as a functional imaging method, given that brain activity occurs in the form of a finite number of distributed hot spots. Despite all previous efforts, linear solutions, at best, produced images with systematic nonzero localization errors. A solution reported here yields images of standardized current density with zero localization error. The purpose of this paper is to present the technical details of the method, allowing researchers to test, check, reproduce and validate the new method.",2002,28,2763,205,1,3,9,24,35,51,70,113,127,157
52290de9c21c139bf8edc000e3c9e0eb6814d7d0,"Preface. Acknowledgments. Acronyms. 1 Introduction. 1.1 Definition of Metamaterials (MTMs) and Left-Handed (LH) MTMs. 1.2 Theoretical Speculation by Viktor Veselago. 1.3 Experimental Demonstration of Left-Handedness. 1.4 Further Numerical and Experimental Confirmations. 1.5 ""Conventional"" Backward Waves and Novelty of LH MTMs. 1.6 Terminology. 1.7 Transmission Line (TL) Approach. 1.8 Composite Right/Left-Handed (CRLH) MTMs. 1.9 MTMs and Photonic Band-Gap (PBG) Structures. 1.10 Historical ""Germs"" of MTMs. References. 2 Fundamentals of LH MTMs. 2.1 Left-Handedness from Maxwell's Equations. 2.2 Entropy Conditions in Dispersive Media. 2.3 Boundary Conditions. 2.4 Reversal of Doppler Effect. 2.5 Reversal of Vavilov- Cerenkov Radiation. 2.6 Reversal of Snell's Law: Negative Refraction. 2.7 Focusing by a ""Flat LH Lens"". 2.8 Fresnel Coefficients. 2.9 Reversal of Goos-H anchen Effect. 2.10 Reversal of Convergence and Divergence in Convex and Concave Lenses. 2.11 Subwavelength Diffraction. References. 3 TLTheoryofMTMs. 3.1 Ideal Homogeneous CRLH TLs. 3.1.1 Fundamental TL Characteristics. 3.1.2 Equivalent MTM Constitutive Parameters. 3.1.3 Balanced and Unbalanced Resonances. 3.1.4 Lossy Case. 3.2 LC Network Implementation. 3.2.1 Principle. 3.2.2 Difference with Conventional Filters. 3.2.3 Transmission Matrix Analysis. 3.2.4 Input Impedance. 3.2.5 Cutoff Frequencies. 3.2.6 Analytical Dispersion Relation. 3.2.7 Bloch Impedance. 3.2.8 Effect of Finite Size in the Presence of Imperfect Matching. 3.3 Real Distributed 1D CRLH Structures. 3.3.1 General Design Guidelines. 3.3.2 Microstrip Implementation. 3.3.3 Parameters Extraction. 3.4 Experimental Transmission Characteristics. 3.5 Conversion from Transmission Line to Constitutive Parameters. References. 4 Two-Dimensional MTMs. 4.1 Eigenvalue Problem. 4.1.1 General Matrix System. 4.1.2 CRLH Particularization. 4.1.3 Lattice Choice, Symmetry Points, Brillouin Zone, and 2D Dispersion Representations. 4.2 Driven Problem by the Transmission Matrix Method (TMM). 4.2.1 Principle of the TMM. 4.2.2 Scattering Parameters. 4.2.3 Voltage and Current Distributions. 4.2.4 Interest and Limitations of the TMM. 4.3 Transmission Line Matrix (TLM) Modeling Method. 4.3.1 TLM Modeling of the Unloaded TL Host Network. 4.3.2 TLM Modeling of the Loaded TL Host Network (CRLH). 4.3.3 Relationship between Material Properties and the TLM Model Parameters. 4.3.4 Suitability of the TLM Approach for MTMs. 4.4 Negative Refractive Index (NRI) Effects. 4.4.1 Negative Phase Velocity. 4.4.2 Negative Refraction. 4.4.3 Negative Focusing. 4.4.4 RH-LH Interface Surface Plasmons. 4.4.5 Reflectors with Unusual Properties. 4.5 Distributed 2D Structures. 4.5.1 Description of Possible Structures. 4.5.2 Dispersion and Propagation Characteristics. 4.5.3 Parameter Extraction. 4.5.4 Distributed Implementation of the NRI Slab. References. 5 Guided-Wave Applications. 5.1 Dual-Band Components. 5.1.1 Dual-Band Property of CRLH TLs. 5.1.2 Quarter-Wavelength TL and Stubs. 5.1.3 Passive Component Examples: Quadrature Hybrid and Wilkinson Power Divider. 5.1.3.1 Quadrature Hybrid. 5.1.3.2 Wilkinson Power Divider. 5.1.4 Nonlinear Component Example: Quadrature Subharmonically Pumped Mixer. 5.2 Enhanced-Bandwidth Components. 5.2.1 Principle of Bandwidth Enhancement. 5.2.2 Rat-Race Coupler Example. 5.3 Super-compact Multilayer ""Vertical"" TL. 5.3.1 ""Vertical"" TL Architecture. 5.3.2 TL Performances. 5.3.3 Diplexer Example. 5.4 Tight Edge-Coupled Coupled-Line Couplers (CLCs). 5.4.1 Generalities on Coupled-Line Couplers. 5.4.1.1 TEM and Quasi-TEM Symmetric Coupled-Line Structures with Small Interspacing: Impedance Coupling (IC). 5.4.1.2 Non-TEM Symmetric Coupled-Line Structures with Relatively Large Spacing: Phase Coupling (PC). 5.4.1.3 Summary on Symmetric Coupled-Line Structures. 5.4.1.4 Asymmetric Coupled-Line Structures. 5.4.1.5 Advantages of MTM Couplers. 5.4.2 Symmetric Impedance Coupler. 5.4.3 Asymmetric Phase Coupler. 5.5 Negative and Zeroth-Order Resonator. 5.5.1 Principle. 5.5.2 LC Network Implementation. 5.5.3 Zeroth-Order Resonator Characteristics. 5.5.4 Circuit Theory Verification. 5.5.5 Microstrip Realization. References. 6 Radiated-Wave Applications. 6.1 Fundamental Aspects of Leaky-Wave Structures. 6.1.1 Principle of Leakage Radiation. 6.1.2 Uniform and Periodic Leaky-Wave Structures. 6.1.2.1 Uniform LW Structures. 6.1.2.2 Periodic LW Structures. 6.1.3 Metamaterial Leaky-Wave Structures. 6.2 Backfire-to-Endfire (BE) Leaky-Wave (LW) Antenna. 6.3 Electronically Scanned BE LW Antenna. 6.3.1 Electronic Scanning Principle. 6.3.2 Electronic Beamwidth Control Principle. 6.3.3 Analysis of the Structure and Results. 6.4 Reflecto-Directive Systems. 6.4.1 Passive Retro-Directive Reflector. 6.4.2 Arbitrary-Angle Frequency Tuned Reflector. 6.4.3 Arbitrary-Angle Electronically Tuned Reflector. 6.5 Two-Dimensional Structures. 6.5.1 Two-Dimensional LW Radiation. 6.5.2 Conical-Beam Antenna. 6.5.3 Full-Space Scanning Antenna. 6.6 Zeroth Order Resonating Antenna. 6.7 Dual-Band CRLH-TL Resonating Ring Antenna. 6.8 Focusing Radiative ""Meta-Interfaces"". 6.8.1 Heterodyne Phased Array. 6.8.2 Nonuniform Leaky-Wave Radiator. References. 7 The Future of MTMs. 7.1 ""Real-Artificial"" Materials: the Challenge of Homogenization. 7.2 Quasi-Optical NRI Lenses and Devices. 7.3 Three-Dimensional Isotropic LH MTMs. 7.4 Optical MTMs. 7.5 ""Magnetless"" Magnetic MTMs. 7.6 Terahertz Magnetic MTMs. 7.7 Surface Plasmonic MTMs. 7.8 Antenna Radomes and Frequency Selective Surfaces. 7.9 Nonlinear MTMs. 7.10 Active MTMs. 7.11 Other Topics of Interest. References. Index.",2005,0,2590,214,8,57,101,134,223,215,224,243,211,182
d11f1ac25a813dda546cd2cb7a6cd157afa194dd,"This paper presents a new method for localizing the electric activity in the brain based on multichannel surface EEG recordings. In contrast to the models presented up to now the new method does not assume a limited number of dipolar point sources nor a distribution on a given known surface, but directly computes a current distribution throughout the full brain volume. In order to find a unique solution for the 3-dimensional distribution among the infinite set of different possible solutions, the method assumes that neighboring neurons are simultaneously and synchronously activated. The basic assumption rests on evidence from single cell recordings in the brain that demonstrates strong synchronization of adjacent neurons. In view of this physiological consideration the computational task is to select the smoothest of all possible 3-dimensional current distributions, a task that is a common procedure in generalized signal processing. The result is a true 3-dimensional tomography with the characteristic that localization is preserved with a certain amount of dispersion, i.e., it has a relatively low spatial resolution. The new method, which we call Low Resolution Electromagnetic Tomography (LORETA) is illustrated with two different sets of evoked potential data, the first showing the tomography of the P100 component to checkerboard stimulation of the left, right, upper and lower hemiretina, and the second showing the results for the auditory N100 component and the two cognitive components CNV and P300. A direct comparison of the tomography results with those obtained from fitting one and two dipoles illustrates that the new method provides physiologically meaningful results while dipolar solutions fail in many situations. In the case of the cognitive components, the method offers new hypotheses on the location of higher cognitive functions in the brain.",1994,50,2599,220,0,1,12,16,35,31,28,70,64,68
f576d363baecb17d5b1d1a4ab03a68da016f1fc2,Historical introduction 1. Basic properties of the electromagnetic field 2. Electromagnetic potentials and polarization 3. Foundations of geometrical optics 4. Geometrical theory of optical imaging 5. Geometrical theory of aberrations 6. Image-forming instruments 7. Elements of the theory of interference and interferometers 8. Elements of the theory of diffraction 9. The diffraction theory of aberrations 10. Interference and diffraction with partially coherent light 11. Rigorous diffraction theory 12. Diffraction of light by ultrasonic waves 13. Scattering from inhomogeneous media 14. Optics of metals 15. Optics of crystals 16. Appendices Author index Subject index.,1999,0,3100,57,14,25,34,35,51,57,69,106,106,139
af526b9350f53fa150c2582a2cb5b2cc263330d8,,1977,0,2913,157,0,7,0,3,5,5,4,0,8,10
b2167e98e19ba19a61e88acc773ee1f3e8b39a91,,1969,0,3008,92,0,8,18,35,23,27,33,31,39,39
dcc529acbebd28d2951dbda122f8274e0e52bb0b,"We discuss the validity of standard retrieval methods that assign bulk electromagnetic properties, such as the electric permittivity epsilon and the magnetic permeability mu, from calculations of the scattering (S) parameters for finite-thickness samples. S-parameter retrieval methods have recently become the principal means of characterizing artificially structured metamaterials, which, by nature, are inherently inhomogeneous. While the unit cell of a metamaterial can be made considerably smaller than the free space wavelength, there remains a significant variation of the phase across the unit cell at operational frequencies in nearly all metamaterial structures reported to date. In this respect, metamaterials do not rigorously satisfy an effective medium limit and are closer conceptually to photonic crystals. Nevertheless, we show here that a modification of the standard S-parameter retrieval procedure yields physically reasonable values for the retrieved electromagnetic parameters, even when there is significant inhomogeneity within the unit cell of the structure. We thus distinguish a metamaterial regime, as opposed to the effective medium or photonic crystal regimes, in which a refractive index can be rigorously established but where the wave impedance can only be approximately defined. We present numerical simulations on typical metamaterial structures to illustrate the modified retrieval algorithm and the impact on the retrieved material parameters. We find that no changes to the standard retrieval procedures are necessary when the inhomogeneous unit cell is symmetric along the propagation axis; however, when the unit cell does not possess this symmetry, a modified procedure--in which a periodic structure is assumed--is required to obtain meaningful electromagnetic material parameters.",2005,2,2111,50,1,23,42,48,87,86,127,152,148,169
008d140ab2aad3a95380207f47f2c8a08497f7a2,"When time-domain electromagnetic-field equations are solved using finite-difference techniques in unbounded space, there must be a method limiting the domain in which the field is computed. This is achieved by truncating the mesh and using absorbing boundary conditions at its artificial boundaries to simulate the unbounded surroundings. This paper presents highly absorbing boundary conditions for electromagnetic-field equations that can be used for both two-and three-dimensional configurations. Numerical results are given that clearly exhibit the accuracy and limits of applicability of highly absorbing boundary conditions. A simplified, but equally accurate, absorbing condition is derived for two- dimensional time-domain electromagnetic-field problems.",1981,20,2441,94,0,2,1,1,6,3,10,12,19,37
51d8bcee2a5b1648bf2087f99a1d6e8b7d43097c,"Achieving control of light-material interactions for photonic device applications at nanoscale dimensions will require structures that guide electromagnetic energy with a lateral mode confinement below the diffraction limit of light. This cannot be achieved by using conventional waveguides1 or photonic crystals2. It has been suggested that electromagnetic energy can be guided below the diffraction limit along chains of closely spaced metal nanoparticles3,4 that convert the optical mode into non-radiating surface plasmons5. A variety of methods such as electron beam lithography6 and self-assembly7 have been used to construct metal nanoparticle plasmon waveguides. However, all investigations of the optical properties of these waveguides have so far been confined to collective excitations8,9,10, and direct experimental evidence for energy transport along plasmon waveguides has proved elusive. Here we present observations of electromagnetic energy transport from a localized subwavelength source to a localized detector over distances of about 0.5 μm in plasmon waveguides consisting of closely spaced silver rods. The waveguides are excited by the tip of a near-field scanning optical microscope, and energy transport is probed by using fluorescent nanospheres.",2003,17,2027,11,18,46,65,94,142,111,145,132,139,170
67f6126040381d0b5c89376892a65a7df4fff918,"One of the most striking phenomena in condensed-matter physics is the quantum Hall effect, which arises in two-dimensional electron systems subject to a large magnetic field applied perpendicular to the plane in which the electrons reside. In such circumstances, current is carried by electrons along the edges of the system, in so-called chiral edge states (CESs). These are states that, as a consequence of nontrivial topological properties of the bulk electronic band structure, have a unique directionality and are robust against scattering from disorder. Recently, it was theoretically predicted that electromagnetic analogues of such electronic edge states could be observed in photonic crystals, which are materials having refractive-index variations with a periodicity comparable to the wavelength of the light passing through them. Here we report the experimental realization and observation of such electromagnetic CESs in a magneto-optical photonic crystal fabricated in the microwave regime. We demonstrate that, like their electronic counterparts, electromagnetic CESs can travel in only one direction and are very robust against scattering from disorder; we find that even large metallic scatterers placed in the path of the propagating edge modes do not induce reflections. These modes may enable the production of new classes of electromagnetic device and experiments that would be impossible using conventional reciprocal photonic states alone. Furthermore, our experimental demonstration and study of photonic CESs provides strong support for the generalization and application of topological band theories to classical and bosonic systems, and may lead to the realization and observation of topological phenomena in a generally much more controlled and customizable fashion than is typically possible with electronic systems.",2009,37,1553,14,1,15,43,58,55,67,102,117,143,235
462770e4b0b8f794a40185c7e99273bec9b7ab22,"A first year graduate text on electromagnetic field theory emphasizing mathematical approaches, problem solving and physical interpretation. Examples deal with guidance propagation, radiation, and scattering of electromagnetic waves; metallic and dielectric wave guides, resonators, antennas and radiating structures, Cerenkov radiation, moving media, plasmas, crystals, integrated optics, lasers and fibers, remote sensing, geophysical probing, dipole antennas and stratified media.",1986,9,1943,84,2,2,5,14,14,23,25,31,33,26
4424ae3af143d7d7b3dec6669e95efff825a7e95,"We review the basic physics of surface-plasmon excitations occurring at metal/dielectric interfaces with special emphasis on the possibility of using such excitations for the localization of electromagnetic energy in one, two, and three dimensions, in a context of applications in sensing and waveguiding for functional photonic devices. Localized plasmon resonances occurring in metallic nanoparticles are discussed both for single particles and particle ensembles, focusing on the generation of confined light fields enabling enhancement of Raman-scattering and nonlinear processes. We then survey the basic properties of interface plasmons propagating along flat boundaries of thin metallic films, with applications for waveguiding along patterned films, stripes, and nanowires. Interactions between plasmonic structures and optically active media are also discussed.",2005,185,1669,13,3,48,68,78,96,110,134,147,135,122
1423c21de05c4345a5907ad55f9f6a0e0bdc148f,"The fast multipole method (FMM) and multilevel fast multipole algorithm (MLFMA) are reviewed. The number of modes required, block-diagonal preconditioner, near singularity extraction, and the choice of initial guesses are discussed to apply the MLFMA to calculating electromagnetic scattering by large complex objects. Using these techniques, we can solve the problem of electromagnetic scattering by large complex three-dimensional (3-D) objects such as an aircraft (VFY218) on a small computer.",1997,40,1469,172,0,20,24,30,30,27,55,46,45,53
e3083dcadcc1a9c4294d6f83b40d4fb27b8c4666,"There has been tremendous advances in our ability to produce images of human brain function. Applications of functional brain imaging extend from improving our understanding of the basic mechanisms of cognitive processes to better characterization of pathologies that impair normal function. Magnetoencephalography (MEG) and electroencephalography (EEG) (MEG/EEG) localize neural electrical activity using noninvasive measurements of external electromagnetic signals. Among the available functional imaging techniques, MEG and EEG uniquely have temporal resolutions below 100 ms. This temporal precision allows us to explore the timing of basic neural processes at the level of cell assemblies. MEG/EEG source localization draws on a wide range of signal processing techniques including digital filtering, three-dimensional image analysis, array signal processing, image modeling and reconstruction, and, blind source separation and phase synchrony estimation. We describe the underlying models currently used in MEG/EEG source estimation and describe the various signal processing steps required to compute these sources. In particular we describe methods for computing the forward fields for known source distributions and parametric and imaging-based approaches to the inverse problem.",2001,75,1531,119,0,11,21,50,51,56,64,61,83,75
c26c55df0cfd4f81f70942cd8cc7965cee2d32ff,"We use the discrete dipole approximation to investigate the electromagnetic fields induced by optical excitation of localized surface plasmon resonances of silver nanoparticles, including monomers and dimers, with emphasis on what size, shape, and arrangement leads to the largest local electric field (E-field) enhancement near the particle surfaces. The results are used to determine what conditions are most favorable for producing enhancements large enough to observe single molecule surface enhanced Raman spectroscopy. Most of the calculations refer to triangular prisms, which exhibit distinct dipole and quadrupole resonances that can easily be controlled by varying particle size. In addition, for the dimer calculations we study the influence of dimer separation and orientation, especially for dimers that are separated by a few nanometers. We find that the largest /E/2 values for dimers are about a factor of 10 larger than those for all the monomers examined. For all particles and particle orientations, the plasmon resonances which lead to the largest E-fields are those with the longest wavelength dipolar excitation. The spacing of the particles in the dimer plays a crucial role, and we find that the spacing needed to achieve a given /E/2 is proportional to nanoparticle size for particles below 100 nm in size. Particle shape and curvature are of lesser importance, with a head to tail configuration of two triangles giving enhanced fields comparable to head to head, or rounded head to tail. The largest /E/2 values we have calculated for spacings of 2 nm or more is approximately 10(5).",2004,52,1562,23,9,19,37,48,77,74,93,121,145,145
af418f2a017b505fb6cd3bac87d491b704790c30,"An investigation is made of the structure of the electromagnetic field near the focus of an aplanatic system which images a point source. First the case of a linearly polarized incident field is examined and expressions are derived for the electric and magnetic vectors in the image space. Some general consequences of the formulae are then discussed. In particular the symmetry properties of the field with respect to the focal plane are noted and the state of polarization of the image region is investigated. The distribution of the time-averaged electric and magnetic energy densities and of the energy flow (Poynting vector) in the focal plane is studied in detail, and the results are illustrated by diagrams and in a tabulated form based on data obtained by extensive calculations on an electronic computor. The case of an unpolarized field is also investigated. The solution is riot restricted to systems of low aperture, and the computational results cover, in fact, selected values of the angular semi-aperture a on the image side, in the whole range 0 ≤ α ≤ 90°. The limiting case α → 0 is examined in detail and it is shown that the field is then completely characterized by a single, generally complex, scalar function, which turns out to be identical with that of the classical scalar theory of Airy, Lommel and Struve. The results have an immediate bearing on the resolving power of image forming systems; they also help our understanding of the significance of the scalar diffraction theory, which is customarily employed, without a proper justification, in the analysis of images in lowaperture systems.",1959,7,2454,81,0,0,1,0,0,1,1,1,1,0
45d875db775c224c225311e496fab93d1fb6a51d,"In this paper basic mathematical and physical concepts of the biomagnetic inverse problem are reviewed with some new approaches. The forward problem is discussed for both homogeneous and inhomogeneous media. Geselowitz' formulae and a surface integral equation are presented to handle a piecewise homogeneous conductor. The special cases of a spherically symmetric conductor and a horizontally layered medium are discussed in detail. The non-uniqueness of the solution of the magnetic inverse problem is discussed and the difficulty caused by the contribution of the electric potential to the magnetic field outside the conductor is studied. As practical methods of solving the inverse problem, a weighted least-squares search with confidence limits and the method of minimum norm estimate are discussed.",1987,13,1773,104,4,2,18,8,13,18,20,25,26,35
63af0e90729f983539a0eaba62eaf04b61926a70,"Several methods have been previously used to approximate free boundaries in finite-difference numerical simulations. A simple, but powerful, method is described that is based on the concept of a fractional volume of fluid (VOF). This method is shown to be more flexible and efficient than other methods for treating complicated free boundary configurations. To illustrate the method, a description is given for an incompressible hydrodynamics code, SOLA-VOF, that uses the VOF technique to track free fluid surfaces.",1981,35,11261,505,0,0,0,0,0,0,0,0,0,0
b904e9b7cd539936a2058f623a06c8fea10651c8,"This brief paper derives Euler’s equations for an inviscid fluid, summarizes the Cauchy momentum equation, derives the Navier-Stokes equation from that, and then talks about finite difference method approaches to solutions. Typical texts for this material are apparently Acheson, Elementary Fluid Dynamics and Landau and Lifschitz, Fluid Mechanics. 1. Basic Definitions We describe a fluid flow in three-dimensional space R as a vector field representing the velocity at all locations in the fluid. Concretely, then, a fluid flow is a function ~v : R× R → R that assigns to each point (t, ~x) in spacetime a velocity ~v(t, ~x) in space. In the special situation where ~v does not depend on t we say that the flow is steady. A trajectory or particle path is a curve ~x : R→ R such that for all t ∈ R, d dt ~x(t) = ~v(t, ~x(t)). Fix a t0 ∈ R; a streamline at time t0 is a curve ~x : R→ R such that for all t ∈ R, d dt ~x(t) = ~v(t0, ~x(t)). In the special case of steady flow the streamlines are constant across times t0 and any trajectory is a streamline. In non-steady flows, particle paths need not be streamlines. Consider the 2-dimensional example ~v = [− sin t cos t]>. At t0 = 0 the velocities all point up and the streamlines are vertical straight lines. At t0 = π/2 the velocities all point left and the streamlines are horizontal straight lines. Any trajectory is of the form ~x = [cos t + C1 sin t + C2] >; this traces out a radius-1 circle centered at [C1 C2] >. Indeed, all radius-1 circles in the plane arise as trajectories. These circles cross each other at many (in fact, all) points. If you find it counterintuitive that distinct trajectories can pass through a single point, remember that they do so at different times. 2. Acceleration Let f : R × R → R be some scalar field (such as temperature). Then ∂f/∂t is the rate of change of f at some fixed point in space. If we precompose f with a 1 Fluid Dynamics Math 211, Fall 2014, Carleton College trajectory ~x, then the chain rule gives us the rate of change of f with respect to time along that curve: D Dt f := d dt f(t, x(t), y(t), z(t)) = ∂f ∂t + ∂f ∂x dx dt + ∂f ∂y dy dt + ∂f ∂z dz dt = ( ∂ ∂t + dx dt ∂ ∂x + dy dt ∂ ∂y + dz dt ∂ ∂z ) f = ( ∂ ∂t + ~v · ∇ ) f. Intuitively, if ~x describes the trajectory of a small sensor for the quantity f (such as a thermometer), then Df/Dt gives the rate of change of the output of the sensor with respect to time. The ∂f/∂t term arises because f varies with time. The ~v ·∇f term arises because f is being measured at varying points in space. If we apply this idea to each component function of ~v, then we obtain an acceleration (or force per unit mass) vector field ~a(t, x) := D~v Dt = ∂~v ∂t + (~v · ∇)~v. That is, for any spacetime point (t, ~x), the vector ~a(t, ~x) is the acceleration of the particle whose trajectory happens to pass through ~x at time t. Let’s check that it agrees with our usual notion of acceleration. Suppose that a curve ~x describes the trajectory of a particle. The acceleration should be d dt d dt~x. By the definition of trajectory, d dt d dt ~x = d dt ~v(t, ~x(t)). The right-hand side is precisely D~v/Dt. Returning to our 2-dimensional example ~v = [− sin t cos t]>, we have ~a = [− cos t − sin t]>. Notice that ~v · ~a = 0. This is the well-known fact that in constant-speed circular motion the centripetal acceleration is perpendicular to the velocity. (In fact, the acceleration of any constant-speed trajectory is perpendicular to its velocity.) 3. Ideal Fluids An ideal fluid is one of constant density ρ, such that for any surface within the fluid the only stresses on the surface are normal. That is, there exists a scalar field p : R × R → R, called the pressure, such that for any surface element ∆S with outward-pointing unit normal vector ~n, the force exerted by the fluid inside ∆S on the fluid outside ∆S is p~n ∆S. The constant density condition implies that the fluid is incompressible, meaning ∇ · ~v = 0, as follows. For any region of space R, the rate of flow of mass out of the region is ∫∫ ∂R ρ~v · ~n dS = ∫∫∫",1968,0,10112,994,0,0,0,0,0,0,2,21,29,25
2d4f6af07a0bb851ecd0f3e2243676518dc555b8,"Ludwig Krinner (Dated: November 5th 2012) Abstract This is a script made with the help of Landau Lifshitz, Book VI [1] on fluid mechanics, that gives a short introduction to basic fluid mechanics. This short script includes, various equations of continuity, Eulers equation for motion of nonviscous fluid, gravitational waves in nonviscous fluids, Navier-Stokes Equation for viscous fluids, viscous flow within a pipe, some turbulence and laminar wake (can be used to calculate the lift of a wing), all along with some basic thermodynamics and vector calculus. Plagiarism: For creation of this script we have closely oriented ourselves towards Laundau Lifschitz Course in theoretical physics Vol 6 Fluid Mechanics. This is no original work! Even though we have tried to write everyting in our own words (in order to have to understand everything), most of the formulas are similar or equal to the book, while some of the steps which seemed too fast are supplemented by own “fill in” calculations. I will not cite anything properly, since this is not an official paper or homework or term paper, and since I basically cite every line of calculation and from the content also every line of text.",1980,4,7139,953,6,4,4,11,6,8,7,6,4,11
08236a76e2355ee36c154f51b6b2159e054a3191,*Introduction. *Conservation Laws of Fluid Motion and Boundary Conditions. *Turbulence and its Modelling. *The Finite Volume Method for Diffusion Problems. *The Finite Volume Method for Convection-Diffusion Problems. *Solution Algorithms for Pressure-Velocity Coupling in Steady Flows. *Solution of Discretised Equations. *The Finite Volume Method for Unsteady Flows. *Implementation of Boundary Conditions. *Advanced topics and applications. Appendices. References. Index.,2007,0,7119,527,194,239,276,330,368,429,451,568,552,526
83f1b137f2528e8a5c8a26771dd91cf970a0789c,"This text develops and applies the techniques used to solve problems in fluid mechanics on computers and describes in detail those most often used in practice. It includes advanced techniques in computational fluid dynamics, such as direct and large-eddy simulation of turbulence, multigrid methods, parallel computing, moving grids, structured, block-structured and unstructured boundary-fitted grids, and free surface flows.",1996,0,6178,362,4,17,31,36,71,94,137,160,187,212
ba2712feae036b228a3b142629b8a6593187a87a,"Applications of second-moment turbulent closure hypotheses to geophysical fluid problems have developed rapidly since 1973, when genuine predictive skill in coping with the effects of stratification was demonstrated. The purpose here is to synthesize and organize material that has appeared in a number of articles and add new useful material so that a complete (and improved) description of a turbulence model from conception to application is condensed in a single article. It is hoped that this will be a useful reference to users of the model for application to either atmospheric or oceanic boundary layers.",1982,103,6206,973,1,5,6,12,14,14,14,27,32,19
8705e8fe0632bd11f200455a5125692a2547a018,"Note: Includes references and index Reference Record created on 2004-09-07, modified on 2016-08-08",1997,20,6000,610,1,11,29,43,88,102,97,114,169,180
8fafdfc080df616e449cbda5fd9dc86d70f4ce7c,Preliminaries * Fundamentals * Inviscid Shallow-Water Theory * Friction and Viscous Flow * Homogeneous Models of the Wind-Driven Oceanic Circulation * Quasigeostrophic Motion of a Stratified Fluid on a Sphere * Instability Theory * Ageostrophic Motion,1979,0,4580,311,5,13,16,23,32,35,37,54,60,56
c04a2060f080cc8dec616929bff1ea8f9dd19e60,A theory is developed for the propagation of stress waves in a porous elastic solid containing compressible viscous fluid. The emphasis of the present treatment is on materials where fluid and solid are of comparable densities as for instance in the case of water‐saturated rock. The paper denoted here as Part I is restricted to the lower frequency range where the assumption of Poiseuille flow is valid. The extension to the higher frequencies will be treated in Part II. It is found that the material may be described by four nondimensional parameters and a characteristic frequency. There are two dilatational waves and one rotational wave. The physical interpretation of the result is clarified by treating first the case where the fluid is frictionless. The case of a material containing viscous fluid is then developed and discussed numerically. Phase velocity dispersion curves and attenuation coefficients for the three types of waves are plotted as a function of the frequency for various combinations of the characteristic parameters.,1956,0,6388,319,1,0,1,1,2,6,5,4,6,1
202870134de1f771f678cb540d2ea082b1ab9c5d,"§1. We shall denote by uα(P) = uα (x1, x2, x3, t), α = 1, 2, 3, the components of velocity at the moment t at the point with rectangular cartesian coordinates x1, x2, x3. In considering the turbulence it is natural to assume the components of the velocity uα (P) at every point P = (x1, x2, x3, t) of the considered domain G of the four-dimensional space (x1, x2, x3, t) are random variables in the sense of the theory of probabilities (cf. for this approach to the problem Millionshtchikov (1939) Denoting by Ᾱ the mathematical expectation of the random variable A we suppose that ῡ2α and (duα /dxβ)2― are finite and bounded in every bounded subdomain of the domain G.",1991,2,4611,453,26,30,32,30,46,54,51,65,78,63
fb98d93832bf3e2c825e08371c4680e9500750d6,,1981,0,3996,526,1,0,1,2,1,4,4,5,7,8
65afba3daff41d488f11e017bc02ba99854e52b7,"We present an overview of the lattice Boltzmann method (LBM), a parallel and efficient algorithm for simulating single-phase and multiphase fluid flows and for incorporating additional physical complexities. The LBM is especially useful for modeling complicated boundary conditions and multiphase interfaces. Recent extensions of this method are described, including simulations of fluid turbulence, suspension flows, and reaction diffusion systems.",2001,184,6056,217,62,109,118,114,142,211,241,226,290,295
f791e34262a8909299027036132288d68eb07a99,,1952,0,6154,404,0,0,1,0,1,1,0,1,2,1
425cb3106a15e476a2d338e32b896e704cd38a81,"This overview of diffusion and separation processes brings unsurpassed, engaging clarity to this complex topic. Diffusion is a key part of the undergraduate chemical engineering curriculum and at the core of understanding chemical purification and reaction engineering. This spontaneous mixing process is also central to our daily lives, with importance in phenomena as diverse as the dispersal of pollutants to digestion in the small intestine. For students, Diffusion goes from the basics of mass transfer and diffusion itself, with strong support through worked examples and a range of student questions. It also takes the reader right through to the cutting edge of our understanding, and the new examples in this third edition will appeal to professional scientists and engineers. Retaining the trademark enthusiastic style, the broad coverage now extends to biology and medicine.",1984,25,4862,302,0,3,7,6,14,20,18,26,24,35
b691bea88b6cd666499b4cc30141f24a46a37037,1 Preliminary Concepts 2 Fundamental Equations of Compressible Viscous Flow 3 Solutions of the Newtonian Viscous-Flow Equations 4 Laminar Boundary Layers 5 The Stability of Laminar Flows 6 Incompressible Turbulent Mean Flow 7 Compressible Boundary Layer Flow Appendices A Transport Properties of Various Newtonian Fluids B Equations of Motion of Incompressible Newtonian Fluids in Cylindrical and Spherical Coordinates C A Runge-Kutta Subroutine for N Simultaneous Differential Equations Bibliography Index,1974,0,5912,283,1,3,1,6,5,12,18,14,11,20
1350fb3231b36d942b6ff1dacb8df68a6df9c9b0,"An important achievement of modern experimental fluid mechanics is the invention and development of techniques for the measurement of whole, instantaneous fields of scalars and vectors. These techniques include tomographic interferometry (Hesselink 1988) and planar laser-induced fluorescence for scalars (Hassa et al 1987), and nuclear-magnetic-resonance imaging (Lee et al 1987), planar laser-induced fluorescence, laser-speckle velocimetry, particle-tracking velocimetry, molecular-tracking velocimetry (Miles et al 1989), and particle-image velocimetry for velocity fields. Reviews of these methods can be found in articles by Lauterborn & Vogel (1984), Adrian (1986a), Hesselink (1988), and Dudderar et al (1988), in books written by Merzkirch (1987) and edited by Chiang & Reid (1988) and Gad-el-Hak (1989).",1991,0,3309,258,3,24,48,39,70,65,85,71,73,104
00021f14a02740a3ada3b9e090dce979f5003e43,"A new technique is described for the numerical investigation of the time‐dependent flow of an incompressible fluid, the boundary of which is partially confined and partially free. The full Navier‐Stokes equations are written in finite‐difference form, and the solution is accomplished by finite‐time‐step advancement. The primary dependent variables are the pressure and the velocity components. Also used is a set of marker particles which move with the fluid. The technique is called the marker and cell method. Some examples of the application of this method are presented. All non‐linear effects are completely included, and the transient aspects can be computed for as much elapsed time as desired.",1965,7,5593,193,0,1,5,7,4,16,17,9,13,9
e0069fc257328c8f362a5e4c37c59f19eb719523,,2003,0,3648,223,53,79,107,171,156,168,222,241,267,223
d9a5c17a17c29bc5b50f55c1fe4f168034ae0279,"1. Introduction.- 1.1. Historical Background.- 1.2. Some Examples of Spectral Methods.- 1.2.1. A Fourier Galerkin Method for the Wave Equation.- 1.2.2. A Chebyshev Collocation Method for the Heat Equation.- 1.2.3. A Legendre Tau Method for the Poisson Equation.- 1.2.4. Basic Aspects of Galerkin, Tau and Collocation Methods.- 1.3. The Equations of Fluid Dynamics.- 1.3.1. Compressible Navier-Stokes.- 1.3.2. Compressible Euler.- 1.3.3. Compressible Potential.- 1.3.4. Incompressible Flow.- 1.3.5. Boundary Layer.- 1.4. Spectral Accuracy for a Two-Dimensional Fluid Calculation.- 1.5. Three-Dimensional Applications in Fluids.- 2. Spectral Approximation.- 2.1. The Fourier System.- 2.1.1. The Continuous Fourier Expansion.- 2.1.2. The Discrete Fourier Expansion.- 2.1.3. Differentiation.- 2.1.4. The Gibbs Phenomenon.- 2.2. Orthogonal Polynomials in ( - 1, 1).- 2.2.1. Sturm-Liouville Problems.- 2.2.2. Orthogonal Systems of Polynomials.- 2.2.3. Gauss-Type Quadratures and Discrete Polynomial Transforms.- 2.3. Legendre Polynomials.- 2.3.1. Basic Formulas.- 2.3.2. Differentiation.- 2.4. Chebyshev Polynomials.- 2.4.1. Basic Formulas.- 2.4.2. Differentiation.- 2.5. Generalizations.- 2.5.1. Jacobi Polynomials.- 2.5.2. Mapping.- 2.5.3. Semi-Infinite Intervals.- 2.5.4. Infinite Intervals.- 3. Fundamentals of Spectral Methods for PDEs.- 3.1. Spectral Projection of the Burgers Equation.- 3.1.1. Fourier Galerkin.- 3.1.2. Fourier Collocation.- 3.1.3. Chebyshev Tau.- 3.1.4. Chebyshev Collocation.- 3.2. Convolution Sums.- 3.2.1. Pseudospectral Transform Methods.- 3 2 2 Aliasing Removal by Padding or Truncation.- 3.2.3. Aliasing Removal by Phase Shifts.- 3.2.4. Convolution Sums in Chebyshev Methods.- 3.2.5. Relation Between Collocation and Pseudospectral Methods.- 3.3. Boundary Conditions.- 3.4. Coordinate Singularities.- 3.4.1. Polar Coordinates.- 3.4.2. Spherical Polar Coordinates.- 3.5. Two-Dimensional Mapping.- 4. Temporal Discretization.- 4.1. Introduction.- 4.2. The Eigenvalues of Basic Spectral Operators.- 4.2.1. The First-Derivative Operator.- 4.2.2. The Second-Derivative Operator.- 4.3. Some Standard Schemes.- 4.3.1. Multistep Schemes.- 4.3.2. Runge-Kutta Methods.- 4.4. Special Purpose Schemes.- 4.4.1. High Resolution Temporal Schemes.- 4.4.2. Special Integration Techniques.- 4.4.3. Lerat Schemes.- 4.5. Conservation Forms.- 4.6. Aliasing.- 5. Solution Techniques for Implicit Spectral Equations.- 5.1. Direct Methods.- 5.1.1. Fourier Approximations.- 5.1.2. Chebyshev Tau Approximations.- 5.1.3. Schur-Decomposition and Matrix-Diagonalization.- 5.2. Fundamentals of Iterative Methods.- 5.2.1. Richardson Iteration.- 5.2.2. Preconditioning.- 5.2.3. Non-Periodic Problems.- 5.2.4. Finite-Element Preconditioning.- 5.3. Conventional Iterative Methods.- 5.3.1. Descent Methods for Symmetric, Positive-Definite Systems.- 5.3.2. Descent Methods for Non-Symmetric Problems.- 5.3.3. Chebyshev Acceleration.- 5.4. Multidimensional Preconditioning.- 5.4.1. Finite-Difference Solvers.- 5.4.2. Modified Finite-Difference Preconditioners.- 5.5. Spectral Multigrid Methods.- 5.5.1. Model Problem Discussion.- 5.5.2. Two-Dimensional Problems.- 5.5.3. Interpolation Operators.- 5.5.4. Coarse-Grid Operators.- 5.5.5. Relaxation Schemes.- 5.6. A Semi-Implicit Method for the Navier-Stokes Equations.- 6. Simple Incompressible Flows.- 6.1. Burgers Equation.- 6.2. Shear Flow Past a Circle.- 6.3. Boundary-Layer Flows.- 6.4. Linear Stability.- 7. Some Algorithms for Unsteady Navier-Stokes Equations.- 7.1. Introduction.- 7.2. Homogeneous Flows.- 7.2.1. A Spectral Galerkin Solution Technique.- 7.2.2. Treatment of the Nonlinear Terms.- 7.2.3. Refinements.- 7.2.4. Pseudospectral and Collocation Methods.- 7.3. Inhomogeneous Flows.- 7.3.1. Coupled Methods.- 7.3.2. Splitting Methods.- 7.3.3. Galerkin Methods.- 7.3.4. Other Confined Flows.- 7.3.5. Unbounded Flows.- 7.3.6. Aliasing in Transition Calculations.- 7.4. Flows with Multiple Inhomogeneous Directions.- 7.4.1. Choice of Mesh.- 7.4.2. Coupled Methods.- 7.4.3. Splitting Methods.- 7.4.4. Other Methods.- 7.5. Mixed Spectral/Finite-Difference Methods.- 8. Compressible Flow.- 8.1. Introduction.- 8.2. Boundary Conditions for Hyperbolic Problems.- 8.3. Basic Results for Scalar Nonsmooth Problems.- 8.4. Homogeneous Turbulence.- 8.5. Shock-Capturing.- 8.5.1. Potential Flow.- 8.5.2. Ringleb Flow.- 8.5.3. Astrophysical Nozzle.- 8.6. Shock-Fitting.- 8.7. Reacting Flows.- 9. Global Approximation Results.- 9.1. Fourier Approximation.- 9.1.1. Inverse Inequalities for Trigonometric Polynomials.- 9.1.2. Estimates for the Truncation and Best Approximation Errors.- 9.1.3. Estimates for the Interpolation Error.- 9.2. Sturm-Liouville Expansions.- 9.2.1. Regular Sturm-Liouville Problems.- 9.2.2. Singular Sturm-Liouville Problems.- 9.3. Discrete Norms.- 9.4. Legendre Approximations.- 9.4.1. Inverse Inequalities for Algebraic Polynomials.- 9.4.2. Estimates for the Truncation and Best Approximation Errors.- 9.4.3. Estimates for the Interpolation Error.- 9.5. Chebyshev Approximations.- 9.5.1. Inverse Inequalities for Polynomials.- 9.5.2. Estimates for the Truncation and Best Approximation Errors.- 9.5.3. Estimates for the Interpolation Error.- 9.5.4. Proofs of Some Approximation Results.- 9.6. Other Polynomial Approximations.- 9.6.1. Jacobi Polynomials.- 9.6.2. Laguerre and Hermite Polynomials.- 9.7. Approximation Results in Several Dimensions.- 9.7.1. Fourier Approximations.- 9.7.2. Legendre Approximations.- 9.7.3. Chebyshev Approximations.- 9.7.4. Blended Fourier and Chebyshev Approximations.- 10. Theory of Stability and Convergence for Spectral Methods.- 10.1. The Three Examples Revisited.- 10.1.1. A Fourier Galerkin Method for the Wave Equation.- 10.1.2. A Chebyshev Collocation Method for the Heat Equation.- 10.1.3. A Legendre Tau Method for the Poisson Equation.- 10.2. Towards a General Theory.- 10.3. General Formulation of Spectral Approximations to Linear Steady Problems.- 10.4. Galerkin, Collocation and Tau Methods.- 10.4.1. Galerkin Methods.- 10.4.2. Tau Methods.- 10.4.3. Collocation Methods.- 10.5. General Formulation of Spectral Approximations to Linear Evolution Equations.- 10.5.1. Conditions for Stability and Convergence: The Parabolic Case.- 10.5.2. Conditions for Stability and Convergence: The Hyperbolic Case.- 10.6. The Error Equation.- 11. Steady, Smooth Problems.- 11.1. The Poisson Equation.- 11.1.1. Legendre Methods.- 11.1.2. Chebyshev Methods.- 11.1.3. Other Boundary Value Problems.- 11.2. Advection-Diffusion Equation.- 11.2.1. Linear Advection-Diffusion Equation.- 11.2.2. Steady Burgers Equation.- 11.3. Navier-Stokes Equations.- 11.3.1. Compatibility Conditions Between Velocity and Pressure.- 11.3.2. Direct Discretization of the Continuity Equation: The ""inf-sup"" Condition.- 11.3.3. Discretizations of the Continuity Equation by an Influence-Matrix Technique: The Kleiser-Schumann Method.- 11.3.4. Navier-Stokes Equations in Streamfunction Formulation.- 11.4. The Eigenvalues of Some Spectral Operators.- 11.4.1. The Discrete Eigenvalues for Lu = ? uxx.- 11.4.2. The Discrete Eigenvalues for Lu = ? vuxx + bux.- 11.4.3. The Discrete Eigenvalues for Lu = ux.- 12. Transient, Smooth Problems.- 12.1. Linear Hyperbolic Equations.- 12.1.1. Periodic Boundary Conditions.- 12.1.2. Non-Periodic Boundary Conditions.- 12.1.3. Hyperbolic Systems.- 12.1.4. Spectral Accuracy for Non-Smooth Solutions.- 12.2. Heat Equation.- 12.2.1. Semi-Discrete Approximation.- 12.2.2. Fully Discrete Approximation.- 12.3. Advection-Diffusion Equation.- 12.3.1. Semi-Discrete Approximation.- 12.3.2. Fully Discrete Approximation.- 13. Domain Decomposition Methods.- 13.1. Introduction.- 13.2. Patching Methods.- 13.2.1. Notation.- 13.2.2. Discretization.- 13.2.3. Solution Techniques.- 13.2.4. Examples.- 13.3. Variational Methods.- 13.3.1. Formulation.- 13.3.2. The Spectral-Element Method.- 13.4. The Alternating Schwarz Method.- 13.5. Mathematical Aspects of Domain Decomposition Methods.- 13.5.1. Patching Methods.- 13.5.2. Equivalence Between Patching and Variational Methods.- 13.6. Some Stability and Convergence Results.- 13.6.1. Patching Methods.- 13.6.2. Variational Methods.- Appendices.- A. Basic Mathematical Concepts.- B. Fast Fourier Transforms.- C. Jacobi-Gauss-Lobatto Roots.- References.",1987,0,4102,147,6,15,45,72,78,69,87,95,93,129
58cb1e26e323ae41fad6f4638110e58116e9923a,"1. The Phase Equilibrium Problem. 2. Classical Thermodynamics of Phase Equilibria. 3. Thermodynamic Properties from Volumetric Data. 4. Intermolecular Forces, Corresponding States and Osmotic Systems. 5. Fugacities in Gas Mixtures. 6. Fugacities in Liquid Mixtures: Excess Functions. 7. Fugacities in Liquid Mixtures: Models and Theories of Solutions. 8. Polymers: Solutions, Blends, Membranes, and Gels. 9. Electrolyte Solutions. 10. Solubilities of Gases in Liquids. 11. Solubilities of Solids in Liquids. 12. High-Pressure Phase Equilibria. Appendix A. Uniformity of Intensive Potentials as a Criterion of Phase Equilibrium. Appendix B. A Brief Introduction to Statistical Thermodynamics. Appendix C. Virial Coefficients for Quantum Gases. Appendix D. The Gibbs-Duhem Equation. Appendix E. Liquid-Liquid Equilibria in Binary and Multicomponent Systems. Appendix F. Estimation of Activity Coefficients. Appendix G. A General Theorem for Mixtures with Associating or Solvating Molecules. Appendix H. Brief Introduction to Perturbation Theory of Dense Fluids. Appendix I. The Ion-Interaction Model of Pitzer for Multielectrolyte Solutions. Appendix J. Conversion Factors and Constants. Index.",1969,0,4433,200,0,7,6,10,4,5,16,8,12,16
ed587e9f225d4e264962e07d64ba652fc8a2b6d9,"BACKGROUND
Retinal ischemia induces intraocular neovascularization, which often leads to glaucoma, vitreous hemorrhage, and retinal detachment, presumably by stimulating the release of angiogenic molecules. Vascular endothelial growth factor (VEGF) is an endothelial-cell-specific angiogenic factor whose production is increased by hypoxia.


METHODS
We measured the concentration of VEGF in 210 specimens of ocular fluid obtained from 164 patients undergoing intraocular surgery, using both radioimmuno-assays and radioreceptor assays. Vitreous proliferative potential was measured with in vitro assays of the growth of retinal endothelial cells and with VEGF-neutralizing antibody.


RESULTS
VEGF was detected in 69 of 136 ocular-fluid samples from patients with diabetic retinopathy, 29 of 38 samples from patients with neovascularization of the iris, and 3 of 4 samples from patients with ischemic occlusion of the central retinal vein, as compared with 2 of 31 samples from patients with no neovascular disorders (P < 0.001, P < 0.001, and P = 0.006, respectively). The mean (+/- SD) VEGF concentration in 70 samples of ocular fluid from patients with active proliferative diabetic retinopathy (3.6 +/- 6.3 ng per milliliter) was higher than that in 25 samples from patients with nonproliferative diabetic retinopathy (0.1 +/- 0.1 ng per milliliter, P = 0.008), 41 samples from patients with quiescent proliferative diabetic retinopathy (0.2 +/- 0.6 ng per milliliter, P < 0.001), or 31 samples from nondiabetic patients (0.1 +/- 0.2 ng per milliliter, P = 0.003). Concentrations of VEGF in vitreous fluid (8.8 +/- 9.9 ng per milliliter) were higher than those in aqueous fluid (5.6 +/- 8.6 ng per milliliter, P = 0.033) in all 10 pairs of samples obtained simultaneously from the same patient; VEGF concentrations in vitreous fluid declined after successful laser photocoagulation. VEGF stimulated the growth of retinal endothelial cells in vitro, as did vitreous fluid containing measurable VEGF. Stimulation was inhibited by VEGF-neutralizing antibodies.


CONCLUSIONS
Our data suggest that VEGF plays a major part in mediating active intraocular neovascularization in patients with ischemic retinal diseases, such as diabetic retinopathy and retinal-vein occlusion.",1994,28,3507,138,1,28,73,84,77,96,94,91,105,96
2f01b5dc74c8c918a8f3cf026f6e0bebca28c94c,"Tumor ascites fluids from guinea pigs, hamsters, and mice contain activity that rapidly increases microvascular permeability. Similar activity is also secreted by these tumor cells and a variety of other tumor cell lines in vitro. The permeability-increasing activity purified from either the culture medium or ascites fluid of one tumor, the guinea pig line 10 hepatocarcinoma, is a 34,000- to 42,000-dalton protein distinct from other known permeability factors.",1983,8,3889,68,2,6,7,8,5,12,6,9,20,18
584db0eb1badb25b3938823e76d1e705cb60df18,"A fluid mosaic model is presented for the gross organization and structure of the proteins and lipids of biological membranes. The model is consistent with the restrictions imposed by thermodynamics. In this model, the proteins that are integral to the membrane are a heterogeneous set of globular molecules, each arranged in an amphipathic structure, that is, with the ionic and highly polar groups protruding from the membrane into the aqueous phase, and the nonpolar groups largely buried in the hydrophobic interior of the membrane. These globular molecules are partially embedded in a matrix of phospholipid. The bulk of the phospholipid is organized as a discontinuous, fluid bilayer, although a small fraction of the lipid may interact specifically with the membrane proteins. The fluid mosaic structure is therefore formally analogous to a two-dimensional oriented solution of integral proteins (or lipoproteins) in the viscous phospholipid bilayer solvent. Recent experiments with a wide variety of techniqes and several different membrane systems are described, all of which abet consistent with, and add much detail to, the fluid mosaic model. It therefore seems appropriate to suggest possible mechanisms for various membrane functions and membrane-mediated phenomena in the light of the model. As examples, experimentally testable mechanisms are suggested for cell surface changes in malignant transformation, and for cooperative effects exhibited in the interactions of membranes with some specific ligands. Note added in proof: Since this article was written, we have obtained electron microscopic evidence (69) that the concanavalin A binding sites on the membranes of SV40 virus-transformed mouse fibroblasts (3T3 cells) are more clustered than the sites on the membranes of normal cells, as predicted by the hypothesis represented in Fig. 7B. T-here has also appeared a study by Taylor et al. (70) showing the remarkable effects produced on lymphocytes by the addition of antibodies directed to their surface immunoglobulin molecules. The antibodies induce a redistribution and pinocytosis of these surface immunoglobulins, so that within about 30 minutes at 37�C the surface immunoglobulins are completely swept out of the membrane. These effects do not occur, however, if the bivalent antibodies are replaced by their univalent Fab fragments or if the antibody experiments are carried out at 0�C instead of 37�C. These and related results strongly indicate that the bivalent antibodies produce an aggregation of the surface immunoglobulin molecules in the plane of the membrane, which can occur only if the immunoglobulin molecules are free to diffuse in the membrane. This aggregation then appears to trigger off the pinocytosis of the membrane components by some unknown mechanism. Such membrane transformations may be of crucial importance in the induction of an antibody response to an antigen, as well as iv other processes of cell differentiation.",1972,77,5674,16,49,179,216,203,264,217,221,185,163,157
37491ed087839d86c9ba7783cb12ccc6c9918bf6,"Microfabricated integrated circuits revolutionized computation by vastly reducing the space, labor, and time required for calculations. Microfluidic systems hold similar promise for the large-scale automation of chemistry and biology, suggesting the possibility of numerous experiments performed rapidly and in parallel, while consuming little reagent. While it is too early to tell whether such a vision will be realized, significant progress has been achieved, and various applications of significant scientific and practical interest have been developed. Here a review of the physics of small volumes (nanoliters) of fluids is presented, as parametrized by a series of dimensionless numbers expressing the relative importance of various physical phenomena. Specifically, this review explores the Reynolds number Re, addressing inertial effects; the Peclet number Pe, which concerns convective and diffusive transport; the capillary number Ca expressing the importance of interfacial tension; the Deborah, Weissenberg, and elasticity numbers De, Wi, and El, describing elastic effects due to deformable microstructural elements like polymers; the Grashof and Rayleigh numbers Gr and Ra, describing density-driven flows; and the Knudsen number, describing the importance of noncontinuum molecular effects. Furthermore, the long-range nature of viscous flows and the small device dimensions inherent in microfluidics mean that the influence of boundaries is typically significant. A variety of strategies have been developed to manipulate fluids by exploiting boundary effects; among these are electrokinetic effects, acoustic streaming, and fluid-structure interactions. The goal is to describe the physics behind the rich variety of fluid phenomena occurring on the nanoliter scale using simple scaling arguments, with the hopes of developing an intuitive sense for this occasionally counterintuitive world.",2005,1080,3427,70,17,77,129,221,219,226,241,278,285,260
f6af0b7accdc3ca99195b991e50232392fa4b4b8,This Letter presents variational ground-state and excited-state wave functions which describe the condensation of a two-dimensional electron gas into a new state of matter.,1983,8,3228,111,4,30,46,46,33,37,36,45,38,57
cf15817ee5f9c1536ee4da2c4c018555600ca91b,"A study was conducted in which 133 participants performed 11 memory tasks (some thought to reflect working memory and some thought to reflect short-term memory), 2 tests of general fluid intelligence, and the Verbal and Quantitative Scholastic Aptitude Tests. Structural equation modeling suggested that short-term and working memories reflect separate but highly related constructs and that many of the tasks used in the literature as working memory tasks reflect a common construct. Working memory shows a strong connection to fluid intelligence, but short-term memory does not. A theory of working memory capacity and general fluid intelligence is proposed: The authors argue that working memory capacity and fluid intelligence reflect the ability to keep a representation active, particularly in the face of interference and distraction. The authors also discuss the relationship of this capability to controlled attention, and the functions of the prefrontal cortex.",1999,109,2892,149,5,13,49,42,69,63,99,92,115,130
0ceb798602f4a466b1e3c943a73a892c25d8abbc,"Perturbation Methods in Fluid MechanicsBy Milton Van Dyke. (Applied Mathematics and Mechanics: an International Series of Monographs, Vol. 8.) Pp. x + 229. (New York: Academic Press, Inc.; London: Academic Press, Inc. (London), Ltd., 1964.) 56s.",1965,0,3063,188,3,21,33,26,41,41,53,63,46,48
265d2bbf765c4ed9784ab0a574810231e3b14671,"Keywords: dynamique des : fluides Reference Record created on 2005-11-18, modified on 2016-08-08",1969,0,3419,110,7,3,11,12,15,8,17,23,21,27
fee733d1816ac29a2c38fa51ec2e25242b6b2dc7,"A fluid mosaic model is presented for the gross organization and structure of the proteins and lipids of biological membranes. The model is consistent with the restrictions imposed by thermodynamics. In this model, the proteins that are integral to the membrane are a heterogeneous set of globular molecules, each arranged in an amphipathic structure, that is, with the ionic and highly polar groups protruding from the membrane into the aqueous phase, and the nonpolar groups largely buried in the hydrophobic interior of the membrane. These globular molecules are partially embedded in a matrix of phospholipid. The bulk of the phospholipid is organized as a discontinuous, fluid bilayer, although a small fraction of the lipid may interact specifically with the membrane proteins. The fluid mosaic structure is therefore formally analogous to a two-dimensional oriented solution of integral proteins (or lipoproteins) in the viscous phospholipid bilayer solvent. Recent experiments with a wide variety of techniqes and several different membrane systems are described, all of which abet consistent with, and add much detail to, the fluid mosaic model. It therefore seems appropriate to suggest possible mechanisms for various membrane functions and membrane-mediated phenomena in the light of the model. As examples, experimentally testable mechanisms are suggested for cell surface changes in malignant transformation, and for cooperative effects exhibited in the interactions of membranes with some specific ligands. Note added in proof: Since this article was written, we have obtained electron microscopic evidence (69) that the concanavalin A binding sites on the membranes of SV40 virus-transformed mouse fibroblasts (3T3 cells) are more clustered than the sites on the membranes of normal cells, as predicted by the hypothesis represented in Fig. 7B. T-here has also appeared a study by Taylor et al. (70) showing the remarkable effects produced on lymphocytes by the addition of antibodies directed to their surface immunoglobulin molecules. The antibodies induce a redistribution and pinocytosis of these surface immunoglobulins, so that within about 30 minutes at 37 degrees C the surface immunoglobulins are completely swept out of the membrane. These effects do not occur, however, if the bivalent antibodies are replaced by their univalent Fab fragments or if the antibody experiments are carried out at 0 degrees C instead of 37 degrees C. These and related results strongly indicate that the bivalent antibodies produce an aggregation of the surface immunoglobulin molecules in the plane of the membrane, which can occur only if the immunoglobulin molecules are free to diffuse in the membrane. This aggregation then appears to trigger off the pinocytosis of the membrane components by some unknown mechanism. Such membrane transformations may be of crucial importance in the induction of an antibody response to an antigen, as well as iv other processes of cell differentiation.",1972,81,2893,124,8,32,53,53,67,35,36,34,22,23
042ad33902b18a6cc9310441d5853afbc17bc5e4,"Keywords: CFD ; numerique ; transfert de chaleur ; ecoulement Reference Record created on 2005-11-18, modified on 2016-08-08",1984,0,3087,51,1,6,12,21,33,49,53,63,76,76
66c57627253ea3dc5eb71892be3e648a67981c6d,"Variations of the SIMPLE method of Patankar and Spalding have been widely used over the past decade to obtain numerical solutions to problems involving incompressible flows. The present paper shows several modifications to the method which both simplify its implementation and reduce solution costs. The performances of SIMPLE, SIMPLER, and SIMPLEC (the present method) are compared for two recirculating flow problems. The paper is addressed to readers who already have experience with SIMPLE or its variants.",1984,11,3189,53,0,5,13,10,22,29,35,37,43,45
f14b8ef5a3edc5fdc9613a8a1c5545aa6cb626ad,"This Position Stand provides guidance on fluid replacement to sustain appropriate hydration of individuals performing physical activity. The goal of prehydrating is to start the activity euhydrated and with normal plasma electrolyte levels. Prehydrating with beverages, in addition to normal meals and fluid intake, should be initiated when needed at least several hours before the activity to enable fluid absorption and allow urine output to return to normal levels. The goal of drinking during exercise is to prevent excessive (>2% body weight loss from water deficit) dehydration and excessive changes in electrolyte balance to avert compromised performance. Because there is considerable variability in sweating rates and sweat electrolyte content between individuals, customized fluid replacement programs are recommended. Individual sweat rates can be estimated by measuring body weight before and after exercise. During exercise, consuming beverages containing electrolytes and carbohydrates can provide benefits over water alone under certain circumstances. After exercise, the goal is to replace any fluid electrolyte deficit. The speed with which rehydration is needed and the magnitude of fluid electrolyte deficits will determine if an aggressive replacement program is merited.",2007,10,1797,207,36,62,53,93,111,114,140,150,158,152
b0b240ab99338da6569f9903228e8f93e533a163,"Abstract Results of an extensive comparison of numerical methods for simulating hydrodynamics are presented and discussed. This study focuses on the simulation of fluid flows with strong shocks in two dimensions. By “strong shocks,” we here refer to shocks in which there is substantial entropy production. For the case of shocks in air, we therefore refer to Mach numbers of three and greater. For flows containing such strong shocks we find that a careful treatment of flow discontinuities is of greatest importance in obtaining accurate numerical results. Three approaches to treating discontinuities in the flow are discussed—artificial viscosity, blending of low- and high-order-accurate fluxes, and the use of nonlinear solutions to Riemann's problem. The advantages and disadvantages of each approach are discussed and illustrated by computed results for three test problems. In this comparison we have focused our attention entirely upon the performance of schemes for differencing the hydrodynamic equations. We have regarded the nature of the grid upon which such differencing schemes are applied as an independent issue outside the scope of this work. Therefore we have restricted our study to the case of uniform, square computational zones in Cartesian coordinates. For simplicity we have further restricted our attention to two-dimensional difference schemes which are built out of symmetrized products of one-dimensional difference operators.",1984,69,2422,175,3,14,15,20,16,36,32,22,24,22
b7e200c497c63b94ed8094fb5e38ecfd01aaed5d,,1997,0,2259,160,8,12,23,14,23,12,28,28,39,44
1484e852af31571652ef794ecd06b9a769576620,"In both physical and biological science, we are often concerned with the properties of a fluid, or plasma, in which small particles or corpuscles are suspended and carried about by the motion of the fluid. The presence of the particles will influence the properties of the suspension in bulk, and, in particular, its viscosity will be increased. The most complete mathematical treatment of the problem, from this point of view, has been that given by Einstein, who considered the case of spherical particles and gave a simple formula for the increase in the viscosity. We have extended this work to the case of particles of ellipsoidal shape. The second section of the paper is occupied with the requisite solution of the equations of motion of the fluid. The problem of the motion of a viscous fluid, due to an ellipsoid moving through it with a small velocity of translation in a direction parallel to one of its axes, has been solved by Oberbeck, and the corresponding problem for an ellipsoid rotating about one of its axes by Edwards. In both cases the equations of motion are approximated by neglecting the terms involving the squares of the velocities. It may be seen, a posteriori , that the condition for the validity of this approximation is that the product of the velocity of the ellipsoid by its linear dimensions shall be small compared with the “kinematic coefficient, of viscosity” of the fluid. In relation to our present problem, it will therefore be satisfied either for sufficiently slow motions, or for sufficiently small particles.",1922,0,3245,262,0,0,0,0,0,0,0,0,0,0
662ac20e8d8b980582c513b38dd964cc56b40e05,,2009,0,1647,162,94,74,105,117,140,138,140,120,110,149
61eaddd04b290b10da09b172ac6063dbb94bd9a9,Pore Structure. Capillarity in Porous Media. Single-Phase Transport Phenomena in Porous Media. Selected Operations Involving Transport of a Single Fluid Phase through a Porous Medium. Multiphase Flow of Immiscible Fluids in Porous Media. Miscible Displacement and Dispersion. Index.,1979,1,2827,63,0,0,1,3,9,10,13,18,27,20
2e776c3af634ab4ff40a67e6295efcf3a3879848,"Fluid intelligence (Gf) refers to the ability to reason and to solve new problems independently of previously acquired knowledge. Gf is critical for a wide variety of cognitive tasks, and it is considered one of the most important factors in learning. Moreover, Gf is closely related to professional and educational success, especially in complex and demanding environments. Although performance on tests of Gf can be improved through direct practice on the tests themselves, there is no evidence that training on any other regimen yields increased Gf in adults. Furthermore, there is a long history of research into cognitive training showing that, although performance on trained tasks can increase dramatically, transfer of this learning to other tasks remains poor. Here, we present evidence for transfer from training on a demanding working memory task to measures of Gf. This transfer results even though the trained task is entirely different from the intelligence test itself. Furthermore, we demonstrate that the extent of gain in intelligence critically depends on the amount of training: the more training, the more improvement in Gf. That is, the training effect is dosage-dependent. Thus, in contrast to many previous studies, we conclude that it is possible to improve Gf without practicing the testing tasks themselves, opening a wide range of applications.",2008,58,1890,133,17,56,77,108,154,163,195,176,152,178
365bc7651a03fa1c1c02dbd72d800a1d97855484,Develop a cerebrospinal fluid biomarker signature for mild Alzheimer's disease (AD) in Alzheimer's Disease Neuroimaging Initiative (ADNI) subjects.,2009,34,1760,77,25,110,156,141,137,146,171,138,158,148
16373b46351bfeb596128a1c49249e44a47ee77e,"Fluid models of gas discharges require the input of transport coefficients and rate coefficients that depend on the electron energy distribution function. Such coefficients are usually calculated from collision cross-section data by solving the electron Boltzmann equation (BE). In this paper we present a new user-friendly BE solver developed especially for this purpose, freely available under the name BOLSIG+, which is more general and easier to use than most other BE solvers available. The solver provides steady-state solutions of the BE for electrons in a uniform electric field, using the classical two-term expansion, and is able to account for different growth models, quasi-stationary and oscillating fields, electron–neutral collisions and electron–electron collisions. We show that for the approximations we use, the BE takes the form of a convection-diffusion continuity-equation with a non-local source term in energy space. To solve this equation we use an exponential scheme commonly used for convection-diffusion problems. The calculated electron transport coefficients and rate coefficients are defined so as to ensure maximum consistency with the fluid equations. We discuss how these coefficients are best used in fluid models and illustrate the influence of some essential parameters and approximations.",2005,33,2083,92,4,11,26,27,49,56,80,93,114,160
48383c4b50853ff1afab6ecb9745c247a97ba6e1,"BACKGROUND
It remains uncertain whether the choice of resuscitation fluid for patients in intensive care units (ICUs) affects survival. We conducted a multicenter, randomized, double-blind trial to compare the effect of fluid resuscitation with albumin or saline on mortality in a heterogeneous population of patients in the ICU.


METHODS
We randomly assigned patients who had been admitted to the ICU to receive either 4 percent albumin or normal saline for intravascular-fluid resuscitation during the next 28 days. The primary outcome measure was death from any cause during the 28-day period after randomization.


RESULTS
Of the 6997 patients who underwent randomization, 3497 were assigned to receive albumin and 3500 to receive saline; the two groups had similar baseline characteristics. There were 726 deaths in the albumin group, as compared with 729 deaths in the saline group (relative risk of death, 0.99; 95 percent confidence interval, 0.91 to 1.09; P=0.87). The proportion of patients with new single-organ and multiple-organ failure was similar in the two groups (P=0.85). There were no significant differences between the groups in the mean (+/-SD) numbers of days spent in the ICU (6.5+/-6.6 in the albumin group and 6.2+/-6.2 in the saline group, P=0.44), days spent in the hospital (15.3+/-9.6 and 15.6+/-9.6, respectively; P=0.30), days of mechanical ventilation (4.5+/-6.1 and 4.3+/-5.7, respectively; P=0.74), or days of renal-replacement therapy (0.5+/-2.3 and 0.4+/-2.0, respectively; P=0.41).


CONCLUSIONS
In patients in the ICU, use of either 4 percent albumin or normal saline for fluid resuscitation results in similar outcomes at 28 days.",2004,28,2296,38,31,102,129,135,140,127,160,140,133,169
fa9d519d34f73f8dafc5be93f38af63e8083b20e,"Abstract A method to simulate unsteady multi-fluid flows in which a sharp interface or a front separates incompressible fluids of different density and viscosity is described. The flow field is discretized by a conservative finite difference approximation on a stationary grid, and the interface is explicitly represented by a separate, unstructured grid that moves through the stationary grid. Since the interface deforms continuously, it is necessary to restructure its grid as the calculations proceed. In addition to keeping the density and viscosity stratification sharp, the tracked interface provides a natural way to include surface tension effects. Both two- and three-dimensional, full numerical simulations of bubble motion are presented.",1992,88,2201,93,1,5,9,19,19,18,24,36,30,48
2413f245a64b9504efdd14d86c2c9483cdf0d6ba,"A new silver stain for electrophoretically separated polypeptides can be rapidly and easily used and can detect as little as 0.01 nanogram of protein per square millimeter. When employed with two-dimensional electrophoresis, it should permit qualitative and quantitative characterization of protein distributions in body fluids and tissues. It has been used to demonstrate regional variations in cerebrospinal fluid proteins.",1981,6,2554,12,12,64,133,169,150,191,171,151,152,138
d26eccb48c61a051378678dfbb5b5943683fcaed,"Background Optimal fluid management in patients with acute lung injury is unknown. Diuresis or fluid restriction may improve lung function but could jeopardize extrapulmonary-organ perfusion. Methods In a randomized study, we compared a conservative and a liberal strategy of fluid management using explicit protocols applied for seven days in 1000 patients with acute lung injury. The primary end point was death at 60 days. Secondary end points included the number of ventilator-free days and organ-failure–free days and measures of lung physiology. Results The rate of death at 60 days was 25.5 percent in the conservative-strategy group and 28.4 percent in the liberal-strategy group (P=0.30; 95 percent confidence interval for the difference, −2.6 to 8.4 percent). The mean (±SE) cumulative fluid balance during the first seven days was –136±491 ml in the conservative-strategy group and 6992±502 ml in the liberal-strategy group (P<0.001). As compared with the liberal strategy, the conservative strategy improved ...",2009,32,1768,1,93,127,112,142,126,122,93,124,120,115
62c0ba833912728a4e79c5c3550b44571b5ce94a,"Effective thermal conductivity of mixtures of e uids and nanometer-size particles is measured by a steady-state parallel-plate method. The tested e uids contain two types of nanoparticles, Al 2O3 and CuO, dispersed in water, vacuum pump e uid, engine oil, and ethylene glycol. Experimental results show that the thermal conductivities of nanoparticle ‐e uid mixtures are higher than those of the base e uids. Using theoretical models of effective thermal conductivity of a mixture, we have demonstrated that the predicted thermal conductivities of nanoparticle ‐e uid mixtures are much lower than our measured data, indicating the dee ciency in the existing models when used for nanoparticle ‐e uid mixtures. Possible mechanisms contributing to enhancement of the thermal conductivity of the mixtures are discussed. A more comprehensive theory is needed to fully explain the behavior of nanoparticle ‐e uid mixtures. Nomenclature cp = specie c heat k = thermal conductivity L = thickness Pe = Peclet number P q = input power to heater 1 r = radius of particle S = cross-sectional area T = temperature U = velocity of particles relative to that of base e uids ® = ratio of thermal conductivity of particle to that of base liquid ¯ = .® i 1/=.® i 2/ ° = shear rate of e ow Ω = density A = volume fraction of particles in e uids Subscripts",1999,43,1970,54,1,0,2,4,4,11,28,36,52,56
a7e4d45f1adab6c0e6ddaccdabbbc086d9547ef9,,1996,0,1586,213,1,6,10,28,40,31,34,43,33,54
1a9c2fa2d12609af7a489648fe68a416075cac00,"The method of characteristics used for numerical computation of solutions of fluid dynamical equations is characterized by a large degree of non standardness and therefore is not suitable for automatic computation on electronic computing machines, especially for problems with a large number of shock waves and contact discontinuities. In 1950 v. Neumann and Richtmyer proposed to use, for the solution of fluid dynamics equations, difference equations into which viscosity was introduced artificially; this has the effect of smearing out the shock wave over several mesh points. Then, it was proposed to proceed with the computations across the shock waves in the ordinary manner. In 1954, Lax published the ""triangle'' scheme suitable for computation across the shock"" waves. A deficiency of this scheme is that it does not allow computation with arbitrarily fine time steps (as compared with the space steps divided by the sound speed) because it then transforms any initial data into linear functions. In addition, this scheme smears out contact discontinuities. The purpose of this paper is to choose a scheme which is in some sense best and which still allows computation across the shock waves. This choice is made for linear equations and then by analogy the scheme is applied to the general equations of fluid dynamics. Following this scheme we carried out a large number of computations on Soviet electronic computers. For a check, some of these computations were compared with the computations carried out by the method of characteristics. The agreement of results was fully satisfactory.",1959,2,2761,73,0,0,0,3,0,1,2,1,1,3
b721f6ba0e10507f1d25b76c3eb37380d16a31a2,This study develops analytical relationships and computations of power dissipation in magnetic fluid (ferrofluid) subjected to alternating magnetic field. The dissipation results from the orientational relaxation of particles having thermal fluctuations in a viscous medium.,2002,20,1775,94,2,5,8,13,23,28,36,68,81,92
cffc507312c01839ef2dc32158f2ad3a57efa5ce,"Dispersions of solid spherical grains of diameter D = 0.13cm were sheared in Newtonian fluids of varying viscosity (water and a glycerine-water-alcohol mixture) in the annular space between two concentric drums. The density σ of the grains was balanced against the density ρ of the fluid, giving a condition of no differential forces due to radial acceleration. The volume concentration C of the grains was varied between 62 and 13 %. A substantial radial dispersive pressure was found to be exerted between the grains. This was measured as an increase of static pressure in the inner stationary drum which had a deformable periphery. The torque on the inner drum was also measured. The dispersive pressure P was found to be proportional to a shear stress λ attributable to the presence of the grains. The linear grain concentration λ is defined as the ratio grain diameter/mean free dispersion distance and is related to C by λ=1(C0/C)12−1 where C0 is the maximum possible static volume concentration. Both the stressesT and P, as dimensionless groups TσD2/λη2, and PσD2/λη 2, were found to bear single-valued empirical relations to a dimensionless shear strain group λ½σD2(dU/dy)lη for all the values of λ< 12(C= 57% approx.) where dU/dy is the rate of shearing of the grains over one another, and η the fluid viscosity. This relation gives Tασ(λD)2(dU/dy)2 and T∝λ12ηdU/dy according as dU/dy is large or small, i.e. according to whether grain inertia or fluid viscosity dominate. An alternative semi-empirical relation F = (1+λ)(1+½λ)ηdU/dy was found for the viscous case, when T is the whole shear stress. The ratio T/P was constant at 0·3 approx, in the inertia region, and at 0.75 approx, in the viscous region. The results are applied to a few hitherto unexplained natural phenomena.",1954,1,2296,201,0,0,0,0,0,0,1,0,0,1
6d0dea3286515ee375a020de8d743eca7f5f1556,"While Eulerian schemes work well for most gas flows, they have been shown to admit nonphysical oscillations near some material interfaces. In contrast, Lagrangian schemes work well at multimaterial interfaces, but suffer from their own difficulties in problems with large deformations and vorticity characteristic of most gas flows. We believe that the most robust schemes will combine the best properties of Eulerian and Lagrangian schemes. In this paper, we propose a new numerical method for treating interfaces in Eulerian schemes that maintains a Heaviside profile of the density with no numerical smearing along the lines of earlier work and most Lagrangian schemes. We use a level set function to track the motion of a multimaterial interface in an Eulerian framework. In addition, the use of ghost cells (actually ghost nodes in our finite difference framework) and a new isobaric fix technique allows us to keep the density profile from smearing out, while still keeping the scheme robust and easy to program with simple extensions to multidimensions and multilevel time integration, e.g., Runge?Kutta methods. In contrast, previous methods used ill-advised dimensional splitting for multidimensional problems and suffered from great complexity when used in conjunction with multilevel time integrators.",1999,47,1796,131,6,13,32,26,38,54,66,59,80,97
51f93535127cb16477ad4e6d56a8cb527969cf72,"We study the stability of steady nonlinear waves on the surface of an infinitely deep fluid [1, 2]. In section 1, the equations of hydrodynamics for an ideal fluid with a free surface are transformed to canonical variables: the shape of the surface η(r, t) and the hydrodynamic potential ψ(r, t) at the surface are expressed in terms of these variables. By introducing canonical variables, we can consider the problem of the stability of surface waves as part of the more general problem of nonlinear waves in media with dispersion [3,4]. The resuits of the rest of the paper are also easily applicable to the general case.In section 2, using a method similar to van der Pohl's method, we obtain simplified equations describing nonlinear waves in the small amplitude approximation. These equations are particularly simple if we assume that the wave packet is narrow. The equations have an exact solution which approximates a periodic wave of finite amplitude.In section 3 we investigate the instability of periodic waves of finite amplitude. Instabilities of two types are found. The first type of instability is destructive instability, similar to the destructive instability of waves in a plasma [5, 6], In this type of instability, a pair of waves is simultaneously excited, the sum of the frequencies of which is a multiple of the frequency of the original wave. The most rapid destructive instability occurs for capillary waves and the slowest for gravitational waves. The second type of instability is the negative-pressure type, which arises because of the dependence of the nonlinear wave velocity on the amplitude; this results in an unbounded increase in the percentage modulation of the wave. This type of instability occurs for nonlinear waves through any media in which the sign of the second derivative in the dispersion law with respect to the wave number (d2ω/dk2) is different from the sign of the frequency shift due to the nonlinearity.As announced by A. N. Litvak and V. I. Talanov [7], this type of instability was independently observed for nonlinear electromagnetic waves.",1968,11,2093,209,0,0,0,1,0,0,2,0,0,2
6635fa7b37888f0f90c64afdfcb8ca09d3ae974f,"Many solid tumours show an increased interstitial fluid pressure (IFP), which forms a barrier to transcapillary transport. This barrier is an obstacle in tumour treatment, as it results in inefficient uptake of therapeutic agents. There are a number of factors that contribute to increased IFP in the tumour, such as vessel abnormalities, fibrosis and contraction of the interstitial matrix. Lowering the tumour IFP with specific signal-transduction antagonists might be a useful approach to improving anticancer drug efficacy.",2004,90,1639,63,0,18,37,44,48,50,69,81,97,123
1d573e6f61e8bb51c90f597a65780382b4a1369a,"Keywords: dynamique des : fluides ; equations : differentielles ; analyse ; elements : finis ; stabilite ; stationnaire ; mathematiques ; methodes : numeriques Reference Record created on 2005-11-18, modified on 2016-08-08",1992,0,1923,62,16,31,39,57,64,64,64,56,71,71
d9b606743d6eeaf3f42ee68298f3067af8201478,"When a viscous fluid filling the voids in a porous medium is driven forwards by the pressure of another driving fluid, the interface between them is liable to be unstable if the driving fluid is the less viscous of the two. This condition occurs in oil fields. To describe the normal modes of small disturbances from a plane interface and their rate of growth, it is necessary to know, or to assume one knows, the conditions which must be satisfied at the interface. The simplest assumption, that the fluids remain completely separated along a definite interface, leads to formulae which are analogous to known expressions developed by scientists working in the oil industry, and also analogous to expressions representing the instability of accelerated interfaces between fluids of different densities. In the latter case the instability develops into round-ended fingers of less dense fluid penetrating into the more dense one. Experiments in which a viscous fluid confined between closely spaced parallel sheets of glass, a Hele-Shaw cell, is driven out by a less viscous one reveal a similar state. The motion in a Hele-Shaw cell is mathematically analogous to two-dimensional flow in a porous medium. Analysis which assumes continuity of pressure through the interface shows that a flow is possible in which equally spaced fingers advance steadily. The ratio λ = (width of finger)/(spacing of fingers) appears as the parameter in a singly infinite set of such motions, all of which appear equally possible. Experiments in which various fluids were forced into a narrow Hele-Shaw cell showed that single fingers can be produced, and that unless the flow is very slow λ = (width of finger)/(width of channel) is close to ½, so that behind the tips of the advancing fingers the widths of the two columns of fluid are equal. When λ = ½ the calculated form of the fingers is very close to that which is registered photographically in the Hele-Shaw cell, but at very slow speeds where the measured value of λ increased from ½ to the limit 1.0 as the speed decreased to zero, there were considerable differences. Assuming that these might be due to surface tension, experiments were made in which a fluid of small viscosity, air or water, displaced a much more viscous oil. It is to be expected in that case that λ would be a function of μU/T only, where μ is the viscosity, U the speed of advance and T the interfacial tension. This was verified using air as the less viscous fluid penetrating two oils of viscosities 0.30 and 4.5 poises.",1958,6,2340,126,0,0,2,1,1,2,0,0,2,1
e599af5bb14076ef4ec04855a75243bbc7c0b3d1,"Part I*Basic Thoughts and Equations 1 Philosophy of Computational Fluid Dynamics 2 The Governing Equations of Fluid Dynamics Their Derivation, A Discussion of Their Physical Meaning, and A Presentation of Forms Particularly Suitable to CFD 3 Mathematical Behavior of Partial Differential Equations The Impact on Computational Fluid Dynamics Part II*Basics of the Numerics 4 Basic Aspects of Discretization 5 Grids and Meshes, With Appropriate Transformations 6 Some Simple CFD Techniques A Beginning Part III*Some Applications 7 Numerical Solutions of Quasi-One-Dimensional Nozzle Flows 8 Numerical Solution of A Two-Dimensional Supersonic Flow Prandtl-Meyer Expansion Wave 9 Incompressible Couette Flow Numerical Solution by Means of an Implicit Method and the Pressure Correction Method 10 Incompressible, Inviscid Slow Over a Circular Cylinder Solution by the Technique Relaxation Part IV*Other Topics 11 Some Advanced Topics in Modern CFD A Discussion 12 The Future of Computational Fluid Dynamics Appendixes A Thomas's Algorithm for the Solution of A Tridiagonal System of Equations References",1995,94,1685,102,0,1,5,11,9,18,19,23,36,31
e51ab939cf5a2faf79ae169f2b794bc27f31cfae,,2006,0,1247,189,3,4,27,30,57,72,76,93,73,105
66fbc867a7a5faf4773581ccb3a2918a0706e1db,"BACKGROUND
Optimal fluid management in patients with acute lung injury is unknown. Diuresis or fluid restriction may improve lung function but could jeopardize extrapulmonary-organ perfusion.


METHODS
In a randomized study, we compared a conservative and a liberal strategy of fluid management using explicit protocols applied for seven days in 1000 patients with acute lung injury. The primary end point was death at 60 days. Secondary end points included the number of ventilator-free days and organ-failure-free days and measures of lung physiology.


RESULTS
The rate of death at 60 days was 25.5 percent in the conservative-strategy group and 28.4 percent in the liberal-strategy group (P=0.30; 95 percent confidence interval for the difference, -2.6 to 8.4 percent). The mean (+/-SE) cumulative fluid balance during the first seven days was -136+/-491 ml in the conservative-strategy group and 6992+/-502 ml in the liberal-strategy group (P<0.001). As compared with the liberal strategy, the conservative strategy improved the oxygenation index ([mean airway pressure x the ratio of the fraction of inspired oxygen to the partial pressure of arterial oxygen]x100) and the lung injury score and increased the number of ventilator-free days (14.6+/-0.5 vs. 12.1+/-0.5, P<0.001) and days not spent in the intensive care unit (13.4+/-0.4 vs. 11.2+/-0.4, P<0.001) during the first 28 days but did not increase the incidence or prevalence of shock during the study or the use of dialysis during the first 60 days (10 percent vs. 14 percent, P=0.06).


CONCLUSIONS
Although there was no significant difference in the primary outcome of 60-day mortality, the conservative strategy of fluid management improved lung function and shortened the duration of mechanical ventilation and intensive care without increasing nonpulmonary-organ failures. These results support the use of a conservative strategy of fluid management in patients with acute lung injury. (ClinicalTrials.gov number, NCT00281268 [ClinicalTrials.gov].).",2006,59,1413,48,11,34,60,77,85,93,110,133,120,112
8454380009625b96576deb493ee3900d48575660,"AbstractA moving-particle semi-implicit (MPS) method for simulating fragmentation of incompressible fluids is presented. The motion of each particle is calculated through interactions with neighboring particles covered with the kernel function. Deterministic particle interaction models representing gradient, Laplacian, and free surfaces are proposed. Fluid density is implicitly required to be constant as the incompressibility condition, while the other terms are explicitly calculated. The Poisson equation of pressure is solved by the incomplete Cholesky conjugate gradient method. Collapse of a water column is calculated using MPS. The effect of parameters in the models is investigated in test calculations. Good agreement with an experiment is obtained even if fragmentation and coalescence of the fluid take place.",1996,21,1492,129,3,0,3,7,4,12,12,18,25,19
c9af6f7c0bbe1e96188277f5965ad8edf1dd7479,"We review the development of diffuse-interface models of hydrodynamics and their application to a wide variety of interfacial phenomena. These models have been applied successfully to situations in which the physical phenomena of interest have a length scale commensurate with the thickness of the interfacial region (e.g. near-critical interfacial phenomena or small-scale flows such as those occurring near contact lines) and fluid flows involving large interface deformations and/or topological changes (e.g. breakup and coalescence events associated with fluid jets, droplets, and large-deformation waves). We discuss the issues involved in formulating diffuse-interface models for single-component and binary fluids. Recent applications and computations using these models are discussed in each case. Further, we address issues including sharp-interface analyses that relate these models to the classical free-boundary problem, computational approaches to describe interfacial phenomena, and models of fully miscible fluids.",1997,116,1614,57,0,4,10,6,22,22,25,22,39,56
d93ffe572b15a163e2ec1336a4e507b0b7a766f0,"Research in IT must address the design tasks faced by practitioners. Real problems must be properly conceptualized and represented, appropriate techniques for their solution must be constructed, and solutions must be implemented and evaluated using appropriate criteria. If significant progress is to be made, IT research must also develop an understanding of how and why IT systems work or do not work. Such an understanding must tie together natural laws governing IT systems with natural laws governing the environments in which they operate. This paper presents a two dimensional framework for research in information technology. The first dimension is based on broad types of design and natural science research activities: build, evaluate, theorize, and justify. The second dimension is based on broad types of outputs produced by design research: representational constructs, models, methods, and instantiations. We argue that both design science and natural science activities are needed to insure that IT research is both relevant and effective.",1995,90,3794,442,1,0,1,2,5,15,22,19,18,40
6fcaad2b337e0786a08486c3f71c9c9afddca930,"One: Descriptive.- One The Scientific Description of Personality.- Personality and Taxonomy: The Problem of Classification.- Type and Trait Theories: The Modern View.- Type-Trait Theories and Factor Analysis.- Situationism versus Type-Trait Theories.- Two The Development of a Paradigm.- Origins of Personality Theory.- The Beginnings of Modern Research.- Psychoticism as a Dimension of Personality.- Impulsiveness and Sensation Seeking: A Special Case.- The Question of Validity.- Three The Universality of P, E, and N.- Genetic Factors.- Personality in Animals.- Cross-cultural Studies.- Longitudinal Studies of Personality.- Four Alternative Systems of Personality Description.- R.B. Cattell and 16PF.- The Guilford-Zimmerman Factors.- The NEO Model of Personality.- The Minnesota Multiphasic Personality Inventor.- The California Psychological Inventory.- The Edwards Personal Preference Schedule and the Jackson Personality Research Form.- Other Systems.- Summary.- Five The Cognitive Dimension: Intelligence as a Component of Personality.- Galton versus Binet: IQ and Reaction Time.- The Psychophysiology of Intelligence.- The Theory of Intelligence.- Six Summary and Conclusions.- Two: Causal.- Seven Theories of Personality and Performance.- H. J. Eysenck (1957).- H. J. Eysenck (1967a).- Gray's Theory.- Brebner's Theory.- Eight The Psychophysiology of Personality.- Theoretical Background.- Extraversion.- Neuroticism.- Theoretical Implications.- Nine Extraversion, Arousal, and Performance.- Conditioning.- Sensitivity to Stimulation.- Vigilance.- Verbal Learning and Verbal Memory.- Psychomotor Performance.- Perceptual Phenomena.- Conclusions.- Ten Neuroticism, Anxiety, and Performance.- The State-Trait Approach.- Theories of Anxiety and Performance.- Worry and Performance.- Efficiency and Effectiveness.- Anxiety ? Task Interactions.- Attentional Mechanisms.- Learning and Memory.- Conclusions.- Eleven Social Behavior.- Social Interaction.- Sexual Behavior.- Educational Achievement.- Occupational Performance.- Antisocial Behavior and Crime.- Psychiatric Disorders.- Conclusions.- Three: Epilogue.- Twelve Is There a Paradigm in Personality Research?.- References and Bibliography.- Author Index.",1985,0,2054,111,1,24,46,14,25,25,41,24,53,36
e25083587debd035d2963d3ccdeb3eef477f083b,"In section 3.3 of [i]Philosophy of Natural Science[/i], Hempel argues that crucial tests are not sufficient enough to prove a given hypothesis or to disprove them. Hempel states what some may believe why a crucial test can prove or disprove a hypothesis. If there are two competing hypothesis which involve the same subject and no available evidence favors one or the other, then there exists a test, which will produce conflicting outcomes for the different hypotheses. This test is the so-called crucial test would then presumably refute one hypothesis while supporting the other. Hempel then presents his side of this argument using an example of past experiments involving the nature of light. He describes how Foucault performed an experiment involving the velocity of light through air and water. This experiment was meant to show whether light consists of waves or extremely small particles as presented by Newton. Foucault’s experiment was performed and the resulting outcome was used to refute Newton’s view of light as small particles traveling at a high velocity. Hempel believed that this test was not strong enough to completely support or refute either view of light. The experiment relied on the assumption that light as waves would travel faster in air than in water. However Hempel argues that the conception of light as streams of particles was too indefinite to assume that it would travel slower in air without additional assumptions about the motion of particles and their surrounding medium. So even though the results may seem to support and prove the wave hypothesis, it doesn’t necessarily disprove the particle theory. In fact Einstein later proposed a theory, which eliminated the classical wave theory, using support from an experiment by Lenard in 1903. But again as in the previous example one of the hypotheses was not definitely refuted, in this case being the wave theory. M.-M. V.",1966,0,1635,44,0,1,1,6,8,10,13,6,10,12
e22c08e168d74b55bd618a9026635fb60d5c7508,"Despite their successful use in many conscientious studies involving outdoor learning applications, mobile learning systems still have certain limitations. For instance, because students cannot obtain real-time, contextaware content in outdoor locations such as historical sites, endangered animal habitats, and geological landscapes, they are unable to search, collect, share, and edit information by using information technology. To address such concerns, this work proposes an environment of ubiquitous learning with educational resources (EULER) based on radio frequency identification (RFID), augmented reality (AR), the Internet, ubiquitous computing, embedded systems, and database technologies. EULER helps teachers deliver lessons on site and cultivate student competency in adopting information technology to improve learning. To evaluate its effectiveness, we used the proposed EULER for natural science learning at the Guandu Nature Park in Taiwan. The participants were elementary school teachers and students. The analytical results revealed that the proposed EULER improves student learning. Moreover, the largely positive feedback from a post-study survey confirms the effectiveness of EULER in supporting outdoor learning and its ability to attract the interest of students.",2009,40,180,9,2,3,10,16,17,19,17,26,11,21
a0e6a2a784c38a0052beec3c23419e9c11f22a60,"This study has three major purposes, including designing mobile natural-science learning activities that rest on the 5E Learning Cycle, examining the effects of these learning activities on students’ performances of learning aquatic plants, and exploring students’ perceptions toward these learning activities. A case-study method is utilized and the science club with 46 fourth-grade students is selected as the study case in the study. Besides, a set of quantitative and qualitative data were collected from the case to document the learning effects of and the students’ perceptions of the learning activities, and to discuss factors underlying these effects and students’ perceptions. The results indicate that the learning activities can enhance students’ scientific performances, including both knowledge and understanding levels. Students’ perceptions of these learning activities appear to be positive. The study identifies two factors that are prominent in the positive effects: students’ engaging in “mobile-technology supported” observation during their scientific inquiry; and students’ engaging in “mobiletechnology supported” manipulation during their scientific inquiry. Finally, the conclusions that our study has drawn could constitute a useful guide for educational practitioners concerned with the potentials of mobile computing in school settings.",2009,33,127,7,0,0,7,10,16,18,21,11,16,9
30a0ba6032b19c5d2ef61ec2281c814482f223bf,"The context and conduct of Arctic research are changing. In Nunavut, funding agencies, licensing bodies, and new regulatory agencies established under the Nunavut Land Claims Agreement require researchers to engage and consult with Inuit communities during all phases of research, to provide local training and other benefits, and to communicate project results effectively. Researchers are also increasingly expected to incorporate traditional knowledge into their work and to design studies that are relevant to local interests and needs. In this paper, we explore the challenges that researchers and communities experience in meeting these requirements by reviewing case studies of three natural science projects in Nunavut. Together, these projects exemplify both success and failure in negotiating research relationships. The case studies highlight three principal sources of researcher-community conflict: 1) debate surrounding acceptable impacts of research and the nature and extent of local benefits that research projects can and should provide; 2) uncertainty over who has the power and authority to dictate terms and conditions under which projects should be licensed; and 3) the appropriate research methodology and design to balance local expectations and research needs. The Nunavut research licensing process under the Scientists Act is an important opportunity for communities, scientists, and regulatory agencies to negotiate power relationships. However, the standards and procedures used to evaluate research impact remain unclear, as does the role of communities in the decision-making process for research licensing. The case studies also demonstrate the critical role of trust and rapport, forged through early and frequent communication, efforts to provide local training, and opportunities for community members to observe, participate in, and derive employment from project activities. Clarifying research policies in Nunavut is one step to improving relations between scientists and communities. In addition, steps need to be taken at both policy and project levels to train researchers, educate funding programs, mobilize institutions, and empower communities, thus strengthening the capacity of all stakeholders in northern research.",2009,15,91,6,5,5,6,8,8,5,6,6,9,12
15f688bdc381f2bc19a1411ec650a38ebec0c7a9,,1998,0,159,3,2,0,1,3,1,10,4,1,3,2
febde76b600bdcb5fa8005eda4da979e2311e965,,1892,0,57,2,0,0,0,0,0,0,0,0,0,0
0bdfc5a5fd0ef6e7bb4e993f7fa4d64228acefea,,1899,0,0,0,0,0,0,0,0,0,0,0,0,0
30c9dbd8c68e009b1205da2c6ed7d3a57aebca7c,,1901,0,0,0,0,0,0,0,0,0,0,0,0,0
61c80563df60e566d3cbfe6d26ea91d557860a9d,"IN order to meet as fully as possible the needs of those who desire work in nature study, the subject will be presented in three closely related courses. Course I deals with the subject in its general aspects, and Courses II and III deal with special topics in considerable detail. During the first three weeks, Course I will be open only to those who enter for the first half of the term. During the last three weeks it will be open (I) to those who enter at that time and (2) to those who are taking either Course II or Course III, and who may desire a somewhat broader survey of the field. The primaryand grammar-grade teachers who elect Course I will be assigned to separate sections, and the work will be adapted to each. The sections will be divided into groups for convenience, and each group will be assigned according to the choice of the individual to definite work upon which each student will be expected to make reports as often as necessary. The subjects given to the groups will be selected from the subjoined syllabus of topics. Each group will report to the entire class, so that the mutual relations of the different lines of study will appear. The topics for discussion will be assigned to different groups for presentation, through which the pedagogic aspects of the subject will be considered.",1901,1,0,0,0,0,0,0,0,0,0,0,0,0
a022df6e1db5f2e1be32388ca33cfc0391bb490d,"EVERY observation is made and repeated for the purpose of defining a picture or image. Number work is a means of defining a picture or image through a determination of quantity. In the application of number, therefore, the same principles must be observed that are employed in defining the image by other means. I. There must be a clear conception of the image to be developed. Number work, not less than drawing, painting, modeling, and written composition, depends upon the primary",1902,0,0,0,0,0,0,0,0,0,0,0,0,0
7fe1bc4a8e62195178294117a3ab64b97fe47d22,"Most behavior analysts emphasize that a science of behavior must be a natural science as opposed to a social science or any other such description. But what does this mean and why is it important? I explore these questions by attempting to characterize the designation “natural science” and briefly surveying what behavior analysis has in common with other putative natural sciences. Included in the discussion are problems of agency, background assumptions, mechanism, and some examples of shared problems and issues with other natural sciences. Special focus is placed on behavior analysis as a biological science and the implications of that status.",2009,34,19,0,0,1,2,0,3,0,4,2,2,1
f502a77d80062f2dd42ed0b065ded02a5868c2e9,"Following an initial discussion of the general nature of interpretation in contemporary psychology, and social and natural science, relevant views of Charles Taylor and Thomas Kuhn are considered in some detail. Although both Taylor and Kuhn agree that interpretation in the social or human sciences differs in some ways from interpretation in the natural sciences, they disagree about the nature and origins of such difference. Our own analysis follows, in which we consider differences in interpretation between the natural and social sciences (psychology in particular) in terms of Ian Hacking's use of Elizabeth Anscombe's conceptualization of actions as intentional acts under particular descriptions. We conclude that both Taylor and Kuhn are correct to point to differences in interpretation between the natural and social sciences. We also argue that in psychology, such interpretive differences, contra Kuhn and pro Taylor, are qualitative rather than quantitative. They arise from the nature of persons as self-interpretive, reactive beings who act under socioculturally sanctioned, linguistic descriptions. The actions of psychological persons may display qualitative differences over time and across contexts as these descriptions, including social scientific and psychological findings and interpretations, change. In contrast, even when descriptions in natural science change, such changes do not spawn changes in the self-interpretations and intentional actions of the focal phenomena of natural science. We also make the point that much current confusion surrounding interpretation in science arises from the unwarranted tendency of some commentators to treat interpretation as subjective, in ways that ignore the objective grounding of interpretation within regulated social practices, including scientific practices sanctioned by scientific communities.",2009,9,15,0,1,0,0,1,2,0,2,2,2,2
7f4f566b160c7abbf57da0fa51ae119640c28770,"The study aims at establishing whether Foundation Phase schooling provides a proper foundation for the promotion of scientific literacy. Natural Science in the Foundation Phase is understood as scientific knowledge, process skills, and values and attitudes, which together should foster scientific literacy. Influential perspectives on learning, and teaching methods appropriate to Natural Science education in the Foundation Phase, are reviewed, and the Natural Science Learning Area in the RNCS discussed in the context of global trends in curriculum development. Finally the findings of an empirical survey on the perceptions of Foundation Phase teachers with regard to Natural Science teaching and learning, are presented. Major findings include the following: (1) Scientific literacy is currently not a curriculum priority in the Foundation Phase, due mainly to meagre time allocation and lack of applicable Learning Outcomes. (2) Although teachers appear predominantly positive towards the Learning Area, significant shortcomings need to be addressed before Natural Science teaching in the Foundation Phase may claim to provide the required basis for promoting scientific literacy.",2009,129,12,1,0,0,0,1,4,1,2,1,0,1
a78d7e14769e88809543d71159c6b16f6b21b702,How was the hypothetical character of theories of experiencethought about throughout the history of science? The essays cover periods from the middle ages to the 19th and 20th centuries. It is fascinating to see how natural scientists and philosophers were increasingly forced to realize that a natural science without hypotheses is not possible.,2009,0,12,0,0,1,1,3,1,2,3,0,0,1
4b4a4b295a5e8221b0165e65a5a2aeaa58dbea85,"Analytical table of contents Preface Introduction: rationality Part I. Representing: 1. What is scientific realism? 2. Building and causing 3. Positivism 4. Pragmatism 5. Incommensurability 6. Reference 7. Internal realism 8. A surrogate for truth Part II. Intervening: 9. Experiment 10. Observation 11. Microscopes 12. Speculation, calculation, models, approximations 13. The creation of phenomena 14. Measurement 15. Baconian topics 16. Experimentation and scientific realism Further reading Index.",1983,0,1243,47,0,1,1,4,1,3,1,3,11,10
1610cf06f9281bfbda5368d15c286ad6a3191d80,"In this study, mixed research methods were adopted to examine the online game-based learning environment for ""Go Go Bugs"". The purpose of this study was to determine if an online game-based learning environment can engage and motivate students to learn more natural science by using game features than the control group, which had the same learning environment, but no game features. This experiment examined the engagement by participants' frequencies of returning to the learning environment after school hours, within a time period of two weeks. The results of the comparison of pre- and post- surveys showed significant improvement in increasing participants' interest in learning natural science in a game-based learning environment over that of the non-game-based learning environment. Both quantitative and qualitative results from this study indicated that this game-based learning environment successfully motivated participants in exploring natural science and engaging in the learning activities. However, there was no significant result showing that the game-based learning environment improves students' learning achievement more than with the non-game-based learning environment.",2007,28,50,3,0,1,0,2,7,2,7,3,1,8
e28fc8015226ba662b5a6468e3b2f33837358fd2,"Information processes and computation continue to be found abundantly in the deep structures of many fields. Computing is not---in fact, never was---a science only of the artificial.",2007,11,203,9,4,14,25,15,16,23,20,15,13,7
4d09ecd48768f8945dbec99323d03f7eb62b8316,Preface 1. Metaphysical foundations of phoronomy 2. Metaphysical foundations of dynamics 3. Metaphysical foundations of mechanics 4. Metaphysical foundations of phenomenology.,2007,0,209,8,10,10,9,10,9,8,16,19,16,14
cec073bd5d5418c7ff112e5807fef1089a59d453,"This study examines the relationship between citation frequency and the human capital of teams of authors. Analysis of a random sample of articles published in top natural science journals shows that articles co-authored by teams including frequently cited scholars and teams whose members have diverse disciplinary backgrounds have greater citation frequency. The institutional prestige, the percentage of team members at U. S. institutions and the variety of disciplines represented by team member backgrounds do not influence citation frequency. The study introduces a method for evaluating the extent of multidisciplinarity that accounts for the relatedness of disciplines or authors.",2009,46,22,1,1,1,0,2,4,2,1,4,2,0
050f600c03ea03d13839cd108acdbd1e0d59b96d,"The psychology classic a detailed study of scientific theories of human nature and the possible ways in which human behavior can be predicted and controlled from one of the most influential behaviorists of the twentieth century and the author of ""Walden Two."" This is an important book, exceptionally well written, and logically consistent with the basic premise of the unitary nature of science. Many students of society and culture would take violent issue with most of the things that Skinner has to say, but even those who disagree most will find this a stimulating book. Samuel M. Strong, ""The American Journal of Sociology"" This is a remarkable book remarkable in that it presents a strong, consistent, and all but exhaustive case for a natural science of human behavior It ought to be valuable for those whose preferences lie with, as well as those whose preferences stand against, a behavioristic approach to human activity. Harry Prosch, ""Ethics""""",1953,13,6841,265,0,5,2,10,12,16,11,10,17,15
413886ce56be596e068c8e069f1c3ad7a6e563b8,"I. METAPHYSICS, ONTOLOGY, AND LOGIC II. OBJECTS AND PROPERTIES III. METAPHYSICS AND NATURAL SCIENCE IV. TRUTH, TRUTHMAKING, AND METAPHYSICAL REALISM",2006,0,266,12,4,4,5,10,8,15,9,36,14,22
7fa07c973ff7d7a84c8658c25a6532fb1e018b1c,"Regional-scale restoration is a tool of growing importance in environmental management, and the number, scope, and complexity of restoration programs is increasing. Although the importance of natural science to the success of such projects generally is recognized, the actual use of natural science in these programs rarely has been evaluated. We used techniques of program evaluation to examine the use of natural science in six American and three Western European regional-scale restoration programs. Our results suggest that ensuring the technical rigor and directed application of the science is important to program development and delivery. However, the influence of science may be constrained if strategies for its integration into the broader program are lacking. Consequently, the influence of natural science in restoration programs is greatest when formal mechanisms exist for incorporating science into programs, for example, via a framework for integration of science and policy. Our evaluation proposes a model that can be used to enhance the influence of natural science in regional-scale restoration programs in the United States and elsewhere.",2006,39,23,5,0,1,3,4,5,1,0,3,1,2
8a2ce9609450a02cffe7d6c90ef98ed900880f2f,"In this paper, we studied the research areas of Chinese natural science basic research from a point view of complex network. Two research areas are considered to be connected if they appear in one fund proposal. The explicit network of such connections using data from 1999 to 2004 is constructed. The analysis of the real data shows that the degree distribution of the research areas network (RAN) may be better fitted by the exponential distribution. It displays small world effect in which randomly chosen pairs of research areas are typically separated by only a short path of intermediate research areas. The average distance of RAN decreases with time, while the average clustering coefficient increases with time, which indicates that the scientific study would like to be integrated together in terms of the studied areas. The relationship between the clustering coefficient C(k) and the degree k indicates that there is no hierarchical organization in RAN.",2005,112,26,0,0,1,2,4,4,5,1,1,1,1
63b37ee975c40e74bd26819ec6c3fe341656fe7f,"This paper reviews the recent literature on monetary policy rules. We exploit the monetary policy design problem within a simple baseline theoretical framework. We then consider the implications of adding various real world complications. Among other things, we show that the optimal policy implicitly incorporates inflation targeting. We also characterize the gains from making a credible commitment to fight inflation. In contrast to conventional wisdom, we show that gains from commitment may emerge even if the central bank is not trying to inadvisedly push output above its natural level. We also consider the implications of frictions such as imperfect information.",1999,186,5463,621,20,85,194,227,305,292,274,320,368,343
ec1d6bbda7331fc074cca94f0bc07127327dea8b,"Integrated studies of coupled human and natural systems reveal new and complex patterns and processes not evident when studied by social or natural scientists separately. Synthesis of six case studies from around the world shows that couplings between human and natural systems vary across space, time, and organizational units. They also exhibit nonlinear dynamics with thresholds, reciprocal feedback loops, time lags, resilience, heterogeneity, and surprises. Furthermore, past couplings have legacy effects on present conditions and future possibilities.",2007,75,2590,91,3,31,88,116,135,149,190,212,232,223
310beca34914ca073a88f04a2b3ae12b302dcd81,"Abstract The second phase of the Mpumalanga Secondary Science Initiative (MSSI) was launched in Mpumalanga Province, South Africa in 2003. The MSSI seeks to improve the teaching and learning of mathematics and science in all secondary schools in the province over a period of three years. To achieve this goal an in-service system has been developed. The long-term research of the MSSI is aimed at examining the effect of the intervention on the three science learning outcomes as defined by the new curriculum. Tests were developed to assess 10 learners in each of Grades 8 and 9 in 40 schools, representing a 6% sample. This paper reports on the methodology of the baseline assessments undertaken and the extent of learners' attainment levels in the three Natural Science outcomes. It then looks at the results in terms of learners' socio-economic backgrounds. Finally it discusses some of the dilemmas encountered in this research, such as the issue of validity in the light of possible misalignments between the instrument, practice and policy.",2005,26,22,3,1,1,1,1,6,3,3,0,1,2
c3d62bc0ab3e900f0fbf33949754ed9919139164,"This article discusses the results of a qualitative study, based on case studies, aimed at: (a) assessing a group of Portuguese secondary school natural science teachers regarding their conceptions of the nature, teaching and learning of science; (b) studying possible impacts of recent controversies surrounding scientific and technological issues on these conceptions and on teachers' classroom practices. Five teachers, with different backgrounds and teaching experience, were observed during classes and interviewed with the purpose of studying: (a) the relationship between their conceptions and classroom practices; and (b) the factors that impede or enhance this relationship. Subsequently, observation notes and interview transcriptions were systematically analysed. The socio-scientific controversies recently discussed in Portugal seem to have had an impact on teachers' (1) conceptions about the nature, teaching and learning of science; and (2) classroom practice. However, not all teachers were able to teach according to their conceptions. Some factors seem to mediate the relationship between teachers' conceptions and classroom practice: National Curriculum, national exams, teachers' previous experience as scientists, and personal educational priorities or aims. Based on the results obtained, some remarks and educational implications are discussed.",2004,23,49,1,1,1,2,3,5,5,0,1,5,4
2bfbe016c77a55119c84d93bfb1b5f3139e83771,"Interdisciplinary collaboration occurs when people with different educational and research backgrounds bring complementary skills to bear on a problem or task. The strength of interdisciplinary scientific research collaboration is its capacity to bring together diverse scientific knowledge to address complex problems and questions. However, interdisciplinary scientific research can be difficult to initiate and sustain. We do not yet fully understand factors that impact interdisciplinary scientific research collaboration. This study synthesizes empirical data from two empirical studies to provide a more comprehensive understanding of interdisciplinary scientific research collaboration within the natural sciences in ac ademia. Data analysis confirmed factors previously identified in various literatures and yielded new factors. A total of twenty factors were identified, and classified into four categories: personal, resources, motivation and common ground. These categorie s and their factors are described, and implications for academic policies and practices to facilitate and sustain interdisciplinary collaboration are discussed.",2005,17,50,3,0,1,4,2,0,0,4,5,2,3
2294cddf83c01629c138b307bffcecb19822e307,"Abstract The Chomskyan revolution in linguistics in the 1950s in essence turned linguistics into a branch of cognitive science (and ultimately biology) by both changing the linguistic landscape and forcing a radical change in cognitive science to accommodate linguistics as many of us conceive of it today. More recently Chomsky has advanced the boldest version of his naturalistic approach to language by proposing a Minimalist Program for linguistic theory. In this article, we wish to examine the foundations of the Minimalist Program and its antecedents and draw parallelisms with (meta-)methodological foundations in better-developed sciences such as physics. Once established, such parallelisms, we argue, help direct inquiry in linguistics and cognitive science/biology and unify both disciplines.",2005,150,54,1,1,0,2,4,6,5,2,1,5,2
9fff1ce502fd37eb5512b972177c4cff3b038c4b,"The global tendency is obvious: interest in science is on the decrease, the number of pupils choosing university science curriculums has been constantly declining, and scientific knowledge in society (especially among young people) is inadequate. In our opinion, humanity verges on social cataclysms owing to inadequate natural science education as well as on insufficient and often improper knowledge of nature and human. Natural sciences give us most fundamental knowledge about the world of nature. Encouragement of young people’s interest in science is the essential scientific problem. As educational paradigms are being altered we must search for new quality approaches to teaching chemistry and other science subjects. The research, which involved 350 senior pupils from Latvia and 762 from Lithuania, analyzes present-day situation in natural science education. We tried to analyze the factors that cause the interest in natural sciences to decline: inadequate content of teaching, issues related to teachers‘ competence, general attitude of society to natural sciences etc.",2004,13,17,0,2,1,1,1,0,2,0,0,2,1
9e86f2a2d795ac16288d030844b918e125658025,"From the viewpoint of biology, learning and education can be defined as the processes of forming neuronal connections in response to external environmental stimuli, and of controlling or adding appropriate stimuli, respectively. Learning and education can thus be studied as a new field of natural sciences with the entire human life span as its subject, thus including various problems such as fetal environment, childcare, language acquisition, general/special education, and rehabilitation. Non-invasive imaging of higher-order brain functions in humans will clarify the brain's developmental processes, and will provide various evidence for learning sciences. This new approach is called 'developing the brain' or 'brain science and education'. The origin of the concept and its present state are described and its future prospects are briefly analyzed.",2004,52,70,3,1,3,6,3,3,7,5,5,2,5
7b83b725196e087b2b96a6f185c93eff259c6182,,2004,0,67,0,0,0,1,0,3,1,1,0,3,3
7017309de7e23dfd4705bf0c8423384484334c5a,"B.D. Ratner, Biomaterials Science: An Interdisciplinary Endeavor. Materials Science and Engineering--Properties of Materials: J.E. Lemons, Introduction. F.W. Cooke, Bulk Properties of Materials. B.D. Ratner, Surface Properties of Materials. Classes of Materials Used in Medicine: A.S. Hoffman, Introduction. J.B. Brunski, Metals. S.A. Visser, R.W. Hergenrother, and S.L. Cooper, Polymers. N.A. Peppas, Hydrogels. J. Kohnand R. Langer, Bioresorbable and Bioerodible Materials. L.L. Hench, Ceramics, Glasses, and Glass Ceramics. I.V. Yannas, Natural Materials. H. Alexander, Composites. B.D. Ratner and A.S. Hoffman, Thin Films, Grafts, and Coatings. S.W. Shalaby, Fabrics. A.S. Hoffman, Biologically Functional Materials. Biology, Biochemistry, and Medicine--Some Background Concepts: B.D. Ratner, Introduction. T.A. Horbett, Proteins: Structure, Properties, and Adsorption to Surfaces. J.M. Schakenraad, Cells: Their Surfaces and Interactions with Materials. F.J. Schoen, Tissues. Host Reactions to Biomaterials and Their Evaluations: F.J. Schoen, Introduction. J.M. Anderson, Inflammation, Wound Healing, and the Foreign Body Response. R.J. Johnson, Immunology and the Complement System. K. Merritt, Systemic Toxicity and Hypersensitivity. S.R. Hanson and L.A. Harker, Blood Coagulation and Blood-Materials Interaction. F.J.Schoen, Tumorigenesis and Biomaterials. A.G. Gristina and P.T. Naylor, Implant-Associated Infection. Testing Biomaterials: B.D. Ratner, Introduction. S.J. Northup, In Vitro Assessment of Tissue Compatibility. M. Spector and P.A. Lalor, In Vivo Assessment of Tissue Compatibility. S. Hanson and B.D. Ratner, Testing of Blood-Material Interactions. B.H. Vale, J.E. Willson, and S.M. Niemi, Animal Models. Degradation of Materials in the Biological Environment: B.D. Ratner, Introduction. A.J. Coury, Chemical and Biochemical Degradation of Polymers. D.F. Williams and R.L. Williams, Degradative Effects of the Biological Environment on Metals and Ceramics. C.R. McMillin, Mechanical Breakdown in the Biological Environment. Y. Pathak, F.J. Schoen, and R.J. Levy, Pathologic Calcification of Biomaterials. Application of Materials in Medicine and Dentistry: J.E. Lemons, Introduction. D. Didisheim and J.T. Watson, Cardiovascular Applications. S.W. Kim, Nonthrombogenic Treatments and Strategies. J.E. Lemons, Dental Implants. D.C. Smith, Adhesives and Sealants. M.F. Refojo, Ophthalmologic Applications. J.L. Katz, Orthopedic Applications. J. Heller, Drug Delivery Systems. D. Goupil, Sutures. J.B. Kane, R.G. Tompkins, M.L. Yarmush, and J.F. Burke, Burn Dressings. L.S. Robblee and J.D. Sweeney, Bioelectrodes. P. Yager, Biomedical Sensors and Biosensors. Artificial Organs: F.J. Schoen, Introduction. K.D. Murray and D.B. Olsen, Implantable Pneumatic Artificial Hearts. P. Malchesky, Extracorporeal Artificial Organs. Practical Aspects of Biomaterials--Implants and Devices: F.J. Schoen, Introduction. J.B. Kowalski and R.F. Morrissey, Sterilization of Implants. L.M. Graham, D. Whittlesey, and B. Bevacqua, Cardiovascular Implantation. A.N. Cranin, M. Klein, and A. Sirakian, Dental Implantation. S.A. Obstbaum, Ophthalmic Implantation. A.E. Hoffman, Implant and Device Failure. B.D. Ratner, Correlations of Material Surface Properties with Biological Responses. J.M. Anderson, Implant Retrieval and Evaluation. New Products and Standards: J.E. Lemons, Introduction. S.A. Brown, Voluntary Consensus Standards. N.B. Mateo, Product Development and Regulation. B. Ratner, Perspectives and Possibilities in Biomaterials Science. Appendix: S. Slack, Properties of Biological Fluids. Subject Index.",1996,0,4044,185,1,6,14,18,31,38,66,54,80,131
9b981fa3d1559ac7f17930a25abcb592d03419e3,"Marrying the biological and social sciences: culture, social constructions and natural science possible frameworks evolution and the theory of evolution alternative theories to NeoDarwinism how good a theory is evolutionary theory? suggested readings. The evolution of intelligence: why intelligence ever evolved at all the limits of reductionism intelligence unlimited? Fodor poses a problem human intelligence as adaptation or exaptation suggested readings. The emergence of culture: broadening the picture the trouble with ""levels"" a solution to the levels problem suggested readings. Naturalizing culture the process way: the puzzle of war universal Darwinism modelling co-evolution the ""new"" science of memetics suggested readings. Causal mechanisms: a general framework for understanding psychological mechanism what those mechanisms may be concepts, schemata and other higher-order knowledge structures imitation language theory of mind social force a single magical mechanism? suggested readings. Individuals, groups and culture: the behavioural ecology of group living the units and levels of selection vehicles, interactors and the revival of group selection niche construction suggested readings. The strangeness of culture: the construction of social reality a sociological turn social representations cultural psychology a tentative conclusion suggested readings.",2002,0,77,8,0,3,1,8,7,4,4,1,6,9
78360a9b8edc5f4b2884024a39c318b484e3f3ce,"Given the growing multicultural composition of South African classrooms, educators of science and technology, like educators across the spectrum of all learning areas, are increasingly challenged to reflect how they and their learners conceive of and, as a result, construct knowledge. The reality is that in an expanding globalised world, learners can easily become alienated from what is taught in science and technology, as well as the way it is taught. Indigenous Knowledge Systems (IKS), as a broad framework of thinking about our local context, seeks to problematise the insufficient integration of the cultural-social and the canonical-academic dimensions of natural science and technology education. In this article I conceptualise and clarify IKS vis-a-vis knowledge production, particularly towards educational transformation in which educators may assume that all learners are the same in terms of identity and cultural dynamics. Natural science and technology, in particular, have assumed a definite culture of power, which has marginalized the majority of learners in the past. IKS strategically wishes to transform this view and therefore holds valuable implications for educators in the learning areas of natural science and technology.",2002,58,72,4,0,0,0,1,0,2,2,4,5,3
5d2af227cfafe9b468ac48d97658a8cc051ed08f,"Abstract We investigated the significance of risk assessment studies in the public discussion on CO 2 emissions. Politicians and representatives from the public where interviewed by using the social-science technique of qualitative in-depth interviews. Three different types of attitudes towards natural science were found among politicians. Depending on which attitude a politician holds, risk assessment studies can have an impact on his/her readiness to support environmental policy measures. Regarding lay people, key factors affecting the acceptance of environmental policy measures are knowledge of environmental problems, their impacts on ecosystems or human health as well as direct personal perception of those impacts. Since direct perception is not always possible in everyday life, natural science experiments might be a means for successfully mediating this lacking perception.",2003,36,23,0,0,0,0,1,2,2,2,0,0,3
f8ea30c78c8d3693bdf6960182d84667100abfa3,"When mathematician Hermann Weyl decided to write a book on philosophy, he faced what he referred to as ""conflicts of conscience""--the objective nature of science, he felt, did not mesh easily with the incredulous, uncertain nature of philosophy. Yet the two disciplines were already intertwined. In Philosophy of Mathematics and Natural Science, Weyl examines how advances in philosophy were led by scientific discoveries--the more humankind understood about the physical world, the more curious we became. The book is divided into two parts, one on mathematics and the other on the physical sciences. Drawing on work by Descartes, Galileo, Hume, Kant, Leibniz, and Newton, Weyl provides readers with a guide to understanding science through the lens of philosophy. This is a book that no one but Weyl could have written--and, indeed, no one has written anything quite like it since.",1950,0,315,4,0,1,3,0,1,0,0,1,1,0
