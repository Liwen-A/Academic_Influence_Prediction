{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Starter Code for Retrieving, and Analyzing Data Using API\n",
    "In this notebook I include a basic example of \n",
    "1. retrieving data using [SemanticScholar APIs](https://api.semanticscholar.org/graph/v1)\n",
    "2. store it in a pandas dataframe  \n",
    "3. write it to a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the following API performs a search by keyword and:\n",
    "1. Returns with total=639637, offset=0, next=100, and data is a list of 100 papers.\n",
    "2. Each paper has paperId, abstract, year, referenceCount, citationCount, influentialCitationCount and fieldsOfStudy \n",
    "\n",
    "Feel free to change the strings after 'query=' and 'fields='to specify what keyword you want to search and what fields, i.e. data, you want the API to return.  Add 'limit=' to specify how many data you want it to return.\n",
    "For more information on other APIs refer to [SemanticScholar APIs](https://api.semanticscholar.org/graph/v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.semanticscholar.org/graph/v1/paper/search?&query=convex&fields=abstract,year,referenceCount,authors,citationCount,influentialCitationCount,fieldsOfStudy&offest=0&limit=100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using api to access a author using authorID, and calculate the h-index of the specified author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H-index of 1741103 is 19\n"
     ]
    }
   ],
   "source": [
    "def get_h_index(authorID):\n",
    "    \"\"\"given string authorID, calculate H-Index\"\"\"\n",
    "    response_author = requests.get('https://api.semanticscholar.org/graph/v1/author/{}?fields=name,papers,papers.citationCount'.format(authorID))\n",
    "    papers = response_author.json()['papers']\n",
    "\n",
    "    paper_citation = []\n",
    "    for i in papers:\n",
    "        paper_citation.append(i['citationCount'])\n",
    "    paper_citation.sort(key = lambda x: -x)\n",
    "    \n",
    "    h = 0\n",
    "    for i, c in enumerate(paper_citation):\n",
    "        if i + 1 > c:\n",
    "            h = i \n",
    "            break\n",
    "    return h\n",
    "\n",
    "\n",
    "exampleID = '1741103'\n",
    "print('H-index of {} is {}'.format(exampleID,get_h_index(exampleID)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API only supports 100 resquests per 5 minutes. Hence after every 99 requests are made the code will sleep for 5 mins. The keywords contains the disciplines and their corresponding keywords that are used to conduct a search using the API. The data collected are saved to the current working directory as csv files. \n",
    "\n",
    "The features that are collected are:\n",
    "1. paperid \n",
    "2. abstract\n",
    "3. Year of publish\n",
    "4. fields of study\n",
    "5. list of authors (to be transformed to h-index later using get_h_index func above)\n",
    "6. reference_count\n",
    "7. citation_count\n",
    "8. influential_citation_count \n",
    "\n",
    "To collect data, run the following cell. Modify keywords dict if you want to collect data from other disciplines or containg other keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "num_requests = 33\n",
    "keywords = {\n",
    "    'Aeronautics': ['Aerospace','aircraft','fluid','aerodynamics', 'radar', 'orbital', 'combustion'],\n",
    "    'Mathematics':['Analysis', 'Algebraic', 'Arithmetic', 'Number', 'Vector', 'Set', 'Geometric'],\n",
    "    'Chemistry': ['Chemical', 'Thermodynamic', 'kinetics', 'electrochemistry', 'spectroscopy', 'molecular', 'geochemistry'],\n",
    "    'Computer science': ['Algorithm', 'Computation', 'Intelligent', 'System', 'Graphics', 'Visualization', 'Architecture'],\n",
    "    'Physics': ['Force', 'Newtonian', 'Mechanics', 'Relativity', 'Equilibrium', 'Quantum', 'Nuclear', 'Electromagnetic'],\n",
    "    'Material Science': ['Solids', 'metallurgy', 'mineralogy', 'nanotechnology', 'biomaterials', 'metallurgy', 'failure'],\n",
    "    'Civil Engineering': ['Geology', 'Soils', 'Environmental', 'Design', 'pavement', 'construction', 'residential', 'commercial'],\n",
    "    'Biology': ['natural science', 'organisms', 'physiology', 'anatomy', 'plants', 'animals', 'earth', 'ecosystem' ],\n",
    "    'Medicine': ['Cardiology', 'Cardiovascular Surgery', 'Dermatology', 'Dentistry', 'Emergency Medicine', 'Endocrinology', 'Gastroenterology', 'General Practice'],\n",
    "    'Economics':['Goods', 'services', 'production', 'consumption', 'macroeconomics', 'microeconomics', 'contract', 'econophysics', 'political economy']\n",
    "}\n",
    "\n",
    "for fields in keywords.keys():\n",
    "    data = []\n",
    "    for counter,q in enumerate(keywords[fields]):\n",
    "        for i in range(num_requests):\n",
    "            query = 'https://api.semanticscholar.org/graph/v1/paper/search?&query={}&fields=abstract,year,referenceCount,citationCount,influentialCitationCount,fieldsOfStudy,authors&offest={}&limit=100'.format(q,i*100)\n",
    "            response = requests.get(query)\n",
    "            data += response.json()['data']\n",
    "        if (counter + 1) % 3 == 0:\n",
    "            time.sleep(301) #sleep for 5 min\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[df['year'] < 2010]\n",
    "    df.to_csv(fields+'data.csv',index=False) # this writes a csv file to the current working directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a example of storing the retrived data into a pandas dataframe and write it into a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search by some keyword and then filter the data by year/discipline. Get 10k datapoints for 10 different disciplines each.   \n",
    "\n",
    "Potential disciplines to consider: [Math,Physics,Chemetry,Computer science, Aeronautics, Material Science, Civil Engineering, Biology, Medicine, scociology,economics]\n",
    "\n",
    "For each paper, get author ID, perform a search with author ID, get papers and citation count published by the author, calculate H-index.       \n",
    "\n",
    "\n",
    "train Models:        \n",
    "\n",
    "1.Linear Regression (with kernel)           \n",
    "2.NN: fully connected, 1st layer: D * 20, 2nd layer 20 * 1 , activation function: relu, loss: MSE  Try regularization. \n",
    "\n",
    "The following cells demonstrate how to define and train a simple regression model using Pytorch. We will use the data collecred above. The model will be a linear regression model that takes citationCount as input and predicts influentialCitationCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train = df['citationCount'].to_numpy(dtype=np.float32)\n",
    "y_train = df['influentialCitationCount'].to_numpy(dtype=np.float32)\n",
    "sc = MinMaxScaler() #scale the input so the gradient won't explode. \n",
    "X_train=sc.fit_transform(X_train.reshape(-1,1))\n",
    "y_train =y_train.reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "input_size,output_size = 1,1\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "learning_rate = 0.01\n",
    "l = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr =learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 9078.8662109375\n",
      "epoch 1000, loss 2224.139404296875\n",
      "epoch 2000, loss 1482.6478271484375\n",
      "epoch 3000, loss 1100.35009765625\n",
      "epoch 4000, loss 903.2459716796875\n",
      "epoch 5000, loss 801.622314453125\n",
      "epoch 6000, loss 749.2279663085938\n",
      "epoch 7000, loss 722.2144775390625\n",
      "epoch 8000, loss 708.2868041992188\n",
      "epoch 9000, loss 701.106201171875\n",
      "epoch 10000, loss 697.4038696289062\n",
      "epoch 11000, loss 695.4951171875\n",
      "epoch 12000, loss 694.510986328125\n",
      "epoch 13000, loss 694.0035400390625\n",
      "epoch 14000, loss 693.741943359375\n",
      "epoch 15000, loss 693.6071166992188\n",
      "epoch 16000, loss 693.5374755859375\n",
      "epoch 17000, loss 693.501708984375\n",
      "epoch 18000, loss 693.483154296875\n",
      "epoch 19000, loss 693.4736938476562\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "     #forward feed\n",
    "    y_pred = model(X_train.requires_grad_())\n",
    "\n",
    "    #calculate the loss\n",
    "    loss= l(y_pred, y_train)\n",
    "\n",
    "    #backward propagation: calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #clear out the gradients from the last step loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "     print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[457.44882 ],\n",
       "       [325.22952 ],\n",
       "       [155.64798 ],\n",
       "       ...,\n",
       "       [ 33.368767],\n",
       "       [ 35.577686],\n",
       "       [ 29.992285]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X_train).detach().numpy() #make prediction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8794517f13bcfee9fc9e64fa68815e36a23c0d95e014b70721951cbb766e370b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
